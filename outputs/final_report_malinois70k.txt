================================================================================
Experimental Design Report: Cross-Cell-Type CRE Activity Prediction Model Construction
================================================================================

SUMMARY
--------------------------------------------------------------------------------
Based on the task background and data set information, after 0 rounds of expert design, a complete experimental design scheme is generated. The overall feasibility score: 9.2/10

Overall Feasibility Score: 9.2/10

TASK INFORMATION
--------------------------------------------------------------------------------
Description: Cross-Cell-Type CRE Activity Prediction Model Construction

Background:
Goal: Construct a deep learning model to simultaneously predict the cell-type-specific activity of 200bp cis-regulatory elements (CREs) across three human cell lines: K562, HepG2, and SK-N-SH.
Requirements: The model should be a deep learning model with a shared convolutional backbone (using weight normalization) and a multi-task branched output. Input sequence length is 200bp, converted to 4-channel One-hot. The backbone should consist of multiple Conv1d layers with Weight Norm to identify DNA motifs. The final output must be a 3-dimensional vector (one for each cell line) predicted via independent branched linear layers to capture cell-type-specific regulatory logic.

Dataset Information:
File path: D:/RE-Agent/task/data/malinois_200/malinois_200.csv; Data type: ; Input features: s, e, q, u, e, n, c, e; Target variable: N/A; Constraint: Input: 200bp DNA One-hot (4, 200). Targets: K562_l2fc, HepG2_l2fc, SKNSH_l2fc. Data Cleaning: Filter rows with lfcSE > 0.5 prior to training. Metrics: Evaluate performance using Pearson correlation per cell type and MinGap for specificity validation.

================================================================================
EXPERIMENTAL DESIGN IMPLEMENTATION PLAN
================================================================================

1. DATA USAGE PLAN
--------------------------------------------------------------------------------

Design Recommendations:
1) Dataset profiling and type confirmation: Using the provided columns ['sequence','target_cell','K562_l2fc','HepG2_l2fc','SKNSH_l2fc','MinGap','K562_lfcSE','HepG2_lfcSE','SKNSH_lfcSE'] and 77,157 rows, this is best characterized as an MPRA-derived multi-output regression dataset where each row is a synthetic/natural 200 bp regulatory sequence with measured activity in three cell lines, reported as log2 fold change (l2fc) with associated standard errors (lfcSE). This implies label noise is heteroskedastic and should be addressed by uncertainty-aware filtering and/or loss weighting; for data management, we prioritize filtering rules that are explicit and reproducible. Sequence length should be validated by computing length for every row and summarizing mean/median/min/max and outlier counts (e.g., count of length != 200). The expected distribution is a delta at 200 bp; if not, the plan defines strict handling: (a) preferred: drop sequences with length != 200 if they are rare (<0.5% of rows), (b) otherwise: trim longer sequences to the central 200 bp and pad shorter sequences with 'N' to 200 and mark them with a QC flag for downstream sensitivity analyses. Because the dataset is large, we also recommend computing duplication rate (exact identical sequences), GC-content distribution (mean/SD and tails), and per-target summary statistics (l2fc mean/SD, percentiles) as part of profiling to guide outlier policies. These recommendations align with community emphasis on robust MPRA QC and variability control (e.g., standardized QC frameworks and outlier identification described in MPRA QC pipelines such as esMPRA and uniform processing standards such as MPRAsnakeflow/MPRAlib from IGVF efforts).

2) QC / cleaning with explicit lfcSE filtering rule: Implement a deterministic filter based on measurement uncertainty using the three standard error columns K562_lfcSE, HepG2_lfcSE, SKNSH_lfcSE with the threshold lfcSE > 0.5. Because ambiguity exists (filter per cell line vs. only the target_cell’s SE), define and document two exact, mutually exclusive rules: Rule A (strict multi-task reliability): DROP a row if ANY of the three SE values exceeds 0.5, i.e., drop if max(K562_lfcSE, HepG2_lfcSE, SKNSH_lfcSE) > 0.5; this ensures all three regression labels for a retained sequence are relatively reliable. Rule B (target-cell-only reliability): DROP a row only if the SE corresponding to target_cell exceeds 0.5; concretely, map target_cell in {K562,HepG2,SKNSH} to the matching *_lfcSE column and drop if that single SE > 0.5, allowing retention if non-target tasks are noisy. Recommendation: prefer Rule A for multi-task training where the model jointly predicts all three activities, because noisy targets in any head can destabilize optimization and bias shared feature learning; Rule B is acceptable if target_cell is used as the primary supervision and other heads are auxiliary/weak. In addition, record the number of rows removed under each rule and the post-filter distributions of l2fc to ensure filtering does not collapse dynamic range; if Rule A removes >30% of data, consider raising threshold to 0.6 or switching to Rule B plus per-task loss weighting by 1/(lfcSE^2 + 1e-6). This approach is consistent with MPRA best-practice emphasis on identifying technical variability sources and filtering out unreliable measurements (as highlighted in MPRA QC/standardization efforts like esMPRA and IGVF MPRA processing standards).

3) Missing/invalid values, sequence sanitization, and N handling: Define invalid sequences as any containing characters outside {A,C,G,T,N} (case-insensitive); immediately drop rows with other IUPAC codes (e.g., R,Y,S,W,K,M) unless you explicitly implement probabilistic mapping for them. For Ns, implement a two-tier policy: if N_fraction = count('N')/length(sequence) > 0.0 (strict) then drop, OR if you want a less aggressive option, drop only if N_fraction > 0.01 (i.e., >2 Ns in 200 bp) and otherwise keep. Recommendation for this large dataset: use N_fraction > 0.0 (drop any N) as the default because you have ample data volume (77k) and Ns introduce ambiguous k-mers that can degrade motif learning; keep the relaxed threshold (1%) as a fallback if too many sequences are removed. If sequences with small N_fraction are kept, encode N as a uniform distribution [0.25,0.25,0.25,0.25] at that position (float32), which preserves expected base frequency without injecting a specific nucleotide. Also drop rows with missing numeric targets/SEs (NaN/Inf) and define explicit range checks: lfcSE must be >0 and <5 (hard sanity bounds), and l2fc should be finite with optional winsorization at the 0.1 and 99.9 percentiles per cell line if extreme outliers are present. Maintain a QC report that logs counts removed by each rule (invalid chars, length mismatch, N filter, lfcSE filter, NaNs) to support reproducibility and auditability.

4) Encoding specification (exact tensor layout and dtype): Use a channel-first one-hot encoding with float32 output tensor shape (B, 4, 200), where B is batch size. Define base-to-channel mapping exactly as: A->[1,0,0,0], C->[0,1,0,0], G->[0,0,1,0], T->[0,0,0,1]; for N (if allowed under the relaxed N policy) use [0.25,0.25,0.25,0.25]. Enforce uppercase transformation and verify post-encoding that each position sums to 1.0 (within float tolerance) for A/C/G/T/N; for dropped invalid bases this check is not needed because they are excluded upstream. Keep a deterministic sequence order (5’->3’) and only apply reverse-complement augmentation explicitly as described later; do not implicitly canonicalize sequences because strand information can matter in some MPRA designs. Store encoded arrays in a memory-efficient format for 77k sequences: either on-the-fly encoding in the DataLoader (CPU) or pre-encode and save as an mmap/NPY/Zarr array; with 77,157*4*200 float32 ≈ 246 MB, pre-encoding is feasible and can improve training throughput. If pre-encoding, also store an index mapping from row_id to original CSV row for traceability.

5) Target construction and normalization (train-only scalers): Treat targets as a 3D regression vector y = [K562_l2fc, HepG2_l2fc, SKNSH_l2fc] per row, regardless of target_cell; use target_cell as auxiliary metadata (see splitting/bias checks) rather than the label. Normalize each target dimension separately using only the training split statistics: for cell line c in {K562,HepG2,SKNSH}, compute mean μ_c and std σ_c on train rows, then transform y_c_norm = (y_c - μ_c) / (σ_c + 1e-8). Save μ and σ in a versioned artifact (e.g., JSON) and apply the same scalers to validation and test; at evaluation time, compute metrics in both normalized space (for stable training curves) and original l2fc space (for interpretability). If you adopt SE-aware weighting instead of hard filtering, define per-target weight w_c = 1/(lfcSE_c^2 + 1e-6) and optionally cap weights at w_max=25 to prevent single observations dominating; this is complementary to normalization and is especially useful if you choose the less strict Rule B. Also check for systematic shifts in l2fc distributions across target_cell values; if strong, consider including target_cell as a categorical covariate input (one-hot of size 3 concatenated to model head) but keep it out of the leakage-safe grouping keys.

6) Data splitting strategy (leakage-safe, deterministic, and stratified where possible): Primary risk is sequence leakage due to duplicate (identical) sequences appearing multiple times; therefore split by groups defined as the exact 'sequence' string after canonical cleaning (uppercase, validated length, no Ns if dropped). Algorithm: (i) create cleaned_sequence, (ii) group rows by cleaned_sequence and assign each group an integer group_id, (iii) shuffle group_ids with a fixed seed (seed=2025), (iv) allocate groups to train/val/test by cumulative group size to achieve 80/10/10 of total rows as closely as possible while keeping groups intact. After the group split, verify that there are zero identical sequences shared across splits (assert disjointness) and report group counts and row counts per split. If you want additional robustness against near-duplicates (1–2 mismatches), optionally cluster sequences by MinHash/LSH or by Hamming distance <=2 within 200 bp, but this is computationally heavier; as a practical default, exact-dedup group splitting is required and sufficient for many MPRA tables. Because 'target_cell' may be imbalanced, add a secondary constraint: during group allocation, approximate stratification by target_cell by tracking per-split target_cell proportions and using a greedy assignment that minimizes deviation from the overall distribution (tolerance ±2%). Keep 'MinGap' out of grouping keys but use it for post-split diagnostics; if MinGap correlates with measurement quality, ensure its distribution is similar across splits.

7) Augmentation policy (lightweight due to large N, optional RC): Since the dataset is large (>10K), prioritize QC and do not use heavy synthetic augmentation that could distort measured activity. Optional reverse-complement (RC) augmentation can be applied with probability p=0.5 during training only: for each sequence in a batch, with Bernoulli(p) replace it with its reverse complement; labels (all three l2fc) remain unchanged because MPRA reporter constructs typically measure sequence function independent of strand orientation in many setups, but you must confirm assay design—therefore treat RC augmentation as optional and evaluate with/without it. If you suspect strand-specific effects (e.g., promoter-directional constructs), set p=0.0 and do not RC augment. If you enable RC, implement it at the one-hot tensor level: reverse the length dimension and swap channels A<->T and C<->G; this is deterministic and avoids string operations. Do not use random base mutations for MPRA regression because it changes the underlying biological function and breaks label validity; similarly, do not use random shuffling except as a negative control dataset for sanity checks.

8) Data loader and throughput specifications: For 4x200 float32 tensors, a batch size of 256 is a strong default on a single modern GPU (e.g., 16–24 GB VRAM); provide candidates {128, 256, 512} and choose the largest that maintains >90% GPU utilization without OOM. Use num_workers=4–8 (Linux) depending on CPU cores, pin_memory=True for CUDA training, persistent_workers=True, and prefetch_factor=2 to reduce input stalls; if pre-encoding to mmap/Zarr, you can reduce CPU overhead and increase batch size. Shuffle training data each epoch (shuffle=True) but keep validation/test deterministic (shuffle=False). For reproducibility, set global seeds (Python, NumPy, PyTorch) to 2025 and enforce deterministic ops where feasible; still record that CuDNN determinism may reduce speed. Finally, maintain a data manifest per split containing row indices, cleaned_sequence hash (e.g., SHA1), and QC flags so that any model run can be traced back exactly.

9) Step-by-step preprocessing pipeline (pseudocode-level clarity with exact parameters): Step 0 Load CSV and assert required columns exist; Step 1 Standardize sequence strings: seq = upper(seq), strip whitespace; Step 2 Validate alphabet: drop if regex '[^ACGTN]'; Step 3 Compute len(seq) and profile distribution; apply length policy: if len!=200 then drop (default) unless outlier_fraction>0.5% then trim/pad to 200 with 'N' and flag; Step 4 Handle Ns: compute N_fraction; default drop if N_fraction>0.0 (alternative drop if >0.01 and map remaining N to uniform encoding); Step 5 Validate numeric columns: drop rows with NaN/Inf in any l2fc or lfcSE; enforce lfcSE in (0,5); Step 6 Apply lfcSE filter with threshold 0.5 using Rule A (default) or Rule B (alternative) and log removals; Step 7 Deduplicate/group: group_id = hash(cleaned_sequence), create group table; Step 8 Group split with seed=2025 into train/val/test 0.8/0.1/0.1 by groups; Step 9 Compute train-only μ_c, σ_c and normalize targets; Step 10 Encode sequences to one-hot float32 (B,4,200); Step 11 Save artifacts: split manifests, scaler JSON, QC report, and optional pre-encoded arrays; Step 12 Train with optional RC augmentation p=0.5 on train only. This pipeline follows the general principle from MPRA community tooling that robust QC, outlier handling, and standardized processing are essential for reproducibility (as emphasized by MPRA QC pipelines like esMPRA and uniform processing/standard formats like MPRAsnakeflow/MPRAlib).


2. METHOD DESIGN
--------------------------------------------------------------------------------

Design Recommendations:
1) Dataset characterization & preprocessing: Assume the dataset is an MPRA-style sequence-to-activity regression dataset because the targets are per-cell-type l2fc (K562/HepG2/SKNSH), which are commonly derived from reporter assays; this implies labels are noisy with occasional outliers and can be heteroscedastic across cell types. If sequence length is fixed (common in MPRA libraries), set input_len=200 bp (adjust to actual; if variable, pad/truncate to 200 with center-crop for longer sequences and N-padding for shorter) and record length distribution; if >95% are identical length, disable variable-length masking for simplicity. Treat dataset size as medium-to-large (>10k is common for MPRA), so prioritize strict QC: remove sequences with >5% ambiguous bases (N), remove duplicated sequences keeping the median label per cell type, and winsorize extreme l2fc values at the 0.5th/99.5th percentiles per task to reduce leverage points while keeping ranking. Standardize labels per task using training-set mean/std (z-score) so multi-task losses are on comparable scale; at inference, invert standardization to report l2fc. Use a leakage-safe split: sequence-level unique split (no exact duplicates across train/val/test), optionally family-aware split by k-mer MinHash clustering to reduce near-duplicate leakage; keep val=10%, test=10%. Implement batch sampling uniformly over sequences; do not stratify by label unless label distribution is extremely skewed.

2) Model architecture (shared backbone + 3 heads): Use one-hot DNA encoding (A,C,G,T plus optional N channel) with shape [B, L, 4] (or 5), and a CNN-residual backbone for strong inductive bias on motifs and motif syntax. Backbone example (exact): Conv1D(4->256, kernel=15, stride=1, padding=same) + BatchNorm(momentum=0.9, eps=1e-5) + GELU + Dropout(p=0.10); then 6 residual blocks each: Conv1D(256->256, k=7) + BN + GELU + Dropout(0.10) + Conv1D(256->256, k=7) + BN + GELU with residual connection; after blocks, DilatedConv stack: 3 layers Conv1D(256->256, k=3, dilation=2/4/8) each with BN+GELU+Dropout(0.10) to extend receptive field without huge depth. Pooling: attention pooling or global average pooling; to keep it simple and stable, use GlobalAveragePooling over length producing 256-d vector. Heads: three independent MLP heads: Linear(256->128) + GELU + Dropout(0.20) + Linear(128->1) for each task; initialize with Xavier uniform (gain=1.0) and zero bias. Apply weight normalization (as required) to all Conv1D and Linear layers; in PyTorch, use torch.nn.utils.weight_norm at module construction and optionally remove at inference. This design matches the “shared trunk + multiple expression heads” pattern seen in sequence-to-expression models (e.g., Proformer uses multiple heads and strand embedding ideas), while retaining CNN interpretability and stable training.

3) Loss function (exact choice + weighting + masking): Use Huber loss (a robust alternative to MSE) because MPRA l2fc labels often contain outliers and heavy-tailed noise; set delta=1.0 in standardized label units. Reduction: compute per-sample per-task Huber, then reduce by mean over batch and tasks after weighting (i.e., ‘none’ then manual reduction). Per-task weights: start equal weights w=[1.0,1.0,1.0] for the first 5 epochs to avoid early collapse; then switch to inverse-variance weights estimated from training residual EMA to balance tasks (w_t = 1/(sigma_t^2 + 1e-4), normalized so sum(w)=3.0). Missing-task masking: assume all present, but implement a mask tensor m in {0,1}^3 per sample; final loss = sum_t w_t * m_t * huber(yhat_t,y_t) / (sum_t w_t*m_t + 1e-8) so training remains correct if any labels are absent in future data. Add RC-consistency regularization term (see augmentation/regularization): L_total = L_regression + lambda_rccr * mean_t |yhat_fwd_t - yhat_rc_t| with lambda_rccr=0.05 (after standardization), motivated by the RCCR idea that explicitly penalizing RC prediction divergence improves reliability on genomic tasks (Reverse-Complement Consistency for DNA Language Models, arXiv 2025). If you later detect strong heteroscedasticity per-task (e.g., HepG2 variance far higher), a Gaussian NLL head can be ablated, but Huber is the default for stability and simplicity.

4) Optimization + LR schedule (exact hyperparameters): Use AdamW (not SGD) for faster convergence on sequence models with BatchNorm/weight norm and for robustness to gradient scale; set lr=3e-4, betas=(0.9,0.98), eps=1e-8, weight_decay=0.05. Apply weight decay to all weights except biases, BatchNorm parameters, and weight-norm “g” scalars (exclude via parameter groups). Gradient clipping: clip global norm at 1.0 (gradient_clip_norm=1.0) to prevent rare batch spikes from destabilizing training (common with noisy l2fc). LR schedule: cosine decay with linear warmup; warmup_epochs=5 (or warmup_steps = 5 * steps_per_epoch), start_lr=3e-6, peak_lr=3e-4, min_lr=3e-6, then cosine down to min_lr by the final epoch. Use batch_size=256 if GPU memory permits for stable BatchNorm; if constrained, use batch_size=128 and set BN to sync-bn or switch to GroupNorm(32 groups). Use mixed precision (fp16/bf16) with dynamic loss scaling; keep gradient clip after unscale.

5) Regularization (dropout, input dropout, weight norm, mixup rationale): Backbone dropout p=0.10 in conv/residual layers; head dropout p=0.20 because heads overfit more easily than shared features. Input dropout (random base masking): with probability p_mask=0.02 per position, replace the one-hot base with all-zeros (or an explicit N channel if used); this simulates sequencing/library noise and discourages reliance on single nucleotide positions. Do not use label smoothing because this is regression and smoothing would bias continuous targets; instead, rely on Huber and winsorization/standardization for robustness. Weight norm: apply to every Conv1D and Linear (excluding BN) to stabilize training dynamics; monitor for training slowdowns and consider removing weight norm after convergence if needed. Mixup: do not apply raw-sequence mixup at one-hot level because convex combinations of bases are not biologically valid sequences; if needed as an ablation, restrict mixup to the pooled embedding vector (post-backbone) with alpha=0.2 and mix only within-batch while also mixing labels, but treat as optional because it can blur motif syntax. Add L2 on outputs is not needed; instead, use early stopping and RC regularization.

6) Data augmentation + biological prior integration (motifs/PWMs and RC): Reverse-complement augmentation: with probability 0.5, replace input with its RC during training; at evaluation, optionally average predictions of forward and RC for robustness (test-time augmentation). Add explicit RC-consistency regularization (lambda_rccr=0.05) by computing predictions on both forward and RC in the same batch and penalizing their absolute difference; this is directly inspired by RCCR which targets inconsistent forward/RC predictions in DNA models (arXiv 2025). Small random shift augmentation: if sequences represent a window around a central element, allow shift in [-3,+3] bp with circular padding disabled (pad with N) to reflect uncertainty in exact motif alignment; set shift_prob=0.5. Motif prior: compute PWM scan features (e.g., max log-odds score per TF motif from a curated motif set relevant to K562/HepG2/SKNSH, such as ETS/GATA for hematopoietic contexts, HNF4A/CEBPA for HepG2 liver-like, and neural TFs for SKNSH) using a fixed library; concatenate a low-dim motif feature vector (e.g., 64 motifs) to the pooled 256-d backbone embedding, giving 320-d to each head. Regularize motif branch by Dropout(0.30) on motif features to prevent the model from ignoring sequence and overfitting motif counts. This “feature fusion” keeps the model biologically grounded and improves interpretability; it is also compatible with multi-task learning as in multi-head designs (e.g., Proformer highlights multi-head strategies and strand handling).

7) Training length, early stopping, validation metrics (including specificity/MinGap): Train for max_epochs=80 with early stopping on a single scalar: mean Pearson correlation across the three tasks on the validation set (computed on de-standardized predictions), because Pearson aligns with ranking/relative effect accuracy often desired in regulatory activity prediction. Early stopping parameters: patience=12 epochs, min_delta=0.002 (absolute improvement in mean Pearson), and require at least min_epochs=20 before stopping to avoid premature termination during warmup/cosine transition. Track additional metrics: per-task Pearson, Spearman, RMSE, and calibration of residuals; log each epoch. Specificity validation: compute “MinGap” per sample as gap between top predicted cell-type l2fc and second-best (or between target and mean of others if target label known); then report mean MinGap and fraction of samples with correct top-cell-type (if you have a designated “intended” cell type for each sequence). Use MinGap only for monitoring/model selection tie-breaker: if mean Pearson is within 0.001 of best, select the checkpoint with higher mean MinGap to favor specificity. For robustness, also compute Pearson on RC-averaged predictions (fwd/rc mean) and ensure the drop from fwd-only is <0.01; large gaps indicate strand inconsistency issues.

8) Multi-task handling, balancing, and runnable-like configuration: Optimize weighted-sum loss (not average of independent optimizers) using the dynamic inverse-variance weighting after epoch 5; additionally cap any task weight to at most 2.0x the mean weight to prevent one task from dominating (w_t = min(w_t, 2.0 * mean(w))). If one task’s gradients dominate (detect via per-task gradient norms on the last shared layer), enable GradNorm-style balancing as an optional ablation: update weights every 50 steps to equalize relative training rates, but default to inverse-variance because it is simple and usually effective. Reproducibility: set seeds for Python/NumPy/PyTorch to 1337, enable deterministic flags (torch.backends.cudnn.deterministic=True, benchmark=False), and log git commit + dataset hash + exact config. Ablation plan: (A1) backbone depth {4,6,8 residual blocks} with fixed kernels; (A2) kernel sizes: first conv k in {11,15,21} and residual k in {5,7}; (A3) remove dilated conv stack; (A4) RC augmentation off vs on; (A5) RC augmentation on but RCCR lambda_rccr in {0.0,0.02,0.05,0.1}; (A6) motif feature fusion off vs on; evaluate all by mean Pearson and mean MinGap. Below is the exact YAML-like configuration to run:

experiment:
  name: multitask_l2fc_k562_hepg2_sknsh
  seed: 1337
  deterministic: true
  mixed_precision: bf16

data:
  task_names: [K562, HepG2, SKNSH]
  target_type: l2fc
  input_len_bp: 200
  encoding: onehot
  allow_ambiguous_N: true
  qc:
    max_N_fraction: 0.05
    deduplicate: true
    dedup_reduce: median
    winsorize:
      enabled: true
      lower_q: 0.005
      upper_q: 0.995
  label_standardize: true
  split:
    strategy: sequence_unique
    val_fraction: 0.10
    test_fraction: 0.10

augmentation:
  reverse_complement:
    enabled: true
    prob: 0.50
  shift:
    enabled: true
    prob: 0.50
    max_shift_bp: 3
    pad_with: N
  input_base_masking:
    enabled: true
    p_mask_per_bp: 0.02
    mask_token: zero

model:
  backbone:
    type: cnn_resnet_dilated
    in_channels: 4
    stem:
      conv_out_channels: 256
      kernel_size: 15
      stride: 1
      padding: same
      norm: batchnorm
      bn_momentum: 0.90
      activation: gelu
      dropout_p: 0.10
    res_blocks: 6
    res_block:
      channels: 256
      kernel_size: 7
      dropout_p: 0.10
      norm: batchnorm
      activation: gelu
    dilated_stack:
      enabled: true
      layers: 3
      kernel_size: 3
      dilations: [2, 4, 8]
      dropout_p: 0.10
    pooling: global_avg
    weight_norm: true
  motif_prior:
    enabled: true
    motif_feature_dim: 64
    scan_stat: max_log_odds
    motif_dropout_p: 0.30
    fusion: concat_to_pooled_embedding
  heads:
    shared_in_dim: 256
    fused_in_dim_if_motif: 320
    per_task_mlp:
      hidden_dim: 128
      activation: gelu
      dropout_p: 0.20
      out_dim: 1

loss:
  type: huber
  huber_delta: 1.0
  reduction: none
  multitask_reduction: weighted_mean
  task_weights:
    schedule:
      epoch_0_to_4: [1.0, 1.0, 1.0]
      epoch_5_plus: inverse_variance_ema
    inverse_variance_ema:
      ema_decay: 0.98
      eps: 1.0e-4
      normalize_sum_to: 3.0
      max_weight_ratio_to_mean: 2.0
  missing_task_masking:
    enabled: true
    mask_value: 0
  rc_consistency:
    enabled: true
    lambda: 0.05
    metric: l1

optimizer:
  type: adamw
  lr: 3.0e-4
  betas: [0.9, 0.98]
  eps: 1.0e-8
  weight_decay: 0.05
  exclude_from_weight_decay: [bias, batchnorm, weightnorm_g]
  gradient_clip_norm: 1.0

lr_schedule:
  type: cosine
  warmup_epochs: 5
  start_lr: 3.0e-6
  min_lr: 3.0e-6

training:
  batch_size: 256
  epochs: 80
  min_epochs: 20
  early_stopping:
    monitor: val_mean_pearson
    mode: max
    patience: 12
    min_delta: 0.002
  checkpoint:
    save_best_k: 3
    metric: val_mean_pearson
    tie_breaker_metric: val_mean_mingap

evaluation:
  primary_metric: mean_pearson
  secondary_metrics: [per_task_pearson, spearman, rmse, mean_mingap]
  test_time_rc_ensemble:
    enabled: true
    method: mean

ablations:
  - name: depth
    res_blocks: [4, 6, 8]
  - name: kernel_sizes
    stem_kernel: [11, 15, 21]
    res_kernel: [5, 7]
  - name: rc_aug
    reverse_complement_enabled: [false, true]
  - name: rccr_lambda
    lambda: [0.0, 0.02, 0.05, 0.10]
  - name: motif_prior
    motif_enabled: [false, true]



3. MODEL DESIGN
--------------------------------------------------------------------------------

Design Recommendations:
Dataset and task characterization: With N=77,157 labeled sequences of fixed length 200 bp and one-hot encoding (4 channels), this is a large dataset (>10K) suitable for a moderate CNN with residual/dilated blocks and multi-task heads, targeting ~0.5–5M parameters to balance expressiveness and generalization. The dataset type is not explicitly stated, but the setup (200 bp one-hot, 3 outputs) is consistent with MPRA-like regression, STARR-seq activity prediction, or multi-task epigenomic signal prediction; therefore, preprocessing should include strict deduplication, train/val/test splitting by genomic region (or chromosome holdout) to avoid leakage, and label normalization (z-score per task for regression, or log1p transform if count-like). Because length is fixed at 200, no padding/truncation is needed; however, quality control should remove sequences with ambiguous bases (N) or map them to uniform probabilities (recommended: drop if >1% ambiguous, else replace with all-zero and track a mask). For large datasets, favor cleaning/filtering over heavy augmentation; use light augmentation only if biologically plausible (reverse-complement augmentation with probability 0.5, and random 0–5 bp shift if the assay is shift-invariant, implemented by circular or padded shift). Model selection rationale: choose a shared WeightNorm CNN backbone because it is efficient on 200 bp inputs, naturally learns motif detectors, and supports multi-task learning; multi-scale first-layer kernels (7/11/15) act as motif scanners at different widths while downstream dilated residual blocks capture spacing/interactions. The retrieved literature emphasizes interpretability through convolutional filters and in silico mutagenesis (“un-box” models using convolutional filters, attention maps, and in silico mutagenesis; Valeri et al., 2020, PMC), motivating explicit interpretability hooks in the experimental plan.

Exact architecture (layer-by-layer, with dimensions): Input X is (B,4,200). Stage A (multi-scale motif stem, parallel branches; all Conv1d use torch.nn.utils.weight_norm): Branch A1: WN-Conv1d(4->64, k=7, s=1, p=3), LeakyReLU(negative_slope=0.1), Dropout(p=0.10). Branch A2: WN-Conv1d(4->64, k=11, s=1, p=5), LeakyReLU(0.1), Dropout(0.10). Branch A3: WN-Conv1d(4->64, k=15, s=1, p=7), LeakyReLU(0.1), Dropout(0.10). Concatenate along channels: (B,192,200). Fuse + downsample: WN-Conv1d(192->256, k=1, s=1, p=0), LeakyReLU(0.1), then MaxPool1d(kernel=2, stride=2) giving (B,256,100). Stage B (residual blocks, same-length within block): Block B1 (no dilation): Main path: WN-Conv1d(256->256, k=3, s=1, p=1), LeakyReLU(0.1), Dropout(0.15), then WN-Conv1d(256->256, k=3, s=1, p=1); Residual add (identity) then LeakyReLU(0.1) output (B,256,100). Block B2 (dilation=2): WN-Conv1d(256->256, k=3, s=1, p=2, dilation=2), LeakyReLU(0.1), Dropout(0.15), WN-Conv1d(256->256, k=3, s=1, p=2, dilation=2), residual add, LeakyReLU -> (B,256,100). Block B3 (dilation=4): WN-Conv1d(256->256, k=3, s=1, p=4, dilation=4), LeakyReLU(0.1), Dropout(0.15), WN-Conv1d(256->256, k=3, s=1, p=4, dilation=4), residual add, LeakyReLU -> (B,256,100). Stage C (channel expansion + second downsample): WN-Conv1d(256->384, k=3, s=1, p=1), LeakyReLU(0.1), then MaxPool1d(k=2,s=2) -> (B,384,50). Stage D (residual blocks at 50 positions): Block D1 (no dilation): WN-Conv1d(384->384, k=3, s=1, p=1), LeakyReLU(0.1), Dropout(0.20), WN-Conv1d(384->384, k=3, s=1, p=1), residual add, LeakyReLU -> (B,384,50). Block D2 (dilation=2): WN-Conv1d(384->384, k=3, s=1, p=2, dilation=2), LeakyReLU(0.1), Dropout(0.20), WN-Conv1d(384->384, k=3, s=1, p=2, dilation=2), residual add, LeakyReLU -> (B,384,50). Backbone output pooling: compute GlobalAvgPool1d over length (50) -> (B,384) and GlobalMaxPool1d over length (50) -> (B,384), then concatenate -> backbone feature vector h of size (B,768). This pooling choice preserves both “presence” (max) and “overall activity” (avg), and yields a fixed 768-dim representation for all tasks.

Multi-scale motif capture specifics (required): The explicit parallel conv stem uses kernels 7/11/15 to learn motif detectors of different effective lengths; because padding is (k-1)/2, each branch preserves length 200, enabling clean concatenation. After concatenation, a 1x1 convolution fuses across scales into 256 channels, letting the model learn weighted combinations of motif detectors and reduce dimensionality before deeper processing. Long-range dependencies are modeled by the dilated residual blocks (dilation 2 and 4 at length 100, then dilation 2 at length 50), increasing receptive field without excessive pooling; for example, at length 100, two stacked k=3 dilated convs with dilation=4 expand the receptive field substantially while preserving resolution. Residual connections ensure stable gradient flow for a deeper backbone and mitigate performance degradation as depth increases. If an ablation is desired, swap the parallel branches for a sequential dilated stack (dilations 1/2/4/8) while keeping parameter count similar; however, the parallel design typically improves motif-width coverage on short sequences. This design choice is consistent with common regulatory CNN motif-learning patterns and supports downstream interpretability by aligning first-layer filters with motif-like patterns (a practice commonly paired with in silico mutagenesis in interpretability workflows, as described in the retrieved “un-boxing” literature).

Heads (three independent outputs) and exact dimensions: Shared feature h is (B,768). Head1 MLP: Linear(768->256), LeakyReLU(0.1), Dropout(p=0.30), Linear(256->64), LeakyReLU(0.1), Dropout(p=0.20), Linear(64->1). Head2 is identical but with independent parameters, producing y2 (B,1). Head3 is identical, producing y3 (B,1). Final output is torch.cat([y1,y2,y3], dim=1) -> (B,3) or return a tuple of three scalars depending on training loop preference. If tasks have different scales/noise, optionally add per-head LayerNorm(768) before the first Linear to stabilize, but keep it off by default to minimize moving parts alongside WeightNorm. For classification tasks (if applicable), apply per-head Sigmoid for binary or softmax for multi-class; for regression, output is linear and use MSE/Huber. To encourage shared representation but avoid negative transfer, the heads are moderately deep yet small; dropout is higher in heads (0.30) than backbone (0.10–0.20) to regularize task-specific fitting.

Initialization, normalization, and training hyperparameters: All Conv1d and Linear layers use Kaiming/He initialization suited for LeakyReLU: torch.nn.init.kaiming_normal_(weight, a=0.1, mode='fan_out', nonlinearity='leaky_relu'); biases initialized to 0.0. WeightNorm is applied as torch.nn.utils.weight_norm(conv_layer) (and optionally on Linear layers in heads if desired, but keep Linear without WN initially for simplicity); when using WeightNorm, still initialize the underlying weight_v with Kaiming. BatchNorm is optional; if included, use BatchNorm1d with momentum=0.1 and eps=1e-5 after conv and before activation, but given explicit WeightNorm, the default recommendation is to omit BN to reduce interaction effects and keep inference simple. Optimizer: AdamW with lr=1e-3, betas=(0.9,0.999), weight_decay=1e-4; gradient clipping at 1.0 to stabilize multi-task training. Training schedule: cosine decay with 5% warmup steps (e.g., warmup for first 1–2 epochs), total 30–50 epochs with early stopping patience=6 on mean validation metric across tasks. Batch size suggestion: 256 if GPU memory allows (input is small), else 128; mixed precision (AMP) recommended for speed. Loss: if regression, per-task Huber(delta=1.0) or MSE; combine as weighted sum with weights inversely proportional to task variance (or use uncertainty weighting), and track Pearson/Spearman per task.

Parameter count estimate, target rationale, and compute efficiency: The design targets ~1.8–2.2M parameters (within the requested 0.5–5M). Rough breakdown: multi-scale stem convs (4->64 with k=7/11/15) contribute ~1.8k+2.8k+3.9k weights; fuse 1x1 (192->256) adds ~49k; residual stacks dominate (multiple 256->256 k=3 and 384->384 k=3 convs) contributing on the order of ~1.6–1.9M total; heads add ~0.66M (each head ~0.22M for 768->256->64->1). This parameter budget is appropriate for 77k samples because it provides sufficient capacity to learn motif combinations and task-shared features while remaining small enough to train quickly and limit overfitting risk compared with very large transformers. Computationally, all operations are 1D convolutions over max length 200 (then 100/50), making FLOPs modest; pooling reduces length early, and dilation increases receptive field without increasing length or parameters. WeightNorm adds negligible overhead and often improves optimization stability for CNNs, particularly when BN is omitted. For deployment, the model is a single backbone pass plus three small MLP heads; latency is low, and batch inference is efficient.

Interpretability hooks and exact procedures (saliency, IG, in-silico mutagenesis, motif extraction): For gradient saliency, enable requires_grad on input one-hot tensor X, run forward to get y_task (choose one head or a weighted sum), then compute grads = torch.autograd.grad(y_task.sum(), X)[0], producing (B,4,200) attributions; aggregate to per-position importance via abs(grads).sum(dim=1) or grads*X (gradient×input) summed over channels. For Integrated Gradients, use baseline X0 = all-zeros (or GC-matched random baseline), choose steps m=64, compute IG = (X-X0) * (1/m) * sum_{i=1..m} grad(f(X0 + i/m*(X-X0))) across steps; return (B,4,200) attribution maps per task. For in-silico mutagenesis, for each position p, mutate within a window size W=11 (centered at p; clamp at edges) by flipping the one-hot base to each alternative base (3 alternatives per position), recompute output delta for each mutation, and store max |delta|; this yields a (200,) sensitivity profile and a 4x200 mutation effect map. Motif extraction: take top-K (e.g., K=5000) highest-saliency windows of length 15 from correctly predicted/high-confidence sequences, align them by peak position, and compute PWM by averaging one-hot in aligned windows; optionally also inspect first-layer convolutional filters by converting weight tensors to sequence logos. Provide hooks in code by exposing (a) the stem branch conv outputs (before pooling) and (b) the input gradients; register_forward_hook on stem convs and register_full_backward_hook for gradient capture. This aligns with the retrieved interpretability emphasis on “un-boxing” models via convolutional filters and in silico mutagenesis (Valeri et al., 2020, PMC) and complements regulatory genomics practice where motif-like patterns are derived from first-layer filters plus attribution maps.

Required architecture table and PyTorch skeleton (unambiguous, minimal but precise): The architecture table should list each layer with input/output shapes: (1) Input (B,4,200). (2) Branch7: WN-Conv1d(4,64,7,1,3)->(B,64,200), LReLU(0.1), Dropout0.10. (3) Branch11: WN-Conv1d(4,64,11,1,5)->(B,64,200), LReLU, Dropout0.10. (4) Branch15: WN-Conv1d(4,64,15,1,7)->(B,64,200), LReLU, Dropout0.10. (5) Concat ->(B,192,200). (6) Fuse: WN-Conv1d(192,256,1,1,0)->(B,256,200), LReLU. (7) MaxPool1d(2,2)->(B,256,100). (8) ResBlock256 d=1 ->(B,256,100). (9) ResBlock256 d=2 ->(B,256,100). (10) ResBlock256 d=4 ->(B,256,100). (11) WN-Conv1d(256,384,3,1,1)->(B,384,100), LReLU. (12) MaxPool1d(2,2)->(B,384,50). (13) ResBlock384 d=1 ->(B,384,50). (14) ResBlock384 d=2 ->(B,384,50). (15) GAP->(B,384), GMP->(B,384), Concat->(B,768). (16) Head t in {1,2,3}: Linear 768->256, LReLU, Dropout0.30; Linear 256->64, LReLU, Dropout0.20; Linear 64->1. PyTorch skeleton: define class SharedBackbone(nn.Module) with stem convs wrapped by weight_norm, forward returns h (B,768) and optionally intermediates; define class MultiTaskModel(nn.Module) with backbone + three nn.Sequential heads; forward returns torch.cat outputs. Ensure Conv1d layers are created then wrapped: conv = nn.Conv1d(...); conv = torch.nn.utils.weight_norm(conv). Residual block skeleton: two WN-Conv1d layers, dropout between, add identity, final activation; keep channels constant to avoid projection shortcuts in this baseline (add 1x1 projection only if channels change).


4. RESULT SUMMARY
--------------------------------------------------------------------------------

Design Recommendations:
1) Dataset characteristics and evaluation unit definition: Treat each sequence/regulatory element as the sampling unit, with a vector of continuous targets across K cell lines (multi-output regression), and ensure that all metrics and bootstrap resamples operate at the element level to preserve within-element cross-cell correlation structure. Before any metric computation, summarize target distributions per cell line (mean, SD, skew, % near-zero), and summarize lfcSE (if present) per cell line (median, IQR) to anticipate heteroscedastic errors; use these summaries to justify stratified diagnostics (error vs lfcSE bins). If the dataset includes replicated measurements per element, aggregate replicates into a single target per cell line using inverse-variance weighting w=1/(lfcSE^2) when lfcSE is available; otherwise use the arithmetic mean, and carry forward an aggregated uncertainty estimate (e.g., sqrt(1/sum(w))). Define the held-out test set at the element level with no overlap in sequence identity between splits; if sequences are highly similar (e.g., tiled variants), split by group/cluster to avoid leakage, consistent with evaluation cautions in CRM assessment literature (Su et al., 2010, PMC). If there are distinct assay types or libraries, treat them as strata and enforce stratified splitting to maintain comparable distributions across train/val/test, reflecting that prediction accuracy varies systematically by technique (Nowling et al., 2023, PMC). 2) Primary metric suite and exact computation specs: Primary endpoint is Pearson correlation r computed separately for each cell line c on the held-out test set of N elements: r_c = cov(y_c, yhat_c) / (sd(y_c)*sd(yhat_c)), where y_c and yhat_c are length-N vectors of de-standardized true and predicted values for that cell line. De-standardization rule (if training used z-scoring per cell line): yhat_c,orig = yhat_c,std * sigma_c,train + mu_c,train and y_c,orig = y_c,std * sigma_c,train + mu_c,train, using mu/sigma computed on training targets only to avoid leakage; Pearson is computed on orig scale, and ties to reporting in many sequence-to-function regression tasks where correlation is favored for scale-invariant performance (Nowling et al., 2023, PMC). Also report Spearman rho per cell line (rank correlation, computed on orig scale, with average ranks for ties), plus MSE_c = mean((y_c,orig - yhat_c,orig)^2) and MAE_c = mean(|y_c,orig - yhat_c,orig|). Report macro-averaged mean Pearson across tasks: r_mean = (1/K) * sum_c r_c; additionally report a sample-size-weighted mean Pearson r_wt = sum_c (N_c * r_c)/sum_c N_c if missingness differs by cell line. For completeness, report a “micro-Pearson” computed by concatenating all cell line targets/predictions into a single vector (after de-standardization) only as a secondary summary, because it can be dominated by high-variance cell lines; macro-averaging is the default selection criterion. Thresholds for contextual interpretation: r_c > 0.60 is labeled “good”, 0.40–0.60 “moderate”, 0.20–0.40 “weak”, and <0.20 “poor”, with the caveat that achievable r depends on assay noise and technique (Nowling et al., 2023, PMC). 3) Bootstrap confidence intervals (per task) with parameterization: For each cell line c, compute 95% CIs for Pearson, Spearman, MSE, and MAE using nonparametric bootstrap with B=1000 resamples of the test elements. Resampling procedure: sample N indices with replacement from {1..N}; for each b, compute metric m_c^(b) on the resampled pairs (y_c,orig[i_b], yhat_c,orig[i_b]); then CI is the percentile interval [q_0.025, q_0.975] of {m_c^(b)}. Use the same bootstrap indices across models when comparing ablations (paired bootstrap) to reduce variance and enforce pairing, and use a fixed random seed (e.g., seed=2025) for reproducibility in reporting. For Pearson specifically, if bootstrap distribution is highly skewed or near ±1, optionally compute CI on Fisher z space: z = atanh(r), bootstrap z, then transform back with tanh; report which method is used, defaulting to percentile on r for simplicity and Fisher-z for edge cases. Also compute CI for r_mean via bootstrap by computing r_c^(b) for all c on the same resample and then averaging to r_mean^(b); CI is percentiles of {r_mean^(b)}. Report bootstrap failure rates (e.g., sd(yhat)=0 in a resample) and handle by skipping those draws; if >1% draws fail for a task, flag it as an instability diagnostic. 4) MinGap-based specificity validation (exact use of MinGap column): Interpret MinGap_i as a label-derived specificity proxy per element i (larger MinGap indicates stronger separation between a “preferred” and “non-preferred” cell context, or generally higher specificity). First define predicted specificity per element using predicted cell-type contrasts: for element i with predictions across K cell lines, let pred_gap_i = max_c(yhat_i,c,orig) - secondmax_c(yhat_i,c,orig) (top-1 minus top-2 predicted activity); optionally also compute pred_range_i = max_c(yhat_i,c,orig) - min_c(yhat_i,c,orig) as a broader contrast. Correlation analysis: compute Spearman correlation between MinGap and pred_gap across test elements: rho_gap = Spearman(MinGap, pred_gap); bootstrap CI with B=1000 resamples of elements, reporting 95% CI and p-value via bootstrap percentile or permutation (10,000 permutations recommended if feasible) for robustness. Stratified performance: split test elements into Q=5 quantiles (quintiles) by MinGap (Q1 lowest specificity to Q5 highest) using cutpoints on the test set only; within each quantile q, compute per-cell-line Pearson r_c,q and macro-mean r_mean,q, then test monotonic trend across quantiles using Spearman correlation between quantile index (1..5) and r_mean,q (or use Jonckheere–Terpstra if implementing a formal ordered alternative). Additionally, compute “specificity classification consistency” by checking whether argmax_c(yhat_i,c,orig) matches argmax_c(y_i,c,orig) (winner-takes-all cell line) and report accuracy per MinGap quantile; expectation is higher accuracy in higher MinGap bins if MinGap encodes clearer cell preference. Report effect sizes: delta_r = r_mean,Q5 - r_mean,Q1 and its bootstrap CI to quantify how much specificity strength changes predictability. This aligns with broader regulatory element evaluation themes that emphasize cell-type-specific performance stratification (e.g., motif-based prediction varies across cell contexts in Cornejo-Páramo et al., 2025, PMC). 5) Model selection and early stopping (validation metric and parameters): Select the checkpoint that maximizes mean validation Pearson across cell lines (macro-average) computed on de-standardized values, denoted r_mean,val, evaluated once per epoch. Early stopping rule: patience=10 epochs, min_delta=0.002 in r_mean,val (i.e., require at least +0.002 improvement to reset patience), with a maximum of 200 epochs; store the best checkpoint by r_mean,val and break training after patience is exhausted. If validation set is small or noisy, smooth r_mean,val with an exponential moving average (EMA) with decay=0.6 for early-stopping decisions while still reporting raw values; explicitly state which is used for stopping versus reporting. Tie-breakers: if two checkpoints have r_mean,val within 0.001, choose the one with lower val MSE (macro-averaged) to prefer better absolute calibration. Guard against overfitting to a single cell line by also requiring that at least 70% of cell lines have non-decreasing Pearson relative to the previous best checkpoint (a stability constraint); if violated, flag and optionally use a multi-objective rank (maximize r_mean,val and minimize variance of r_c,val across cell lines). This selection design is consistent with correlation-centric reporting in regulatory element prediction comparisons, where per-context performance can differ materially (Nowling et al., 2023, PMC). 6) Statistical comparisons for ablations (paired tests, Fisher z, multiple testing): For each ablation A vs baseline B, compute paired differences in Pearson per cell line on the same test set: d_c = r_c(B) - r_c(A), and report d_c with 95% paired bootstrap CI using the shared bootstrap indices across models (B=1000). For a global comparison across cell lines, compute d_mean = (1/K) * sum_c d_c and its bootstrap CI; declare significance if the Holm–Bonferroni-adjusted p-value < 0.05. P-values from paired bootstrap: p = 2*min(P(d_mean^(b) <= 0), P(d_mean^(b) >= 0)) using the bootstrap distribution of d_mean^(b) (centered at 0), with the same approach optionally applied per cell line. As an alternative or confirmation for Pearson differences per cell line, apply Fisher z-transform: z = atanh(r), standard error approx se = 1/sqrt(N-3); test statistic t = (z_B - z_A)/sqrt(2*se^2) if correlations are treated as independent; however, because predictions are paired on the same y, prefer paired bootstrap as the primary test and use Fisher-z only as a sensitivity analysis. Correct for multiple ablations and/or multiple cell lines using Holm–Bonferroni (family defined as all hypothesis tests reported in the main table; alpha=0.05). Report both adjusted and unadjusted p-values, and emphasize effect sizes (delta Pearson, delta MSE) over p-values to avoid overinterpretation, consistent with best practice in method comparison studies (Su et al., 2010, PMC). 7) Diagnostics and plots (required panels and parameter settings): Per-cell-type scatter plots: for each cell line, plot y_orig vs yhat_orig on test with an overlaid y=x line; annotate r, rho, MAE, and n, and set axis limits to [p1, p99] percentiles to reduce outlier domination (keep outliers plotted with transparency alpha=0.2). Calibration for regression: bin predictions into M=10 equal-frequency bins per cell line, compute mean(yhat) and mean(y) per bin, plot mean(y) vs mean(yhat) with error bars (±1 SE via bootstrap within-bin) and report calibration slope/intercept from a linear fit y = a + b*yhat; “good” calibration is b in [0.9, 1.1] and |a| < 0.1*sd(y). Residual analysis: compute residual e_i,c = y_i,c,orig - yhat_i,c,orig; plot residual histograms (50 bins), Q–Q plots, and residual vs prediction to inspect heteroscedasticity; flag if Breusch–Pagan p<0.01 (optional) or if residual variance increases >2x from lowest to highest prediction decile. Error vs lfcSE bins: within each cell line, bin elements by lfcSE into 5 quantiles and compute MAE and Pearson per bin; expectation is worse error at higher lfcSE, and deviations (e.g., flat or inverted trend) can indicate model mis-specification or label noise handling issues. Also include “influence/outlier” diagnostics: report the top 1% absolute residual elements and whether they cluster by sequence family or extreme MinGap, to guide biological follow-up. These plots support interpretation of why correlations differ across contexts and are commonly used in sequence-to-function model evaluation beyond single summary metrics (Nowling et al., 2023, PMC). 8) Reporting template (tables, figures, and acceptance thresholds): Main Table 1 (Test performance per cell line): columns = cell_line, N_test, Pearson r (95% CI), Spearman rho (95% CI), MSE (95% CI), MAE (95% CI); add a final row with macro-mean across cell lines with bootstrap CI. Main Table 2 (Ablation deltas): for each ablation, report delta r_mean (95% CI), adjusted p-value (Holm), and per-cell-line delta r_c (optionally in supplement). Table 3 (MinGap specificity): report rho(MinGap, pred_gap) with CI, r_mean per MinGap quintile with CI, and delta_r (Q5-Q1) with CI; include winner-cell accuracy per quintile if applicable. Figure set: Fig 1 per-cell-type scatter grid (K panels), Fig 2 calibration curves (K panels or representative), Fig 3 residual diagnostics (aggregate + per cell line), Fig 4 MinGap stratification curves and pred_gap vs MinGap scatter with smooth (LOESS span=0.75). Predefine “good performance” as: (i) r_mean,test >= 0.60 with lower CI bound >= 0.50, (ii) at least 70% of cell lines have r_c >= 0.50, and (iii) positive specificity association rho_gap >= 0.30 with CI excluding 0; these are contextual thresholds and should be interpreted relative to assay noise and baseline models. Provide a concise model card appendix listing data split IDs, target scaling parameters (mu/sigma), bootstrap seed, and exact metric definitions to ensure reproducibility, reflecting reproducibility emphasis in method assessments (Su et al., 2010, PMC).


================================================================================
EXPERT IMPLEMENTATION PLANS
================================================================================

DATA MANAGEMENT EXPERT (Score: 9.2/10)
--------------------------------------------------------------------------------

Design Summary:
This dataset is an MPRA-style cis-regulatory element (CRE) activity dataset structured for 3-task regression (K562/HepG2/SKNSH log2 fold-change outcomes) with per-row uncertainty estimates (lfcSE) and metadata columns (target_cell, MinGap). With 77,157 rows, it is a large dataset (>10K), so the plan prioritizes aggressive QC/cleaning, uncertainty-based filtering, deduplication, and leakage-safe splits rather than heavy augmentation. The primary input is a fixed-length DNA sequence expected to be 200 bp; the plan specifies explicit checks for length distribution and deterministic handling of any deviations (trim/pad policy or exclusion). Targets are treated as continuous regression labels per cell line, with normalization (z-scoring) computed only on the training split and stored for reproducibility. Splitting is designed to avoid sequence leakage by grouping identical sequences and then splitting groups into train/val/test with fixed seed, while retaining representative distributions of target_cell and l2fc. Optional reverse-complement augmentation is included with a controlled probability to improve invariance without materially changing dataset statistics, and data loader parameters are provided for efficient throughput on this scale.


Implementation Plan (Complete, No Summarization):

Design Recommendations:
------------------------------------------------------------
1) Dataset profiling and type confirmation: Using the provided columns ['sequence','target_cell','K562_l2fc','HepG2_l2fc','SKNSH_l2fc','MinGap','K562_lfcSE','HepG2_lfcSE','SKNSH_lfcSE'] and 77,157 rows, this is best characterized as an MPRA-derived multi-output regression dataset where each row is a synthetic/natural 200 bp regulatory sequence with measured activity in three cell lines, reported as log2 fold change (l2fc) with associated standard errors (lfcSE). This implies label noise is heteroskedastic and should be addressed by uncertainty-aware filtering and/or loss weighting; for data management, we prioritize filtering rules that are explicit and reproducible. Sequence length should be validated by computing length for every row and summarizing mean/median/min/max and outlier counts (e.g., count of length != 200). The expected distribution is a delta at 200 bp; if not, the plan defines strict handling: (a) preferred: drop sequences with length != 200 if they are rare (<0.5% of rows), (b) otherwise: trim longer sequences to the central 200 bp and pad shorter sequences with 'N' to 200 and mark them with a QC flag for downstream sensitivity analyses. Because the dataset is large, we also recommend computing duplication rate (exact identical sequences), GC-content distribution (mean/SD and tails), and per-target summary statistics (l2fc mean/SD, percentiles) as part of profiling to guide outlier policies. These recommendations align with community emphasis on robust MPRA QC and variability control (e.g., standardized QC frameworks and outlier identification described in MPRA QC pipelines such as esMPRA and uniform processing standards such as MPRAsnakeflow/MPRAlib from IGVF efforts).

2) QC / cleaning with explicit lfcSE filtering rule: Implement a deterministic filter based on measurement uncertainty using the three standard error columns K562_lfcSE, HepG2_lfcSE, SKNSH_lfcSE with the threshold lfcSE > 0.5. Because ambiguity exists (filter per cell line vs. only the target_cell’s SE), define and document two exact, mutually exclusive rules: Rule A (strict multi-task reliability): DROP a row if ANY of the three SE values exceeds 0.5, i.e., drop if max(K562_lfcSE, HepG2_lfcSE, SKNSH_lfcSE) > 0.5; this ensures all three regression labels for a retained sequence are relatively reliable. Rule B (target-cell-only reliability): DROP a row only if the SE corresponding to target_cell exceeds 0.5; concretely, map target_cell in {K562,HepG2,SKNSH} to the matching *_lfcSE column and drop if that single SE > 0.5, allowing retention if non-target tasks are noisy. Recommendation: prefer Rule A for multi-task training where the model jointly predicts all three activities, because noisy targets in any head can destabilize optimization and bias shared feature learning; Rule B is acceptable if target_cell is used as the primary supervision and other heads are auxiliary/weak. In addition, record the number of rows removed under each rule and the post-filter distributions of l2fc to ensure filtering does not collapse dynamic range; if Rule A removes >30% of data, consider raising threshold to 0.6 or switching to Rule B plus per-task loss weighting by 1/(lfcSE^2 + 1e-6). This approach is consistent with MPRA best-practice emphasis on identifying technical variability sources and filtering out unreliable measurements (as highlighted in MPRA QC/standardization efforts like esMPRA and IGVF MPRA processing standards).

3) Missing/invalid values, sequence sanitization, and N handling: Define invalid sequences as any containing characters outside {A,C,G,T,N} (case-insensitive); immediately drop rows with other IUPAC codes (e.g., R,Y,S,W,K,M) unless you explicitly implement probabilistic mapping for them. For Ns, implement a two-tier policy: if N_fraction = count('N')/length(sequence) > 0.0 (strict) then drop, OR if you want a less aggressive option, drop only if N_fraction > 0.01 (i.e., >2 Ns in 200 bp) and otherwise keep. Recommendation for this large dataset: use N_fraction > 0.0 (drop any N) as the default because you have ample data volume (77k) and Ns introduce ambiguous k-mers that can degrade motif learning; keep the relaxed threshold (1%) as a fallback if too many sequences are removed. If sequences with small N_fraction are kept, encode N as a uniform distribution [0.25,0.25,0.25,0.25] at that position (float32), which preserves expected base frequency without injecting a specific nucleotide. Also drop rows with missing numeric targets/SEs (NaN/Inf) and define explicit range checks: lfcSE must be >0 and <5 (hard sanity bounds), and l2fc should be finite with optional winsorization at the 0.1 and 99.9 percentiles per cell line if extreme outliers are present. Maintain a QC report that logs counts removed by each rule (invalid chars, length mismatch, N filter, lfcSE filter, NaNs) to support reproducibility and auditability.

4) Encoding specification (exact tensor layout and dtype): Use a channel-first one-hot encoding with float32 output tensor shape (B, 4, 200), where B is batch size. Define base-to-channel mapping exactly as: A->[1,0,0,0], C->[0,1,0,0], G->[0,0,1,0], T->[0,0,0,1]; for N (if allowed under the relaxed N policy) use [0.25,0.25,0.25,0.25]. Enforce uppercase transformation and verify post-encoding that each position sums to 1.0 (within float tolerance) for A/C/G/T/N; for dropped invalid bases this check is not needed because they are excluded upstream. Keep a deterministic sequence order (5’->3’) and only apply reverse-complement augmentation explicitly as described later; do not implicitly canonicalize sequences because strand information can matter in some MPRA designs. Store encoded arrays in a memory-efficient format for 77k sequences: either on-the-fly encoding in the DataLoader (CPU) or pre-encode and save as an mmap/NPY/Zarr array; with 77,157*4*200 float32 ≈ 246 MB, pre-encoding is feasible and can improve training throughput. If pre-encoding, also store an index mapping from row_id to original CSV row for traceability.

5) Target construction and normalization (train-only scalers): Treat targets as a 3D regression vector y = [K562_l2fc, HepG2_l2fc, SKNSH_l2fc] per row, regardless of target_cell; use target_cell as auxiliary metadata (see splitting/bias checks) rather than the label. Normalize each target dimension separately using only the training split statistics: for cell line c in {K562,HepG2,SKNSH}, compute mean μ_c and std σ_c on train rows, then transform y_c_norm = (y_c - μ_c) / (σ_c + 1e-8). Save μ and σ in a versioned artifact (e.g., JSON) and apply the same scalers to validation and test; at evaluation time, compute metrics in both normalized space (for stable training curves) and original l2fc space (for interpretability). If you adopt SE-aware weighting instead of hard filtering, define per-target weight w_c = 1/(lfcSE_c^2 + 1e-6) and optionally cap weights at w_max=25 to prevent single observations dominating; this is complementary to normalization and is especially useful if you choose the less strict Rule B. Also check for systematic shifts in l2fc distributions across target_cell values; if strong, consider including target_cell as a categorical covariate input (one-hot of size 3 concatenated to model head) but keep it out of the leakage-safe grouping keys.

6) Data splitting strategy (leakage-safe, deterministic, and stratified where possible): Primary risk is sequence leakage due to duplicate (identical) sequences appearing multiple times; therefore split by groups defined as the exact 'sequence' string after canonical cleaning (uppercase, validated length, no Ns if dropped). Algorithm: (i) create cleaned_sequence, (ii) group rows by cleaned_sequence and assign each group an integer group_id, (iii) shuffle group_ids with a fixed seed (seed=2025), (iv) allocate groups to train/val/test by cumulative group size to achieve 80/10/10 of total rows as closely as possible while keeping groups intact. After the group split, verify that there are zero identical sequences shared across splits (assert disjointness) and report group counts and row counts per split. If you want additional robustness against near-duplicates (1–2 mismatches), optionally cluster sequences by MinHash/LSH or by Hamming distance <=2 within 200 bp, but this is computationally heavier; as a practical default, exact-dedup group splitting is required and sufficient for many MPRA tables. Because 'target_cell' may be imbalanced, add a secondary constraint: during group allocation, approximate stratification by target_cell by tracking per-split target_cell proportions and using a greedy assignment that minimizes deviation from the overall distribution (tolerance ±2%). Keep 'MinGap' out of grouping keys but use it for post-split diagnostics; if MinGap correlates with measurement quality, ensure its distribution is similar across splits.

7) Augmentation policy (lightweight due to large N, optional RC): Since the dataset is large (>10K), prioritize QC and do not use heavy synthetic augmentation that could distort measured activity. Optional reverse-complement (RC) augmentation can be applied with probability p=0.5 during training only: for each sequence in a batch, with Bernoulli(p) replace it with its reverse complement; labels (all three l2fc) remain unchanged because MPRA reporter constructs typically measure sequence function independent of strand orientation in many setups, but you must confirm assay design—therefore treat RC augmentation as optional and evaluate with/without it. If you suspect strand-specific effects (e.g., promoter-directional constructs), set p=0.0 and do not RC augment. If you enable RC, implement it at the one-hot tensor level: reverse the length dimension and swap channels A<->T and C<->G; this is deterministic and avoids string operations. Do not use random base mutations for MPRA regression because it changes the underlying biological function and breaks label validity; similarly, do not use random shuffling except as a negative control dataset for sanity checks.

8) Data loader and throughput specifications: For 4x200 float32 tensors, a batch size of 256 is a strong default on a single modern GPU (e.g., 16–24 GB VRAM); provide candidates {128, 256, 512} and choose the largest that maintains >90% GPU utilization without OOM. Use num_workers=4–8 (Linux) depending on CPU cores, pin_memory=True for CUDA training, persistent_workers=True, and prefetch_factor=2 to reduce input stalls; if pre-encoding to mmap/Zarr, you can reduce CPU overhead and increase batch size. Shuffle training data each epoch (shuffle=True) but keep validation/test deterministic (shuffle=False). For reproducibility, set global seeds (Python, NumPy, PyTorch) to 2025 and enforce deterministic ops where feasible; still record that CuDNN determinism may reduce speed. Finally, maintain a data manifest per split containing row indices, cleaned_sequence hash (e.g., SHA1), and QC flags so that any model run can be traced back exactly.

9) Step-by-step preprocessing pipeline (pseudocode-level clarity with exact parameters): Step 0 Load CSV and assert required columns exist; Step 1 Standardize sequence strings: seq = upper(seq), strip whitespace; Step 2 Validate alphabet: drop if regex '[^ACGTN]'; Step 3 Compute len(seq) and profile distribution; apply length policy: if len!=200 then drop (default) unless outlier_fraction>0.5% then trim/pad to 200 with 'N' and flag; Step 4 Handle Ns: compute N_fraction; default drop if N_fraction>0.0 (alternative drop if >0.01 and map remaining N to uniform encoding); Step 5 Validate numeric columns: drop rows with NaN/Inf in any l2fc or lfcSE; enforce lfcSE in (0,5); Step 6 Apply lfcSE filter with threshold 0.5 using Rule A (default) or Rule B (alternative) and log removals; Step 7 Deduplicate/group: group_id = hash(cleaned_sequence), create group table; Step 8 Group split with seed=2025 into train/val/test 0.8/0.1/0.1 by groups; Step 9 Compute train-only μ_c, σ_c and normalize targets; Step 10 Encode sequences to one-hot float32 (B,4,200); Step 11 Save artifacts: split manifests, scaler JSON, QC report, and optional pre-encoded arrays; Step 12 Train with optional RC augmentation p=0.5 on train only. This pipeline follows the general principle from MPRA community tooling that robust QC, outlier handling, and standardized processing are essential for reproducibility (as emphasized by MPRA QC pipelines like esMPRA and uniform processing/standard formats like MPRAsnakeflow/MPRAlib).


Recommendations:
  1. Confirm assay strand/orientation invariance before using RC augmentation or RC-consistency loss. If the MPRA library is orientation-fixed (common), disable RC augmentation and instead rely on standard regularization; if orientation-randomized or bidirectional by design, enable RC augmentation at p=0.5 and consider test-time RC ensembling.
  2. Prefer uncertainty-aware learning over hard drops when lfcSE is available: implement inverse-variance sample weights w=1/(lfcSE^2+ε) with caps (e.g., w_max=25) and/or robust loss (Huber). Use filtering only for extreme outliers (e.g., lfcSE > 0.8–1.0) after inspecting retention rates. This aligns with MPRA QC emphasis on standardized metrics and systematic monitoring (as in esMPRA) rather than ad hoc removal.
  3. Run the proposed QC ablations and log outcomes: (A) strict drop if any-task lfcSE>t vs (B) drop only for the labeled target_cell’s lfcSE>t vs (C) no-drop + weighting. Choose the policy that improves held-out performance without collapsing label range or removing >30% of rows.
  4. Make split strategy conditional on available identifiers: (i) If genomic coordinates exist, do region/chromosome holdout; (ii) otherwise do GroupKFold by exact sequence hash, and optionally add a near-duplicate guardrail using k-mer sketching/minhash buckets before splitting to reduce similarity leakage.
  5. Sequence length policy: compute and report length distribution; default drop len!=200. If off-length fraction >0.5%, adopt deterministic trimming-to-center for len>200 and padding for len<200, then constrain N_fraction<=0.01 to avoid excessive ambiguity injection (and record a flag feature indicating padded).
  6. Bias checks: quantify per-cell-line label distributions, lfcSE distributions, and any platform/batch identifiers if present; ensure splits are balanced by target_cell and other key strata (or at least report imbalance). Perform post-split audits: duplicate/near-duplicate leakage checks, label range checks, and lfcSE distribution checks.

Retrieved Knowledge Base Items (for Explainability):
------------------------------------------------------------

[1] Knowledge ID: 0332668308ce5474
    Title: Automated Statistical and Machine Learning Platform for Biological Research
    Source: arXiv
    Relevance Score: 0.2240
    Content:
    categorical encoding, and adaptive model configuration based on dataset characteristics. Initial testing protocols are designed to evaluate classification accuracy across diverse chemical datasets with varying feature distributions. This work demonstrates that integrating statistical rigor with machine learning interpretability can accelerate biological discovery workflows while maintaining methodological soundness. The platform's modular architecture enables future extensions to additional machine learning algorithms and statistical procedures relevant to bioinformatics.


[2] Knowledge ID: 22aa543cc23f2463
    Title: AIAP: A Quality Control and Integrative Analysis Package to Improve ATAC-seq Data Analysis
    Source: PMC
    Relevance Score: 0.1622
    Content:
    (BG), promoter enrichment (ProEn), subsampling enrichment (SubEn), and other measurements. We incorporated these QC tests into our recently developed ATAC-seq Integrative Analysis Package (AIAP) to provide a complete ATAC-seq analysis system, including quality assurance, improved peak calling, and downstream differential analysis . We demonstrated a significant improvement of sensitivity (20%–60%) in both peak calling and differential analysis by processing paired-end ATAC-seq datasets using AIAP. AIAP is compiled into Docker/Singularity, and it can be executed by one command line to generate a comprehensive QC report. We used ENCODE ATAC-seq data to benchmark and generate QC recommendations, and developed qATACViewer for the user-friendly interaction with the QC report. The software, source code, and documentation of AIAP are freely available at https://github.com/Zhang-lab/ATAC-seq_QC_analysis .


[3] Knowledge ID: 3f1450b2b045e22f
    Title: scMRDR: A scalable and flexible framework for unpaired single-cell multi-omics data integration
    Source: arXiv
    Relevance Score: 0.1432
    Content:
     and masked reconstruction loss strategy to address the issue of missing features across modalities. Our method achieves excellent performance on benchmark datasets in terms of batch correction, modality alignment, and biological signal preservation. Crucially, it scales effectively to large-level datasets and supports integration of more than two omics, offering a powerful and flexible solution for large-scale multi-omics data integration and downstream biological discovery.


[4] Knowledge ID: f0cf4c63ddb21d06
    Title: A CRISP approach to QSP: XAI enabling fit-for-purpose models
    Source: arXiv
    Relevance Score: 0.1428
    Content:
    A CRISP approach to QSP: XAI enabling fit-for-purpose models


[5] Knowledge ID: f0a9cb13315a21ed
    Title: RadDiff: Retrieval-Augmented Denoising Diffusion for Protein Inverse Folding
    Source: arXiv
    Relevance Score: 0.1422
    Content:
     acid profile, which serves as an evolutionary-informed prior that conditions the denoising process. A lightweight integration module is further designed to incorporate this prior effectively. Experimental results on the CATH, PDB, and TS50 datasets show that RadDiff consistently outperforms existing methods, improving sequence recovery rate by up to 19%. Experimental results also demonstrate that RadDiff generates highly foldable sequences and scales effectively with database size.


[6] Knowledge ID: 
    Source: PubMed
    Relevance Score: 0.2468
    Content:
    processing parameters hamper data integration, reproducibility, and meta-analyses across studies. To address these challenges, the Impact of Genomic Variation on Function (IGVF) Consortium established an MPRA focus group to develop community standards, including harmonized file formats, and robust analysis pipelines for a wide range of library types and experimental designs. Here, we present these formats and comprehensive computational tools, MPRAlib and MPRAsnakeflow, for uniform processing fr


[7] Knowledge ID: 
    Source: PMC
    Relevance Score: 0.1409
    Content:
    functional effects, and most of them fail to characterize the regulatory effect that MPRA detects. Using MpraNet, we predict potential MPRA functional variants across the genome and identify the distributions of MPRA effect relative to other characteristics of genetic variation, including allele frequency, alternative functional annotations specified by FAVOR, and phenome-wide associations. We also observed that the predicted MPRA positives are not uniformly distributed across the genome; instea


METHODOLOGY EXPERT (Score: 9.3/10)
--------------------------------------------------------------------------------

Design Summary:
This methodology targets 3-output regression of MPRA-like activity (l2fc) in K562, HepG2, and SKNSH using a shared sequence backbone with three task-specific heads. The training objective uses a robust regression loss (Huber) to handle heavy-tailed experimental noise typical for fold-change assays while maintaining stable gradients. Optimization is performed with AdamW and a cosine decay schedule with warmup, which is a strong default for deep sequence models and supports rapid early learning without destabilizing later convergence. Regularization combines dropout in the backbone, input base-masking (input dropout) to reduce reliance on single k-mers, and reverse-complement (RC) augmentation plus an explicit RC-consistency regularizer to enforce strand-symmetry (motivated by RCCR). Biological priors are integrated through an auxiliary motif-scoring branch (PWM scanning features) and optional motif-preserving augmentations, ensuring learned representations remain consistent with known TF-binding logic. Validation emphasizes not only mean Pearson across tasks but also specificity metrics (MinGap between top vs. non-target cell types) to ensure designs are cell-type-discriminative rather than globally strong. A structured ablation plan tests backbone depth, kernel sizes, and RC augmentation/consistency to quantify which components most impact accuracy and specificity.


Implementation Plan (Complete, No Summarization):

Design Recommendations:
------------------------------------------------------------
1) Dataset characterization & preprocessing: Assume the dataset is an MPRA-style sequence-to-activity regression dataset because the targets are per-cell-type l2fc (K562/HepG2/SKNSH), which are commonly derived from reporter assays; this implies labels are noisy with occasional outliers and can be heteroscedastic across cell types. If sequence length is fixed (common in MPRA libraries), set input_len=200 bp (adjust to actual; if variable, pad/truncate to 200 with center-crop for longer sequences and N-padding for shorter) and record length distribution; if >95% are identical length, disable variable-length masking for simplicity. Treat dataset size as medium-to-large (>10k is common for MPRA), so prioritize strict QC: remove sequences with >5% ambiguous bases (N), remove duplicated sequences keeping the median label per cell type, and winsorize extreme l2fc values at the 0.5th/99.5th percentiles per task to reduce leverage points while keeping ranking. Standardize labels per task using training-set mean/std (z-score) so multi-task losses are on comparable scale; at inference, invert standardization to report l2fc. Use a leakage-safe split: sequence-level unique split (no exact duplicates across train/val/test), optionally family-aware split by k-mer MinHash clustering to reduce near-duplicate leakage; keep val=10%, test=10%. Implement batch sampling uniformly over sequences; do not stratify by label unless label distribution is extremely skewed.

2) Model architecture (shared backbone + 3 heads): Use one-hot DNA encoding (A,C,G,T plus optional N channel) with shape [B, L, 4] (or 5), and a CNN-residual backbone for strong inductive bias on motifs and motif syntax. Backbone example (exact): Conv1D(4->256, kernel=15, stride=1, padding=same) + BatchNorm(momentum=0.9, eps=1e-5) + GELU + Dropout(p=0.10); then 6 residual blocks each: Conv1D(256->256, k=7) + BN + GELU + Dropout(0.10) + Conv1D(256->256, k=7) + BN + GELU with residual connection; after blocks, DilatedConv stack: 3 layers Conv1D(256->256, k=3, dilation=2/4/8) each with BN+GELU+Dropout(0.10) to extend receptive field without huge depth. Pooling: attention pooling or global average pooling; to keep it simple and stable, use GlobalAveragePooling over length producing 256-d vector. Heads: three independent MLP heads: Linear(256->128) + GELU + Dropout(0.20) + Linear(128->1) for each task; initialize with Xavier uniform (gain=1.0) and zero bias. Apply weight normalization (as required) to all Conv1D and Linear layers; in PyTorch, use torch.nn.utils.weight_norm at module construction and optionally remove at inference. This design matches the “shared trunk + multiple expression heads” pattern seen in sequence-to-expression models (e.g., Proformer uses multiple heads and strand embedding ideas), while retaining CNN interpretability and stable training.

3) Loss function (exact choice + weighting + masking): Use Huber loss (a robust alternative to MSE) because MPRA l2fc labels often contain outliers and heavy-tailed noise; set delta=1.0 in standardized label units. Reduction: compute per-sample per-task Huber, then reduce by mean over batch and tasks after weighting (i.e., ‘none’ then manual reduction). Per-task weights: start equal weights w=[1.0,1.0,1.0] for the first 5 epochs to avoid early collapse; then switch to inverse-variance weights estimated from training residual EMA to balance tasks (w_t = 1/(sigma_t^2 + 1e-4), normalized so sum(w)=3.0). Missing-task masking: assume all present, but implement a mask tensor m in {0,1}^3 per sample; final loss = sum_t w_t * m_t * huber(yhat_t,y_t) / (sum_t w_t*m_t + 1e-8) so training remains correct if any labels are absent in future data. Add RC-consistency regularization term (see augmentation/regularization): L_total = L_regression + lambda_rccr * mean_t |yhat_fwd_t - yhat_rc_t| with lambda_rccr=0.05 (after standardization), motivated by the RCCR idea that explicitly penalizing RC prediction divergence improves reliability on genomic tasks (Reverse-Complement Consistency for DNA Language Models, arXiv 2025). If you later detect strong heteroscedasticity per-task (e.g., HepG2 variance far higher), a Gaussian NLL head can be ablated, but Huber is the default for stability and simplicity.

4) Optimization + LR schedule (exact hyperparameters): Use AdamW (not SGD) for faster convergence on sequence models with BatchNorm/weight norm and for robustness to gradient scale; set lr=3e-4, betas=(0.9,0.98), eps=1e-8, weight_decay=0.05. Apply weight decay to all weights except biases, BatchNorm parameters, and weight-norm “g” scalars (exclude via parameter groups). Gradient clipping: clip global norm at 1.0 (gradient_clip_norm=1.0) to prevent rare batch spikes from destabilizing training (common with noisy l2fc). LR schedule: cosine decay with linear warmup; warmup_epochs=5 (or warmup_steps = 5 * steps_per_epoch), start_lr=3e-6, peak_lr=3e-4, min_lr=3e-6, then cosine down to min_lr by the final epoch. Use batch_size=256 if GPU memory permits for stable BatchNorm; if constrained, use batch_size=128 and set BN to sync-bn or switch to GroupNorm(32 groups). Use mixed precision (fp16/bf16) with dynamic loss scaling; keep gradient clip after unscale.

5) Regularization (dropout, input dropout, weight norm, mixup rationale): Backbone dropout p=0.10 in conv/residual layers; head dropout p=0.20 because heads overfit more easily than shared features. Input dropout (random base masking): with probability p_mask=0.02 per position, replace the one-hot base with all-zeros (or an explicit N channel if used); this simulates sequencing/library noise and discourages reliance on single nucleotide positions. Do not use label smoothing because this is regression and smoothing would bias continuous targets; instead, rely on Huber and winsorization/standardization for robustness. Weight norm: apply to every Conv1D and Linear (excluding BN) to stabilize training dynamics; monitor for training slowdowns and consider removing weight norm after convergence if needed. Mixup: do not apply raw-sequence mixup at one-hot level because convex combinations of bases are not biologically valid sequences; if needed as an ablation, restrict mixup to the pooled embedding vector (post-backbone) with alpha=0.2 and mix only within-batch while also mixing labels, but treat as optional because it can blur motif syntax. Add L2 on outputs is not needed; instead, use early stopping and RC regularization.

6) Data augmentation + biological prior integration (motifs/PWMs and RC): Reverse-complement augmentation: with probability 0.5, replace input with its RC during training; at evaluation, optionally average predictions of forward and RC for robustness (test-time augmentation). Add explicit RC-consistency regularization (lambda_rccr=0.05) by computing predictions on both forward and RC in the same batch and penalizing their absolute difference; this is directly inspired by RCCR which targets inconsistent forward/RC predictions in DNA models (arXiv 2025). Small random shift augmentation: if sequences represent a window around a central element, allow shift in [-3,+3] bp with circular padding disabled (pad with N) to reflect uncertainty in exact motif alignment; set shift_prob=0.5. Motif prior: compute PWM scan features (e.g., max log-odds score per TF motif from a curated motif set relevant to K562/HepG2/SKNSH, such as ETS/GATA for hematopoietic contexts, HNF4A/CEBPA for HepG2 liver-like, and neural TFs for SKNSH) using a fixed library; concatenate a low-dim motif feature vector (e.g., 64 motifs) to the pooled 256-d backbone embedding, giving 320-d to each head. Regularize motif branch by Dropout(0.30) on motif features to prevent the model from ignoring sequence and overfitting motif counts. This “feature fusion” keeps the model biologically grounded and improves interpretability; it is also compatible with multi-task learning as in multi-head designs (e.g., Proformer highlights multi-head strategies and strand handling).

7) Training length, early stopping, validation metrics (including specificity/MinGap): Train for max_epochs=80 with early stopping on a single scalar: mean Pearson correlation across the three tasks on the validation set (computed on de-standardized predictions), because Pearson aligns with ranking/relative effect accuracy often desired in regulatory activity prediction. Early stopping parameters: patience=12 epochs, min_delta=0.002 (absolute improvement in mean Pearson), and require at least min_epochs=20 before stopping to avoid premature termination during warmup/cosine transition. Track additional metrics: per-task Pearson, Spearman, RMSE, and calibration of residuals; log each epoch. Specificity validation: compute “MinGap” per sample as gap between top predicted cell-type l2fc and second-best (or between target and mean of others if target label known); then report mean MinGap and fraction of samples with correct top-cell-type (if you have a designated “intended” cell type for each sequence). Use MinGap only for monitoring/model selection tie-breaker: if mean Pearson is within 0.001 of best, select the checkpoint with higher mean MinGap to favor specificity. For robustness, also compute Pearson on RC-averaged predictions (fwd/rc mean) and ensure the drop from fwd-only is <0.01; large gaps indicate strand inconsistency issues.

8) Multi-task handling, balancing, and runnable-like configuration: Optimize weighted-sum loss (not average of independent optimizers) using the dynamic inverse-variance weighting after epoch 5; additionally cap any task weight to at most 2.0x the mean weight to prevent one task from dominating (w_t = min(w_t, 2.0 * mean(w))). If one task’s gradients dominate (detect via per-task gradient norms on the last shared layer), enable GradNorm-style balancing as an optional ablation: update weights every 50 steps to equalize relative training rates, but default to inverse-variance because it is simple and usually effective. Reproducibility: set seeds for Python/NumPy/PyTorch to 1337, enable deterministic flags (torch.backends.cudnn.deterministic=True, benchmark=False), and log git commit + dataset hash + exact config. Ablation plan: (A1) backbone depth {4,6,8 residual blocks} with fixed kernels; (A2) kernel sizes: first conv k in {11,15,21} and residual k in {5,7}; (A3) remove dilated conv stack; (A4) RC augmentation off vs on; (A5) RC augmentation on but RCCR lambda_rccr in {0.0,0.02,0.05,0.1}; (A6) motif feature fusion off vs on; evaluate all by mean Pearson and mean MinGap. Below is the exact YAML-like configuration to run:

experiment:
  name: multitask_l2fc_k562_hepg2_sknsh
  seed: 1337
  deterministic: true
  mixed_precision: bf16

data:
  task_names: [K562, HepG2, SKNSH]
  target_type: l2fc
  input_len_bp: 200
  encoding: onehot
  allow_ambiguous_N: true
  qc:
    max_N_fraction: 0.05
    deduplicate: true
    dedup_reduce: median
    winsorize:
      enabled: true
      lower_q: 0.005
      upper_q: 0.995
  label_standardize: true
  split:
    strategy: sequence_unique
    val_fraction: 0.10
    test_fraction: 0.10

augmentation:
  reverse_complement:
    enabled: true
    prob: 0.50
  shift:
    enabled: true
    prob: 0.50
    max_shift_bp: 3
    pad_with: N
  input_base_masking:
    enabled: true
    p_mask_per_bp: 0.02
    mask_token: zero

model:
  backbone:
    type: cnn_resnet_dilated
    in_channels: 4
    stem:
      conv_out_channels: 256
      kernel_size: 15
      stride: 1
      padding: same
      norm: batchnorm
      bn_momentum: 0.90
      activation: gelu
      dropout_p: 0.10
    res_blocks: 6
    res_block:
      channels: 256
      kernel_size: 7
      dropout_p: 0.10
      norm: batchnorm
      activation: gelu
    dilated_stack:
      enabled: true
      layers: 3
      kernel_size: 3
      dilations: [2, 4, 8]
      dropout_p: 0.10
    pooling: global_avg
    weight_norm: true
  motif_prior:
    enabled: true
    motif_feature_dim: 64
    scan_stat: max_log_odds
    motif_dropout_p: 0.30
    fusion: concat_to_pooled_embedding
  heads:
    shared_in_dim: 256
    fused_in_dim_if_motif: 320
    per_task_mlp:
      hidden_dim: 128
      activation: gelu
      dropout_p: 0.20
      out_dim: 1

loss:
  type: huber
  huber_delta: 1.0
  reduction: none
  multitask_reduction: weighted_mean
  task_weights:
    schedule:
      epoch_0_to_4: [1.0, 1.0, 1.0]
      epoch_5_plus: inverse_variance_ema
    inverse_variance_ema:
      ema_decay: 0.98
      eps: 1.0e-4
      normalize_sum_to: 3.0
      max_weight_ratio_to_mean: 2.0
  missing_task_masking:
    enabled: true
    mask_value: 0
  rc_consistency:
    enabled: true
    lambda: 0.05
    metric: l1

optimizer:
  type: adamw
  lr: 3.0e-4
  betas: [0.9, 0.98]
  eps: 1.0e-8
  weight_decay: 0.05
  exclude_from_weight_decay: [bias, batchnorm, weightnorm_g]
  gradient_clip_norm: 1.0

lr_schedule:
  type: cosine
  warmup_epochs: 5
  start_lr: 3.0e-6
  min_lr: 3.0e-6

training:
  batch_size: 256
  epochs: 80
  min_epochs: 20
  early_stopping:
    monitor: val_mean_pearson
    mode: max
    patience: 12
    min_delta: 0.002
  checkpoint:
    save_best_k: 3
    metric: val_mean_pearson
    tie_breaker_metric: val_mean_mingap

evaluation:
  primary_metric: mean_pearson
  secondary_metrics: [per_task_pearson, spearman, rmse, mean_mingap]
  test_time_rc_ensemble:
    enabled: true
    method: mean

ablations:
  - name: depth
    res_blocks: [4, 6, 8]
  - name: kernel_sizes
    stem_kernel: [11, 15, 21]
    res_kernel: [5, 7]
  - name: rc_aug
    reverse_complement_enabled: [false, true]
  - name: rccr_lambda
    lambda: [0.0, 0.02, 0.05, 0.10]
  - name: motif_prior
    motif_enabled: [false, true]



Recommendations:
  1. RC augmentation gatekeeping: treat RC as an experimental factor. Run an explicit ablation (no-RC vs RC) and only keep RC + RC-consistency if it improves validation metrics and does not worsen strand-specific residual structure. If assay orientation is known to be directional, disable RC entirely (or restrict to palindromic/strand-invariant subsets).
  2. Prefer SE-aware loss weighting over aggressive filtering when retention would otherwise drop too much: implement clipped inverse-variance weights w=1/(SE^2+eps), then cap (e.g., w<=25) and optionally warm-start with uniform weights for 1–3 epochs before enabling SE weights. This matches the Data Management expert’s suggestion and reduces data loss.
  3. Multi-task balancing: start with per-task target z-scoring on training split; then either (a) keep equal task weights, or (b) use a stable balancing method (e.g., learned homoscedastic task uncertainty or GradNorm) with caps on weight ratios to avoid one task dominating.
  4. Motif prior integration: make motif/PWM use auxiliary or optional. Safer default is an auxiliary head that predicts motif presence/occupancy (multi-label) from intermediate features, rather than hard feature concatenation; keep motif dropout high and always report an ablation (motif-off baseline).
  5. Optimization refinement: AdamW + cosine decay + warmup is appropriate; if validation saturates early, follow the proposed adjustment (slightly higher peak LR and lower weight decay). Keep gradient clipping (e.g., 1.0) if using any dynamic weighting.
  6. Report robustness: add winsorized Pearson sensitivity (1st/99th per task) and weighted MSE using lfcSE-derived weights to explicitly show noise sensitivity, as suggested by the Result Analyst.
  7. Tie methodology explicitly to MPRA context: cite that MPRA datasets can generalize across cellular contexts with performance loss tied to cell-type-specific features (Kreimer et al., 2019) and that deep learning has been applied successfully to MPRA-related prediction tasks (Lu et al., 2022; NAR: 10.1093/nar/gkac990).

Retrieved Knowledge Base Items (for Explainability):
------------------------------------------------------------

[1] Knowledge ID: e9547d693912e631
    Title: Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning
    Source: arXiv
    Relevance Score: 0.1544
    Content:
    Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning


[2] Knowledge ID: e9547d693912e631
    Title: Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning
    Source: arXiv
    Relevance Score: 0.1543
    Content:
    Large Language Models (LLMs) demonstrate remarkable generalizability across diverse tasks, yet genomic foundation models (GFMs) still require separate finetuning for each downstream application, creating significant overhead as model sizes grow. Moreover, existing GFMs are constrained by rigid output formats, limiting their applicability to various genomic tasks. In this work, we revisit the transformer-based auto-regressive models and introduce Omni-DNA, a family of cross-modal multi-task models ranging from 20 million to 1 billion parameters. Our approach consists of two stages: (i) pretraining on DNA sequences with next token prediction objective, and (ii) expanding the multi-modal task-specific tokens and finetuning for multiple downstream tasks simultaneously. When evaluated on the Nucleotide Transformer and GB benchmarks, Omni-DNA achieves state-of-the-art performance on 18 out of 26 tasks.


[3] Knowledge ID: 44c38b61ceadbaed
    Title: Contrastive Multi-Task Learning with Solvent-Aware Augmentation for Drug Discovery
    Source: arXiv
    Relevance Score: 0.1395
    Content:
    Contrastive Multi-Task Learning with Solvent-Aware Augmentation for Drug Discovery


[4] Knowledge ID: 9d463f3bb7a24c9d
    Title: Machine Learning-Driven Enzyme Mining: Opportunities, Challenges, and Future Perspectives
    Source: arXiv
    Relevance Score: 0.1359
    Content:
    challenges, including multi-task learning, integration of multi-modal data, and explainable AI. Together, these developments establish ML-guided enzyme mining as a scalable and predictive framework for uncovering novel biocatalysts, with broad applications in biocatalysis, biotechnology, and synthetic biology.


[5] Knowledge ID: 0bf8fe1698bbc988
    Title: DISPROTBENCH: A Disorder-Aware, Task-Rich Benchmark for Evaluating Protein Structure Prediction in Realistic Biological Contexts
    Source: arXiv
    Relevance Score: 0.1351
    Content:
    via the DisProtBench Portal, which provides precomputed 3D structures and visual error analyses. Our results reveal significant variability in model robustness under disorder, with low-confidence regions linked to functional prediction failures. Notably, global accuracy metrics often fail to predict task performance in disordered settings, emphasizing the need for function-aware evaluation. DisProtBench establishes a reproducible, extensible, and biologically grounded framework for assessing next-generation PSPMs in realistic biomedical scenarios.


[6] Knowledge ID: 
    Source: arXiv
    Relevance Score: 0.1558
    Content:
    Large Language Models (LLMs) demonstrate remarkable generalizability across diverse tasks, yet genomic foundation models (GFMs) still require separate finetuning for each downstream application, creating significant overhead as model sizes grow. Moreover, existing GFMs are constrained by rigid output formats, limiting their applicability to various genomic tasks. In this work, we revisit the transformer-based auto-regressive models and introduce Omni-DNA, a family of cross-modal multi-task model


[7] Knowledge ID: 
    Source: PMC
    Relevance Score: 0.1522
    Content:
     predict and interpret cell-type-specific functional genomics data that span DNA and RNA regulation. Our findings suggest that probing the representations of pre-trained gLMs do not offer substantial advantages over conventional machine learning approaches that use one-hot encoded sequences. This work highlights a major gap with current gLMs, raising potential issues in conventional pre-training strategies for the non-coding genome.


[8] Knowledge ID: 
    Source: PMC
    Relevance Score: 0.1596
    Content:
    and strand embedding (forward strand vs. reverse complemented strand) as the sequence input. Moreover, Proformer introduced multiple expression heads with mask filling to prevent the transformer models from collapsing when training on relatively small amount of data. We empirically determined that this design had significantly better performance than the conventional design such as using the global pooling layer as the output layer for the regression task. These analyses support the notion that 


[9] Knowledge ID: 
    Source: arXiv
    Relevance Score: 0.1534
    Content:
    bases. We explored different training techniques to address the issue of highly unbalanced data. Among the seven most popular non-linearities for feed-forward ANNs, only three: Rectified Linear Unit (ReLU), Gated Linear Unit (GLU), and Hyperbolic Tangent (Tanh) yielded converging models. Common re-balancing techniques, such as under- and over-sampling of training sets, proved ineffective, whereas increasing the volume of training data and using model ensembles significantly improved performance.


MODEL ARCHITECT (Score: 9.2/10)
--------------------------------------------------------------------------------

Design Summary:
This plan targets gene regulatory element prediction from 200 bp one-hot DNA (B,4,200) with ~77,157 samples, which supports a moderate-capacity 1D CNN (≈1–3M parameters) without being overly prone to overfitting. The model uses a shared Conv1d backbone with explicit Weight Normalization on every Conv1d layer to stabilize optimization and decouple weight magnitude/direction while keeping the architecture computationally efficient. To capture motifs at multiple lengths, the first stage performs parallel multi-scale convolutions with kernel sizes 7/11/15 (all stride 1 with same padding), concatenates branch outputs channel-wise, and then uses a sequence of residual Conv1d blocks (including a dilated block stack) to model motif interactions and longer-range dependencies across the 200 bp window. The backbone ends with a combined global average pooling and global max pooling to preserve both presence and distributional information, producing a fixed-dimensional feature vector regardless of sequence length. Three independent heads (one per task) are implemented as separate small MLPs, each mapping the shared feature vector to one scalar output, collectively yielding a 3D output vector. Interpretability is built in through gradient-based saliency (dY/dX), Integrated Gradients baselines, and in-silico mutagenesis with a fixed window size to extract motif-like sensitivity patterns, aligning with common regulatory genomics interpretation practices (e.g., filter visualization and in silico mutagenesis approaches highlighted in the retrieved literature on “un-boxing” sequence models using convolutional filters and mutagenesis).


Implementation Plan (Complete, No Summarization):

Design Recommendations:
------------------------------------------------------------
Dataset and task characterization: With N=77,157 labeled sequences of fixed length 200 bp and one-hot encoding (4 channels), this is a large dataset (>10K) suitable for a moderate CNN with residual/dilated blocks and multi-task heads, targeting ~0.5–5M parameters to balance expressiveness and generalization. The dataset type is not explicitly stated, but the setup (200 bp one-hot, 3 outputs) is consistent with MPRA-like regression, STARR-seq activity prediction, or multi-task epigenomic signal prediction; therefore, preprocessing should include strict deduplication, train/val/test splitting by genomic region (or chromosome holdout) to avoid leakage, and label normalization (z-score per task for regression, or log1p transform if count-like). Because length is fixed at 200, no padding/truncation is needed; however, quality control should remove sequences with ambiguous bases (N) or map them to uniform probabilities (recommended: drop if >1% ambiguous, else replace with all-zero and track a mask). For large datasets, favor cleaning/filtering over heavy augmentation; use light augmentation only if biologically plausible (reverse-complement augmentation with probability 0.5, and random 0–5 bp shift if the assay is shift-invariant, implemented by circular or padded shift). Model selection rationale: choose a shared WeightNorm CNN backbone because it is efficient on 200 bp inputs, naturally learns motif detectors, and supports multi-task learning; multi-scale first-layer kernels (7/11/15) act as motif scanners at different widths while downstream dilated residual blocks capture spacing/interactions. The retrieved literature emphasizes interpretability through convolutional filters and in silico mutagenesis (“un-box” models using convolutional filters, attention maps, and in silico mutagenesis; Valeri et al., 2020, PMC), motivating explicit interpretability hooks in the experimental plan.

Exact architecture (layer-by-layer, with dimensions): Input X is (B,4,200). Stage A (multi-scale motif stem, parallel branches; all Conv1d use torch.nn.utils.weight_norm): Branch A1: WN-Conv1d(4->64, k=7, s=1, p=3), LeakyReLU(negative_slope=0.1), Dropout(p=0.10). Branch A2: WN-Conv1d(4->64, k=11, s=1, p=5), LeakyReLU(0.1), Dropout(0.10). Branch A3: WN-Conv1d(4->64, k=15, s=1, p=7), LeakyReLU(0.1), Dropout(0.10). Concatenate along channels: (B,192,200). Fuse + downsample: WN-Conv1d(192->256, k=1, s=1, p=0), LeakyReLU(0.1), then MaxPool1d(kernel=2, stride=2) giving (B,256,100). Stage B (residual blocks, same-length within block): Block B1 (no dilation): Main path: WN-Conv1d(256->256, k=3, s=1, p=1), LeakyReLU(0.1), Dropout(0.15), then WN-Conv1d(256->256, k=3, s=1, p=1); Residual add (identity) then LeakyReLU(0.1) output (B,256,100). Block B2 (dilation=2): WN-Conv1d(256->256, k=3, s=1, p=2, dilation=2), LeakyReLU(0.1), Dropout(0.15), WN-Conv1d(256->256, k=3, s=1, p=2, dilation=2), residual add, LeakyReLU -> (B,256,100). Block B3 (dilation=4): WN-Conv1d(256->256, k=3, s=1, p=4, dilation=4), LeakyReLU(0.1), Dropout(0.15), WN-Conv1d(256->256, k=3, s=1, p=4, dilation=4), residual add, LeakyReLU -> (B,256,100). Stage C (channel expansion + second downsample): WN-Conv1d(256->384, k=3, s=1, p=1), LeakyReLU(0.1), then MaxPool1d(k=2,s=2) -> (B,384,50). Stage D (residual blocks at 50 positions): Block D1 (no dilation): WN-Conv1d(384->384, k=3, s=1, p=1), LeakyReLU(0.1), Dropout(0.20), WN-Conv1d(384->384, k=3, s=1, p=1), residual add, LeakyReLU -> (B,384,50). Block D2 (dilation=2): WN-Conv1d(384->384, k=3, s=1, p=2, dilation=2), LeakyReLU(0.1), Dropout(0.20), WN-Conv1d(384->384, k=3, s=1, p=2, dilation=2), residual add, LeakyReLU -> (B,384,50). Backbone output pooling: compute GlobalAvgPool1d over length (50) -> (B,384) and GlobalMaxPool1d over length (50) -> (B,384), then concatenate -> backbone feature vector h of size (B,768). This pooling choice preserves both “presence” (max) and “overall activity” (avg), and yields a fixed 768-dim representation for all tasks.

Multi-scale motif capture specifics (required): The explicit parallel conv stem uses kernels 7/11/15 to learn motif detectors of different effective lengths; because padding is (k-1)/2, each branch preserves length 200, enabling clean concatenation. After concatenation, a 1x1 convolution fuses across scales into 256 channels, letting the model learn weighted combinations of motif detectors and reduce dimensionality before deeper processing. Long-range dependencies are modeled by the dilated residual blocks (dilation 2 and 4 at length 100, then dilation 2 at length 50), increasing receptive field without excessive pooling; for example, at length 100, two stacked k=3 dilated convs with dilation=4 expand the receptive field substantially while preserving resolution. Residual connections ensure stable gradient flow for a deeper backbone and mitigate performance degradation as depth increases. If an ablation is desired, swap the parallel branches for a sequential dilated stack (dilations 1/2/4/8) while keeping parameter count similar; however, the parallel design typically improves motif-width coverage on short sequences. This design choice is consistent with common regulatory CNN motif-learning patterns and supports downstream interpretability by aligning first-layer filters with motif-like patterns (a practice commonly paired with in silico mutagenesis in interpretability workflows, as described in the retrieved “un-boxing” literature).

Heads (three independent outputs) and exact dimensions: Shared feature h is (B,768). Head1 MLP: Linear(768->256), LeakyReLU(0.1), Dropout(p=0.30), Linear(256->64), LeakyReLU(0.1), Dropout(p=0.20), Linear(64->1). Head2 is identical but with independent parameters, producing y2 (B,1). Head3 is identical, producing y3 (B,1). Final output is torch.cat([y1,y2,y3], dim=1) -> (B,3) or return a tuple of three scalars depending on training loop preference. If tasks have different scales/noise, optionally add per-head LayerNorm(768) before the first Linear to stabilize, but keep it off by default to minimize moving parts alongside WeightNorm. For classification tasks (if applicable), apply per-head Sigmoid for binary or softmax for multi-class; for regression, output is linear and use MSE/Huber. To encourage shared representation but avoid negative transfer, the heads are moderately deep yet small; dropout is higher in heads (0.30) than backbone (0.10–0.20) to regularize task-specific fitting.

Initialization, normalization, and training hyperparameters: All Conv1d and Linear layers use Kaiming/He initialization suited for LeakyReLU: torch.nn.init.kaiming_normal_(weight, a=0.1, mode='fan_out', nonlinearity='leaky_relu'); biases initialized to 0.0. WeightNorm is applied as torch.nn.utils.weight_norm(conv_layer) (and optionally on Linear layers in heads if desired, but keep Linear without WN initially for simplicity); when using WeightNorm, still initialize the underlying weight_v with Kaiming. BatchNorm is optional; if included, use BatchNorm1d with momentum=0.1 and eps=1e-5 after conv and before activation, but given explicit WeightNorm, the default recommendation is to omit BN to reduce interaction effects and keep inference simple. Optimizer: AdamW with lr=1e-3, betas=(0.9,0.999), weight_decay=1e-4; gradient clipping at 1.0 to stabilize multi-task training. Training schedule: cosine decay with 5% warmup steps (e.g., warmup for first 1–2 epochs), total 30–50 epochs with early stopping patience=6 on mean validation metric across tasks. Batch size suggestion: 256 if GPU memory allows (input is small), else 128; mixed precision (AMP) recommended for speed. Loss: if regression, per-task Huber(delta=1.0) or MSE; combine as weighted sum with weights inversely proportional to task variance (or use uncertainty weighting), and track Pearson/Spearman per task.

Parameter count estimate, target rationale, and compute efficiency: The design targets ~1.8–2.2M parameters (within the requested 0.5–5M). Rough breakdown: multi-scale stem convs (4->64 with k=7/11/15) contribute ~1.8k+2.8k+3.9k weights; fuse 1x1 (192->256) adds ~49k; residual stacks dominate (multiple 256->256 k=3 and 384->384 k=3 convs) contributing on the order of ~1.6–1.9M total; heads add ~0.66M (each head ~0.22M for 768->256->64->1). This parameter budget is appropriate for 77k samples because it provides sufficient capacity to learn motif combinations and task-shared features while remaining small enough to train quickly and limit overfitting risk compared with very large transformers. Computationally, all operations are 1D convolutions over max length 200 (then 100/50), making FLOPs modest; pooling reduces length early, and dilation increases receptive field without increasing length or parameters. WeightNorm adds negligible overhead and often improves optimization stability for CNNs, particularly when BN is omitted. For deployment, the model is a single backbone pass plus three small MLP heads; latency is low, and batch inference is efficient.

Interpretability hooks and exact procedures (saliency, IG, in-silico mutagenesis, motif extraction): For gradient saliency, enable requires_grad on input one-hot tensor X, run forward to get y_task (choose one head or a weighted sum), then compute grads = torch.autograd.grad(y_task.sum(), X)[0], producing (B,4,200) attributions; aggregate to per-position importance via abs(grads).sum(dim=1) or grads*X (gradient×input) summed over channels. For Integrated Gradients, use baseline X0 = all-zeros (or GC-matched random baseline), choose steps m=64, compute IG = (X-X0) * (1/m) * sum_{i=1..m} grad(f(X0 + i/m*(X-X0))) across steps; return (B,4,200) attribution maps per task. For in-silico mutagenesis, for each position p, mutate within a window size W=11 (centered at p; clamp at edges) by flipping the one-hot base to each alternative base (3 alternatives per position), recompute output delta for each mutation, and store max |delta|; this yields a (200,) sensitivity profile and a 4x200 mutation effect map. Motif extraction: take top-K (e.g., K=5000) highest-saliency windows of length 15 from correctly predicted/high-confidence sequences, align them by peak position, and compute PWM by averaging one-hot in aligned windows; optionally also inspect first-layer convolutional filters by converting weight tensors to sequence logos. Provide hooks in code by exposing (a) the stem branch conv outputs (before pooling) and (b) the input gradients; register_forward_hook on stem convs and register_full_backward_hook for gradient capture. This aligns with the retrieved interpretability emphasis on “un-boxing” models via convolutional filters and in silico mutagenesis (Valeri et al., 2020, PMC) and complements regulatory genomics practice where motif-like patterns are derived from first-layer filters plus attribution maps.

Required architecture table and PyTorch skeleton (unambiguous, minimal but precise): The architecture table should list each layer with input/output shapes: (1) Input (B,4,200). (2) Branch7: WN-Conv1d(4,64,7,1,3)->(B,64,200), LReLU(0.1), Dropout0.10. (3) Branch11: WN-Conv1d(4,64,11,1,5)->(B,64,200), LReLU, Dropout0.10. (4) Branch15: WN-Conv1d(4,64,15,1,7)->(B,64,200), LReLU, Dropout0.10. (5) Concat ->(B,192,200). (6) Fuse: WN-Conv1d(192,256,1,1,0)->(B,256,200), LReLU. (7) MaxPool1d(2,2)->(B,256,100). (8) ResBlock256 d=1 ->(B,256,100). (9) ResBlock256 d=2 ->(B,256,100). (10) ResBlock256 d=4 ->(B,256,100). (11) WN-Conv1d(256,384,3,1,1)->(B,384,100), LReLU. (12) MaxPool1d(2,2)->(B,384,50). (13) ResBlock384 d=1 ->(B,384,50). (14) ResBlock384 d=2 ->(B,384,50). (15) GAP->(B,384), GMP->(B,384), Concat->(B,768). (16) Head t in {1,2,3}: Linear 768->256, LReLU, Dropout0.30; Linear 256->64, LReLU, Dropout0.20; Linear 64->1. PyTorch skeleton: define class SharedBackbone(nn.Module) with stem convs wrapped by weight_norm, forward returns h (B,768) and optionally intermediates; define class MultiTaskModel(nn.Module) with backbone + three nn.Sequential heads; forward returns torch.cat outputs. Ensure Conv1d layers are created then wrapped: conv = nn.Conv1d(...); conv = torch.nn.utils.weight_norm(conv). Residual block skeleton: two WN-Conv1d layers, dropout between, add identity, final activation; keep channels constant to avoid projection shortcuts in this baseline (add 1x1 projection only if channels change).


Recommendations:
  1. Adopt uncertainty-aware training using provided lfcSE: either (a) SE-weighted Huber/MSE with capped weights w=1/(SE^2+eps), w_max≈25, or (b) learned per-task log-variance weighting; start with a warm-up phase using equal weights to avoid early instability.
  2. Treat RC augmentation as conditional: enable only after confirming strand invariance for this MPRA setup. If enabled, also add RC-consistency regularization (RCCR) to reduce fwd/RC prediction divergence, consistent with published RCCR objectives (rag_search source: “Reverse-Complement Consistency for DNA Language Models”, arXiv 2025).
  3. Add a lightweight global-context module after the convolutional backbone: e.g., 2-layer efficient attention or a short transformer/“macaron” style block to stabilize regression heads and improve long-range aggregation; this aligns with reports that specialized heads/sequence handling can improve expression-value prediction (rag_search source: Proformer, 2024). Keep this module small (e.g., d_model 128, 2 heads) to stay within the target parameter budget.
  4. Implement and log strict leakage control: group split by exact sequence (and optionally near-duplicates if detected) as the primary requirement; if genomic coordinates exist, prefer region/chromosome holdouts. Persist split manifests for reproducibility.
  5. Run ablations that isolate architectural contributions at matched parameter counts: (i) multi-kernel stem vs single-kernel, (ii) dilations on/off, (iii) WN-only vs WN+BN/LayerNorm, (iv) RC off/on (only if valid).
  6. Regularization tuning for 77k samples: backbone dropout 0.10–0.20, head dropout 0.25–0.35, AdamW weight_decay 5e-5–2e-4 with early stopping; prefer Huber loss for robustness to heavy-tailed MPRA noise (consistent with methodology expert).
  7. Interpretability robustness: compute IG with multiple GC-matched baselines and average; report signed and absolute ISM deltas; and, if RC is enabled, also check attribution symmetry between fwd and RC inputs.

Retrieved Knowledge Base Items (for Explainability):
------------------------------------------------------------

[1] Knowledge ID: bdf76dce39127527
    Title: Flexibility-Conditioned Protein Structure Design with Flow Matching
    Source: arXiv
    Relevance Score: 0.1422
    Content:
    the inverse problem, that is, generating backbones that display a target flexibility profile. In our experiments, we show that FliPS is able to generate novel and diverse protein backbones with the desired flexibility, verified by Molecular Dynamics (MD) simulations. FliPS and BackFlip are available at https://github.com/graeter-group/flips .


[2] Knowledge ID: 5ebd6af4064f2191
    Title: SimpleFold: Folding Proteins is Simpler than You Think
    Source: arXiv
    Relevance Score: 0.1334
    Content:
    layers and is trained via a generative flow-matching objective with an additional structural term. We scale SimpleFold to 3B parameters and train it on approximately 9M distilled protein structures together with experimental PDB data. On standard folding benchmarks, SimpleFold-3B achieves competitive performance compared to state-of-the-art baselines, in addition SimpleFold demonstrates strong performance in ensemble prediction which is typically difficult for models trained via deterministic reconstruction objectives. Due to its general-purpose architecture, SimpleFold shows efficiency in deployment and inference on consumer-level hardware. SimpleFold challenges the reliance on complex domain-specific architectures designs in protein folding, opening up an alternative design space for future progress.


[3] Knowledge ID: bdf76dce39127527
    Title: Flexibility-Conditioned Protein Structure Design with Flow Matching
    Source: arXiv
    Relevance Score: 0.1327
    Content:
    Flexibility-Conditioned Protein Structure Design with Flow Matching


[4] Knowledge ID: fee3f05afbfea727
    Title: HelixVS: Deep Learning-Enhanced Structure-Based Platform for Screening and Design
    Source: arXiv
    Relevance Score: 0.1321
    Content:
    HelixVS: Deep Learning-Enhanced Structure-Based Platform for Screening and Design


[5] Knowledge ID: b73c45ad2c06395d
    Title: Torsion-Space Diffusion for Protein Backbone Generation with Geometric Refinement
    Source: arXiv
    Relevance Score: 0.1316
    Content:
    Torsion-Space Diffusion for Protein Backbone Generation with Geometric Refinement


[6] Knowledge ID: 
    Source: PMC
    Relevance Score: 0.1669
    Content:
    we ‘un-box’ our models using convolutional filters, attention maps, and in silico mutagenesis. Through transfer-learning, we redesign sub-optimal toehold sensors, even with sparse training data, experimentally validating their improved performance. This work provides sequence-to-function deep learning frameworks for toehold selection and design, augmenting our ability to construct potent biological circuit components and precision diagnostics.


[7] Knowledge ID: 
    Source: arXiv
    Relevance Score: 0.1642
    Content:
    Network for Accurate DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings DNA-binding proteins (DBPs) are integral to gene regulation and cellular processes, making their accurate identification essential for understanding biological functions and disease mechanisms. Experimental methods for DBP identification are time-consuming and costly, driving the need for efficient computational prediction techniques. In this study, we propose a novel deep learning framework, ResCap-DBP, that


RESULT ANALYST (Score: 9.2/10)
--------------------------------------------------------------------------------

Design Summary:
This evaluation scheme treats the task as multi-task regression across cell lines (tasks), with Pearson correlation per cell line on a held-out test set as the primary endpoint, complemented by Spearman correlation, MSE/MAE, and a macro-averaged (mean) Pearson across tasks for a single headline number. Targets are evaluated in the original biological scale whenever training used standardization; Pearson is computed on de-standardized predictions/targets to preserve interpretable effect sizes and avoid scale artifacts, while also optionally reporting the “raw standardized-space” Pearson as a diagnostic. Uncertainty is quantified per task via nonparametric bootstrap with 1,000 resamples at the sequence level to produce 95% confidence intervals for Pearson, Spearman, MSE, and MAE. Cell-type specificity is validated using the provided MinGap column by (i) correlating predicted cell-type contrasts with MinGap (a label-derived specificity proxy) and (ii) stratifying elements by MinGap quantiles and computing within-stratum performance, explicitly testing whether higher MinGap corresponds to stronger, more predictable specificity. Model selection is driven by mean validation Pearson across cell lines with early stopping, while comparisons among ablations use paired bootstrap tests and/or Fisher z-transform tests on Pearson differences with Holm–Bonferroni correction at alpha=0.05. Diagnostics include calibration plots for regression, residual distribution/heteroscedasticity checks, per-cell-type scatter plots, and error versus lfcSE bins to assess uncertainty-aware failure modes, aligning with common evaluation practices in regulatory element prediction studies where correlation and stratified analyses are emphasized (e.g., comparative assessments across assay types and cell contexts in Nowling et al., 2023, PMC; and general CRM method assessment principles in Su et al., 2010, PMC).


Implementation Plan (Complete, No Summarization):

Design Recommendations:
------------------------------------------------------------
1) Dataset characteristics and evaluation unit definition: Treat each sequence/regulatory element as the sampling unit, with a vector of continuous targets across K cell lines (multi-output regression), and ensure that all metrics and bootstrap resamples operate at the element level to preserve within-element cross-cell correlation structure. Before any metric computation, summarize target distributions per cell line (mean, SD, skew, % near-zero), and summarize lfcSE (if present) per cell line (median, IQR) to anticipate heteroscedastic errors; use these summaries to justify stratified diagnostics (error vs lfcSE bins). If the dataset includes replicated measurements per element, aggregate replicates into a single target per cell line using inverse-variance weighting w=1/(lfcSE^2) when lfcSE is available; otherwise use the arithmetic mean, and carry forward an aggregated uncertainty estimate (e.g., sqrt(1/sum(w))). Define the held-out test set at the element level with no overlap in sequence identity between splits; if sequences are highly similar (e.g., tiled variants), split by group/cluster to avoid leakage, consistent with evaluation cautions in CRM assessment literature (Su et al., 2010, PMC). If there are distinct assay types or libraries, treat them as strata and enforce stratified splitting to maintain comparable distributions across train/val/test, reflecting that prediction accuracy varies systematically by technique (Nowling et al., 2023, PMC). 2) Primary metric suite and exact computation specs: Primary endpoint is Pearson correlation r computed separately for each cell line c on the held-out test set of N elements: r_c = cov(y_c, yhat_c) / (sd(y_c)*sd(yhat_c)), where y_c and yhat_c are length-N vectors of de-standardized true and predicted values for that cell line. De-standardization rule (if training used z-scoring per cell line): yhat_c,orig = yhat_c,std * sigma_c,train + mu_c,train and y_c,orig = y_c,std * sigma_c,train + mu_c,train, using mu/sigma computed on training targets only to avoid leakage; Pearson is computed on orig scale, and ties to reporting in many sequence-to-function regression tasks where correlation is favored for scale-invariant performance (Nowling et al., 2023, PMC). Also report Spearman rho per cell line (rank correlation, computed on orig scale, with average ranks for ties), plus MSE_c = mean((y_c,orig - yhat_c,orig)^2) and MAE_c = mean(|y_c,orig - yhat_c,orig|). Report macro-averaged mean Pearson across tasks: r_mean = (1/K) * sum_c r_c; additionally report a sample-size-weighted mean Pearson r_wt = sum_c (N_c * r_c)/sum_c N_c if missingness differs by cell line. For completeness, report a “micro-Pearson” computed by concatenating all cell line targets/predictions into a single vector (after de-standardization) only as a secondary summary, because it can be dominated by high-variance cell lines; macro-averaging is the default selection criterion. Thresholds for contextual interpretation: r_c > 0.60 is labeled “good”, 0.40–0.60 “moderate”, 0.20–0.40 “weak”, and <0.20 “poor”, with the caveat that achievable r depends on assay noise and technique (Nowling et al., 2023, PMC). 3) Bootstrap confidence intervals (per task) with parameterization: For each cell line c, compute 95% CIs for Pearson, Spearman, MSE, and MAE using nonparametric bootstrap with B=1000 resamples of the test elements. Resampling procedure: sample N indices with replacement from {1..N}; for each b, compute metric m_c^(b) on the resampled pairs (y_c,orig[i_b], yhat_c,orig[i_b]); then CI is the percentile interval [q_0.025, q_0.975] of {m_c^(b)}. Use the same bootstrap indices across models when comparing ablations (paired bootstrap) to reduce variance and enforce pairing, and use a fixed random seed (e.g., seed=2025) for reproducibility in reporting. For Pearson specifically, if bootstrap distribution is highly skewed or near ±1, optionally compute CI on Fisher z space: z = atanh(r), bootstrap z, then transform back with tanh; report which method is used, defaulting to percentile on r for simplicity and Fisher-z for edge cases. Also compute CI for r_mean via bootstrap by computing r_c^(b) for all c on the same resample and then averaging to r_mean^(b); CI is percentiles of {r_mean^(b)}. Report bootstrap failure rates (e.g., sd(yhat)=0 in a resample) and handle by skipping those draws; if >1% draws fail for a task, flag it as an instability diagnostic. 4) MinGap-based specificity validation (exact use of MinGap column): Interpret MinGap_i as a label-derived specificity proxy per element i (larger MinGap indicates stronger separation between a “preferred” and “non-preferred” cell context, or generally higher specificity). First define predicted specificity per element using predicted cell-type contrasts: for element i with predictions across K cell lines, let pred_gap_i = max_c(yhat_i,c,orig) - secondmax_c(yhat_i,c,orig) (top-1 minus top-2 predicted activity); optionally also compute pred_range_i = max_c(yhat_i,c,orig) - min_c(yhat_i,c,orig) as a broader contrast. Correlation analysis: compute Spearman correlation between MinGap and pred_gap across test elements: rho_gap = Spearman(MinGap, pred_gap); bootstrap CI with B=1000 resamples of elements, reporting 95% CI and p-value via bootstrap percentile or permutation (10,000 permutations recommended if feasible) for robustness. Stratified performance: split test elements into Q=5 quantiles (quintiles) by MinGap (Q1 lowest specificity to Q5 highest) using cutpoints on the test set only; within each quantile q, compute per-cell-line Pearson r_c,q and macro-mean r_mean,q, then test monotonic trend across quantiles using Spearman correlation between quantile index (1..5) and r_mean,q (or use Jonckheere–Terpstra if implementing a formal ordered alternative). Additionally, compute “specificity classification consistency” by checking whether argmax_c(yhat_i,c,orig) matches argmax_c(y_i,c,orig) (winner-takes-all cell line) and report accuracy per MinGap quantile; expectation is higher accuracy in higher MinGap bins if MinGap encodes clearer cell preference. Report effect sizes: delta_r = r_mean,Q5 - r_mean,Q1 and its bootstrap CI to quantify how much specificity strength changes predictability. This aligns with broader regulatory element evaluation themes that emphasize cell-type-specific performance stratification (e.g., motif-based prediction varies across cell contexts in Cornejo-Páramo et al., 2025, PMC). 5) Model selection and early stopping (validation metric and parameters): Select the checkpoint that maximizes mean validation Pearson across cell lines (macro-average) computed on de-standardized values, denoted r_mean,val, evaluated once per epoch. Early stopping rule: patience=10 epochs, min_delta=0.002 in r_mean,val (i.e., require at least +0.002 improvement to reset patience), with a maximum of 200 epochs; store the best checkpoint by r_mean,val and break training after patience is exhausted. If validation set is small or noisy, smooth r_mean,val with an exponential moving average (EMA) with decay=0.6 for early-stopping decisions while still reporting raw values; explicitly state which is used for stopping versus reporting. Tie-breakers: if two checkpoints have r_mean,val within 0.001, choose the one with lower val MSE (macro-averaged) to prefer better absolute calibration. Guard against overfitting to a single cell line by also requiring that at least 70% of cell lines have non-decreasing Pearson relative to the previous best checkpoint (a stability constraint); if violated, flag and optionally use a multi-objective rank (maximize r_mean,val and minimize variance of r_c,val across cell lines). This selection design is consistent with correlation-centric reporting in regulatory element prediction comparisons, where per-context performance can differ materially (Nowling et al., 2023, PMC). 6) Statistical comparisons for ablations (paired tests, Fisher z, multiple testing): For each ablation A vs baseline B, compute paired differences in Pearson per cell line on the same test set: d_c = r_c(B) - r_c(A), and report d_c with 95% paired bootstrap CI using the shared bootstrap indices across models (B=1000). For a global comparison across cell lines, compute d_mean = (1/K) * sum_c d_c and its bootstrap CI; declare significance if the Holm–Bonferroni-adjusted p-value < 0.05. P-values from paired bootstrap: p = 2*min(P(d_mean^(b) <= 0), P(d_mean^(b) >= 0)) using the bootstrap distribution of d_mean^(b) (centered at 0), with the same approach optionally applied per cell line. As an alternative or confirmation for Pearson differences per cell line, apply Fisher z-transform: z = atanh(r), standard error approx se = 1/sqrt(N-3); test statistic t = (z_B - z_A)/sqrt(2*se^2) if correlations are treated as independent; however, because predictions are paired on the same y, prefer paired bootstrap as the primary test and use Fisher-z only as a sensitivity analysis. Correct for multiple ablations and/or multiple cell lines using Holm–Bonferroni (family defined as all hypothesis tests reported in the main table; alpha=0.05). Report both adjusted and unadjusted p-values, and emphasize effect sizes (delta Pearson, delta MSE) over p-values to avoid overinterpretation, consistent with best practice in method comparison studies (Su et al., 2010, PMC). 7) Diagnostics and plots (required panels and parameter settings): Per-cell-type scatter plots: for each cell line, plot y_orig vs yhat_orig on test with an overlaid y=x line; annotate r, rho, MAE, and n, and set axis limits to [p1, p99] percentiles to reduce outlier domination (keep outliers plotted with transparency alpha=0.2). Calibration for regression: bin predictions into M=10 equal-frequency bins per cell line, compute mean(yhat) and mean(y) per bin, plot mean(y) vs mean(yhat) with error bars (±1 SE via bootstrap within-bin) and report calibration slope/intercept from a linear fit y = a + b*yhat; “good” calibration is b in [0.9, 1.1] and |a| < 0.1*sd(y). Residual analysis: compute residual e_i,c = y_i,c,orig - yhat_i,c,orig; plot residual histograms (50 bins), Q–Q plots, and residual vs prediction to inspect heteroscedasticity; flag if Breusch–Pagan p<0.01 (optional) or if residual variance increases >2x from lowest to highest prediction decile. Error vs lfcSE bins: within each cell line, bin elements by lfcSE into 5 quantiles and compute MAE and Pearson per bin; expectation is worse error at higher lfcSE, and deviations (e.g., flat or inverted trend) can indicate model mis-specification or label noise handling issues. Also include “influence/outlier” diagnostics: report the top 1% absolute residual elements and whether they cluster by sequence family or extreme MinGap, to guide biological follow-up. These plots support interpretation of why correlations differ across contexts and are commonly used in sequence-to-function model evaluation beyond single summary metrics (Nowling et al., 2023, PMC). 8) Reporting template (tables, figures, and acceptance thresholds): Main Table 1 (Test performance per cell line): columns = cell_line, N_test, Pearson r (95% CI), Spearman rho (95% CI), MSE (95% CI), MAE (95% CI); add a final row with macro-mean across cell lines with bootstrap CI. Main Table 2 (Ablation deltas): for each ablation, report delta r_mean (95% CI), adjusted p-value (Holm), and per-cell-line delta r_c (optionally in supplement). Table 3 (MinGap specificity): report rho(MinGap, pred_gap) with CI, r_mean per MinGap quintile with CI, and delta_r (Q5-Q1) with CI; include winner-cell accuracy per quintile if applicable. Figure set: Fig 1 per-cell-type scatter grid (K panels), Fig 2 calibration curves (K panels or representative), Fig 3 residual diagnostics (aggregate + per cell line), Fig 4 MinGap stratification curves and pred_gap vs MinGap scatter with smooth (LOESS span=0.75). Predefine “good performance” as: (i) r_mean,test >= 0.60 with lower CI bound >= 0.50, (ii) at least 70% of cell lines have r_c >= 0.50, and (iii) positive specificity association rho_gap >= 0.30 with CI excluding 0; these are contextual thresholds and should be interpreted relative to assay noise and baseline models. Provide a concise model card appendix listing data split IDs, target scaling parameters (mu/sigma), bootstrap seed, and exact metric definitions to ensure reproducibility, reflecting reproducibility emphasis in method assessments (Su et al., 2010, PMC).


Recommendations:
  1. Resolve MinGap formally and lock the evaluation definition to the dataset’s exact rule. If MinGap is pair-specific, compute pred_gap with the same pair mapping; if it is best-vs-rest, compute predicted margin accordingly, then rerun both gap-correlation and stratified analyses.
  2. Add a robustness layer to correlation reporting: (i) winsorized Pearson (1st/99th per task) and (ii) median absolute error (MdAE). Flag instability if |ΔPearson| > 0.05 after winsorization.
  3. Adopt group-aware uncertainty estimation: if sequences are deduplicated into groups (exact matches or near-duplicate clusters), bootstrap over groups (cluster bootstrap) rather than rows to avoid overly narrow CIs.
  4. Include lfcSE-aware evaluation: report weighted MSE (weights 1/(SE^2+eps)) and SE-stratified metrics (e.g., quartiles of SE) to demonstrate performance on high-confidence vs noisy measurements; this complements MPRA best-practice emphasis on technical variability/outliers [Keukeleire et al. 2025: 7e82169680e9e6d2; Rosen et al. 2025 MPRAsnakeflow: 5aa56ceacd6300f1].
  5. Statistical testing: for model/ablation comparisons, use paired bootstrap or paired permutation tests on per-sequence residuals or per-sequence Fisher-z-transformed correlations, and correct across the pre-registered family using Holm–Bonferroni (keep α=0.05).
  6. Strengthen validation strategy: maintain leakage-safe group splits by sequence; additionally, if genomic coordinates exist, consider chromosome/region holdout or LOCO-style evaluation to measure generalization under distribution shift [LOCO-EPI arXiv 2025: dcde68d8f42cbee4].
  7. Add an external validation plan if feasible: evaluate on an independent MPRA dataset or a held-out library/batch if present; MPRA cross-context transfer can degrade due to cell-type-specific features and should be quantified [Kreimer et al. 2019: e92ba9a50867a854].
  8. Report format: provide a single headline metric (macro-mean Pearson across 3 tasks) with 95% CI, plus a table per task (Pearson/Spearman/MAE/MSE, weighted MSE, winsorized Pearson), and a MinGap section (gap-correlation, specificity-stratified metrics, and calibration-by-SE plots).

Retrieved Knowledge Base Items (for Explainability):
------------------------------------------------------------

[1] Knowledge ID: f0b5d4d92329c117
    Title: Combining Machine Learning and Multiplexed, In Situ Profiling to Engineer Cell Type and Behavioral Specificity
    Source: PMC
    Relevance Score: 0.1609
    Content:
    Combining Machine Learning and Multiplexed, In Situ Profiling to Engineer Cell Type and Behavioral Specificity


[2] Knowledge ID: 8394ae4736af071b
    Title: scKAN: interpretable single-cell analysis for cell-type-specific gene discovery and drug repurposing via Kolmogorov-Arnold networks.
    Source: PubMed
    Relevance Score: 0.1554
    Content:
    scKAN: interpretable single-cell analysis for cell-type-specific gene discovery and drug repurposing via Kolmogorov-Arnold networks.


[3] Knowledge ID: 02c356a7ca81b071
    Title: ShortCake: An integrated platform for efficient and reproducible single-cell analysis
    Source: arXiv
    Relevance Score: 0.1533
    Content:
    ShortCake: An integrated platform for efficient and reproducible single-cell analysis


[4] Knowledge ID: ab40d7e7902b41c7
    Title: Capturing cell-type-specific activities of cis-regulatory elements from peak-based single-cell ATAC-seq.
    Source: PubMed
    Relevance Score: 0.1526
    Content:
    to identify differential usage of candidate regulatory elements (CREs) across cell types. Our research advocates for moving away from traditional peak-based quantification in scATAC-seq toward a more robust framework that relies on a standardized reference of annotated CREs, enhancing both the accuracy and reproducibility of genomic studies.


[5] Knowledge ID: 403e2dba4ab3fa0e
    Title: scDIFF: automatic cell type annotation using scATAC-seq data by incorporating bulk-level genomic and epigenomic information in a deep diffusive transformer
    Source: PMC
    Relevance Score: 0.1521
    Content:
    scDIFF: automatic cell type annotation using scATAC-seq data by incorporating bulk-level genomic and epigenomic information in a deep diffusive transformer


[6] Knowledge ID: 
    Source: PMC
    Relevance Score: 0.1895
    Content:
    Motif-based models accurately predict cell type-specific distal regulatory elements


[7] Knowledge ID: 
    Source: arXiv
    Relevance Score: 0.1780
    Content:
    Learning to Discover Regulatory Elements for Gene Expression Prediction


================================================================================
PRIORITY RECOMMENDATIONS
================================================================================

1. Confirm assay strand/orientation invariance before using RC augmentation or RC-consistency loss. If the MPRA library is orientation-fixed (common), disable RC augmentation and instead rely on standard regularization; if orientation-randomized or bidirectional by design, enable RC augmentation at p=0.5 and consider test-time RC ensembling.
2. Prefer uncertainty-aware learning over hard drops when lfcSE is available: implement inverse-variance sample weights w=1/(lfcSE^2+ε) with caps (e.g., w_max=25) and/or robust loss (Huber). Use filtering only for extreme outliers (e.g., lfcSE > 0.8–1.0) after inspecting retention rates. This aligns with MPRA QC emphasis on standardized metrics and systematic monitoring (as in esMPRA) rather than ad hoc removal.
3. Run the proposed QC ablations and log outcomes: (A) strict drop if any-task lfcSE>t vs (B) drop only for the labeled target_cell’s lfcSE>t vs (C) no-drop + weighting. Choose the policy that improves held-out performance without collapsing label range or removing >30% of rows.
4. RC augmentation gatekeeping: treat RC as an experimental factor. Run an explicit ablation (no-RC vs RC) and only keep RC + RC-consistency if it improves validation metrics and does not worsen strand-specific residual structure. If assay orientation is known to be directional, disable RC entirely (or restrict to palindromic/strand-invariant subsets).
5. Prefer SE-aware loss weighting over aggressive filtering when retention would otherwise drop too much: implement clipped inverse-variance weights w=1/(SE^2+eps), then cap (e.g., w<=25) and optionally warm-start with uniform weights for 1–3 epochs before enabling SE weights. This matches the Data Management expert’s suggestion and reduces data loss.
6. Multi-task balancing: start with per-task target z-scoring on training split; then either (a) keep equal task weights, or (b) use a stable balancing method (e.g., learned homoscedastic task uncertainty or GradNorm) with caps on weight ratios to avoid one task dominating.
7. Adopt uncertainty-aware training using provided lfcSE: either (a) SE-weighted Huber/MSE with capped weights w=1/(SE^2+eps), w_max≈25, or (b) learned per-task log-variance weighting; start with a warm-up phase using equal weights to avoid early instability.
8. Treat RC augmentation as conditional: enable only after confirming strand invariance for this MPRA setup. If enabled, also add RC-consistency regularization (RCCR) to reduce fwd/RC prediction divergence, consistent with published RCCR objectives (rag_search source: “Reverse-Complement Consistency for DNA Language Models”, arXiv 2025).
9. Add a lightweight global-context module after the convolutional backbone: e.g., 2-layer efficient attention or a short transformer/“macaron” style block to stabilize regression heads and improve long-range aggregation; this aligns with reports that specialized heads/sequence handling can improve expression-value prediction (rag_search source: Proformer, 2024). Keep this module small (e.g., d_model 128, 2 heads) to stay within the target parameter budget.
10. Resolve MinGap formally and lock the evaluation definition to the dataset’s exact rule. If MinGap is pair-specific, compute pred_gap with the same pair mapping; if it is best-vs-rest, compute predicted margin accordingly, then rerun both gap-correlation and stratified analyses.
