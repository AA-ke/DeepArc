"""
RAG/benchmark.py
RAGç³»ç»Ÿè¯„ä¼°è„šæœ¬ - è®¡ç®—ç­”æ¡ˆè´¨é‡å’Œæ£€ç´¢è´¨é‡æŒ‡æ ‡
"""

import json
import asyncio
import sys
from pathlib import Path
from typing import List, Dict, Any, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime
import re
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import SystemMessage, HumanMessage
from config.settings import get_settings
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# å¯¼å…¥RAGæ¨¡å—ï¼ˆä½¿ç”¨ç›¸å¯¹å¯¼å…¥ï¼‰
try:
    from RAG.rag import HybridRAGSystem, RetrievalResult
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç›´æ¥å¯¼å…¥
    import importlib.util
    rag_path = script_dir / "rag.py"
    spec = importlib.util.spec_from_file_location("rag", rag_path)
    rag_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(rag_module)
    HybridRAGSystem = rag_module.HybridRAGSystem
    RetrievalResult = rag_module.RetrievalResult


# ==================== æ•°æ®æ¨¡å‹ ====================

@dataclass
class EvaluationResult:
    """å•ä¸ªQA pairçš„è¯„ä¼°ç»“æœ"""
    question: str
    ground_truth_answer: str
    no_rag_answer: str = ""
    rag_answer: str = ""
    retrieved_documents: List[RetrievalResult] = field(default_factory=list)
    # ç­”æ¡ˆæ­£ç¡®æ€§æŒ‡æ ‡ï¼ˆå¤šç§å®šé‡æŒ‡æ ‡ï¼‰
    # è¯­ä¹‰ç›¸ä¼¼åº¦
    rag_answer_correctness_semantic: float = 0.0  # åŸºäºembeddingçš„è¯­ä¹‰ç›¸ä¼¼åº¦
    no_rag_answer_correctness_semantic: float = 0.0
    # BLEUåˆ†æ•°
    rag_answer_correctness_bleu: float = 0.0  # BLEU-4åˆ†æ•°
    no_rag_answer_correctness_bleu: float = 0.0
    # ROUGEåˆ†æ•°
    rag_answer_correctness_rouge_l: float = 0.0  # ROUGE-Låˆ†æ•°
    no_rag_answer_correctness_rouge_l: float = 0.0
    # ç¼–è¾‘è·ç¦»
    rag_answer_correctness_edit_distance: float = 0.0  # å½’ä¸€åŒ–çš„ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
    no_rag_answer_correctness_edit_distance: float = 0.0
    # Jaccardç›¸ä¼¼åº¦ï¼ˆè¯æ±‡é‡å ï¼‰
    rag_answer_correctness_jaccard: float = 0.0  # Jaccardç›¸ä¼¼åº¦
    no_rag_answer_correctness_jaccard: float = 0.0
    # å­—ç¬¦çº§åˆ«ç›¸ä¼¼åº¦
    rag_answer_correctness_char_sim: float = 0.0  # å­—ç¬¦çº§åˆ«ç›¸ä¼¼åº¦
    no_rag_answer_correctness_char_sim: float = 0.0
    # å…¶ä»–æŒ‡æ ‡
    rag_faithfulness: float = 0.0  # ç­”æ¡ˆå¯¹æ£€ç´¢æ–‡æ¡£çš„å¿ å®åº¦
    # æ£€ç´¢è´¨é‡æŒ‡æ ‡
    retrieval_precision: float = 0.0  # æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­åŒ…å«ground truthæ–‡æ¡£çš„æ¯”ä¾‹
    retrieval_recall: float = 0.0  # æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­ground truthæ–‡æ¡£çš„æ’å
    evaluation_time_ms: float = 0.0


@dataclass
class BenchmarkStats:
    """æ•´ä½“è¯„ä¼°ç»Ÿè®¡"""
    total_questions: int = 0
    avg_retrieval_time_ms: float = 0.0
    avg_evaluation_time_ms: float = 0.0
    results: List[EvaluationResult] = field(default_factory=list)


# ==================== é…ç½® ====================

QA_PAIRS_FILE = Path(__file__).parent.parent / "Knowledge_Corpus" / "data" / "qa_pairs.json"
SOURCE_DOCUMENTS_FILE = Path(__file__).parent.parent / "Knowledge_Corpus" / "data" / "cleaned" / "documents_deduped.json"
OUTPUT_FILE = Path(__file__).parent / "benchmark_results.json"
PROGRESS_FILE = Path(__file__).parent / "benchmark_progress.json"

# è¯„ä¼°é…ç½®
MAX_EVALUATION_QUESTIONS = 500  # None è¡¨ç¤ºè¯„ä¼°æ‰€æœ‰é—®é¢˜
BATCH_SIZE = 10  # æ¯æ‰¹å¤„ç†çš„é—®é¢˜æ•°é‡
DELAY_BETWEEN_BATCHES = 1.0  # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆç§’ï¼‰


# ==================== å·¥å…·å‡½æ•° ====================

def load_qa_pairs(file_path: Path) -> List[Dict[str, Any]]:
    """åŠ è½½QA pairs"""
    print(f"ğŸ“– æ­£åœ¨åŠ è½½ QA pairs: {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f:
        qa_pairs = json.load(f)
    print(f"âœ“ å·²åŠ è½½ {len(qa_pairs)} ä¸ª QA pairs")
    return qa_pairs


def load_source_documents(file_path: Path) -> Dict[str, Dict[str, Any]]:
    """åŠ è½½åŸå§‹æ–‡æ¡£ï¼Œè¿”å›doc_idåˆ°æ–‡æ¡£çš„æ˜ å°„"""
    print(f"ğŸ“– æ­£åœ¨åŠ è½½åŸå§‹æ–‡æ¡£: {file_path}")
    if not file_path.exists():
        print(f"âš ï¸ åŸå§‹æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return {}
    
    with open(file_path, 'r', encoding='utf-8') as f:
        documents = json.load(f)
    
    # æ„å»ºdoc_idåˆ°æ–‡æ¡£çš„æ˜ å°„
    doc_map = {}
    for doc in documents:
        doc_id = doc.get("doc_id", "")
        if doc_id:
            doc_map[doc_id] = doc
    
    print(f"âœ“ å·²åŠ è½½ {len(doc_map)} ä¸ªåŸå§‹æ–‡æ¡£")
    return doc_map


def create_retrieval_result_from_doc(doc: Dict[str, Any], doc_id: str) -> RetrievalResult:
    """ä»åŸå§‹æ–‡æ¡£åˆ›å»ºRetrievalResultå¯¹è±¡"""
    title = doc.get("title") or ""
    abstract = doc.get("abstract") or ""
    title = str(title).strip() if title else ""
    abstract = str(abstract).strip() if abstract else ""
    content = f"{title}\n\n{abstract}".strip()
    
    return RetrievalResult(
        document_id=doc_id,
        content=content,
        metadata={
            "doc_id": doc.get("doc_id", ""),
            "source": doc.get("source", ""),
            "source_id": doc.get("source_id", ""),
            "title": doc.get("title", ""),
            "authors": doc.get("authors", ""),
            "journal": doc.get("journal", ""),
            "date": doc.get("date", ""),
            "doi": doc.get("doi", ""),
            "url": doc.get("url", ""),
        },
        score=1.0,  # åŸå§‹æ–‡æ¡£çš„åˆ†æ•°è®¾ä¸º1.0
        source="source_document"
    )


def load_progress() -> Dict[str, Any]:
    """åŠ è½½è¿›åº¦ä¿¡æ¯"""
    if PROGRESS_FILE.exists():
        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "processed_indices": [],
        "last_processed_index": -1,
        "start_time": datetime.now().isoformat()
    }


def save_progress(progress: Dict[str, Any]):
    """ä¿å­˜è¿›åº¦ä¿¡æ¯"""
    progress["last_update"] = datetime.now().isoformat()
    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)


def save_results(results: List[EvaluationResult], output_file: Path):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
    serializable_results = []
    for result in results:
            serializable_results.append({
            "question": result.question,
            "ground_truth_answer": result.ground_truth_answer,
            "no_rag_answer": result.no_rag_answer,
            "rag_answer": result.rag_answer,
            # RAGæŒ‡æ ‡
            "rag_retrieved_documents": [
                {
                    "document_id": doc.document_id,
                    "content": doc.content[:500] + "..." if len(doc.content) > 500 else doc.content,
                    "score": doc.score,
                    "source": doc.source
                }
                for doc in result.retrieved_documents
            ],
            # RAGæ­£ç¡®æ€§æŒ‡æ ‡
            "rag_answer_correctness_semantic": result.rag_answer_correctness_semantic,
            "rag_answer_correctness_bleu": result.rag_answer_correctness_bleu,
            "rag_answer_correctness_rouge_l": result.rag_answer_correctness_rouge_l,
            "rag_answer_correctness_edit_distance": result.rag_answer_correctness_edit_distance,
            "rag_answer_correctness_jaccard": result.rag_answer_correctness_jaccard,
            "rag_answer_correctness_char_sim": result.rag_answer_correctness_char_sim,
            "rag_faithfulness": result.rag_faithfulness,
            "retrieval_precision": result.retrieval_precision,
            "retrieval_recall": result.retrieval_recall,
            # æ— RAGæ­£ç¡®æ€§æŒ‡æ ‡
            "no_rag_answer_correctness_semantic": result.no_rag_answer_correctness_semantic,
            "no_rag_answer_correctness_bleu": result.no_rag_answer_correctness_bleu,
            "no_rag_answer_correctness_rouge_l": result.no_rag_answer_correctness_rouge_l,
            "no_rag_answer_correctness_edit_distance": result.no_rag_answer_correctness_edit_distance,
            "no_rag_answer_correctness_jaccard": result.no_rag_answer_correctness_jaccard,
            "no_rag_answer_correctness_char_sim": result.no_rag_answer_correctness_char_sim,
            "evaluation_time_ms": result.evaluation_time_ms
        })
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, ensure_ascii=False, indent=2)


# ==================== LLM è°ƒç”¨å‡½æ•° ====================

async def generate_answer_without_rag(
    llm: ChatOpenAI,
    question: str
) -> str:
    """ä¸ä½¿ç”¨RAGç”Ÿæˆç­”æ¡ˆ"""
    messages = [
        SystemMessage(content="You are a helpful assistant that answers questions based on your knowledge."),
        HumanMessage(content=f"Question: {question}\n\nPlease provide a clear and concise answer.")
    ]
    
    response = await llm.ainvoke(messages)
    return response.content.strip()


async def generate_answer_with_rag(
    llm: ChatOpenAI,
    question: str,
    retrieved_docs: List[RetrievalResult]
) -> str:
    """ä½¿ç”¨RAGç”Ÿæˆç­”æ¡ˆ"""
    # æ„å»ºä¸Šä¸‹æ–‡
    context_parts = []
    for i, doc in enumerate(retrieved_docs, 1):
        context_parts.append(f"[Document {i}]\n{doc.content}")
    
    context = "\n\n".join(context_parts)
    
    messages = [
        SystemMessage(content="You are a helpful assistant that answers questions based on the provided context documents. Use the information from the context to answer the question accurately."),
        HumanMessage(content=f"Context Documents:\n{context}\n\nQuestion: {question}\n\nPlease provide a clear and concise answer based on the context documents.")
    ]
    
    response = await llm.ainvoke(messages)
    return response.content.strip()


# ==================== æŒ‡æ ‡è®¡ç®— ====================

def calculate_bleu_score(candidate: str, reference: str, n: int = 4) -> float:
    """
    è®¡ç®—BLEUåˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆï¼ŒåŸºäºn-gramé‡å ï¼‰
    
    Args:
        candidate: å€™é€‰ç­”æ¡ˆ
        reference: å‚è€ƒç­”æ¡ˆï¼ˆground truthï¼‰
        n: n-gramçš„æœ€å¤§nå€¼ï¼ˆé»˜è®¤4ï¼Œå³BLEU-4ï¼‰
    
    Returns:
        BLEUåˆ†æ•° (0-1)
    """
    def get_ngrams(text: str, n: int) -> List[Tuple]:
        """è·å–n-gramåˆ—è¡¨"""
        words = text.lower().split()
        if len(words) < n:
            return [tuple(words)]
        return [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]
    
    if not candidate or not reference:
        return 0.0
    
    candidate_ngrams = get_ngrams(candidate, n)
    reference_ngrams = get_ngrams(reference, n)
    
    if not candidate_ngrams or not reference_ngrams:
        return 0.0
    
    # è®¡ç®—ç²¾ç¡®åŒ¹é…çš„n-gramæ•°é‡
    candidate_counter = Counter(candidate_ngrams)
    reference_counter = Counter(reference_ngrams)
    
    matches = sum(min(candidate_counter[ngram], reference_counter[ngram]) 
                  for ngram in candidate_counter)
    
    # BLEUåˆ†æ•° = åŒ¹é…æ•° / å€™é€‰n-gramæ€»æ•°
    bleu = matches / len(candidate_ngrams) if candidate_ngrams else 0.0
    
    # åº”ç”¨é•¿åº¦æƒ©ç½šï¼ˆç®€åŒ–ç‰ˆï¼‰
    if len(candidate.split()) < len(reference.split()):
        brevity_penalty = len(candidate.split()) / len(reference.split())
        bleu *= brevity_penalty
    
    return max(0.0, min(1.0, bleu))


def calculate_rouge_l(candidate: str, reference: str) -> float:
    """
    è®¡ç®—ROUGE-Låˆ†æ•°ï¼ˆåŸºäºæœ€é•¿å…¬å…±å­åºåˆ—LCSï¼‰
    
    Args:
        candidate: å€™é€‰ç­”æ¡ˆ
        reference: å‚è€ƒç­”æ¡ˆï¼ˆground truthï¼‰
    
    Returns:
        ROUGE-Låˆ†æ•° (0-1)
    """
    def lcs_length(seq1: List[str], seq2: List[str]) -> int:
        """è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—çš„é•¿åº¦"""
        m, n = len(seq1), len(seq2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if seq1[i-1] == seq2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        return dp[m][n]
    
    if not candidate or not reference:
        return 0.0
    
    candidate_words = candidate.lower().split()
    reference_words = reference.lower().split()
    
    if not candidate_words or not reference_words:
        return 0.0
    
    lcs = lcs_length(candidate_words, reference_words)
    
    # ROUGE-L = LCSé•¿åº¦ / å‚è€ƒç­”æ¡ˆé•¿åº¦
    rouge_l = lcs / len(reference_words) if reference_words else 0.0
    
    return max(0.0, min(1.0, rouge_l))


def calculate_edit_distance_similarity(candidate: str, reference: str) -> float:
    """
    è®¡ç®—åŸºäºç¼–è¾‘è·ç¦»çš„ç›¸ä¼¼åº¦ï¼ˆLevenshteinè·ç¦»ï¼‰
    
    Args:
        candidate: å€™é€‰ç­”æ¡ˆ
        reference: å‚è€ƒç­”æ¡ˆï¼ˆground truthï¼‰
    
    Returns:
        å½’ä¸€åŒ–çš„ç›¸ä¼¼åº¦åˆ†æ•° (0-1)ï¼Œ1è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼Œ0è¡¨ç¤ºå®Œå…¨ä¸åŒ
    """
    def levenshtein_distance(s1: str, s2: str) -> int:
        """è®¡ç®—Levenshteinç¼–è¾‘è·ç¦»"""
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    if not candidate and not reference:
        return 1.0
    
    if not candidate or not reference:
        return 0.0
    
    # è®¡ç®—ç¼–è¾‘è·ç¦»
    edit_dist = levenshtein_distance(candidate.lower(), reference.lower())
    
    # å½’ä¸€åŒ–ï¼šç›¸ä¼¼åº¦ = 1 - (ç¼–è¾‘è·ç¦» / æœ€å¤§é•¿åº¦)
    max_len = max(len(candidate), len(reference))
    similarity = 1.0 - (edit_dist / max_len) if max_len > 0 else 0.0
    
    return max(0.0, min(1.0, similarity))


def calculate_jaccard_similarity(candidate: str, reference: str) -> float:
    """
    è®¡ç®—Jaccardç›¸ä¼¼åº¦ï¼ˆåŸºäºè¯æ±‡é›†åˆçš„é‡å ï¼‰
    
    Args:
        candidate: å€™é€‰ç­”æ¡ˆ
        reference: å‚è€ƒç­”æ¡ˆï¼ˆground truthï¼‰
    
    Returns:
        Jaccardç›¸ä¼¼åº¦ (0-1)
    """
    if not candidate or not reference:
        return 0.0
    
    candidate_words = set(candidate.lower().split())
    reference_words = set(reference.lower().split())
    
    if not candidate_words and not reference_words:
        return 1.0
    
    if not candidate_words or not reference_words:
        return 0.0
    
    # Jaccard = |A âˆ© B| / |A âˆª B|
    intersection = len(candidate_words & reference_words)
    union = len(candidate_words | reference_words)
    
    jaccard = intersection / union if union > 0 else 0.0
    
    return max(0.0, min(1.0, jaccard))


def calculate_char_similarity(candidate: str, reference: str) -> float:
    """
    è®¡ç®—å­—ç¬¦çº§åˆ«çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºå­—ç¬¦é›†åˆçš„Jaccardç›¸ä¼¼åº¦ï¼‰
    
    Args:
        candidate: å€™é€‰ç­”æ¡ˆ
        reference: å‚è€ƒç­”æ¡ˆï¼ˆground truthï¼‰
    
    Returns:
        å­—ç¬¦çº§åˆ«ç›¸ä¼¼åº¦ (0-1)
    """
    if not candidate or not reference:
        return 0.0
    
    candidate_chars = set(candidate.lower().replace(" ", ""))
    reference_chars = set(reference.lower().replace(" ", ""))
    
    if not candidate_chars and not reference_chars:
        return 1.0
    
    if not candidate_chars or not reference_chars:
        return 0.0
    
    # Jaccardç›¸ä¼¼åº¦
    intersection = len(candidate_chars & reference_chars)
    union = len(candidate_chars | reference_chars)
    
    similarity = intersection / union if union > 0 else 0.0
    
    return max(0.0, min(1.0, similarity))


async def calculate_answer_correctness_semantic(
    embeddings_model,
    answer: str,
    ground_truth: str
) -> float:
    """
    è®¡ç®—ç­”æ¡ˆä¸ground truthçš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåŸºäºembeddingï¼‰
    ä½¿ç”¨embeddingè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    
    Returns:
        è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    """
    try:
        # ç”Ÿæˆembedding
        answer_embedding = embeddings_model.embed_query(answer)
        gt_embedding = embeddings_model.embed_query(ground_truth)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = cosine_similarity(
            [answer_embedding],
            [gt_embedding]
        )[0][0]
        
        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´æ˜¯-1åˆ°1ï¼‰
        normalized_similarity = (similarity + 1) / 2
        
        return max(0.0, min(1.0, normalized_similarity))
    except Exception as e:
        print(f"    âš ï¸ è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¤±è´¥: {e}")
        return 0.0


async def calculate_faithfulness(
    llm: ChatOpenAI,
    question: str,
    answer: str,
    retrieved_documents: List[RetrievalResult]
) -> float:
    """
    è®¡ç®—ç­”æ¡ˆå¯¹æ£€ç´¢æ–‡æ¡£çš„å¿ å®åº¦ï¼ˆFaithfulnessï¼‰
    è¯„ä¼°ç­”æ¡ˆæ˜¯å¦å®Œå…¨åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œæ²¡æœ‰å¼•å…¥å¤–éƒ¨çŸ¥è¯†æˆ–å¹»è§‰
    
    Returns:
        å¿ å®åº¦åˆ†æ•° (0-1)
    """
    if not retrieved_documents:
        return 0.0
    
    # æ„å»ºæ–‡æ¡£ä¸Šä¸‹æ–‡
    context_parts = []
    for i, doc in enumerate(retrieved_documents, 1):
        context_parts.append(f"[Document {i}]\n{doc.content}")
    context = "\n\n".join(context_parts)
    
    prompt = f"""Evaluate whether the answer is fully supported by the retrieved context documents.
Rate the faithfulness on a scale of 0 to 1, where:
- 1.0: The answer is completely supported by the context, with no unsupported claims
- 0.5: The answer is partially supported, but contains some unsupported information
- 0.0: The answer contains significant unsupported claims or contradicts the context

Question: {question}

Retrieved Context Documents:
{context}

Answer: {answer}

Output ONLY a single float number between 0 and 1, without any explanation."""

    messages = [
        SystemMessage(content="You are an expert at evaluating answer faithfulness to source documents."),
        HumanMessage(content=prompt)
    ]
    
    try:
        response = await llm.ainvoke(messages)
        score = float(response.content.strip())
        return max(0.0, min(1.0, score))
    except Exception as e:
        print(f"    âš ï¸ è®¡ç®—å¿ å®åº¦å¤±è´¥: {e}")
        return 0.0


def calculate_retrieval_metrics(
    retrieved_documents: List[RetrievalResult],
    ground_truth_doc_id: str
) -> Tuple[float, float]:
    """
    è®¡ç®—æ£€ç´¢è´¨é‡æŒ‡æ ‡
    
    Args:
        retrieved_documents: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        ground_truth_doc_id: ground truthæ–‡æ¡£çš„doc_id
    
    Returns:
        (precision, recall)
        - precision: æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­åŒ…å«ground truthæ–‡æ¡£çš„æ¯”ä¾‹ï¼ˆå¦‚æœground truthåœ¨top-kä¸­ï¼‰
        - recall: ground truthæ–‡æ¡£åœ¨æ£€ç´¢ç»“æœä¸­çš„æ’åå€’æ•°ï¼ˆ1/rankï¼Œå¦‚æœæ‰¾åˆ°çš„è¯ï¼‰
    """
    if not retrieved_documents:
        return 0.0, 0.0
    
    # æ£€æŸ¥ground truthæ–‡æ¡£æ˜¯å¦åœ¨æ£€ç´¢ç»“æœä¸­
    found_gt = False
    gt_rank = -1
    
    for i, doc in enumerate(retrieved_documents):
        # æ£€æŸ¥doc_idæ˜¯å¦åŒ¹é…ï¼ˆå¯èƒ½åœ¨ä¸åŒçš„chunkä¸­ï¼‰
        doc_id_from_metadata = doc.metadata.get("doc_id", "")
        if doc_id_from_metadata == ground_truth_doc_id:
            found_gt = True
            gt_rank = i + 1  # æ’åä»1å¼€å§‹
            break
    
    # Precision: å¦‚æœæ‰¾åˆ°ground truthï¼Œprecision = 1/æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°
    # è¿™è¡¨ç¤º"æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­æœ‰å¤šå°‘æ¯”ä¾‹æ˜¯ground truth"
    if found_gt:
        precision = 1.0 / len(retrieved_documents)
    else:
        precision = 0.0
    
    # Recall: å¦‚æœæ‰¾åˆ°ground truthï¼Œrecall = 1/rankï¼ˆæ’åè¶Šé å‰ï¼Œrecallè¶Šé«˜ï¼‰
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œrecall = 0
    if found_gt:
        recall = 1.0 / gt_rank
    else:
        recall = 0.0
    
    return precision, recall


# ==================== è¯„ä¼°ä¸»å‡½æ•° ====================

async def evaluate_single_qa(
    llm: ChatOpenAI,
    rag_system: HybridRAGSystem,
    qa_pair: Dict[str, Any],
    qa_index: int,
    total_qa: int,
    source_docs_map: Dict[str, Dict[str, Any]] = None,
    embeddings_model = None
) -> EvaluationResult:
    """è¯„ä¼°å•ä¸ªQA pair"""
    import time
    start_time = time.time()
    
    question = qa_pair["question"]
    ground_truth_answer = qa_pair["answer"]
    
    print(f"\n[{qa_index + 1}/{total_qa}] è¯„ä¼°é—®é¢˜: {question[:80]}...")
    
    result = EvaluationResult(
        question=question,
        ground_truth_answer=ground_truth_answer
    )
    
    # è·å–doc_idç”¨äºæ£€ç´¢è´¨é‡è¯„ä¼°
    doc_id = qa_pair.get("doc_id", "")
    
    try:
        # 1. ä¸ä½¿ç”¨RAGç”Ÿæˆç­”æ¡ˆ
        print("  ğŸ“ ç”Ÿæˆæ— RAGç­”æ¡ˆ...")
        result.no_rag_answer = await generate_answer_without_rag(llm, question)
        
        # 2. ä½¿ç”¨RAGæ£€ç´¢æ–‡æ¡£
        print("  ğŸ” æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
        rag_result = await rag_system.retrieve(
            query=question,
            top_k=5,
            similarity_threshold=0.3
        )
        result.retrieved_documents = rag_result.shared_results
        print(f"  âœ“ æ£€ç´¢åˆ° {len(result.retrieved_documents)} ä¸ªæ–‡æ¡£")
        
        # 3. ä½¿ç”¨RAGç”Ÿæˆç­”æ¡ˆ
        print("  ğŸ“ ç”ŸæˆRAGç­”æ¡ˆ...")
        result.rag_answer = await generate_answer_with_rag(llm, question, result.retrieved_documents)
        
        # 4. è®¡ç®—ç­”æ¡ˆæ­£ç¡®æ€§æŒ‡æ ‡ï¼ˆå¤šç§å®šé‡æŒ‡æ ‡ï¼‰
        print("  ğŸ“Š è®¡ç®—ç­”æ¡ˆæ­£ç¡®æ€§æŒ‡æ ‡...")
        
        # è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåŸºäºembeddingï¼‰
        if embeddings_model:
            result.rag_answer_correctness_semantic = await calculate_answer_correctness_semantic(
                embeddings_model, result.rag_answer, ground_truth_answer
            )
            result.no_rag_answer_correctness_semantic = await calculate_answer_correctness_semantic(
                embeddings_model, result.no_rag_answer, ground_truth_answer
            )
        
        # BLEUåˆ†æ•°
        result.rag_answer_correctness_bleu = calculate_bleu_score(
            result.rag_answer, ground_truth_answer
        )
        result.no_rag_answer_correctness_bleu = calculate_bleu_score(
            result.no_rag_answer, ground_truth_answer
        )
        
        # ROUGE-Låˆ†æ•°
        result.rag_answer_correctness_rouge_l = calculate_rouge_l(
            result.rag_answer, ground_truth_answer
        )
        result.no_rag_answer_correctness_rouge_l = calculate_rouge_l(
            result.no_rag_answer, ground_truth_answer
        )
        
        # ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
        result.rag_answer_correctness_edit_distance = calculate_edit_distance_similarity(
            result.rag_answer, ground_truth_answer
        )
        result.no_rag_answer_correctness_edit_distance = calculate_edit_distance_similarity(
            result.no_rag_answer, ground_truth_answer
        )
        
        # Jaccardç›¸ä¼¼åº¦
        result.rag_answer_correctness_jaccard = calculate_jaccard_similarity(
            result.rag_answer, ground_truth_answer
        )
        result.no_rag_answer_correctness_jaccard = calculate_jaccard_similarity(
            result.no_rag_answer, ground_truth_answer
        )
        
        # å­—ç¬¦çº§åˆ«ç›¸ä¼¼åº¦
        result.rag_answer_correctness_char_sim = calculate_char_similarity(
            result.rag_answer, ground_truth_answer
        )
        result.no_rag_answer_correctness_char_sim = calculate_char_similarity(
            result.no_rag_answer, ground_truth_answer
        )
        
        # å¿ å®åº¦ï¼ˆä»…RAGï¼‰
        if result.retrieved_documents:
            result.rag_faithfulness = await calculate_faithfulness(
                llm, question, result.rag_answer, result.retrieved_documents
            )
        
        # æ£€ç´¢è´¨é‡æŒ‡æ ‡
        if doc_id:
            result.retrieval_precision, result.retrieval_recall = calculate_retrieval_metrics(
                result.retrieved_documents, doc_id
            )
        
        # è¾“å‡ºæŒ‡æ ‡
        if embeddings_model:
            print(f"    âœ“ RAG Semantic Similarity: {result.rag_answer_correctness_semantic:.3f}")
            print(f"    âœ“ æ— RAG Semantic Similarity: {result.no_rag_answer_correctness_semantic:.3f}")
        print(f"    âœ“ RAG BLEU-4: {result.rag_answer_correctness_bleu:.3f}")
        print(f"    âœ“ æ— RAG BLEU-4: {result.no_rag_answer_correctness_bleu:.3f}")
        print(f"    âœ“ RAG ROUGE-L: {result.rag_answer_correctness_rouge_l:.3f}")
        print(f"    âœ“ æ— RAG ROUGE-L: {result.no_rag_answer_correctness_rouge_l:.3f}")
        print(f"    âœ“ RAG Edit Distance Sim: {result.rag_answer_correctness_edit_distance:.3f}")
        print(f"    âœ“ æ— RAG Edit Distance Sim: {result.no_rag_answer_correctness_edit_distance:.3f}")
        print(f"    âœ“ RAG Jaccard: {result.rag_answer_correctness_jaccard:.3f}")
        print(f"    âœ“ æ— RAG Jaccard: {result.no_rag_answer_correctness_jaccard:.3f}")
        print(f"    âœ“ RAG Char Similarity: {result.rag_answer_correctness_char_sim:.3f}")
        print(f"    âœ“ æ— RAG Char Similarity: {result.no_rag_answer_correctness_char_sim:.3f}")
        if result.retrieved_documents:
            print(f"    âœ“ RAG Faithfulness: {result.rag_faithfulness:.3f}")
        if doc_id:
            print(f"    âœ“ Retrieval Precision: {result.retrieval_precision:.3f}")
            print(f"    âœ“ Retrieval Recall: {result.retrieval_recall:.3f}")
        
    except Exception as e:
        print(f"  âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    result.evaluation_time_ms = (time.time() - start_time) * 1000
    return result


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("RAGç³»ç»Ÿè¯„ä¼°è„šæœ¬")
    print("=" * 80)
    print()
    
    # åŠ è½½è®¾ç½®
    settings = get_settings()
    if not settings.openai_api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®")
        return
    
    # åˆå§‹åŒ–LLM
    llm_model = "gpt-4o-mini"
    print(f"ğŸ¤– åˆå§‹åŒ– LLM: {llm_model}")
    llm = ChatOpenAI(
        model=llm_model,
        temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„è¯„ä¼°ç»“æœ
        openai_api_key=settings.openai_api_key
    )
    print("âœ“ LLM åˆå§‹åŒ–å®Œæˆ")
    
    # åˆå§‹åŒ–Embeddingsæ¨¡å‹ï¼ˆç”¨äºè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
    print(f"ğŸ”¤ åˆå§‹åŒ– Embeddings æ¨¡å‹...")
    try:
        embeddings_model = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=settings.openai_api_key
        )
        print("âœ“ Embeddings æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ Embeddings æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†è·³è¿‡è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—")
        embeddings_model = None
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    print("\nğŸ”§ åˆå§‹åŒ– RAG ç³»ç»Ÿ...")
    rag_system = HybridRAGSystem()
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å·²åŠ è½½
    stats = rag_system.get_all_stats()
    if stats["total_documents"] == 0:
        print("âš ï¸ å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œæ­£åœ¨åŠ è½½çŸ¥è¯†åº“...")
        rag_system.load_knowledge_base()
    else:
        print(f"âœ“ RAG ç³»ç»Ÿå·²å°±ç»ªï¼ˆ{stats['total_documents']} ä¸ªæ–‡æ¡£ï¼‰")
    
    print()
    
    # åŠ è½½QA pairs
    if not QA_PAIRS_FILE.exists():
        print(f"âŒ é”™è¯¯: QA pairs æ–‡ä»¶ä¸å­˜åœ¨: {QA_PAIRS_FILE}")
        return
    
    qa_pairs = load_qa_pairs(QA_PAIRS_FILE)
    
    # åŠ è½½åŸå§‹æ–‡æ¡£
    source_docs_map = load_source_documents(SOURCE_DOCUMENTS_FILE)
    
    # é™åˆ¶è¯„ä¼°æ•°é‡
    if MAX_EVALUATION_QUESTIONS:
        qa_pairs = qa_pairs[:MAX_EVALUATION_QUESTIONS]
        print(f"ğŸ“Š é™åˆ¶è¯„ä¼°æ•°é‡ä¸º: {MAX_EVALUATION_QUESTIONS}")
    
    # åŠ è½½è¿›åº¦
    progress = load_progress()
    start_index = progress.get("last_processed_index", -1) + 1
    
    if start_index >= len(qa_pairs):
        print("âœ“ æ‰€æœ‰é—®é¢˜å·²è¯„ä¼°å®Œæˆï¼")
        return
    
    print(f"ğŸ“Š è¿›åº¦ä¿¡æ¯:")
    print(f"  - å·²è¯„ä¼°: {start_index}/{len(qa_pairs)}")
    print(f"  - å¾…è¯„ä¼°: {len(qa_pairs) - start_index}")
    print()
    
    # åˆ†æ‰¹è¯„ä¼°
    all_results = []
    total_batches = (len(qa_pairs) - start_index + BATCH_SIZE - 1) // BATCH_SIZE
    
    for batch_num in range(total_batches):
        batch_start = start_index + batch_num * BATCH_SIZE
        batch_end = min(batch_start + BATCH_SIZE, len(qa_pairs))
        
        print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{total_batches} (é—®é¢˜ {batch_start + 1}-{batch_end})")
        
        batch_results = []
        for i in range(batch_start, batch_end):
            qa_pair = qa_pairs[i]
            result = await evaluate_single_qa(
                llm, rag_system, qa_pair, i, len(qa_pairs), 
                source_docs_map, embeddings_model
            )
            batch_results.append(result)
            all_results.append(result)
            
            # é—®é¢˜é—´çŸ­æš‚å»¶è¿Ÿ
            if i < batch_end - 1:
                await asyncio.sleep(0.5)
        
        # ä¿å­˜è¿›åº¦å’Œç»“æœ
        progress["last_processed_index"] = batch_end - 1
        progress["processed_indices"] = list(range(batch_start, batch_end))
        save_progress(progress)
        
        # å¢é‡ä¿å­˜ç»“æœ
        save_results(all_results, OUTPUT_FILE)
        print(f"  âœ“ å·²ä¿å­˜æ‰¹æ¬¡ç»“æœï¼ˆæ€»è®¡: {len(all_results)} ä¸ªç»“æœï¼‰")
        
        # æ‰¹æ¬¡é—´å»¶è¿Ÿ
        if batch_num < total_batches - 1:
            print(f"  â³ ç­‰å¾… {DELAY_BETWEEN_BATCHES} ç§’...")
            await asyncio.sleep(DELAY_BETWEEN_BATCHES)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 80)
    print("ğŸ“Š è¯„ä¼°ç»Ÿè®¡")
    print("=" * 80)
    
    valid_rag_results = [r for r in all_results if r.rag_answer]
    valid_no_rag_results = [r for r in all_results if r.no_rag_answer]
    
    # åªç»Ÿè®¡ precision > 0 çš„ RAG ç»“æœï¼ˆæ£€ç´¢åˆ° ground truth æ–‡æ¡£çš„æƒ…å†µï¼‰
    precision_positive_rag_results = [r for r in valid_rag_results if r.retrieval_precision > 0]
    # å¯¹äºæ— RAGç»“æœï¼Œä½¿ç”¨ç›¸åŒçš„ç´¢å¼•ï¼ˆå¯¹åº”ç›¸åŒçš„é—®é¢˜ï¼‰
    precision_positive_no_rag_results = [r for r in valid_no_rag_results 
                                          if any(r.question == rag_r.question for rag_r in precision_positive_rag_results)]
    
    print(f"æ€»è¯„ä¼°é—®é¢˜æ•°: {len(all_results)}")
    print(f"æœ‰æ•ˆRAGè¯„ä¼°æ•°: {len(valid_rag_results)}")
    print(f"æœ‰æ•ˆæ— RAGè¯„ä¼°æ•°: {len(valid_no_rag_results)}")
    print(f"æ£€ç´¢æˆåŠŸæ•° (precision > 0): {len(precision_positive_rag_results)}")
    print()
    
    print("=" * 80)
    print("ğŸ“Š ç»Ÿè®¡ç»“æœï¼ˆä»… precision > 0 çš„æ¡ˆä¾‹ï¼‰")
    print("=" * 80)
    print()
    
    if precision_positive_rag_results:
        # è®¡ç®—å„ç§æ­£ç¡®æ€§æŒ‡æ ‡çš„å¹³å‡å€¼ï¼ˆä»… precision > 0 çš„æ¡ˆä¾‹ï¼‰
        rag_avg_semantic = sum(r.rag_answer_correctness_semantic for r in precision_positive_rag_results if r.rag_answer_correctness_semantic > 0) / max(1, sum(1 for r in precision_positive_rag_results if r.rag_answer_correctness_semantic > 0))
        rag_avg_bleu = sum(r.rag_answer_correctness_bleu for r in precision_positive_rag_results) / len(precision_positive_rag_results)
        rag_avg_rouge_l = sum(r.rag_answer_correctness_rouge_l for r in precision_positive_rag_results) / len(precision_positive_rag_results)
        rag_avg_edit_dist = sum(r.rag_answer_correctness_edit_distance for r in precision_positive_rag_results) / len(precision_positive_rag_results)
        rag_avg_jaccard = sum(r.rag_answer_correctness_jaccard for r in precision_positive_rag_results) / len(precision_positive_rag_results)
        rag_avg_char_sim = sum(r.rag_answer_correctness_char_sim for r in precision_positive_rag_results) / len(precision_positive_rag_results)
        rag_avg_faithfulness = sum(r.rag_faithfulness for r in precision_positive_rag_results if r.rag_faithfulness > 0) / max(1, sum(1 for r in precision_positive_rag_results if r.rag_faithfulness > 0))
        rag_avg_retrieval_precision = sum(r.retrieval_precision for r in precision_positive_rag_results) / len(precision_positive_rag_results)
        rag_avg_retrieval_recall = sum(r.retrieval_recall for r in precision_positive_rag_results) / len(precision_positive_rag_results)
        
        print(f"ğŸ“Š RAGæŒ‡æ ‡ (åŸºäº {len(precision_positive_rag_results)} ä¸ªæ£€ç´¢æˆåŠŸæ¡ˆä¾‹):")
        print(f"  ã€ç­”æ¡ˆæ­£ç¡®æ€§æŒ‡æ ‡ã€‘")
        if rag_avg_semantic > 0:
            print(f"    - Semantic Similarity: {rag_avg_semantic:.3f}")
        print(f"    - BLEU-4: {rag_avg_bleu:.3f}")
        print(f"    - ROUGE-L: {rag_avg_rouge_l:.3f}")
        print(f"    - Edit Distance Similarity: {rag_avg_edit_dist:.3f}")
        print(f"    - Jaccard Similarity: {rag_avg_jaccard:.3f}")
        print(f"    - Character Similarity: {rag_avg_char_sim:.3f}")
        if rag_avg_faithfulness > 0:
            print(f"    - Faithfulness: {rag_avg_faithfulness:.3f}")
        print(f"  ã€æ£€ç´¢è´¨é‡æŒ‡æ ‡ã€‘")
        print(f"    - Retrieval Precision: {rag_avg_retrieval_precision:.3f}")
        print(f"    - Retrieval Recall: {rag_avg_retrieval_recall:.3f}")
    else:
        print("âš ï¸ æ²¡æœ‰æ£€ç´¢æˆåŠŸçš„RAGè¯„ä¼°ç»“æœ (precision > 0)")
    
    print()
    
    if precision_positive_no_rag_results:
        no_rag_avg_semantic = sum(r.no_rag_answer_correctness_semantic for r in precision_positive_no_rag_results if r.no_rag_answer_correctness_semantic > 0) / max(1, sum(1 for r in precision_positive_no_rag_results if r.no_rag_answer_correctness_semantic > 0))
        no_rag_avg_bleu = sum(r.no_rag_answer_correctness_bleu for r in precision_positive_no_rag_results) / len(precision_positive_no_rag_results)
        no_rag_avg_rouge_l = sum(r.no_rag_answer_correctness_rouge_l for r in precision_positive_no_rag_results) / len(precision_positive_no_rag_results)
        no_rag_avg_edit_dist = sum(r.no_rag_answer_correctness_edit_distance for r in precision_positive_no_rag_results) / len(precision_positive_no_rag_results)
        no_rag_avg_jaccard = sum(r.no_rag_answer_correctness_jaccard for r in precision_positive_no_rag_results) / len(precision_positive_no_rag_results)
        no_rag_avg_char_sim = sum(r.no_rag_answer_correctness_char_sim for r in precision_positive_no_rag_results) / len(precision_positive_no_rag_results)
        
        print(f"ğŸ“Š æ— RAGæŒ‡æ ‡ (åŸºäº {len(precision_positive_no_rag_results)} ä¸ªæ£€ç´¢æˆåŠŸæ¡ˆä¾‹):")
        print(f"  ã€ç­”æ¡ˆæ­£ç¡®æ€§æŒ‡æ ‡ã€‘")
        if no_rag_avg_semantic > 0:
            print(f"    - Semantic Similarity: {no_rag_avg_semantic:.3f}")
        print(f"    - BLEU-4: {no_rag_avg_bleu:.3f}")
        print(f"    - ROUGE-L: {no_rag_avg_rouge_l:.3f}")
        print(f"    - Edit Distance Similarity: {no_rag_avg_edit_dist:.3f}")
        print(f"    - Jaccard Similarity: {no_rag_avg_jaccard:.3f}")
        print(f"    - Character Similarity: {no_rag_avg_char_sim:.3f}")
    else:
        print("âš ï¸ æ²¡æœ‰æ£€ç´¢æˆåŠŸçš„æ— RAGè¯„ä¼°ç»“æœ (precision > 0)")
    
    print()
    
    # å¯¹æ¯”åˆ†æï¼ˆä»… precision > 0 çš„æ¡ˆä¾‹ï¼‰
    if precision_positive_rag_results and precision_positive_no_rag_results:
        print("ğŸ“ˆ å¯¹æ¯”åˆ†æ (RAG vs æ— RAG, ä»…æ£€ç´¢æˆåŠŸæ¡ˆä¾‹):")
        rag_avg_semantic = sum(r.rag_answer_correctness_semantic for r in precision_positive_rag_results if r.rag_answer_correctness_semantic > 0) / max(1, sum(1 for r in precision_positive_rag_results if r.rag_answer_correctness_semantic > 0))
        rag_avg_bleu = sum(r.rag_answer_correctness_bleu for r in precision_positive_rag_results) / len(precision_positive_rag_results)
        rag_avg_rouge_l = sum(r.rag_answer_correctness_rouge_l for r in precision_positive_rag_results) / len(precision_positive_rag_results)
        rag_avg_edit_dist = sum(r.rag_answer_correctness_edit_distance for r in precision_positive_rag_results) / len(precision_positive_rag_results)
        rag_avg_jaccard = sum(r.rag_answer_correctness_jaccard for r in precision_positive_rag_results) / len(precision_positive_rag_results)
        rag_avg_char_sim = sum(r.rag_answer_correctness_char_sim for r in precision_positive_rag_results) / len(precision_positive_rag_results)
        
        no_rag_avg_semantic = sum(r.no_rag_answer_correctness_semantic for r in precision_positive_no_rag_results if r.no_rag_answer_correctness_semantic > 0) / max(1, sum(1 for r in precision_positive_no_rag_results if r.no_rag_answer_correctness_semantic > 0))
        no_rag_avg_bleu = sum(r.no_rag_answer_correctness_bleu for r in precision_positive_no_rag_results) / len(precision_positive_no_rag_results)
        no_rag_avg_rouge_l = sum(r.no_rag_answer_correctness_rouge_l for r in precision_positive_no_rag_results) / len(precision_positive_no_rag_results)
        no_rag_avg_edit_dist = sum(r.no_rag_answer_correctness_edit_distance for r in precision_positive_no_rag_results) / len(precision_positive_no_rag_results)
        no_rag_avg_jaccard = sum(r.no_rag_answer_correctness_jaccard for r in precision_positive_no_rag_results) / len(precision_positive_no_rag_results)
        no_rag_avg_char_sim = sum(r.no_rag_answer_correctness_char_sim for r in precision_positive_no_rag_results) / len(precision_positive_no_rag_results)
        
        print(f"  ã€ç­”æ¡ˆæ­£ç¡®æ€§æŒ‡æ ‡ã€‘")
        if rag_avg_semantic > 0 and no_rag_avg_semantic > 0:
            print(f"    - Semantic Similarity: RAG {rag_avg_semantic:.3f} vs æ— RAG {no_rag_avg_semantic:.3f} (å·®å¼‚: {rag_avg_semantic - no_rag_avg_semantic:+.3f})")
        print(f"    - BLEU-4: RAG {rag_avg_bleu:.3f} vs æ— RAG {no_rag_avg_bleu:.3f} (å·®å¼‚: {rag_avg_bleu - no_rag_avg_bleu:+.3f})")
        print(f"    - ROUGE-L: RAG {rag_avg_rouge_l:.3f} vs æ— RAG {no_rag_avg_rouge_l:.3f} (å·®å¼‚: {rag_avg_rouge_l - no_rag_avg_rouge_l:+.3f})")
        print(f"    - Edit Distance Sim: RAG {rag_avg_edit_dist:.3f} vs æ— RAG {no_rag_avg_edit_dist:.3f} (å·®å¼‚: {rag_avg_edit_dist - no_rag_avg_edit_dist:+.3f})")
        print(f"    - Jaccard: RAG {rag_avg_jaccard:.3f} vs æ— RAG {no_rag_avg_jaccard:.3f} (å·®å¼‚: {rag_avg_jaccard - no_rag_avg_jaccard:+.3f})")
        print(f"    - Char Similarity: RAG {rag_avg_char_sim:.3f} vs æ— RAG {no_rag_avg_char_sim:.3f} (å·®å¼‚: {rag_avg_char_sim - no_rag_avg_char_sim:+.3f})")
    
    if all_results:
        avg_eval_time = sum(r.evaluation_time_ms for r in all_results) / len(all_results)
        print(f"\nâ±ï¸  å¹³å‡è¯„ä¼°æ—¶é—´: {avg_eval_time:.1f} ms")
    
    print(f"\nâœ“ è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_FILE}")


if __name__ == "__main__":
    asyncio.run(main())

