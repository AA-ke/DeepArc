"""
Agents/workflow.py
LangGraphå·¥ä½œæµå®šä¹‰ - å¤šAgentåè°ƒç³»ç»Ÿ
"""

from typing import Dict, Any, Literal, Optional
from pathlib import Path
import json

from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from Agents.state import REAgentState, CritiqueResult, OptimizationReport
from Agents.prompt import initialize_supervisor, initialize_agents_with_rag
from Agents.agent import Agent, summarize_messages
from RAG.rag import HybridRAGSystem


# ==================== å·¥ä½œæµèŠ‚ç‚¹å®šä¹‰ ====================

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨Supervisorã€ä¸“å®¶Agentå’ŒRAGç³»ç»Ÿå®ä¾‹ï¼ˆåœ¨å·¥ä½œæµæ‰§è¡ŒæœŸé—´å…±äº«ï¼‰
_workflow_supervisor: Agent = None
_workflow_expert_agents: Dict[str, Agent] = None
_workflow_rag_system: HybridRAGSystem = None


def set_workflow_components(supervisor: Agent = None, expert_agents: Dict[str, Agent] = None, rag_system: HybridRAGSystem = None):
    """è®¾ç½®å·¥ä½œæµç»„ä»¶ï¼ˆSupervisorã€ä¸“å®¶Agentå’ŒRAGç³»ç»Ÿï¼‰"""
    global _workflow_supervisor, _workflow_expert_agents, _workflow_rag_system
    if supervisor is not None:
        _workflow_supervisor = supervisor
    if expert_agents is not None:
        _workflow_expert_agents = expert_agents
    if rag_system is not None:
        _workflow_rag_system = rag_system


def _check_scores_meet_requirements(state: REAgentState) -> bool:
    """
    æ£€æŸ¥è¯„åˆ†æ˜¯å¦æ»¡è¶³ä¸¥æ ¼çš„è¦æ±‚
    è¦æ±‚ï¼šå¹³å‡åˆ†>=9.0ï¼Œæ¯ä¸ªä¸“å®¶>=8.5
    """
    critiques = {
        "data_management": state.get("data_critique"),
        "methodology": state.get("methodology_critique"),
        "model_architect": state.get("model_critique"),
        "result_analyst": state.get("results_critique")
    }
    
    scores = []
    for role, critique in critiques.items():
        if critique:
            if isinstance(critique, dict):
                score = critique.get("score", 0)
            elif hasattr(critique, "score"):
                score = critique.score
            else:
                score = 0
            
            if score > 0:
                scores.append(score)
                # æ£€æŸ¥æ¯ä¸ªä¸“å®¶æ˜¯å¦>=8.5
                if score < 8.5:
                    print(f"  âœ— {role} è¯„åˆ† {score:.1f} < 8.5 (ä¸æ»¡è¶³è¦æ±‚)")
                    return False
    
    if not scores:
        print("  âœ— æ²¡æœ‰å¯ç”¨çš„è¯„åˆ†")
        return False
    
    # æ£€æŸ¥å¹³å‡åˆ†æ˜¯å¦>=9.0
    avg_score = sum(scores) / len(scores)
    if avg_score < 9.0:
        print(f"  âœ— å¹³å‡åˆ† {avg_score:.1f} < 9.0 (ä¸æ»¡è¶³è¦æ±‚)")
        return False
    
    print(f"  âœ“ è¯„åˆ†æ»¡è¶³è¦æ±‚ï¼šå¹³å‡åˆ† {avg_score:.1f} >= 9.0ï¼Œæ‰€æœ‰ä¸“å®¶ >= 8.5")
    return True


def _create_status_summary(state: REAgentState) -> str:
    """åˆ›å»ºå½“å‰çŠ¶æ€æ‘˜è¦"""
    summary_parts = []
    
    # ä»»åŠ¡ä¿¡æ¯çŠ¶æ€
    task_description = state.get("task_description", "")
    background = state.get("background", "")
    dataset_info = state.get("dataset_info", "")
    
    if task_description:
        summary_parts.append(f"âœ“ ä»»åŠ¡æè¿°: {task_description[:150]}...")
    else:
        summary_parts.append("âœ— ä»»åŠ¡æè¿°ç¼ºå¤±")
    
    if background:
        summary_parts.append(f"âœ“ èƒŒæ™¯è¦æ±‚: {background[:150]}...")
    else:
        summary_parts.append("âœ— èƒŒæ™¯è¦æ±‚ç¼ºå¤±")
    
    if dataset_info:
        summary_parts.append(f"âœ“ æ•°æ®é›†ä¿¡æ¯: {dataset_info[:150]}...")
    else:
        summary_parts.append("âœ— æ•°æ®é›†ä¿¡æ¯ç¼ºå¤±")
    
    # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœSupervisorå·²è¯»å–ï¼‰
    dataset_stats = state.get("dataset_statistics", {})
    if dataset_stats:
        summary_parts.append("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆå·²è¯»å–ï¼‰:")
        for file_path, stats in dataset_stats.items():
            summary_parts.append(f"  æ–‡ä»¶: {file_path}")
            summary_parts.append(f"    - è¡Œæ•°ï¼ˆæ ·æœ¬æ•°ï¼‰: {stats.get('num_rows', 'N/A')}")
            summary_parts.append(f"    - åˆ—æ•°ï¼ˆç‰¹å¾æ•°ï¼‰: {stats.get('num_cols', 'N/A')}")
            col_names = stats.get('column_names', [])
            if col_names:
                col_names_str = ', '.join(col_names[:10])  # æœ€å¤šæ˜¾ç¤º10ä¸ªåˆ—å
                if len(col_names) > 10:
                    col_names_str += f" ... (å…±{len(col_names)}åˆ—)"
                summary_parts.append(f"    - åˆ—å: {col_names_str}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„ä»»åŠ¡ä¿¡æ¯
    has_complete_info = task_description and background and dataset_info
    if has_complete_info:
        summary_parts.append("\nâœ“ ä»»åŠ¡ä¿¡æ¯å®Œæ•´ï¼Œå¯ä»¥è¿›è¡Œåˆ†æ")
    else:
        summary_parts.append("\nâœ— ä»»åŠ¡ä¿¡æ¯ä¸å®Œæ•´ï¼Œéœ€è¦æ›´å¤šä¿¡æ¯")
    
    # ä¸“å®¶åˆ†æçŠ¶æ€
    critiques = {
        "æ•°æ®ç®¡ç†": state.get("data_critique"),
        "æ–¹æ³•å­¦": state.get("methodology_critique"),
        "æ¨¡å‹æ¶æ„": state.get("model_critique"),
        "ç»“æœåˆ†æ": state.get("results_critique")
    }
    
    completed_analyses = sum(1 for c in critiques.values() if c is not None)
    summary_parts.append(f"\nä¸“å®¶åˆ†æ: {completed_analyses}/4 å®Œæˆ")
    
    scores = []
    for name, critique in critiques.items():
        if critique:
            if isinstance(critique, dict):
                score = critique.get("score", 0)
            elif hasattr(critique, "score"):
                score = critique.score
            else:
                score = 0
            
            if score > 0:
                scores.append(score)
                # æ˜¾ç¤ºè¯„åˆ†å’Œæ˜¯å¦æ»¡è¶³è¦æ±‚ï¼ˆ>=8.5ï¼‰
                status = "âœ“" if score >= 8.5 else "âœ—"
                summary_parts.append(f"  {status} {name}: {score:.1f}/10 {'(æ»¡è¶³è¦æ±‚)' if score >= 8.5 else '(ä¸æ»¡è¶³è¦æ±‚ï¼Œéœ€è¦>=8.5)'}")
            else:
                summary_parts.append(f"  âœ“ {name}: å·²å®Œæˆ")
        else:
            summary_parts.append(f"  âœ— {name}: å¾…åˆ†æ")
    
    # æ˜¾ç¤ºå¹³å‡åˆ†å’Œè¯„åˆ†è¦æ±‚
    if scores:
        avg_score = sum(scores) / len(scores)
        avg_status = "âœ“" if avg_score >= 9.0 else "âœ—"
        summary_parts.append(f"\n{avg_status} å¹³å‡åˆ†: {avg_score:.1f}/10 {'(æ»¡è¶³è¦æ±‚)' if avg_score >= 9.0 else '(ä¸æ»¡è¶³è¦æ±‚ï¼Œéœ€è¦>=9.0)'}")
        summary_parts.append("è¯„åˆ†è¦æ±‚: å¹³å‡åˆ†>=9.0ï¼Œæ¯ä¸ªä¸“å®¶>=8.5")
    
    # è¿­ä»£çŠ¶æ€
    iteration = state.get("iteration_count", 0)
    max_iter = state.get("max_iterations", 8)
    summary_parts.append(f"\nè¿­ä»£: {iteration}/{max_iter}")
    
    # æŠ¥å‘ŠçŠ¶æ€
    if state.get("final_report"):
        summary_parts.append("âœ“ æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ")
    
    return "\n".join(summary_parts)


def _parse_decision(response, state: Optional[REAgentState] = None) -> dict:
    """è§£æSupervisorçš„å†³ç­–"""
    import json
    
    content = response.content
    
    # å°è¯•æå–JSON
    try:
        # å¦‚æœå“åº”ä¸­åŒ…å«ä»£ç å—
        if "```json" in content:
            json_str = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            json_str = content.split("```")[1].split("```")[0].strip()
        else:
            json_str = content.strip()
        
        decision = json.loads(json_str)
        return decision
    
    except:
        # é™çº§ï¼šä»å“åº”ä¸­æå–next_action
        if "next_action" in content.lower():
            for action in ["request_info", "analyze", "discuss", "iterate", "report", "end"]:
                if action in content.lower():
                    return {
                        "reasoning": "ä»å“åº”ä¸­æ¨æ–­",
                        "next_action": action,
                        "tool_calls": []
                    }
        
        # é»˜è®¤å†³ç­–ï¼šæ£€æŸ¥ä»»åŠ¡ä¿¡æ¯æ˜¯å¦å®Œæ•´
        if state:
            task_description = state.get("task_description", "")
            background = state.get("background", "")
            dataset_info = state.get("dataset_info", "")
            
            if not (task_description and background and dataset_info):
                default_action = "request_info"
            else:
                default_action = "analyze"
        else:
            default_action = "request_info"
        
        return {
            "reasoning": "æ— æ³•è§£æå†³ç­–ï¼Œä½¿ç”¨é»˜è®¤",
            "next_action": default_action,
            "tool_calls": []
        }


def _build_task_plan(state: REAgentState) -> Dict[str, Any]:
    """ä»çŠ¶æ€ä¸­æ„å»ºä»»åŠ¡è®¡åˆ’å­—å…¸ï¼ˆç”¨äºAgentåˆ†æï¼‰"""
    task_plan = {
        "title": state.get("task_description", "Gene Regulatory Element Design Task"),
        "description": state.get("background", ""),
        "data_source": state.get("dataset_info", ""),
        "methodology": state.get("methodology", ""),
        "model_architecture": state.get("model_architecture", ""),
        "evaluation_metrics": state.get("evaluation_metrics", ""),
        "additional_info": state.get("additional_info", {})
    }
    
    # æ·»åŠ æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœSupervisorå·²è¯»å–ï¼‰
    dataset_stats = state.get("dataset_statistics", {})
    if dataset_stats:
        task_plan["dataset_statistics"] = dataset_stats
    
    return task_plan


async def supervisor_node(state: REAgentState) -> REAgentState:
    """
    SupervisorèŠ‚ç‚¹ - å†³ç­–ä¸‹ä¸€æ­¥è¡ŒåŠ¨
    
    æ ¹æ®å½“å‰çŠ¶æ€ï¼ŒSupervisorå†³å®šä¸‹ä¸€æ­¥ï¼š
    - request_info: éœ€è¦æ›´å¤šä»»åŠ¡ä¿¡æ¯
    - analyze: è°ƒç”¨ä¸“å®¶Agentåˆ†æ
    - discuss: ä¸“å®¶è®¨è®º
    - iterate: è¿­ä»£ä¼˜åŒ–
    - report: ç”ŸæˆæŠ¥å‘Š
    - end: ç»“æŸ
    """
    global _workflow_supervisor, _workflow_rag_system
    
    print("\n" + "="*60)
    print("ğŸ” Supervisor æ­£åœ¨åˆ†æçŠ¶æ€å¹¶å†³ç­–...")
    print("="*60)
    
    # è·å–æˆ–åˆ›å»ºSupervisorå®ä¾‹
    if _workflow_supervisor is None:
        if _workflow_rag_system is None:
            _workflow_rag_system = HybridRAGSystem()
        _workflow_supervisor = initialize_supervisor(rag_system=_workflow_rag_system)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è´¨ç–‘è®¨è®ºç»“æœ
    after_discussion = state.get("after_discussion", False)
    if after_discussion:
        print("\n" + "="*60)
        print("ğŸ” Supervisor æ­£åœ¨è´¨ç–‘å’Œè¯„ä¼°è®¨è®ºç»“æœ...")
        print("="*60)
        
        # æ”¶é›†æ‰€æœ‰ä¸“å®¶çš„åˆ†æç»“æœç”¨äºè´¨ç–‘
        critiques = {
            "data_management": state.get("data_critique"),
            "methodology": state.get("methodology_critique"),
            "model_architect": state.get("model_critique"),
            "result_analyst": state.get("results_critique")
        }
        
        # æ„å»ºè´¨ç–‘ä¸Šä¸‹æ–‡
        critique_summary = ""
        scores = []
        for role, critique in critiques.items():
            if critique:
                if isinstance(critique, dict):
                    score = critique.get("score", 0)
                    design_summary = critique.get("metadata", {}).get("design_summary", "")
                    detailed_design = critique.get("metadata", {}).get("detailed_design", {})
                    strengths = critique.get("strengths", [])
                    weaknesses = critique.get("weaknesses", [])
                    recommendations = critique.get("recommendations", [])
                elif hasattr(critique, "score"):
                    score = critique.score
                    design_summary = critique.metadata.get("design_summary", "") if hasattr(critique, "metadata") else ""
                    detailed_design = critique.metadata.get("detailed_design", {}) if hasattr(critique, "metadata") else {}
                    strengths = critique.strengths
                    weaknesses = critique.weaknesses
                    recommendations = critique.recommendations
                else:
                    continue
                
                role_name = {
                    "data_management": "æ•°æ®ç®¡ç†ä¸“å®¶",
                    "methodology": "æ–¹æ³•å­¦ä¸“å®¶",
                    "model_architect": "æ¨¡å‹æ¶æ„å¸ˆ",
                    "result_analyst": "ç»“æœåˆ†æå¸ˆ"
                }.get(role, role)
                
                scores.append(score)
                critique_summary += f"\nã€{role_name}ã€‘è¯„åˆ†: {score:.1f}/10\n"
                if design_summary:
                    critique_summary += f"è®¾è®¡æ–¹æ¡ˆæ‘˜è¦: {design_summary[:300]}...\n"
                if strengths:
                    critique_summary += f"ä¼˜ç‚¹: {', '.join(strengths[:3])}\n"
                if weaknesses:
                    critique_summary += f"éœ€è¦æ”¹è¿›: {', '.join(weaknesses[:3])}\n"
                if recommendations:
                    critique_summary += f"å»ºè®®: {', '.join(recommendations[:2])}\n"
                critique_summary += "\n"
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_score = sum(scores) / len(scores) if scores else 0
        
        # æ„å»ºè´¨ç–‘æç¤º
        critique_prompt = f"""You have just completed a discussion round with the expert agents. Now you need to CRITICALLY REVIEW and QUESTION their designs.

Current Expert Analysis Results:
{critique_summary}

Average Score: {avg_score:.1f}/10
Individual Scores: {', '.join([f'{s:.1f}' for s in scores])}

âš ï¸ YOUR CRITICAL REVIEW TASKS:
1. **Question the Design Quality**:
   - Are the design specifications comprehensive enough? Do they include detailed parameter values?
   - Are there missing critical parameters or hyperparameters?
   - Are the designs consistent with each other? (e.g., data preprocessing matches model input requirements)
   - Are the designs realistic and implementable?

2. **Question the Scores**:
   - Are the scores justified? Do they reflect the actual quality of the designs?
   - Are there designs that scored too high or too low?
   - Do the designs meet the strict requirements (detailed parameter specifications)?

3. **Identify Specific Issues**:
   - What specific aspects need improvement?
   - What parameters are missing or unclear?
   - What inconsistencies exist between different expert designs?

4. **Provide Critical Questions and Suggestions**:
   - Raise specific questions about each expert's design
   - Point out concrete issues that need to be addressed
   - Suggest specific improvements with parameter-level details

Please provide your critical review in JSON format:
{{
    "critical_questions": [
        "Specific question 1 about data management expert's design...",
        "Specific question 2 about methodology expert's design...",
        ...
    ],
    "identified_issues": [
        "Issue 1: Missing parameter X in model architecture...",
        "Issue 2: Inconsistency between data preprocessing and model input...",
        ...
    ],
    "score_evaluation": "Your evaluation of whether the scores are justified and whether they meet requirements",
    "recommendations": [
        "Specific recommendation 1 with parameter details...",
        "Specific recommendation 2 with parameter details...",
        ...
    ],
    "decision": "iterate" or "report",
    "reasoning": "Your reasoning for the decision based on the critical review"
}}

IMPORTANT:
- Be CRITICAL and SPECIFIC in your questions and issues
- Focus on missing parameters, inconsistencies, and design quality
- Only set decision = "report" if ALL requirements are met (avg >= 9.0, each >= 8.5, comprehensive designs)
- If any issues are found, set decision = "iterate" to continue refinement
"""
        
        # è°ƒç”¨Supervisorè¿›è¡Œè´¨ç–‘
        critique_messages = [
            SystemMessage(content=_workflow_supervisor.prompt() + "\n\nYou are now critically reviewing the expert discussion results. Be thorough and specific in your critique."),
            HumanMessage(content=critique_prompt)
        ]
        
        critique_response = await _workflow_supervisor.llm_with_tools.ainvoke(critique_messages)
        critique_text = critique_response.content if critique_response.content else ""
        
        # è§£æè´¨ç–‘ç»“æœ
        import json
        import re
        critique_data = None
        
        # å°è¯•æå–JSON
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', critique_text, re.DOTALL)
        if json_match:
            try:
                critique_data = json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        if critique_data is None:
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', critique_text, re.DOTALL)
            if json_match:
                try:
                    critique_data = json.loads(json_match.group(0))
                except json.JSONDecodeError:
                    pass
        
        # æ‰“å°è´¨ç–‘ç»“æœ
        if critique_data:
            print("\nğŸ“‹ Supervisor è´¨ç–‘ç»“æœ:")
            if critique_data.get("critical_questions"):
                print("  å…³é”®é—®é¢˜:")
                for i, q in enumerate(critique_data.get("critical_questions", [])[:5], 1):
                    print(f"    {i}. {q[:150]}...")
            if critique_data.get("identified_issues"):
                print("  å‘ç°çš„é—®é¢˜:")
                for i, issue in enumerate(critique_data.get("identified_issues", [])[:5], 1):
                    print(f"    {i}. {issue[:150]}...")
            if critique_data.get("score_evaluation"):
                print(f"  è¯„åˆ†è¯„ä¼°: {critique_data.get('score_evaluation', '')[:200]}...")
            
            # å°†è´¨ç–‘ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
            critique_message_content = f"""Supervisor Critical Review After Discussion:

Critical Questions:
{chr(10).join(f"- {q}" for q in critique_data.get("critical_questions", [])[:10])}

Identified Issues:
{chr(10).join(f"- {issue}" for issue in critique_data.get("identified_issues", [])[:10])}

Score Evaluation: {critique_data.get("score_evaluation", "")}

Recommendations:
{chr(10).join(f"- {rec}" for rec in critique_data.get("recommendations", [])[:10])}

Decision: {critique_data.get("decision", "iterate")}
Reasoning: {critique_data.get("reasoning", "")}
"""
            state["messages"].append(HumanMessage(content=critique_message_content))
            
            # æ ¹æ®è´¨ç–‘ç»“æœå†³å®šä¸‹ä¸€æ­¥
            decision = critique_data.get("decision", "iterate")
            if decision == "report":
                # æ£€æŸ¥è¯„åˆ†æ˜¯å¦çœŸçš„æ»¡è¶³è¦æ±‚
                if _check_scores_meet_requirements(state):
                    state["next_action"] = "report"
                else:
                    print("  âš ï¸ Supervisorå»ºè®®reportï¼Œä½†è¯„åˆ†ä¸æ»¡è¶³è¦æ±‚ï¼Œå¼ºåˆ¶iterate")
                    state["next_action"] = "iterate"
            else:
                state["next_action"] = "iterate"
        else:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨è¯„åˆ†æ£€æŸ¥å†³å®š
            print("  âš ï¸ è´¨ç–‘ç»“æœè§£æå¤±è´¥ï¼Œä½¿ç”¨è¯„åˆ†æ£€æŸ¥å†³å®šä¸‹ä¸€æ­¥")
            if _check_scores_meet_requirements(state):
                state["next_action"] = "report"
            else:
                state["next_action"] = "iterate"
        
        # æ¸…é™¤æ ‡å¿—
        del state["after_discussion"]
        
        print(f"\nâœ“ Supervisorè´¨ç–‘å®Œæˆï¼Œå°†åŸºäºè´¨ç–‘ç»“æœè¿›è¡Œå†³ç­–")
        
        # å°†è´¨ç–‘ç»“æœä¿å­˜åˆ°çŠ¶æ€ä¸­ï¼Œä¾›åç»­å†³ç­–ä½¿ç”¨
        if critique_data:
            state["supervisor_critique"] = critique_data
        else:
            # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„è´¨ç–‘ç»“æœ
            state["supervisor_critique"] = {
                "critical_questions": [],
                "identified_issues": [],
                "score_evaluation": "è¯„åˆ†æ£€æŸ¥ä¸­...",
                "recommendations": [],
                "decision": "iterate" if not _check_scores_meet_requirements(state) else "report",
                "reasoning": "åŸºäºè¯„åˆ†è¦æ±‚è¿›è¡Œå†³ç­–"
            }
        
        # ç»§ç»­æ‰§è¡Œæ­£å¸¸çš„supervisorå†³ç­–æµç¨‹ï¼ˆè´¨ç–‘ç»“æœä¼šå½±å“å†³ç­–ï¼‰
    
    # æ„å»ºç³»ç»Ÿæç¤ºï¼ˆåŒ…å«å·¥ä½œæµè¯´æ˜ï¼‰
    base_prompt = _workflow_supervisor.prompt()
    workflow_details = """

Your Responsibilities:
1. **Read and Understand**:
   - First, use `read_file` tool to read the local task file `task/task_description.json`;
   - Then, extract the task name, goal, constraints, and data set path etc. from the task file;
   - **CRITICAL**: Use `read_file` tool to read the dataset file (CSV/TXT etc.) specified in the task file, and analyze its characteristics:
     * Count the number of rows (samples) and columns (features)
     * Identify all column names
     * Print these statistics clearly for reference
   - Based on the task information and dataset statistics, summarize the background and typical methods for this type of task.
2. **Plan and Assign**:
   - Based on the task JSON and dataset statistics, give a overall experimental plan;
   - Allocate the task to four expert agents (data_management, methodology, model_architect, result_analyst) in a structured and clear format, including the data/method/model/result metrics that each agent needs to focus on, and the detailed parameter specifications that need to be designed.
3. **Question and Evaluate**:
   - Identify the uncertainties or potential risks in the task, and propose questions and suggestions;
4. **Coordinate and Synthesize**:
   - Coordinate the four expert agents to work in parallel and iteratively discuss;
   - Collect their designs (including detailed parameter specifications and configuration details), and synthesize a complete pipeline report.

Workflow (three stages):
1. **Task Understanding and Dataset Analysis Stage (Supervisor-led)**
   - Use `read_file` to read `task/task_description.json`;
   - **MANDATORY**: Use `read_file` to read the dataset file specified in the task file (e.g., CSV file);
   - Analyze and report dataset statistics: number of rows, number of columns, column names;
   - Based on the task information and dataset characteristics, summarize the background and typical methods for this type of task;
   - Give an overall experimental plan;
   - Divide the plan into four sub-tasks, and allocate them to four expert agents.
2. **Expert Agent Design and Execution Stage**
   - data_management:
     - Use `read_file` to read the data set specified in the task (CSV/TXT etc.), and **CAREFULLY ANALYZE** each column's meaning, data type, and potential role:
       * **CRITICAL**: For each column, infer its meaning from column name, data patterns, and context (e.g., sequence columns, expression/activity columns, metadata columns, feature columns);
       * **CRITICAL**: Identify data types (sequences, numeric values, categorical labels, etc.) and their distributions;
       * **CRITICAL**: Determine which columns are input features, which are target variables, and which are metadata/auxiliary information;
       * **CRITICAL**: Analyze sequence length distribution if sequence columns exist (mean, median, range, outliers);
       * **CRITICAL**: Identify dataset type (MPRA/RNA-seq/ChIP-seq/ATAC-seq/etc.) based on column names and data characteristics;
     - CRITICALLY analyze dataset characteristics: dataset type, sequence length distribution, dataset size (number of samples), and data volume;
     - **CRITICAL**: Based on the inferred column meanings and dataset characteristics, select **REASONABLE and APPROPRIATE** data processing strategies:
       * For large datasets (>10K samples): focus on aggressive quality control, cleaning, filtering, and outlier removal;
       * For small datasets (<1K samples): prioritize data augmentation techniques (sequence augmentation, synthetic data generation);
       * For medium datasets (1K-10K): balance between quality control and augmentation;
       * **Tailor preprocessing steps to the specific data types and column meanings** (e.g., sequence-specific preprocessing for sequence columns, normalization strategies for expression columns);
     - Use `rag_search` to search the knowledge related to data preprocessing, division and enhancement strategies tailored to specific dataset types, column meanings, and sizes;
     - Give specific design specifications with detailed parameter values for data reading, preprocessing (tailored to dataset characteristics and column meanings), and division in the design.
   - methodology:
     - Use `rag_search` to search the knowledge related to the training method;
     - Design the corresponding training process with comprehensive parameter specifications and hyperparameter settings.
   - model_architect:
     - Analyze dataset size from dataset statistics to determine appropriate model complexity and parameter count;
     - Use `rag_search` to search the knowledge related to neural network architecture design, innovative architectures (attention, residual connections, etc.), and model complexity control;
     - Flexibly control model parameters and complexity based on data volume: smaller models for limited data, larger models for abundant data;
     - Design innovative, effective, and robust model structures with comprehensive parameter specifications and hyperparameter settings;
     - Provide clear rationale linking dataset characteristics to architectural choices.
   - result_analyst:
     - Use `rag_search` to search the knowledge related to evaluation metrics and statistical testing;
     - Design the corresponding evaluation scheme with comprehensive parameter specifications and hyperparameter settings.
3. **Iterative Discussion and Termination Decision (Supervisor-led)**
   - After the initial analysis, enter the expert discussion stage (discuss), and synthesize the opinions from different perspectives;
   - You need to raise questions and give suggestions to the expert's design, and decide whether to enter the next round of iteration (iterate);
   - When the design scheme with comprehensive parameter specifications is complete, generate the final experimental pipeline report and end (report -> end).

Final Report Structure:
The final report should focus on COMPREHENSIVE DESIGN SPECIFICATIONS with detailed parameter configurations, NOT on analysis of strengths/weaknesses.
The report should include four main sections with detailed design plans and parameter specifications:
1. **Data Usage Plan**: Complete design specifications for data source selection, preprocessing pipeline parameters, split strategy (ratios), augmentation methods with parameters, and quality control procedures
2. **Method Design**: Complete design specifications for training methodology including loss function parameters, optimization algorithm hyperparameters (learning rate, momentum, weight decay, etc.), regularization techniques with coefficients, and prior knowledge integration parameters
3. **Model Design**: Complete design specifications for neural network architecture including:
   - Layer-by-layer architecture with EXACT dimensions (input/output sizes, kernel sizes, stride, padding)
   - ALL hyperparameters: learning rate schedule, batch size, dropout rates, weight decay, optimizer parameters (Adam beta1/beta2, SGD momentum, etc.)
   - Activation functions and their parameters (LeakyReLU negative_slope, ELU alpha, etc.)
   - Regularization parameters (L1/L2 coefficients, dropout probabilities, batch norm momentum)
   - Initialization strategies and parameters (Xavier/Glorot parameters, He initialization parameters)
   - Training configuration (epochs, early stopping criteria, learning rate schedule parameters)
   - Model capacity (total parameter count, FLOPs estimation)
   - Interpretability mechanisms with their parameters
4. **Result Summary**: Complete design specifications for evaluation metrics with thresholds, validation strategy parameters, statistical testing procedures with significance levels, and biological validation methods

Each section MUST include:
- Detailed step-by-step design specifications
- COMPREHENSIVE parameter configurations and hyperparameter settings (this is CRITICAL)
- Specific parameter values, not just general descriptions
- Design rationale and justification for parameter choices
- Expected outcomes and how to interpret results

Decision rules (STRICT SCORING REQUIREMENTS):
- If task information is incomplete or missing, next_action = "request_info" (ask user for more details)
- If task information is complete and design is needed, next_action = "analyze"
- After initial analysis, next_action = "discuss"
- After discussion, check if scores meet requirements:
  * CRITICAL: Only set next_action = "report" if ALL of the following conditions are met:
    1. Average score across all expert agents >= 9.0/10
    2. Each individual expert agent score >= 8.5/10
    3. All agents have provided complete design plans with detailed parameter specifications
  * If scores do NOT meet these requirements, next_action = "iterate" (continue refinement)
- If completed and scores meet requirements, next_action = "end"

Available tools:
- read_file: Read content from local files (for reading task JSON and data files, especially CSV datasets)
- write_file: Write content to local files (for saving intermediate results or final report, if needed)

When using tools:
- Always use `read_file` to read the dataset file specified in the task description
- After reading a CSV dataset file, the system will automatically analyze and display:
  * Number of rows (samples)
  * Number of columns (features)
  * Column names
- Use these statistics to better understand the dataset and provide more accurate task assignments to expert agents

Please always think in a structured way and clearly state your reasoning for the decision.
"""
    system_prompt = base_prompt + workflow_details
    
    # æ„å»ºå½“å‰çŠ¶æ€æ‘˜è¦
    status_summary = _create_status_summary(state)
    
    # å¦‚æœæœ‰Supervisorçš„è´¨ç–‘ç»“æœï¼Œæ·»åŠ åˆ°çŠ¶æ€æ¶ˆæ¯ä¸­
    supervisor_critique = state.get("supervisor_critique", {})
    critique_context = ""
    if supervisor_critique:
        critique_context = f"""

ğŸ” SUPERVISOR CRITICAL REVIEW (After Discussion):
Critical Questions:
{chr(10).join(f"- {q}" for q in supervisor_critique.get("critical_questions", [])[:5])}

Identified Issues:
{chr(10).join(f"- {issue}" for issue in supervisor_critique.get("identified_issues", [])[:5])}

Score Evaluation: {supervisor_critique.get("score_evaluation", "")}

Recommendations:
{chr(10).join(f"- {rec}" for rec in supervisor_critique.get("recommendations", [])[:5])}

You should consider these critical questions and issues when making your decision.
"""
        # æ¸…é™¤è´¨ç–‘ç»“æœï¼ˆå·²ä½¿ç”¨ï¼‰
        del state["supervisor_critique"]
    
    status_message = HumanMessage(content=f"""
Current state:
{status_summary}
{critique_context}

âš ï¸ STRICT SCORING REQUIREMENTS FOR REPORT GENERATION:
- You can ONLY set next_action = "report" if ALL of the following conditions are met:
  1. Average score across all expert agents >= 9.0/10
  2. Each individual expert agent score >= 8.5/10
  3. All agents have provided complete design plans with detailed parameter specifications
- If scores do NOT meet these requirements, you MUST set next_action = "iterate" to continue refinement

Please analyze the current state and decide the next action. Set the next_action field to one of the following:
- "request_info": Need more task information from user
- "analyze": Need to call expert agents for initial design
- "discuss": Need to facilitate discussion among expert agents to refine designs
- "iterate": Need more information or iterative optimization (USE THIS if scores < requirements)
- "report": Can generate a final report (ONLY if scores meet strict requirements above)
- "end": Work completed

Additionally, when you decide to call expert agents for design (next_action = "analyze" or "discuss" or "iterate"), you SHOULD provide a structured task plan for each expert agent.
Return it as a JSON object under the key "agent_task_plans", with the following structure:
{{
  "data_management": {{
    "title": "...",
    "description": "...",
    "task_prompt": "A detailed, specific task prompt for the data management expert. **CRITICAL REQUIREMENTS**:\n1. **Column Analysis**: When reading the dataset, CAREFULLY ANALYZE and INFER the meaning of each column:\n   - Examine column names, data patterns, and values to determine what each column represents (e.g., sequence data, expression/activity values, metadata, features);\n   - Identify data types (sequences, numeric, categorical) and their distributions;\n   - Determine which columns are input features, target variables, or auxiliary metadata;\n   - For sequence columns, analyze sequence length distribution (mean, median, range, outliers);\n   - Infer dataset type (MPRA/RNA-seq/ChIP-seq/ATAC-seq/etc.) based on column characteristics;\n2. **Dataset Characteristics Analysis**: Analyze dataset type, sequence length distribution, dataset size (number of samples), and data volume;\n3. **Reasonable Processing Strategy Selection**: Based on the inferred column meanings and dataset characteristics, select REASONABLE and APPROPRIATE data processing strategies:\n   - For large datasets (>10K samples): focus on aggressive quality control, cleaning, filtering, and outlier removal;\n   - For small datasets (<1K samples): prioritize data augmentation techniques (sequence augmentation, synthetic data generation);\n   - For medium datasets (1K-10K): balance between quality control and augmentation;\n   - **Tailor preprocessing steps to specific data types and column meanings** (e.g., sequence-specific preprocessing for sequence columns, normalization for expression columns);\n4. **Design Specifications**: Explain exactly what data to focus on, what parameter specifications to design for preprocessing tailored to dataset characteristics and column meanings. This should be comprehensive and tailored to the specific task, dataset characteristics, and inferred column meanings.",
  }},
  "methodology": {{
    "title": "...",
    "description": "...",
    "task_prompt": "A detailed, specific task prompt for the methodology expert, explaining exactly what training methodology to design, what loss functions to consider, what optimization strategies to use, etc.",
    ...
  }},
  "model_architect": {{
    "title": "...",
    "description": "...",
    "task_prompt": "A detailed, specific task prompt for the model architect expert. CRITICALLY: First analyze dataset size (number of samples) to determine appropriate model complexity and parameter count. For small datasets (<1K samples), design compact architectures with fewer parameters and strong regularization. For large datasets (>10K samples), design more expressive architectures. Encourage innovative, effective, and robust architecture designs (attention mechanisms, residual connections, multi-scale features, etc.). Explain exactly what architecture to design, what layer specifications to provide, what parameter estimations to make based on dataset size, etc. Provide clear rationale linking dataset characteristics to architectural choices.",
    ...
  }},
  "result_analyst": {{
    "title": "...",
    "description": "...",
    "task_prompt": "A detailed, specific task prompt for the result analyst, explaining exactly what evaluation metrics to use, what statistical tests to design, what validation strategy to implement, etc.",
    ...
  }}
}}

IMPORTANT: The "task_prompt" field for each agent should be:
- Detailed and specific to that agent's role
- Include concrete requirements and expectations
- Specify what parameter specifications need to be designed
- Reference the specific task context and dataset
- Be tailored to the current iteration and refinement needs

Please return the decision in JSON format (you can include optional fields as needed):
{{
    "reasoning": "Your reasoning process",
    "next_action": "request_info/analyze/discuss/iterate/report/end",
    "tool_calls": ["List of tools to call"],
    "agent_task_plans": {{
        "data_management": {{ ... }},
        "methodology": {{ ... }},
        "model_architect": {{ ... }},
        "result_analyst": {{ ... }}
    }}
}}
""")
    
    # è°ƒç”¨LLMï¼ˆå¸¦å·¥å…·ï¼‰
    messages = state.get("messages", [])
    system_message = SystemMessage(content=system_prompt)
    
    # æ¶ˆæ¯å†å²ç®¡ç†ï¼šæ™ºèƒ½æ€»ç»“è€Œä¸æ˜¯ç®€å•æˆªæ–­
    # ä¼°ç®— tokensï¼ˆç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦ï¼‰
    total_chars = sum(len(str(msg.content)) if hasattr(msg, 'content') and msg.content else 0 for msg in messages)
    estimated_tokens = total_chars / 4
    
    MAX_MESSAGES = 50
    MAX_ESTIMATED_TOKENS = 200000  # å¦‚æœè¶…è¿‡ 200k tokensï¼Œè¿›è¡Œæ€»ç»“
    
    if len(messages) > MAX_MESSAGES or estimated_tokens > MAX_ESTIMATED_TOKENS:
        if estimated_tokens > MAX_ESTIMATED_TOKENS:
            print(f"âš ï¸ æ¶ˆæ¯å†å²è¿‡é•¿ï¼ˆä¼°è®¡ {estimated_tokens:.0f} tokensï¼‰ï¼Œè¿›è¡Œæ™ºèƒ½æ€»ç»“...", flush=True)
            # ä¿ç•™æœ€è¿‘çš„ 15 æ¡æ¶ˆæ¯ï¼Œæ€»ç»“ä¹‹å‰çš„æ¶ˆæ¯
            recent_messages = messages[-15:]
            old_messages = messages[:-15]
            
            if old_messages:
                # ä½¿ç”¨ LLM æ€»ç»“æ—§æ¶ˆæ¯
                try:
                    summary = await summarize_messages(old_messages, max_summary_length=2000)
                    # åˆ›å»ºæ€»ç»“æ¶ˆæ¯
                    summary_message = HumanMessage(
                        content=f"[Previous Conversation Summary]\n{summary}\n\n[Continuing with recent messages...]"
                    )
                    # ç”¨æ€»ç»“æ¶ˆæ¯æ›¿æ¢æ—§æ¶ˆæ¯
                    messages = [summary_message] + recent_messages
                    print(f"  âœ“ æ¶ˆæ¯å†å²å·²æ€»ç»“ï¼š{len(old_messages)} æ¡æ—§æ¶ˆæ¯ -> 1 æ¡æ€»ç»“æ¶ˆæ¯", flush=True)
                except Exception as e:
                    print(f"  âš ï¸ æ¶ˆæ¯æ€»ç»“å¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•æˆªæ–­", flush=True)
                    # å¦‚æœæ€»ç»“å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æˆªæ–­
                    messages = messages[-MAX_MESSAGES:]
        else:
            print(f"âš ï¸ æ¶ˆæ¯å†å²è¿‡é•¿ ({len(messages)}æ¡)ï¼Œæˆªæ–­è‡³æœ€è¿‘{MAX_MESSAGES}æ¡", flush=True)
            # ä¿ç•™æœ€è¿‘çš„Næ¡æ¶ˆæ¯ï¼ˆä¼˜å…ˆä¿ç•™éå·¥å…·æ¶ˆæ¯ï¼‰
            recent_messages = messages[-MAX_MESSAGES:]
            messages = recent_messages
    
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆä¸ç›´æ¥ä¿®æ”¹state["messages"]ï¼Œå…ˆå¤„ç†å·¥å…·è°ƒç”¨ï¼‰
    current_messages = [system_message] + messages + [status_message]
    
    # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆæœ€å¤š3è½®ï¼‰
    max_tool_iterations = 3
    tool_iteration = 0
    response = None
    new_messages_to_add = []  # è®°å½•æ‰€æœ‰éœ€è¦æ·»åŠ åˆ°state["messages"]çš„æ–°æ¶ˆæ¯
    
    while tool_iteration < max_tool_iterations:
        response = await _workflow_supervisor.llm_with_tools.ainvoke(current_messages)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        has_tool_calls = False
        if hasattr(response, "tool_calls") and response.tool_calls:
            has_tool_calls = True
        elif hasattr(response, "additional_kwargs") and response.additional_kwargs.get("tool_calls"):
            has_tool_calls = True
        
        if not has_tool_calls:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿™æ˜¯æœ€ç»ˆå“åº”
            break
        
        # æœ‰å·¥å…·è°ƒç”¨ï¼Œéœ€è¦æ‰§è¡Œ
        tool_iteration += 1
        current_messages.append(response)
        new_messages_to_add.append(response)
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        from langchain_core.messages import ToolMessage
        tool_calls_list = []
        if hasattr(response, "tool_calls") and response.tool_calls:
            tool_calls_list = response.tool_calls
        elif hasattr(response, "additional_kwargs") and response.additional_kwargs.get("tool_calls"):
            tool_calls_list = response.additional_kwargs["tool_calls"]
        
        for tool_call in tool_calls_list:
            # å¤„ç†ä¸åŒæ ¼å¼çš„å·¥å…·è°ƒç”¨
            if isinstance(tool_call, dict):
                tool_name = tool_call.get("name", "") or tool_call.get("function", {}).get("name", "")
                tool_args = tool_call.get("args", {}) or tool_call.get("function", {}).get("arguments", {})
                tool_call_id = tool_call.get("id", "") or tool_call.get("function", {}).get("id", "")
            else:
                tool_name = getattr(tool_call, "name", "")
                tool_args = getattr(tool_call, "args", {})
                tool_call_id = getattr(tool_call, "id", "")
            
            if isinstance(tool_args, str):
                import json
                try:
                    tool_args = json.loads(tool_args)
                except:
                    tool_args = {}
            
            # æŸ¥æ‰¾å¹¶æ‰§è¡Œå·¥å…·
            tool = next((t for t in _workflow_supervisor._tools if t.name == tool_name), None)
            if tool:
                try:
                    # æ˜¾ç¤ºå·¥å…·ä½¿ç”¨ä¿¡æ¯
                    print(f"  ğŸ”§ Supervisor æ­£åœ¨ä½¿ç”¨å·¥å…·: {tool_name}", flush=True)
                    if tool_args:
                        print(f"     å‚æ•°: {tool_args}", flush=True)
                    
                    if hasattr(tool, "_arun"):
                        result = await tool._arun(**tool_args)
                    else:
                        result = tool._run(**tool_args)
                    
                    # å¦‚æœè¯»å–äº†æ•°æ®é›†æ–‡ä»¶ï¼Œè§£æå¹¶æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                    if tool_name == "read_file" and isinstance(result, dict) and result.get("success"):
                        file_data = result.get("data", {})
                        file_path = tool_args.get("file_path", "")
                        file_type = file_data.get("file_type", "text")
                        content = file_data.get("content", "")
                        
                        # å¦‚æœæ˜¯CSVæ–‡ä»¶ï¼Œè§£æç»Ÿè®¡ä¿¡æ¯
                        if (file_path.lower().endswith(".csv") or file_type == "csv") and content:
                            try:
                                import csv
                                import io
                                
                                # å¤„ç†ä¸åŒæ ¼å¼çš„content
                                csv_content = content
                                if isinstance(content, dict):
                                    # å¦‚æœcontentæ˜¯å­—å…¸ï¼ˆJSONè§£æåçš„ï¼‰ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                                    csv_content = str(content)
                                elif not isinstance(content, str):
                                    csv_content = str(content)
                                
                                # è§£æCSV
                                csv_reader = csv.reader(io.StringIO(csv_content))
                                rows = list(csv_reader)
                                
                                if rows and len(rows) > 0:
                                    num_rows = len(rows) - 1  # å‡å»è¡¨å¤´
                                    num_cols = len(rows[0]) if rows else 0
                                    column_names = rows[0] if rows else []
                                    
                                    print(f"\n  ğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ ({file_path}):", flush=True)
                                    print(f"     âœ“ è¡Œæ•° (æ ·æœ¬æ•°): {num_rows}", flush=True)
                                    print(f"     âœ“ åˆ—æ•° (ç‰¹å¾æ•°): {num_cols}", flush=True)
                                    print(f"     âœ“ åˆ—åç§° ({len(column_names)}ä¸ª):", flush=True)
                                    for i, col_name in enumerate(column_names, 1):
                                        print(f"        {i}. {col_name}", flush=True)
                                    
                                    # ä¿å­˜åˆ°stateä¸­
                                    if "dataset_statistics" not in state:
                                        state["dataset_statistics"] = {}
                                    state["dataset_statistics"][file_path] = {
                                        "num_rows": num_rows,
                                        "num_cols": num_cols,
                                        "column_names": column_names
                                    }
                                    
                                    # å¯¹äºå¤§å‹CSVæ–‡ä»¶ï¼Œåªè¿”å›ç»Ÿè®¡ä¿¡æ¯æ‘˜è¦ï¼Œä¸è¿”å›å®Œæ•´å†…å®¹
                                    # è¿™æ ·å¯ä»¥é¿å…æ¶ˆæ¯å†å²è¿‡é•¿
                                    MAX_CONTENT_LENGTH = 10000  # 10KB
                                    if len(csv_content) > MAX_CONTENT_LENGTH:
                                        # åªè¿”å›ç»Ÿè®¡ä¿¡æ¯æ‘˜è¦
                                        summary_result = {
                                            "success": True,
                                            "data": {
                                                "file_path": file_path,
                                                "file_type": "csv",
                                                "summary": f"Large CSV file ({num_rows} rows, {num_cols} columns)",
                                                "dataset_statistics": {
                                                    "num_rows": num_rows,
                                                    "num_cols": num_cols,
                                                    "column_names": column_names
                                                },
                                                "size_bytes": len(csv_content),
                                                "line_count": len(rows),
                                                "note": "Full content not included due to size. Statistics extracted."
                                            }
                                        }
                                        result = summary_result
                                    else:
                                        # å°æ–‡ä»¶ï¼Œä¿ç•™å®Œæ•´å†…å®¹ï¼Œä½†æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                                        if isinstance(result, dict) and "data" in result:
                                            result["data"]["dataset_statistics"] = {
                                                "num_rows": num_rows,
                                                "num_cols": num_cols,
                                                "column_names": column_names
                                            }
                            except Exception as e:
                                print(f"  âš ï¸ æ— æ³•è§£æCSVæ–‡ä»¶ {file_path}: {e}", flush=True)
                                import traceback
                                traceback.print_exc()
                    
                    # å¤„ç†å·¥å…·ç»“æœï¼Œå¯¹äºè¿‡é•¿çš„å†…å®¹è¿›è¡Œæˆªæ–­
                    MAX_TOOL_RESULT_LENGTH = 20000  # 20KB
                    if isinstance(result, dict):
                        result_data = result.get("data", {})
                        if isinstance(result_data, dict):
                            # æ£€æŸ¥contentå­—æ®µ
                            if "content" in result_data and isinstance(result_data["content"], str):
                                content = result_data["content"]
                                if len(content) > MAX_TOOL_RESULT_LENGTH:
                                    # æˆªæ–­å†…å®¹å¹¶æ·»åŠ æç¤º
                                    truncated = content[:MAX_TOOL_RESULT_LENGTH]
                                    result_data["content"] = truncated + f"\n\n[å†…å®¹å·²æˆªæ–­ï¼ŒåŸå§‹é•¿åº¦: {len(content)} å­—ç¬¦]"
                                    result_data["truncated"] = True
                                    result_data["original_length"] = len(content)
                                    result["data"] = result_data
                    
                    result_content = str(result) if not isinstance(result, dict) else str(result.get("data", result))
                    # å¦‚æœç»“æœå†…å®¹ä»ç„¶è¿‡é•¿ï¼Œç›´æ¥æˆªæ–­
                    if len(result_content) > MAX_TOOL_RESULT_LENGTH:
                        result_content = result_content[:MAX_TOOL_RESULT_LENGTH] + f"\n\n[å†…å®¹å·²æˆªæ–­ï¼ŒåŸå§‹é•¿åº¦: {len(str(result))} å­—ç¬¦]"
                    
                    tool_msg = ToolMessage(
                        content=result_content,
                        tool_call_id=tool_call_id
                    )
                    current_messages.append(tool_msg)
                    new_messages_to_add.append(tool_msg)
                except Exception as e:
                    tool_msg = ToolMessage(
                        content=f"Tool execution failed: {str(e)}",
                        tool_call_id=tool_call_id
                    )
                    current_messages.append(tool_msg)
                    new_messages_to_add.append(tool_msg)
            else:
                tool_msg = ToolMessage(
                    content=f"Tool '{tool_name}' not found",
                    tool_call_id=tool_call_id
                )
                current_messages.append(tool_msg)
                new_messages_to_add.append(tool_msg)
    
    # æ›´æ–°æ¶ˆæ¯å†å²ï¼ˆå¸¦æˆªæ–­ä¿æŠ¤ï¼‰
    state["messages"].append(status_message)
    # æ·»åŠ æ‰€æœ‰å·¥å…·è°ƒç”¨ç›¸å…³çš„æ¶ˆæ¯å’Œæœ€ç»ˆå“åº”
    for msg in new_messages_to_add:
        state["messages"].append(msg)
    if response and response not in new_messages_to_add:
        state["messages"].append(response)
    
    # å†æ¬¡æ£€æŸ¥æ¶ˆæ¯å†å²é•¿åº¦ï¼Œç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
    MAX_MESSAGES_IN_STATE = 100  # stateä¸­ä¿ç•™æ›´å¤šæ¶ˆæ¯ç”¨äºæ—¥å¿—ï¼Œä½†LLMè°ƒç”¨æ—¶åªä½¿ç”¨æœ€è¿‘çš„
    if len(state["messages"]) > MAX_MESSAGES_IN_STATE:
        print(f"âš ï¸ çŠ¶æ€æ¶ˆæ¯å†å²è¿‡é•¿ ({len(state['messages'])}æ¡)ï¼Œæˆªæ–­è‡³æœ€è¿‘{MAX_MESSAGES_IN_STATE}æ¡", flush=True)
        state["messages"] = state["messages"][-MAX_MESSAGES_IN_STATE:]
    
    # è§£æå†³ç­–
    decision = _parse_decision(response, state)
    proposed_action = decision.get("next_action", state.get("next_action", "request_info"))

    # å¦‚æœSupervisoræä¾›äº†æŒ‰è§’è‰²åˆ’åˆ†çš„ä»»åŠ¡è®¡åˆ’ï¼Œå†™å…¥stateï¼Œä¾›åç»­expert agentä½¿ç”¨
    agent_plans = decision.get("agent_task_plans") or decision.get("agent_tasks")
    if isinstance(agent_plans, dict):
        state["agent_task_plans"] = agent_plans

    # ä¸¥æ ¼è¯„åˆ†æ£€æŸ¥ï¼šå¦‚æœSupervisoræƒ³è¿›å…¥reporté˜¶æ®µï¼Œå¿…é¡»å…ˆæ£€æŸ¥è¯„åˆ†æ˜¯å¦æ»¡è¶³è¦æ±‚
    if proposed_action == "report":
        scores_meet_requirements = _check_scores_meet_requirements(state)
        if not scores_meet_requirements:
            print("âš ï¸ è¯„åˆ†æœªè¾¾åˆ°è¦æ±‚ï¼Œå¼ºåˆ¶è¿›å…¥è¿­ä»£é˜¶æ®µ")
            print("   è¦æ±‚ï¼šå¹³å‡åˆ†>=9.0ï¼Œæ¯ä¸ªä¸“å®¶>=8.5")
            proposed_action = "iterate"
    
    state["next_action"] = proposed_action

    # å®‰å…¨ä¿æŠ¤ï¼šå¦‚æœè¿­ä»£æ¬¡æ•°å·²è¾¾åˆ°ä¸Šé™ï¼Œåˆ™å¼ºåˆ¶è¿›å…¥æŠ¥å‘Šé˜¶æ®µï¼Œé¿å…æ— é™ iterate å¾ªç¯
    iteration = state.get("iteration_count", 0)
    max_iter = state.get("max_iterations", 8)
    if iteration >= max_iter and state["next_action"] in ("iterate", "analyze"):
        print(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iter})ï¼Œå¼ºåˆ¶è¿›å…¥æŠ¥å‘Šé˜¶æ®µ")
        state["next_action"] = "report"
    
    next_action = state.get("next_action", "request_info")
    print(f"âœ“ Supervisorå†³ç­–: next_action = {next_action}")
    
    return state


async def request_info_node(state: REAgentState) -> REAgentState:
    """
    è¯·æ±‚ä¿¡æ¯èŠ‚ç‚¹ - å½“ä»»åŠ¡ä¿¡æ¯ä¸å®Œæ•´æ—¶ï¼Œè¯·æ±‚ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯
    """
    print("\n" + "="*60)
    print("ğŸ“‹ è¯·æ±‚æ›´å¤šä»»åŠ¡ä¿¡æ¯")
    print("="*60)
    
    # æ£€æŸ¥ç¼ºå¤±çš„ä¿¡æ¯
    missing_info = []
    if not state.get("task_description"):
        missing_info.append("ä»»åŠ¡æè¿° (task_description)")
    if not state.get("background"):
        missing_info.append("èƒŒæ™¯è¦æ±‚ (background)")
    if not state.get("dataset_info"):
        missing_info.append("æ•°æ®é›†ä¿¡æ¯ (dataset_info)")
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼Œè¯·æ±‚ç¼ºå¤±ä¿¡æ¯
    request_message = HumanMessage(
        content=f"è¯·æä¾›ä»¥ä¸‹ç¼ºå¤±çš„ä¿¡æ¯ï¼š\n" + "\n".join(f"- {info}" for info in missing_info)
    )
    state["messages"].append(request_message)
    
    # è®¾ç½®next_actionä¸ºç­‰å¾…ç”¨æˆ·è¾“å…¥
    state["next_action"] = "waiting_user_input"
    
    print(f"âš ï¸ éœ€è¦ç”¨æˆ·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š{', '.join(missing_info)}")
    
    return state


async def analyze_node(state: REAgentState) -> REAgentState:
    """
    åˆ†æèŠ‚ç‚¹ - å¹¶è¡Œè°ƒç”¨æ‰€æœ‰ä¸“å®¶Agentè¿›è¡Œåˆ†æ
    """
    global _workflow_supervisor
    
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹å¹¶è¡Œåˆ†æ - è°ƒç”¨æ‰€æœ‰ä¸“å®¶Agent")
    print("="*60)
    
    # ç¡®ä¿ä¸“å®¶Agentå·²åˆå§‹åŒ–
    global _workflow_expert_agents, _workflow_rag_system
    if _workflow_expert_agents is None:
        if _workflow_rag_system is None:
            _workflow_rag_system = HybridRAGSystem()
        _workflow_expert_agents = initialize_agents_with_rag(rag_system=_workflow_rag_system)
    
    # è·å–Supervisoråˆ†é…çš„æŒ‰è§’è‰²ä»»åŠ¡è®¡åˆ’ï¼ˆå¦‚æœæœ‰ï¼‰
    agent_task_plans = state.get("agent_task_plans", {}) or {}

    # å¹¶è¡Œè°ƒç”¨æ‰€æœ‰ä¸“å®¶Agent
    import asyncio
    tasks = {}
    for role, agent in _workflow_expert_agents.items():
        # å¦‚æœSupervisorä¸ºè¯¥è§’è‰²æä¾›äº†ä¸“é—¨çš„task_planï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨
        role_plan = agent_task_plans.get(role)
        if isinstance(role_plan, dict):
            task_plan = role_plan
        else:
            # å›é€€ï¼šä½¿ç”¨å…¨å±€çŠ¶æ€æ„å»ºçš„é€šç”¨ä»»åŠ¡è®¡åˆ’
            task_plan = _build_task_plan(state)
        tasks[role] = agent.analyze(task_plan, state)
    
    results = await asyncio.gather(*tasks.values(), return_exceptions=True)
    
    # å¤„ç†ç»“æœ
    critiques = {}
    for (role, _), result in zip(tasks.items(), results):
        if isinstance(result, Exception):
            print(f"âš ï¸ Agent {role} æ‰§è¡Œå¤±è´¥: {result}")
            critiques[role] = CritiqueResult(
                agent_role=role,
                score=0.0,
                strengths=[],
                weaknesses=[f"æ‰§è¡Œé”™è¯¯: {str(result)}"],
                recommendations=[],
                confidence=0.0,
                metadata={"error": str(result)}
            )
        else:
            critiques[role] = result
    
    # æ›´æ–°çŠ¶æ€ä¸­çš„åˆ†æç»“æœ
    state["data_critique"] = critiques.get("data_management")
    state["methodology_critique"] = critiques.get("methodology")
    state["model_critique"] = critiques.get("model_architect")
    state["results_critique"] = critiques.get("result_analyst")
    
    print("\nâœ“ æ‰€æœ‰ä¸“å®¶Agentåˆ†æå®Œæˆ")
    print(f"  - æ•°æ®ç®¡ç†: {critiques.get('data_management').score if critiques.get('data_management') else 'N/A'}/10")
    print(f"  - æ–¹æ³•å­¦: {critiques.get('methodology').score if critiques.get('methodology') else 'N/A'}/10")
    print(f"  - æ¨¡å‹æ¶æ„: {critiques.get('model_architect').score if critiques.get('model_architect') else 'N/A'}/10")
    print(f"  - ç»“æœåˆ†æ: {critiques.get('result_analyst').score if critiques.get('result_analyst') else 'N/A'}/10")
    
    # åˆ†æå®Œæˆåï¼Œè¿›å…¥è®¨è®ºé˜¶æ®µï¼ˆè®©Agentä»¬è®¨è®ºå½¼æ­¤çš„åˆ†æç»“æœï¼‰
    state["next_action"] = "discuss"
    
    return state


async def discuss_node(state: REAgentState) -> REAgentState:
    """
    è®¨è®ºèŠ‚ç‚¹ - è®©å››ä¸ªä¸“å®¶AgentåŸºäºå½¼æ­¤çš„åˆ†æç»“æœè¿›è¡Œè®¨è®º
    
    æ¯ä¸ªAgentå¯ä»¥çœ‹åˆ°å…¶ä»–Agentçš„åˆ†æç»“æœï¼Œå¹¶åŸºäºæ­¤è¿›è¡Œè¡¥å……æˆ–ä¿®æ­£
    """
    global _workflow_expert_agents, _workflow_rag_system
    
    print("\n" + "="*60)
    print("ğŸ’¬ ä¸“å®¶Agentè®¨è®ºé˜¶æ®µ")
    print("="*60)
    
    # ç¡®ä¿ä¸“å®¶Agentå·²åˆå§‹åŒ–
    if _workflow_expert_agents is None:
        if _workflow_rag_system is None:
            _workflow_rag_system = HybridRAGSystem()
        _workflow_expert_agents = initialize_agents_with_rag(rag_system=_workflow_rag_system)
    
    # æ”¶é›†å½“å‰æ‰€æœ‰åˆ†æç»“æœ
    critiques = {
        "data_management": state.get("data_critique"),
        "methodology": state.get("methodology_critique"),
        "model_architect": state.get("model_critique"),
        "result_analyst": state.get("results_critique")
    }
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æç»“æœ
    if not any(critiques.values()):
        print("âš ï¸ æ²¡æœ‰å¯è®¨è®ºçš„åˆ†æç»“æœï¼Œè·³è¿‡è®¨è®ºé˜¶æ®µ")
        state["next_action"] = "analyze"
        return state
    
    # æ„å»ºè®¨è®ºä¸Šä¸‹æ–‡ï¼ˆæ±‡æ€»æ‰€æœ‰Agentçš„åˆ†æç»“æœï¼‰
    # æ£€æŸ¥æ˜¯å¦æ˜¯è¿­ä»£è®¨è®ºï¼ˆä»iterate_nodeè¿›å…¥ï¼‰
    is_iteration_discussion = state.get("iteration_summary") is not None
    iteration_summary = state.get("iteration_summary", "")
    
    discussion_context = ""
    if is_iteration_discussion:
        # è¿­ä»£è®¨è®ºï¼šåŒ…å«è¿­ä»£æ€»ç»“å’Œå»ºè®®
        discussion_context = f"{iteration_summary}\n\n"
        discussion_context += "=" * 60 + "\n"
        discussion_context += "Current Analysis Results from All Experts:\n"
        discussion_context += "=" * 60 + "\n\n"
    else:
        # é¦–æ¬¡è®¨è®ºï¼šåªåŒ…å«åˆ†æç»“æœ
        discussion_context = "Below are the preliminary analysis results from all expert agents:\n\n"
    
    for role, critique in critiques.items():
        if critique:
            if isinstance(critique, dict):
                score = critique.get("score", 0)
                strengths = critique.get("strengths", [])
                weaknesses = critique.get("weaknesses", [])
                recommendations = critique.get("recommendations", [])
                design_summary = critique.get("metadata", {}).get("design_summary", "")
            elif hasattr(critique, "score"):
                score = critique.score
                strengths = critique.strengths
                weaknesses = critique.weaknesses
                recommendations = critique.recommendations
                design_summary = critique.metadata.get("design_summary", "") if hasattr(critique, "metadata") else ""
            else:
                continue
            
            role_name = {
                "data_management": "Data Management Expert",
                "methodology": "Methodology Expert",
                "model_architect": "Model Architect",
                "result_analyst": "Result Analyst"
            }.get(role, role)
            
            discussion_context += f"ã€{role_name}ã€‘Current Score: {score:.1f}/10\n"
            if design_summary:
                # å¢å¤§ä¸Šä¸‹æ–‡çª—å£ï¼šæ˜¾ç¤ºæ›´å¤šå†…å®¹ï¼ˆä»200å­—ç¬¦å¢åŠ åˆ°800å­—ç¬¦ï¼‰
                summary_preview = design_summary[:800] if len(design_summary) > 800 else design_summary
                discussion_context += f"Design Summary: {summary_preview}"
                if len(design_summary) > 800:
                    discussion_context += "...\n"
                else:
                    discussion_context += "\n"
            if strengths:
                # æ˜¾ç¤ºæ›´å¤šstrengthsï¼ˆä»3ä¸ªå¢åŠ åˆ°5ä¸ªï¼‰
                discussion_context += f"Strengths: {', '.join(strengths[:5])}\n"
            if weaknesses:
                # æ˜¾ç¤ºæ›´å¤šweaknessesï¼ˆä»3ä¸ªå¢åŠ åˆ°5ä¸ªï¼‰
                discussion_context += f"Areas for Improvement: {', '.join(weaknesses[:5])}\n"
            if recommendations and not is_iteration_discussion:
                # é¦–æ¬¡è®¨è®ºæ—¶æ˜¾ç¤ºæ›´å¤šå»ºè®®ï¼ˆä»2ä¸ªå¢åŠ åˆ°5ä¸ªï¼‰
                discussion_context += f"Recommendations: {', '.join(recommendations[:5])}\n"
            discussion_context += "\n"
    
    # è®©æ¯ä¸ªAgentåŸºäºè®¨è®ºä¸Šä¸‹æ–‡è¿›è¡Œè¡¥å……åˆ†æ
    print("ğŸ“ å„Agentæ­£åœ¨åŸºäºè®¨è®ºç»“æœè¿›è¡Œè¡¥å……åˆ†æ...")
    
    task_plan = _build_task_plan(state)
    updated_critiques = {}
    
    for role, agent in _workflow_expert_agents.items():
        current_critique = critiques.get(role)
        if not current_critique:
            continue
        
        print(f"  - {agent.title} æ­£åœ¨è¡¥å……åˆ†æ...")
        
        # æ„å»ºè®¨è®ºæç¤º
        if is_iteration_discussion:
            # è¿­ä»£è®¨è®ºï¼šå¼ºè°ƒåŸºäºå»ºè®®æ›´æ–°è®¾è®¡æ–¹æ¡ˆå’Œè¯„åˆ†
            discussion_prompt = f"""âš ï¸ CRITICAL LANGUAGE REQUIREMENT:
- You MUST write ALL responses in English (EN). Do NOT use Chinese, Japanese, or any other language.
- All text in your response MUST be in English for international publication standards.

You are in an ITERATIVE DISCUSSION round. Based on the following context, please UPDATE your design and score:

{discussion_context}

âš ï¸ ITERATION REQUIREMENTS:
1. **Review the optimization recommendations** provided above - these are specific suggestions for improvement
2. **Update your design** based on these recommendations:
   - Improve your design specifications (detailed_design) to address the recommendations
   - Add missing parameter values, hyperparameter settings, and configuration details
   - For model architect: enhance architecture specifications with exact dimensions and all hyperparameters
   - Correct identified issues and add missing design details
3. **Update your score** based on the improvements:
   - If you have addressed the recommendations and improved your design with detailed parameters, you may increase your score
   - Score should reflect the CURRENT quality of your design after incorporating the recommendations
   - Only give score >= 8.5 if your design is truly comprehensive with detailed parameter specifications
4. **Provide updated recommendations** - new suggestions for further design improvement with specific parameter suggestions (if any)

Consider:
- Are the recommendations from other experts valid and applicable to your design?
- What specific parameter improvements can you make to your design based on these recommendations?
- How do these improvements affect the quality and completeness of your design specifications?

**Available Tools:**
- You CAN use `rag_search` to retrieve relevant knowledge from the knowledge base to support your design improvements
- You CAN use `read_file` to read dataset files or other relevant files if needed
- Use tools as needed to enhance your design specifications with accurate information

Please return the UPDATED analysis in JSON format:
{{
    "score": <0-10 float score, updated based on improvements>,
    "strengths": ["updated strength1", "updated strength2", ...],
    "weaknesses": ["remaining weakness1", "remaining weakness2", ...],
    "recommendations": ["new recommendation1", "new recommendation2", ...],
    "confidence": <0-1 float>,
    "discussion_notes": "Brief notes on what you improved based on the recommendations"
}}

IMPORTANT: Your score should reflect the CURRENT state of your design after incorporating the recommendations.
"""
        else:
            # é¦–æ¬¡è®¨è®ºï¼šåŸºäºå…¶ä»–ä¸“å®¶çš„æ„è§è¿›è¡Œè¡¥å……
            discussion_prompt = f"""âš ï¸ CRITICAL LANGUAGE REQUIREMENT:
- You MUST write ALL responses in English (EN). Do NOT use Chinese, Japanese, or any other language.
- All text in your response MUST be in English for international publication standards.

Based on the following discussion context, please supplement or correct your analysis:

{discussion_context}

Consider:
1. Whether the opinions of other experts are consistent with your analysis?
2. Whether there are places that need to be supplemented or corrected?
3. Whether to agree with the suggestions of other experts?

**Available Tools:**
- You CAN use `rag_search` to retrieve relevant knowledge from the knowledge base to support your analysis
- You CAN use `read_file` to read dataset files or other relevant files if needed
- Use tools as needed to enhance your analysis with accurate information

Please return the updated analysis in JSON format:
{{
    "score": <0-10 float score>,
    "strengths": ["strength1", "strength2", ...],
    "weaknesses": ["weakness1", "weakness2", ...],
    "recommendations": ["recommendation1", "recommendation2", ...],
    "confidence": <0-1 float>,
    "discussion_notes": "Your discussion notes"
}}
"""
        
        try:
            from langchain_core.messages import SystemMessage, HumanMessage
            messages = [
                SystemMessage(content=agent.prompt() + "\n\nYou are now participating in the expert discussion, and you can supplement or correct your analysis based on the opinions of other experts. You can use available tools (rag_search, read_file, etc.) to retrieve relevant knowledge or data to support your analysis."),
                HumanMessage(content=discussion_prompt)
            ]
            
            response = await agent.llm_with_tools.ainvoke(messages)
            
            # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
            max_tool_iterations = 3
            tool_iteration = 0
            while tool_iteration < max_tool_iterations:
                has_tool_calls = False
                if hasattr(response, "tool_calls") and response.tool_calls:
                    has_tool_calls = True
                elif hasattr(response, "additional_kwargs") and response.additional_kwargs.get("tool_calls"):
                    has_tool_calls = True
                
                if not has_tool_calls:
                    break
                
                tool_iteration += 1
                messages.append(response)
                
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                from langchain_core.messages import ToolMessage
                tool_calls_list = []
                if hasattr(response, "tool_calls") and response.tool_calls:
                    tool_calls_list = response.tool_calls
                elif hasattr(response, "additional_kwargs") and response.additional_kwargs.get("tool_calls"):
                    tool_calls_list = response.additional_kwargs["tool_calls"]
                
                for tool_call in tool_calls_list:
                    if isinstance(tool_call, dict):
                        tool_name = tool_call.get("name", "") or tool_call.get("function", {}).get("name", "")
                        tool_args = tool_call.get("args", {}) or tool_call.get("function", {}).get("arguments", {})
                        tool_call_id = tool_call.get("id", "") or tool_call.get("function", {}).get("id", "")
                    else:
                        tool_name = getattr(tool_call, "name", "")
                        tool_args = getattr(tool_call, "args", {})
                        tool_call_id = getattr(tool_call, "id", "")
                    
                    if isinstance(tool_args, str):
                        import json
                        try:
                            tool_args = json.loads(tool_args)
                        except:
                            tool_args = {}
                    
                    # æŸ¥æ‰¾å¹¶æ‰§è¡Œå·¥å…·
                    tool = next((t for t in agent._tools if t.name == tool_name), None)
                    if tool:
                        try:
                            if hasattr(tool, "_arun"):
                                result = await tool._arun(**tool_args)
                            else:
                                result = tool._run(**tool_args)
                            
                            result_content = str(result) if not isinstance(result, dict) else str(result.get("data", result))
                            messages.append(ToolMessage(
                                content=result_content,
                                tool_call_id=tool_call_id
                            ))
                        except Exception as e:
                            messages.append(ToolMessage(
                                content=f"Tool execution failed: {str(e)}",
                                tool_call_id=tool_call_id
                            ))
                
                # ç»§ç»­è°ƒç”¨LLM
                follow_up = HumanMessage(content="Please provide your updated analysis in JSON format as requested.")
                messages.append(follow_up)
                response = await agent.llm_with_tools.ainvoke(messages)
            
            response_text = response.content if response.content else ""
            
            # è§£æå“åº”ï¼ˆä½¿ç”¨æ›´å¥å£®çš„è§£ææ–¹æ³•ï¼‰
            import json
            import re
            
            updated_data = None
            
            # æ–¹æ³•1: å°è¯•æå–JSONä»£ç å—
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                try:
                    updated_data = json.loads(json_match.group(1))
                except json.JSONDecodeError:
                    pass
            
            # æ–¹æ³•2: å°è¯•æå–æ™®é€šJSONå¯¹è±¡
            if updated_data is None:
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
                if json_match:
                    try:
                        updated_data = json.loads(json_match.group(0))
                    except json.JSONDecodeError:
                        pass
            
            # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–å­—æ®µ
            if updated_data is None:
                updated_data = {}
                # è·å–åŸæœ‰å€¼ä½œä¸ºé»˜è®¤å€¼
                old_score = current_critique.score if hasattr(current_critique, "score") else (current_critique.get("score", 5.0) if isinstance(current_critique, dict) else 5.0)
                old_strengths = current_critique.strengths if hasattr(current_critique, "strengths") else (current_critique.get("strengths", []) if isinstance(current_critique, dict) else [])
                old_weaknesses = current_critique.weaknesses if hasattr(current_critique, "weaknesses") else (current_critique.get("weaknesses", []) if isinstance(current_critique, dict) else [])
                old_recommendations = current_critique.recommendations if hasattr(current_critique, "recommendations") else (current_critique.get("recommendations", []) if isinstance(current_critique, dict) else [])
                old_confidence = current_critique.confidence if hasattr(current_critique, "confidence") else (current_critique.get("confidence", 0.5) if isinstance(current_critique, dict) else 0.5)
                
                # å°è¯•æå–åˆ†æ•°
                score_match = re.search(r'"score"\s*:\s*(\d+\.?\d*)', response_text, re.IGNORECASE)
                if score_match:
                    try:
                        updated_data["score"] = float(score_match.group(1))
                    except:
                        updated_data["score"] = old_score
                else:
                    updated_data["score"] = old_score
                
                # å°è¯•æå–åˆ—è¡¨å­—æ®µ
                for key, old_value in [("strengths", old_strengths), ("weaknesses", old_weaknesses), ("recommendations", old_recommendations)]:
                    pattern = rf'"{key}"\s*:\s*\[(.*?)\]'
                    match = re.search(pattern, response_text, re.DOTALL | re.IGNORECASE)
                    if match:
                        items = re.findall(r'"([^"]+)"', match.group(1))
                        updated_data[key] = items if items else old_value
                    else:
                        updated_data[key] = old_value
                
                # å°è¯•æå–confidence
                conf_match = re.search(r'"confidence"\s*:\s*(\d+\.?\d*)', response_text, re.IGNORECASE)
                if conf_match:
                    try:
                        updated_data["confidence"] = float(conf_match.group(1))
                    except:
                        updated_data["confidence"] = old_confidence
                else:
                    updated_data["confidence"] = old_confidence
                
                # å°è¯•æå–discussion_notes
                notes_match = re.search(r'"discussion_notes"\s*:\s*"([^"]+)"', response_text, re.DOTALL | re.IGNORECASE)
                if notes_match:
                    updated_data["discussion_notes"] = notes_match.group(1)
                else:
                    updated_data["discussion_notes"] = f"Discussion response: {response_text[:200]}" if response_text else "No response"
            
            # å¦‚æœä»ç„¶æ— æ³•è§£æï¼Œä½¿ç”¨åŸæœ‰åˆ†æç»“æœ
            if not updated_data or not isinstance(updated_data, dict) or not response_text:
                print(f"    âš ï¸ {agent.title} JSONè§£æå¤±è´¥ï¼ˆå“åº”ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯ï¼‰ï¼Œä¿ç•™åŸæœ‰åˆ†æç»“æœ")
                updated_critiques[role] = current_critique
                continue
            
            # åˆ›å»ºæ›´æ–°åçš„CritiqueResult
            # é‡è¦ï¼šä¿ç•™åŸå§‹metadataä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼ˆç‰¹åˆ«æ˜¯detailed_designï¼‰ï¼Œåªæ·»åŠ è®¨è®ºç›¸å…³çš„å­—æ®µ
            from Agents.state import CritiqueResult
            original_metadata = current_critique.metadata if hasattr(current_critique, "metadata") else {}
            if isinstance(current_critique, dict):
                original_metadata = current_critique.get("metadata", {})
            
            # åˆå¹¶metadataï¼šä¿ç•™åŸå§‹çš„æ‰€æœ‰å­—æ®µï¼Œåªæ›´æ–°è®¨è®ºç›¸å…³å­—æ®µ
            merged_metadata = original_metadata.copy()  # å…ˆå¤åˆ¶åŸå§‹metadata
            merged_metadata.update({
                "discussion_notes": updated_data.get("discussion_notes", ""),
                "updated_after_discussion": True
            })
            # ç¡®ä¿detailed_designå’Œdesign_summaryä¸è¢«è¦†ç›–
            if "detailed_design" not in merged_metadata or not merged_metadata.get("detailed_design"):
                # å¦‚æœdetailed_designä¸¢å¤±äº†ï¼Œå°è¯•ä»åŸå§‹metadataæ¢å¤
                if "detailed_design" in original_metadata:
                    merged_metadata["detailed_design"] = original_metadata["detailed_design"]
            
            updated_critique = CritiqueResult(
                agent_role=role,
                score=updated_data.get("score", current_critique.score if hasattr(current_critique, "score") else current_critique.get("score", 0)),
                strengths=updated_data.get("strengths", []),
                weaknesses=updated_data.get("weaknesses", []),
                recommendations=updated_data.get("recommendations", []),
                confidence=updated_data.get("confidence", 0.5),
                metadata=merged_metadata  # ä½¿ç”¨åˆå¹¶åçš„metadataï¼Œä¿ç•™æ‰€æœ‰åŸå§‹ä¿¡æ¯
            )
            
            updated_critiques[role] = updated_critique
            print(f"    âœ“ {agent.title} è¡¥å……åˆ†æå®Œæˆ")
            
        except Exception as e:
            print(f"    âš ï¸ {agent.title} è®¨è®ºåˆ†æå¤±è´¥: {e}")
            # ä¿ç•™åŸæœ‰åˆ†æç»“æœ
            updated_critiques[role] = current_critique
    
    # æ›´æ–°çŠ¶æ€ä¸­çš„åˆ†æç»“æœ
    if updated_critiques:
        state["data_critique"] = updated_critiques.get("data_management")
        state["methodology_critique"] = updated_critiques.get("methodology")
        state["model_critique"] = updated_critiques.get("model_architect")
        state["results_critique"] = updated_critiques.get("result_analyst")
        
        # æ‰“å°æ›´æ–°åçš„è¯„åˆ†
        print(f"\nâœ“ è®¨è®ºå®Œæˆï¼Œ{len(updated_critiques)} ä¸ªAgentæ›´æ–°äº†åˆ†æç»“æœ")
        print("   æ›´æ–°åçš„è¯„åˆ†ï¼š", end="")
        for role, critique in updated_critiques.items():
            if critique:
                score = critique.score if hasattr(critique, "score") else (critique.get("score", 0) if isinstance(critique, dict) else 0)
                role_short = {
                    "data_management": "æ•°æ®",
                    "methodology": "æ–¹æ³•",
                    "model_architect": "æ¨¡å‹",
                    "result_analyst": "ç»“æœ"
                }.get(role, role)
                print(f"{role_short}:{score:.1f} ", end="")
        print()
    
    # æ¸…é™¤è¿­ä»£æ ‡è®°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œé¿å…å½±å“åç»­æµç¨‹
    if "iteration_summary" in state:
        del state["iteration_summary"]
    if "recommendations_by_role" in state:
        del state["recommendations_by_role"]
    
    # è®¾ç½®æ ‡å¿—ï¼šè®¨è®ºå®Œæˆåéœ€è¦Supervisorè´¨ç–‘å’Œè¯„ä¼°
    state["after_discussion"] = True
    
    # è®¨è®ºåï¼Œè®©Supervisoræ ¹æ®è¯„åˆ†å†³å®šä¸‹ä¸€æ­¥
    # Supervisorä¼šæ£€æŸ¥è¯„åˆ†æ˜¯å¦æ»¡è¶³è¦æ±‚ï¼ˆå¹³å‡åˆ†>=9ï¼Œæ¯ä¸ªä¸“å®¶>=8.5ï¼‰
    # å¦‚æœè¾¾æ ‡ â†’ reportï¼Œå¦‚æœä¸è¾¾æ ‡ â†’ iterateï¼ˆç»§ç»­è¿­ä»£è®¨è®ºï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œä¸è®¾ç½®next_actionï¼Œè®©supervisorèŠ‚ç‚¹è‡ªç„¶å¤„ç†
    # workflowå›¾å·²ç»é…ç½®äº†discuss -> supervisorçš„è¾¹
    
    return state


async def iterate_node(state: REAgentState) -> REAgentState:
    """
    è¿­ä»£èŠ‚ç‚¹ - æ ¹æ®åˆ†æç»“æœè¿›è¡Œè¿­ä»£ä¼˜åŒ–
    
    æ”¶é›†æ‰€æœ‰ä¸“å®¶çš„å»ºè®®ï¼Œç„¶åè§¦å‘è®¨è®ºé˜¶æ®µï¼Œè®©AgentåŸºäºå»ºè®®å’Œä¸Šä¸‹æ–‡å†æ¬¡è®¨è®ºå¹¶æ›´æ–°å¾—åˆ†
    """
    print("\n" + "="*60)
    print("ğŸ”„ è¿­ä»£ä¼˜åŒ–é˜¶æ®µ")
    print("="*60)
    
    # æ¯è¿›å…¥ä¸€æ¬¡è¿­ä»£èŠ‚ç‚¹ï¼Œè§†ä¸ºä¸€è½®æ–°çš„ä¼˜åŒ–è¿­ä»£
    iteration_count = state.get("iteration_count", 0) + 1
    state["iteration_count"] = iteration_count
    max_iterations = state.get("max_iterations", 8)
    
    if iteration_count >= max_iterations:
        print(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})ï¼Œè¿›å…¥æŠ¥å‘Šç”Ÿæˆé˜¶æ®µ")
        state["next_action"] = "report"
        return state
    
    # æ”¶é›†æ‰€æœ‰å»ºè®®ï¼ˆä»æ‰€æœ‰ä¸“å®¶çš„åˆ†æç»“æœä¸­æå–ï¼‰
    all_recommendations = []
    critiques = {
        "data_management": state.get("data_critique"),
        "methodology": state.get("methodology_critique"),
        "model_architect": state.get("model_critique"),
        "result_analyst": state.get("results_critique")
    }
    
    # æŒ‰è§’è‰²ç»„ç»‡å»ºè®®ï¼Œä¾¿äºåœ¨è®¨è®ºä¸­å¼•ç”¨
    recommendations_by_role = {}
    for role, critique in critiques.items():
        if critique:
            if isinstance(critique, dict):
                recommendations = critique.get("recommendations", [])
                score = critique.get("score", 0)
            elif hasattr(critique, "recommendations"):
                recommendations = critique.recommendations
                score = critique.score if hasattr(critique, "score") else 0
            else:
                recommendations = []
                score = 0
            
            if recommendations:
                role_name = {
                    "data_management": "æ•°æ®ç®¡ç†ä¸“å®¶",
                    "methodology": "æ–¹æ³•å­¦ä¸“å®¶",
                    "model_architect": "æ¨¡å‹æ¶æ„å¸ˆ",
                    "result_analyst": "ç»“æœåˆ†æå¸ˆ"
                }.get(role, role)
                recommendations_by_role[role_name] = recommendations
                all_recommendations.extend(recommendations)
    
    # æ„å»ºè¿­ä»£å»ºè®®æ€»ç»“æ¶ˆæ¯ï¼ˆåŒ…å«æ‰€æœ‰å»ºè®®å’Œå½“å‰è¯„åˆ†æƒ…å†µï¼‰
    iteration_summary = f"""ğŸ”„ ç¬¬ {iteration_count} è½®è¿­ä»£è®¨è®º

å½“å‰è¯„åˆ†æƒ…å†µï¼š
"""
    for role, critique in critiques.items():
        if critique:
            if isinstance(critique, dict):
                score = critique.get("score", 0)
            elif hasattr(critique, "score"):
                score = critique.score
            else:
                score = 0
            
            role_name = {
                "data_management": "æ•°æ®ç®¡ç†ä¸“å®¶",
                "methodology": "æ–¹æ³•å­¦ä¸“å®¶",
                "model_architect": "æ¨¡å‹æ¶æ„å¸ˆ",
                "result_analyst": "ç»“æœåˆ†æå¸ˆ"
            }.get(role, role)
            iteration_summary += f"  - {role_name}: {score:.1f}/10\n"
    
    iteration_summary += f"\næ±‡æ€»çš„ä¼˜åŒ–å»ºè®®ï¼ˆå…± {len(all_recommendations)} æ¡ï¼‰ï¼š\n\n"
    for i, rec in enumerate(all_recommendations[:15], 1):  # æœ€å¤šæ˜¾ç¤º15æ¡
        iteration_summary += f"{i}. {rec}\n"
    
    if len(all_recommendations) > 15:
        iteration_summary += f"\n... è¿˜æœ‰ {len(all_recommendations) - 15} æ¡å»ºè®®æœªæ˜¾ç¤º\n"
    
    iteration_summary += "\nè¯·åŸºäºä»¥ä¸Šå»ºè®®å’Œæ‰€æœ‰ä¸“å®¶çš„åˆ†æç»“æœï¼Œè¿›è¡Œè®¨è®ºå¹¶æ›´æ–°æ‚¨çš„è®¾è®¡æ–¹æ¡ˆå’Œè¯„åˆ†ã€‚"
    
    # å°†è¿­ä»£å»ºè®®æ€»ç»“ä¿å­˜åˆ°çŠ¶æ€ä¸­ï¼Œä¾›è®¨è®ºèŠ‚ç‚¹ä½¿ç”¨
    state["iteration_summary"] = iteration_summary
    state["recommendations_by_role"] = recommendations_by_role
    
    # æ·»åŠ è¿­ä»£å»ºè®®æ¶ˆæ¯åˆ°æ¶ˆæ¯å†å²
    iterate_message = HumanMessage(content=iteration_summary)
    state["messages"].append(iterate_message)
    
    # ç›´æ¥è§¦å‘è®¨è®ºé˜¶æ®µï¼ˆä¸æ˜¯é‡æ–°åˆ†æï¼‰
    state["next_action"] = "discuss"
    
    print(f"ğŸ“ æ”¶é›†åˆ° {len(all_recommendations)} æ¡ä¼˜åŒ–å»ºè®®ï¼Œå‡†å¤‡è¿›å…¥è®¨è®ºé˜¶æ®µ...")
    print(f"   å½“å‰è¯„åˆ†ï¼š", end="")
    for role, critique in critiques.items():
        if critique:
            score = critique.score if hasattr(critique, "score") else (critique.get("score", 0) if isinstance(critique, dict) else 0)
            role_short = {
                "data_management": "æ•°æ®",
                "methodology": "æ–¹æ³•",
                "model_architect": "æ¨¡å‹",
                "result_analyst": "ç»“æœ"
            }.get(role, role)
            print(f"{role_short}:{score:.1f} ", end="")
    print()
    
    return state


async def report_node(state: REAgentState) -> REAgentState:
    """
    æŠ¥å‘ŠèŠ‚ç‚¹ - ç”Ÿæˆæœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š
    """
    print("\n" + "="*60)
    print("ğŸ“Š ç”Ÿæˆæœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š")
    print("="*60)
    
    # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
    critiques = {
        "data_management": state.get("data_critique"),
        "methodology": state.get("methodology_critique"),
        "model_architect": state.get("model_critique"),
        "result_analyst": state.get("results_critique")
    }
    
    # è¿‡æ»¤æ‰Noneå€¼
    critiques = {k: v for k, v in critiques.items() if v is not None}
    
    # è®¡ç®—æ€»ä½“è¯„åˆ†
    scores = []
    for critique in critiques.values():
        if critique:
            if isinstance(critique, dict):
                score = critique.get("score", 0)
            elif hasattr(critique, "score"):
                score = critique.score
            else:
                score = 0
            if score > 0:
                scores.append(score)
    
    overall_score = sum(scores) / len(scores) if scores else 0.0
    
    # æ”¶é›†ä¼˜å…ˆå»ºè®®ï¼ˆé‡ç‚¹å…³æ³¨å¯æ‰§è¡Œçš„å®æ–½æ–¹æ¡ˆå»ºè®®ï¼‰
    priority_recommendations = []
    for critique in critiques.values():
        if critique:
            if isinstance(critique, dict):
                recs = critique.get("recommendations", [])
            elif hasattr(critique, "recommendations"):
                recs = critique.recommendations
            else:
                recs = []
            
            # æ”¶é›†æ‰€æœ‰å»ºè®®ï¼ˆé‡ç‚¹å…³æ³¨å®æ–½æ–¹æ¡ˆï¼‰
            if recs:
                priority_recommendations.extend(recs[:3])  # æ¯ä¸ªAgentæœ€å¤š3æ¡
    
    # æ„å»ºæŠ¥å‘Š - ç”Ÿæˆè¯¦ç»†çš„å®éªŒè®¾è®¡æŠ¥å‘Š
    task_title = state.get("task_description", "Gene Regulatory Element Design Task")
    task_background = state.get("background", "")
    dataset_info = state.get("dataset_info", "")
    
    # ä»å„Agentçš„è®¾è®¡ä¸­æå–è¯¦ç»†ä¿¡æ¯ï¼ˆå®Œæ•´ä¿ç•™ï¼Œä¸è¿›è¡Œä»»ä½•æ¦‚æ‹¬æˆ–åˆ å‡ï¼‰
    data_design = {}
    method_design = {}
    model_design = {}
    result_design = {}
    
    if critiques.get("data_management"):
        dm_critique = critiques["data_management"]
        dm_meta = dm_critique.metadata if hasattr(dm_critique, "metadata") else {}
        # å®Œæ•´ä¿ç•™ detailed_designï¼Œä¸åšä»»ä½•ä¿®æ”¹
        data_design = dm_meta.get("detailed_design", {})
        # å¦‚æœ detailed_design ä¸ºç©ºï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–
        if not data_design:
            print("âš ï¸ Data Management detailed_design is empty, checking metadata...")
            print(f"   Metadata keys: {list(dm_meta.keys())}")
    
    if critiques.get("methodology"):
        method_critique = critiques["methodology"]
        method_meta = method_critique.metadata if hasattr(method_critique, "metadata") else {}
        method_design = method_meta.get("detailed_design", {})
        if not method_design:
            print("âš ï¸ Methodology detailed_design is empty, checking metadata...")
            print(f"   Metadata keys: {list(method_meta.keys())}")
    
    if critiques.get("model_architect"):
        model_critique = critiques["model_architect"]
        model_meta = model_critique.metadata if hasattr(model_critique, "metadata") else {}
        model_design = model_meta.get("detailed_design", {})
        if not model_design:
            print("âš ï¸ Model Architect detailed_design is empty, checking metadata...")
            print(f"   Metadata keys: {list(model_meta.keys())}")
    
    if critiques.get("result_analyst"):
        result_critique = critiques["result_analyst"]
        result_meta = result_critique.metadata if hasattr(result_critique, "metadata") else {}
        result_design = result_meta.get("detailed_design", {})
        if not result_design:
            print("âš ï¸ Result Analyst detailed_design is empty, checking metadata...")
            print(f"   Metadata keys: {list(result_meta.keys())}")
    
    report = OptimizationReport(
        title=f"Experimental Design Report: {task_title}",
        summary=f"Based on the task background and data set information, after {state.get('iteration_count', 0)} rounds of expert design, a complete experimental design scheme is generated. The overall feasibility score: {overall_score:.1f}/10",
        critiques={
            role: critique for role, critique in critiques.items() if critique
        },
        overall_score=overall_score,
        priority_recommendations=priority_recommendations[:10],
        metadata={
            "iteration_count": state.get("iteration_count", 0),
            "task_description": task_title,
            "task_background": task_background,
            "dataset_info": dataset_info,
            "data_usage_plan": data_design,
            "method_design": method_design,
            "model_design": model_design,
            "result_summary": result_design
        }
    )
    
    # å°†æŠ¥å‘Šè½¬æ¢ä¸ºå­—å…¸æ ¼å¼å­˜å‚¨ï¼ˆåŒ…å«è¯¦ç»†çš„å®éªŒè®¾è®¡ï¼‰
    # é‡ç‚¹ï¼šçªå‡ºå®æ–½æ–¹æ¡ˆå’Œä»£ç ï¼Œå‡å°‘ä¼˜ç¼ºç‚¹åˆ†æ
    state["final_report"] = {
        "title": report.title,
        "summary": report.summary,
        "overall_score": report.overall_score,
        "priority_recommendations": report.priority_recommendations,
        "task_information": {
            "description": task_title,
            "background": task_background,
            "dataset_info": dataset_info
        },
        "experimental_design": {
            "1_data_usage_plan": report.metadata.get("data_usage_plan", {}),
            "2_method_design": report.metadata.get("method_design", {}),
            "3_model_design": report.metadata.get("model_design", {}),
            "4_result_summary": report.metadata.get("result_summary", {})
        },
        "expert_analyses": {
            role: {
                "score": c.score if hasattr(c, "score") else c.get("score", 0),
                "design_summary": c.metadata.get("design_summary", "") if hasattr(c, "metadata") else "",
                # å®Œæ•´ä¿ç•™ detailed_designï¼Œä¸è¿›è¡Œä»»ä½•æ¦‚æ‹¬æˆ–åˆ å‡ï¼Œç›´æ¥æ‹¼æ¥å„ä¸“å®¶çš„æœ€ç»ˆæ–¹æ¡ˆ
                "implementation_plan": c.metadata.get("detailed_design", {}) if hasattr(c, "metadata") else {},
                "recommendations": c.recommendations if hasattr(c, "recommendations") else c.get("recommendations", []),
                # æ£€ç´¢åˆ°çš„çŸ¥è¯†æ¡ç›®ï¼ˆid + å†…å®¹ï¼‰ï¼Œç”¨äºå¯è§£é‡Šæ€§
                "retrieved_knowledge": c.metadata.get("retrieved_knowledge", []) if hasattr(c, "metadata") else [],
                # åŒæ—¶ä¿ç•™å®Œæ•´çš„ metadataï¼Œç¡®ä¿ä¸ä¸¢å¤±ä»»ä½•ä¿¡æ¯
                "full_metadata": c.metadata if hasattr(c, "metadata") else {}
            }
            for role, c in report.critiques.items()
        },
        "metadata": report.metadata
    }
    
    # æ·»åŠ æŠ¥å‘Šæ¶ˆæ¯ï¼ˆåŒ…å«å®éªŒè®¾è®¡æ‘˜è¦ï¼Œé‡ç‚¹çªå‡ºå®æ–½æ–¹æ¡ˆï¼‰
    report_content = f"""The experimental design report has been generated

{report.summary}

Experimental Design Implementation Plan:
1. Data Usage Plan: Contains {len(data_design)} design sections with detailed parameter specifications
2. Method Design: Contains {len(method_design)} design sections with detailed parameter specifications
3. Model Design: Contains {len(model_design)} design sections with comprehensive parameter configurations (layer dimensions, hyperparameters, etc.)
4. Result Summary: Contains {len(result_design)} design sections with detailed parameter specifications

Key Implementation Recommendations:
""" + "\n".join(f"{i+1}. {rec}" for i, rec in enumerate(report.priority_recommendations))
    
    report_message = AIMessage(content=report_content)
    state["messages"].append(report_message)
    
    print(f"âœ“ The experimental design report has been generated, the overall score: {overall_score:.1f}/10")
    print(f"  - Number of priority recommendations: {len(priority_recommendations)}")
    
    # æŠ¥å‘Šç”Ÿæˆåï¼Œè¿›å…¥ç»“æŸé˜¶æ®µ
    state["next_action"] = "end"
    
    return state


# ==================== è·¯ç”±å‡½æ•° ====================

def route_decision(state: REAgentState) -> Literal["request_info", "analyze", "discuss", "iterate", "report", "end"]:
    """
    æ ¹æ®next_actionè·¯ç”±åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
    """
    next_action = state.get("next_action", "request_info")

    # å…¨å±€å®‰å…¨ä¿æŠ¤ï¼šè¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°åï¼Œå¼ºåˆ¶è¿›å…¥æŠ¥å‘Šé˜¶æ®µï¼Œé¿å…æ— é™å¾ªç¯
    iteration = state.get("iteration_count", 0)
    max_iter = state.get("max_iterations", 8)
    if iteration >= max_iter and next_action in ("iterate", "analyze"):
        print(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iter})ï¼Œè·¯ç”±å¼ºåˆ¶åˆ‡æ¢ä¸º report")
        next_action = "report"
    
    # ç¡®ä¿next_actionæ˜¯æœ‰æ•ˆçš„
    valid_actions = ["request_info", "analyze", "discuss", "iterate", "report", "end"]
    if next_action not in valid_actions:
        print(f"âš ï¸ Invalid next_action: {next_action}, default using request_info")
        next_action = "request_info"
    
    return next_action


# ==================== å·¥ä½œæµå›¾æ„å»º ====================

def create_workflow_graph(rag_system: HybridRAGSystem = None, supervisor: Agent = None) -> StateGraph:
    """
    åˆ›å»ºLangGraphå·¥ä½œæµå›¾
    
    Args:
        rag_system: RAGç³»ç»Ÿå®ä¾‹ï¼ˆå¯é€‰ï¼‰
        supervisor: Supervisorå®ä¾‹ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨ï¼Œå¦åˆ™åˆ›å»ºæ–°å®ä¾‹ï¼‰
    
    Returns:
        ç¼–è¯‘åçš„StateGraphåº”ç”¨
    """
    # è®¾ç½®å·¥ä½œæµç»„ä»¶
    if rag_system is None:
        rag_system = HybridRAGSystem()
    
    if supervisor is None:
        supervisor = initialize_supervisor(rag_system=rag_system)
    
    expert_agents = initialize_agents_with_rag(rag_system=rag_system)
    set_workflow_components(supervisor=supervisor, expert_agents=expert_agents, rag_system=rag_system)
    
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(REAgentState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("supervisor", supervisor_node)
    workflow.add_node("request_info", request_info_node)
    workflow.add_node("analyze", analyze_node)
    workflow.add_node("discuss", discuss_node)
    workflow.add_node("iterate", iterate_node)
    workflow.add_node("report", report_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("supervisor")
    
    # æ·»åŠ æ¡ä»¶è¾¹ï¼šä»supervisoræ ¹æ®next_actionè·¯ç”±
    workflow.add_conditional_edges(
        "supervisor",
        route_decision,
        {
            "request_info": "request_info",
            "analyze": "analyze",
            "discuss": "discuss",
            "iterate": "iterate",
            "report": "report",
            "end": END
        }
    )
    
    # ä»request_infoå›åˆ°supervisorï¼ˆç­‰å¾…ç”¨æˆ·è¾“å…¥åï¼‰
    workflow.add_edge("request_info", "supervisor")
    
    # ä»analyzeåˆ°discussï¼ˆåˆ†æå®Œæˆåè¿›å…¥è®¨è®ºï¼‰
    workflow.add_edge("analyze", "discuss")
    
    # ä»discussåˆ°supervisorï¼ˆè®¨è®ºåé‡æ–°å†³ç­–ï¼‰
    workflow.add_edge("discuss", "supervisor")
    
    # ä»iterateåˆ°discussï¼ˆè¿­ä»£æ—¶ç›´æ¥è¿›å…¥è®¨è®ºï¼Œè®©AgentåŸºäºå»ºè®®æ›´æ–°è®¾è®¡ï¼‰
    workflow.add_edge("iterate", "discuss")
    
    # ä»reportåˆ°ENDï¼ˆæŠ¥å‘Šç”Ÿæˆåç»“æŸï¼‰
    workflow.add_edge("report", END)
    
    # ç¼–è¯‘å›¾
    app = workflow.compile()
    
    print("âœ“ LangGraphå·¥ä½œæµå›¾åˆ›å»ºå®Œæˆ")
    print("  èŠ‚ç‚¹: supervisor -> [request_info/analyze -> discuss/iterate/report] -> end")
    
    return app


# ==================== è¾…åŠ©å‡½æ•° ====================

def create_initial_state(
    task_description: str = None,
    background: str = None,
    dataset_info: str = None,
    methodology: str = None,
    model_architecture: str = None,
    evaluation_metrics: str = None,
    rag_system: HybridRAGSystem = None,
    max_iterations: int = 8
) -> REAgentState:
    """
    åˆ›å»ºåˆå§‹çŠ¶æ€
    
    Args:
        task_description: ä»»åŠ¡æè¿°
        background: èƒŒæ™¯è¦æ±‚
        dataset_info: æ•°æ®é›†ä¿¡æ¯
        methodology: æ–¹æ³•æè¿°ï¼ˆå¯é€‰ï¼‰
        model_architecture: æ¨¡å‹æ¶æ„ï¼ˆå¯é€‰ï¼‰
        evaluation_metrics: è¯„ä¼°æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
        rag_system: RAGç³»ç»Ÿå®ä¾‹ï¼ˆå¯é€‰ï¼‰
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
    
    Returns:
        åˆå§‹åŒ–çš„REAgentState
    """
    from langchain_core.messages import HumanMessage

    # å¦‚æœç”¨æˆ·æœªæ˜¾å¼æä¾›ä»»åŠ¡ä¿¡æ¯ï¼Œå°è¯•ä»æœ¬åœ°JSONè¯»å–é»˜è®¤ä»»åŠ¡æè¿°
    additional_info: Dict[str, Any] = {}
    if not task_description and not background and not dataset_info:
        task_json_path = Path("task/task_description.json")
        try:
            if task_json_path.exists():
                with task_json_path.open("r", encoding="utf-8") as f:
                    task_json = json.load(f)
                additional_info["task_json"] = task_json

                # ä»JSONä¸­æå–ä¿¡æ¯ï¼Œå¡«å……åˆ°æ–‡æœ¬å­—æ®µ
                task_name = task_json.get("task_name", "")
                task_goal = task_json.get("task_goal", "")
                task_requirements = task_json.get("task_requirements", "")
                task_dataset = task_json.get("task_dataset", {})

                if not task_description:
                    # ä»»åŠ¡æè¿°åå‘ä»»åŠ¡åç§°
                    task_description = task_name or task_goal
                if not background:
                    # èƒŒæ™¯/ç›®æ ‡ä¸è¦æ±‚åˆå¹¶
                    background_parts = []
                    if task_goal:
                        background_parts.append(f"Goal: {task_goal}")
                    if task_requirements:
                        background_parts.append(f"Requirements: {task_requirements}")
                    background = "\n".join(background_parts) if background_parts else None
                if not dataset_info and task_dataset:
                    # å°†æ•°æ®é›†ç»“æ„è½¬æˆå¯è¯»æ–‡æœ¬ï¼Œæ–¹ä¾¿Supervisorå’ŒData Agentç†è§£
                    ds_path = task_dataset.get("file_path", "")
                    ds_type = task_dataset.get("data_type", "")
                    ds_inputs = task_dataset.get("input_features", [])
                    ds_target = task_dataset.get("target_variable", "")
                    ds_constraint = task_dataset.get("key_constraint", "")
                    dataset_info = (
                        f"File path: {ds_path}; "
                        f"Data type: {ds_type}; "
                        f"Input features: {', '.join(ds_inputs) if ds_inputs else 'N/A'}; "
                        f"Target variable: {ds_target or 'N/A'}; "
                        f"Constraint: {ds_constraint or 'N/A'}"
                    )
        except Exception as e:
            # å¤±è´¥æ—¶ä»…æ‰“å°è­¦å‘Šï¼Œä¸ä¸­æ–­å·¥ä½œæµ
            print(f"âš ï¸ è¯»å–é»˜è®¤ä»»åŠ¡æ–‡ä»¶ task/task_description.json å¤±è´¥: {e}")

    # åˆ›å»ºåˆå§‹æ¶ˆæ¯ï¼ˆå±•ç¤ºå½“å‰æŒæ¡çš„ä»»åŠ¡ä¿¡æ¯ï¼‰
    initial_message = HumanMessage(
        content=(
            f"ä»»åŠ¡æè¿°: {task_description or 'å¾…æä¾›'}\n"
            f"èƒŒæ™¯è¦æ±‚: {background or 'å¾…æä¾›'}\n"
            f"æ•°æ®é›†ä¿¡æ¯: {dataset_info or 'å¾…æä¾›'}"
        )
    )

    state: REAgentState = {
        "messages": [initial_message],
        "task_description": task_description,
        "background": background,
        "dataset_info": dataset_info,
        "methodology": methodology,
        "model_architecture": model_architecture,
        "evaluation_metrics": evaluation_metrics,
        "next_action": "request_info",
        "data_critique": None,
        "methodology_critique": None,
        "model_critique": None,
        "results_critique": None,
        "iteration_count": 0,
        "max_iterations": max_iterations,
        "final_report": None,
        "additional_info": additional_info or None,
    }

    # å¦‚æœæä¾›äº†rag_systemï¼Œè®¾ç½®å·¥ä½œæµç»„ä»¶
    if rag_system:
        set_workflow_components(rag_system=rag_system)

    return state


# ==================== å¯¼å‡º ====================

__all__ = [
    "create_workflow_graph",
    "create_initial_state",
    "set_workflow_components",
    "supervisor_node",
    "request_info_node",
    "analyze_node",
    "discuss_node",
    "iterate_node",
    "report_node",
    "route_decision"
]

