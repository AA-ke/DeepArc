import json
import os
import sys
import string
from datetime import datetime
from collections import defaultdict
import difflib

def load_unified_data():
    """åŠ è½½ç»Ÿä¸€æ ¼å¼çš„æ•°æ®"""
    try:
        with open("Knowledge_Corpus/data/unified/all_documents_raw.json", "r", encoding="utf-8") as f:
            data = json.load(f)
            if not isinstance(data, list):
                print("âš ï¸ æ•°æ®æ ¼å¼é”™è¯¯ï¼šæœŸæœ›åˆ—è¡¨", file=sys.stderr, flush=True)
                return []
            return data
    except FileNotFoundError:
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨: Knowledge_Corpus/data/unified/all_documents_raw.json", file=sys.stderr, flush=True)
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æé”™è¯¯: {e}", file=sys.stderr, flush=True)
        sys.exit(1)
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}", file=sys.stderr, flush=True)
        sys.exit(1)


def calculate_title_similarity(title1, title2):
    """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦"""
    if not title1 or not title2:
        return 0.0
    
    title1 = title1.lower().strip()
    title2 = title2.lower().strip()
    
    return difflib.SequenceMatcher(None, title1, title2).ratio()


def deduplicate_by_id(docs):
    """åŸºäºIDå»é‡"""
    
    print("1ï¸âƒ£ åŸºäºIDå»é‡...", flush=True)
    
    seen_ids = {}  # key: source_id, value: doc
    duplicates = []
    docs_without_id = []
    
    for doc in docs:
        if not isinstance(doc, dict):
            continue
        
        source = doc.get("source", "")
        source_id = doc.get("source_id", "")
        
        if not source_id:
            docs_without_id.append(doc)
            continue
        
        # æ„å»ºå¤åˆkey
        key = f"{source}:{source_id}"
        
        if key in seen_ids:
            duplicates.append(doc)
        else:
            seen_ids[key] = doc
    
    # åˆå¹¶æœ‰IDå’Œæ²¡æœ‰IDçš„æ–‡æ¡£
    result = list(seen_ids.values()) + docs_without_id
    
    print(f"   å‘ç° {len(duplicates)} ä¸ªIDé‡å¤é¡¹", flush=True)
    if docs_without_id:
        print(f"   ä¿ç•™ {len(docs_without_id)} ä¸ªæ— IDæ–‡æ¡£", flush=True)
    
    return result, duplicates


def deduplicate_by_doi(docs):
    """åŸºäºDOIå»é‡"""
    
    print("2ï¸âƒ£ åŸºäºDOIå»é‡...", flush=True)
    
    seen_dois = {}
    duplicates = []
    unique_docs = []
    
    for doc in docs:
        if not isinstance(doc, dict):
            continue
        
        # å®‰å…¨è·å–DOIï¼Œå¤„ç†Noneå€¼
        doi_raw = doc.get("doi") or ""
        doi = str(doi_raw).strip().lower() if doi_raw else ""
        
        if doi and doi in seen_dois:
            duplicates.append(doc)
        else:
            unique_docs.append(doc)
            if doi:
                seen_dois[doi] = doc
    
    print(f"   å‘ç° {len(duplicates)} ä¸ªDOIé‡å¤é¡¹", flush=True)
    
    return unique_docs, duplicates


def deduplicate_by_title(docs, similarity_threshold=0.9):
    """åŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦å»é‡ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å­—å…¸åŠ é€Ÿï¼‰"""
    
    print(f"3ï¸âƒ£ åŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦å»é‡ (é˜ˆå€¼: {similarity_threshold})...", flush=True)
    
    unique_docs = []
    duplicates = []
    seen_titles_dict = {}  # key: normalized_title, value: doc
    
    for i, doc in enumerate(docs):
        if not isinstance(doc, dict):
            unique_docs.append(doc)
            continue
        
        title = doc.get("title", "").strip()
        
        if not title:
            unique_docs.append(doc)
            continue
        
        # æ ‡å‡†åŒ–æ ‡é¢˜ç”¨äºå¿«é€ŸåŒ¹é…ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·å’Œå¤šä½™ç©ºæ ¼ï¼‰
        title_lower = title.lower()
        title_normalized = ''.join(c for c in title_lower if c not in string.punctuation)
        title_normalized = ' '.join(title_normalized.split())  # è§„èŒƒåŒ–ç©ºæ ¼
        
        is_duplicate = False
        
        # æ£€æŸ¥æ ‡å‡†åŒ–æ ‡é¢˜æ˜¯å¦å·²å­˜åœ¨
        if title_normalized in seen_titles_dict:
            # ç²¾ç¡®åŒ¹é…ï¼Œç›´æ¥åˆ¤å®šä¸ºé‡å¤
            duplicates.append(doc)
            is_duplicate = True
        else:
            # æ£€æŸ¥ç›¸ä¼¼åº¦ï¼ˆåªä¸å·²ä¿å­˜çš„æ ‡é¢˜æ¯”è¾ƒï¼Œå‡å°‘è®¡ç®—é‡ï¼‰
            for seen_title_norm, seen_doc in seen_titles_dict.items():
                similarity = difflib.SequenceMatcher(None, title_normalized, seen_title_norm).ratio()
                
                if similarity >= similarity_threshold:
                    duplicates.append(doc)
                    is_duplicate = True
                    break
        
        if not is_duplicate:
            unique_docs.append(doc)
            seen_titles_dict[title_normalized] = doc
        
        if (i + 1) % 1000 == 0:
            print(f"   å¤„ç†è¿›åº¦: {i+1}/{len(docs)}", flush=True)
    
    print(f"   å‘ç° {len(duplicates)} ä¸ªæ ‡é¢˜ç›¸ä¼¼é¡¹", flush=True)
    
    return unique_docs, duplicates


def analyze_duplicates(original_count, dedup_stages):
    """åˆ†æå»é‡æ•ˆæœ"""
    
    print(f"\n{'='*60}", flush=True)
    print("å»é‡åˆ†ææŠ¥å‘Š", flush=True)
    print(f"{'='*60}\n", flush=True)
    
    print(f"åŸå§‹æ–‡æ¡£æ•°: {original_count:,}", flush=True)
    
    current_count = original_count
    for stage_name, removed_count in dedup_stages:
        current_count -= removed_count
        print(f"{stage_name}: ç§»é™¤ {removed_count:,} æ¡ (å‰©ä½™: {current_count:,})", flush=True)
    
    total_removed = original_count - current_count
    dedup_rate = (total_removed / original_count * 100) if original_count > 0 else 0
    
    print(f"\næ€»è®¡ç§»é™¤: {total_removed:,} æ¡", flush=True)
    print(f"å»é‡ç‡: {dedup_rate:.2f}%", flush=True)
    print(f"æœ€ç»ˆæ–‡æ¡£æ•°: {current_count:,}", flush=True)


if __name__ == "__main__":
    
    print("="*60, flush=True)
    print("æ­¥éª¤2: æ•°æ®å»é‡", flush=True)
    print("="*60 + "\n", flush=True)
    
    try:
        # åŠ è½½æ•°æ®
        docs = load_unified_data()
        original_count = len(docs)
        
        if original_count == 0:
            print("âš ï¸ æœªåŠ è½½åˆ°ä»»ä½•æ–‡æ¡£", flush=True)
            sys.exit(0)
        
        print(f"åŠ è½½æ–‡æ¡£æ•°: {original_count:,}\n", flush=True)
        
        dedup_stages = []
        
        # 1. IDå»é‡
        docs, id_dups = deduplicate_by_id(docs)
        dedup_stages.append(("IDå»é‡", len(id_dups)))
        print(f"   å‰©ä½™æ–‡æ¡£: {len(docs):,}\n", flush=True)
        
        # 2. DOIå»é‡
        docs, doi_dups = deduplicate_by_doi(docs)
        dedup_stages.append(("DOIå»é‡", len(doi_dups)))
        print(f"   å‰©ä½™æ–‡æ¡£: {len(docs):,}\n", flush=True)
        
        # 3. æ ‡é¢˜å»é‡
        docs, title_dups = deduplicate_by_title(docs, similarity_threshold=0.9)
        dedup_stages.append(("æ ‡é¢˜å»é‡", len(title_dups)))
        print(f"   å‰©ä½™æ–‡æ¡£: {len(docs):,}\n", flush=True)
        
        # åˆ†æ
        analyze_duplicates(original_count, dedup_stages)
        
        # ä¿å­˜
        os.makedirs("Knowledge_Corpus/data/cleaned", exist_ok=True)
        os.makedirs("Knowledge_Corpus/data/metadata", exist_ok=True)
        
        try:
            with open("Knowledge_Corpus/data/cleaned/documents_deduped.json", "w", encoding="utf-8") as f:
                json.dump(docs, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: Knowledge_Corpus/data/cleaned/documents_deduped.json", flush=True)
        except Exception as e:
            print(f"\nâŒ ä¿å­˜å»é‡æ–‡æ¡£æ—¶å‡ºé”™: {e}", file=sys.stderr, flush=True)
        
        # ä¿å­˜é‡å¤é¡¹ï¼ˆç”¨äºå®¡æŸ¥ï¼‰
        all_duplicates = id_dups + doi_dups + title_dups
        if all_duplicates:
            try:
                with open("Knowledge_Corpus/data/cleaned/duplicates.json", "w", encoding="utf-8") as f:
                    json.dump(all_duplicates, f, indent=2, ensure_ascii=False)
                print(f"ğŸ’¾ é‡å¤é¡¹å·²ä¿å­˜åˆ°: Knowledge_Corpus/data/cleaned/duplicates.json", flush=True)
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜é‡å¤é¡¹æ—¶å‡ºé”™: {e}", file=sys.stderr, flush=True)
        
        # ä¿å­˜ç»Ÿè®¡
        try:
            with open("Knowledge_Corpus/data/metadata/02_dedup_stats.json", "w", encoding="utf-8") as f:
                json.dump({
                    "timestamp": datetime.now().isoformat(),
                    "original_count": original_count,
                    "final_count": len(docs),
                    "removed_count": original_count - len(docs),
                    "dedup_rate": (original_count - len(docs)) / original_count * 100 if original_count > 0 else 0,
                    "stages": [{"name": name, "removed": count} for name, count in dedup_stages]
                }, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“Š ç»Ÿè®¡å·²ä¿å­˜åˆ°: Knowledge_Corpus/data/metadata/02_dedup_stats.json", flush=True)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç»Ÿè®¡æ—¶å‡ºé”™: {e}", file=sys.stderr, flush=True)
        
        print(f"\nâœ… å®Œæˆï¼", flush=True)
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ·ä¸­æ–­", flush=True)
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}", file=sys.stderr, flush=True)
        import traceback
        traceback.print_exc()
        sys.exit(1)