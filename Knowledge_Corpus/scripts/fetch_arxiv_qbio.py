import arxiv
import json
import time
import os
import sys
from datetime import datetime, timedelta
from collections import defaultdict

def fetch_arxiv_by_category(category, max_results=500, days_back=365):
    """
    æŒ‰åˆ†ç±»è·å–arXivè®ºæ–‡
    
    Args:
        category: arXivåˆ†ç±»ä»£ç ï¼ˆå¦‚ "q-bio.GN"ï¼‰
        max_results: æœ€å¤§ç»“æœæ•°
        days_back: è·å–æœ€è¿‘Nå¤©çš„è®ºæ–‡
    """
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆä½¿ç”¨UTCæ—¶åŒºï¼Œå› ä¸ºarXivè¿”å›çš„æ˜¯UTCæ—¶é—´ï¼‰
    from datetime import timezone
    end_date = datetime.now(timezone.utc)
    start_date = end_date - timedelta(days=days_back)
    
    print(f"ğŸ” æœç´¢ {category} (æœ€è¿‘ {days_back} å¤©)...", end=" ", flush=True)
    
    papers = []
    
    try:
        # ä½¿ç”¨æ–°çš„Client API
        client = arxiv.Client()
        
        # æ„å»ºæŸ¥è¯¢
        search = arxiv.Search(
            query=f"cat:{category}",
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )
        
        result_count = 0
        for result in client.results(search):
            # æ£€æŸ¥æ—¥æœŸèŒƒå›´ï¼ˆç¡®ä¿æ—¶åŒºä¸€è‡´ï¼‰
            if result.published:
                # å¦‚æœpublishedæ˜¯naive datetimeï¼Œè½¬æ¢ä¸ºaware
                if result.published.tzinfo is None:
                    published_date = result.published.replace(tzinfo=timezone.utc)
                else:
                    published_date = result.published
                
                if published_date < start_date:
                    break
            
            try:
                paper = {
                    "arxiv_id": result.entry_id.split('/')[-1] if result.entry_id else "N/A",
                    "title": result.title or "N/A",
                    "abstract": result.summary.replace('\n', ' ') if result.summary else "",
                    "authors": [author.name for author in result.authors] if result.authors else [],
                    "published": result.published.strftime("%Y-%m-%d") if result.published else "N/A",
                    "updated": result.updated.strftime("%Y-%m-%d") if result.updated else None,
                    "categories": list(result.categories) if result.categories else [],
                    "primary_category": result.primary_category or "N/A",
                    "pdf_url": result.pdf_url or "",
                    "doi": result.doi or None,
                    "journal_ref": result.journal_ref or None,
                    "comment": result.comment or None,
                    "source": "arXiv"
                }
                papers.append(paper)
                result_count += 1
                
                # æ¯50ç¯‡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if result_count % 50 == 0:
                    print(f"å·²è·å– {result_count} ç¯‡...", end=" ", flush=True)
                    
            except Exception as e:
                print(f"\n  âš ï¸ è§£æè®ºæ–‡æ—¶å‡ºé”™: {e}", file=sys.stderr, flush=True)
                continue
        
        print(f"âœ… {len(papers)} ç¯‡", flush=True)
        
    except arxiv.UnexpectedEmptyPageError:
        print(f"âœ… {len(papers)} ç¯‡ï¼ˆå·²åˆ°æœ«å°¾ï¼‰", flush=True)
    except arxiv.HTTPError as e:
        print(f"âŒ HTTPé”™è¯¯: {e}", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}", file=sys.stderr, flush=True)
    
    return papers


def save_papers(papers, filename):
    """ä¿å­˜è®ºæ–‡"""
    os.makedirs("Knowledge_Corpus/data/raw", exist_ok=True)
    
    # JSON
    json_path = f"Knowledge_Corpus/data/raw/{filename}.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(papers, f, indent=2, ensure_ascii=False)
    
    # TXT
    txt_path = f"Knowledge_Corpus/data/raw/{filename}.txt"
    with open(txt_path, "w", encoding="utf-8") as f:
        for i, paper in enumerate(papers, 1):
            f.write(f"{'='*100}\n")
            f.write(f"Paper {i}/{len(papers)}\n")
            f.write(f"{'='*100}\n")
            f.write(f"arXiv ID: {paper['arxiv_id']}\n")
            f.write(f"Title: {paper['title']}\n")
            f.write(f"Authors: {', '.join(paper['authors'])}\n")
            f.write(f"Published Date: {paper['published']}\n")
            f.write(f"Primary Category: {paper['primary_category']}\n")
            f.write(f"All Categories: {', '.join(paper['categories'])}\n")
            f.write(f"PDF: {paper['pdf_url']}\n")
            if paper['doi']:
                f.write(f"DOI: {paper['doi']}\n")
            if paper['journal_ref']:
                f.write(f"Journal Reference: {paper['journal_ref']}\n")
            if paper['comment']:
                f.write(f"Comment: {paper['comment']}\n")
            f.write(f"\nAbstract:\n{paper['abstract']}\n\n")
    
    print(f"ğŸ’¾ å·²ä¿å­˜: {json_path}", flush=True)
    print(f"ğŸ’¾ å·²ä¿å­˜: {txt_path}", flush=True)


if __name__ == "__main__":
    
    print("="*60, flush=True)
    print("arXiv Quantitative Biology (q-bio) è®ºæ–‡é‡‡é›†", flush=True)
    print("="*60 + "\n", flush=True)
    
    # arXiv q-bio åˆ†ç±»ï¼ˆèšç„¦ï¼šåŸºå› è°ƒæ§å…ƒä»¶è®¾è®¡ï¼Œå…¼é¡¾åŸºç¡€ä¸æ·±åº¦ï¼‰
    qbio_categories = {
         "q-bio.GN": {
            "name": "Genomics (åŸºå› ç»„å­¦)",
            "priority": "HIGH",
            "max_results": 400,
            "days_back": 730,  # 2å¹´
            "keywords": [
                # åŸºç¡€åŸºå› è°ƒæ§ä¸å…ƒä»¶
                ["gene regulation", "cis-regulatory element", "regulatory element"],
                ["promoter", "enhancer", "silencer", "insulator"],
                ["transcription factor", "TF binding site", "TFBS"],
                ["chromatin accessibility", "ATAC-seq", "DNase-seq"],
                # è°ƒæ§å…ƒä»¶è®¾è®¡ä¸é¢„æµ‹
                ["regulatory element design", "cis-regulatory design", "regulatory grammar"],
                ["synthetic enhancer", "synthetic promoter", "synthetic regulatory element"],
                ["MPRA", "massively parallel reporter assay"],
                ["CRE activity prediction", "enhancer activity prediction"],
            ]
        },
        
        "q-bio.QM": {
            "name": "Quantitative Methods (å®šé‡æ–¹æ³•)",
            "priority": "HIGH",
            "max_results": 400,
            "days_back": 730,
            "keywords": [
                # ç”¨äºè°ƒæ§å…ƒä»¶è®¾è®¡çš„å®šé‡/æœºå™¨å­¦ä¹ æ–¹æ³•
                ["sequence-to-function model", "sequence function prediction"],
                ["deep learning for genomics", "deep learning for gene regulation"],
                ["convolutional neural network", "CNN", "transformer"],
                ["probabilistic model", "Bayesian model", "generative model"],
                # è®¾è®¡ä¼˜åŒ–ä¸æœç´¢
                ["sequence design", "sequence optimization", "in silico design"],
                ["Bayesian optimization", "evolutionary algorithm", "genetic algorithm"],
                ["inverse design", "optimal regulatory sequence"],
                # é«˜é€šé‡æ•°æ®å»ºæ¨¡
                ["MPRA modeling", "massively parallel reporter assay analysis"],
                ["predictive model of enhancer activity", "TF binding prediction"],
            ]
        },
        
        "q-bio.MN": {
            "name": "Molecular Networks (åˆ†å­ç½‘ç»œ)",
            "priority": "MEDIUM",
            "max_results": 300,
            "days_back": 730,
            "keywords": [
                ["gene regulatory network", "GRN", "transcriptional network"],
                ["cis-regulatory network", "enhancer-promoter interaction"],
                ["3D genome", "chromatin looping", "chromatin contact"],
                ["network-based design", "network-constrained design"],
                ["systems biology", "pathway-level regulation"],
            ]
        },
        
        "q-bio.BM": {
            "name": "Biomolecules (ç”Ÿç‰©åˆ†å­)",
            "priority": "MEDIUM",
            "max_results": 300,
            "days_back": 730,
            "keywords": [
                # DNA / RNA è°ƒæ§åºåˆ—
                ["regulatory DNA sequence", "regulatory RNA sequence"],
                ["motif discovery", "PWM", "position weight matrix"],
                ["binding motif", "TF binding motif"],
                ["RNA regulatory element", "UTR element"],
                # ç»“æ„ä¸åŠŸèƒ½è”ç³»
                ["sequence grammar", "regulatory code"],
                ["biophysical model of binding", "binding energetics"],
            ]
        },
        
        "q-bio.PE": {
            "name": "Populations and Evolution (ç¾¤ä½“ä¸è¿›åŒ–)",
            "priority": "MEDIUM",
            "max_results": 250,
            "days_back": 730,
            "keywords": [
                ["regulatory evolution", "cis-regulatory evolution"],
                ["enhancer evolution", "promoter evolution"],
                ["selection on regulatory elements", "adaptive regulatory change"],
                ["comparative regulatory genomics", "conserved non-coding element"],
            ]
        },
        
        "q-bio.CB": {
            "name": "Cell Behavior (ç»†èƒè¡Œä¸º)",
            "priority": "LOW",
            "max_results": 150,
            "days_back": 365,
            "keywords": [
                ["cell-type-specific enhancer", "cell type specific regulatory element"],
                ["gene regulation in cell differentiation", "regulatory program"],
                ["single-cell gene regulation", "single-cell regulatory landscape"],
                ["spatial gene regulation", "spatial enhancer activity"],
            ]
        },
        
        "q-bio.NC": {
            "name": "Neurons and Cognition (ç¥ç»ä¸è®¤çŸ¥)",
            "priority": "LOW",
            "max_results": 100,
            "days_back": 365,
            "keywords": [
                ["neuronal enhancer", "brain-specific regulatory element"],
                ["gene regulation in neural development", "neurodevelopmental regulation"],
                ["regulatory elements in cognition", "brain regulatory landscape"],
            ]
        },
        
        "q-bio.SC": {
            "name": "Subcellular Processes (äºšç»†èƒè¿‡ç¨‹)",
            "priority": "LOW",
            "max_results": 100,
            "days_back": 365,
            "keywords": [
                ["transcription regulation", "transcriptional control"],
                ["promoter architecture", "core promoter elements"],
                ["enhancer-promoter communication", "transcriptional bursting"],
            ]
        },
        
        "q-bio.TO": {
            "name": "Tissues and Organs (ç»„ç»‡ä¸å™¨å®˜)",
            "priority": "LOW",
            "max_results": 100,
            "days_back": 365,
            "keywords": [
                ["tissue-specific enhancer", "tissue-specific promoter"],
                ["regulatory element atlas", "regulatory annotation in tissues"],
            ]
        },
        
        "q-bio.OT": {
            "name": "Other Quantitative Biology (å…¶ä»–)",
            "priority": "SKIP",  # è·³è¿‡ï¼Œå†…å®¹å¤ªæ‚
            "max_results": 0,
            "days_back": 0,
            "keywords": []
        },
    }
    
    all_papers = []
    category_stats = {}
    
    # æŒ‰åˆ†ç±»è·å–
    # è¿‡æ»¤æ‰éœ€è¦è·³è¿‡çš„åˆ†ç±»
    active_categories = {k: v for k, v in qbio_categories.items() if v.get('priority') != 'SKIP'}
    total_categories = len(active_categories)
    
    for i, (category, config) in enumerate(active_categories.items(), 1):
        category_name = config.get('name', category)
        priority = config.get('priority', 'MEDIUM')
        max_results = config.get('max_results', 300)
        days_back = config.get('days_back', 730)
        
        print(f"\n[{i}/{total_categories}] [{category}] {category_name} (ä¼˜å…ˆçº§: {priority})", flush=True)
        
        try:
            papers = fetch_arxiv_by_category(
                category=category,
                max_results=max_results,
                days_back=days_back
            )
            
            if papers:
                # ä¿å­˜è¯¥åˆ†ç±»
                cat_name = category.replace(".", "_")
                save_papers(papers, f"arxiv_{cat_name}")
                
                all_papers.extend(papers)
                category_stats[category] = len(papers)
            else:
                print(f"  âš ï¸ æœªæ‰¾åˆ°è®ºæ–‡", flush=True)
                category_stats[category] = 0
            
            time.sleep(3)  # ç¤¼è²Œå»¶è¿Ÿ
        except Exception as e:
            print(f"  âŒ å¤„ç†åˆ†ç±»æ—¶å‡ºé”™: {e}", file=sys.stderr, flush=True)
            category_stats[category] = 0
            continue
    
    # å»é‡ï¼ˆä¸€ç¯‡è®ºæ–‡å¯èƒ½å±äºå¤šä¸ªåˆ†ç±»ï¼‰
    print(f"\n{'='*60}", flush=True)
    print("å»é‡å¤„ç†...", flush=True)
    print(f"{'='*60}", flush=True)
    
    if not all_papers:
        print("âš ï¸  æœªè·å–åˆ°ä»»ä½•è®ºæ–‡", flush=True)
        sys.exit(0)
    
    unique_papers = {}
    for paper in all_papers:
        arxiv_id = paper.get('arxiv_id', '')
        if arxiv_id and arxiv_id not in unique_papers:
            unique_papers[arxiv_id] = paper
    
    unique_list = list(unique_papers.values())
    
    print(f"åŸå§‹æ€»æ•°: {len(all_papers)} ç¯‡", flush=True)
    print(f"å»é‡å: {len(unique_list)} ç¯‡", flush=True)
    
    # ä¿å­˜æ‰€æœ‰ï¼ˆå»é‡åï¼‰
    if unique_list:
        save_papers(unique_list, "arxiv_qbio_all_unique")
    
    # ç»Ÿè®¡æŠ¥å‘Š
    print(f"\nğŸ“Š ç»Ÿè®¡æŠ¥å‘Š:", flush=True)
    print(f"{'â”€'*60}", flush=True)
    
    print(f"\næŒ‰åˆ†ç±»:", flush=True)
    for cat, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
        cat_config = qbio_categories.get(cat, {})
        desc = cat_config.get('name', cat) if isinstance(cat_config, dict) else str(cat_config)
        print(f"  {cat:12s} ({desc:30s}): {count:3d} ç¯‡", flush=True)
    
    # æŒ‰å¹´ä»½
    years = defaultdict(int)
    for paper in unique_list:
        published = paper.get('published', '')
        if published and len(published) >= 4:
            year = published[:4]
            if year.isdigit():
                years[year] += 1
    
    if years:
        print(f"\næŒ‰å¹´ä»½:", flush=True)
        for year in sorted(years.keys(), reverse=True):
            print(f"  {year}: {years[year]:3d} ç¯‡", flush=True)
    
    # Topä½œè€…
    author_count = defaultdict(int)
    for paper in unique_list:
        authors = paper.get('authors', [])
        for author in authors[:3]:  # åªç»Ÿè®¡å‰3ä½ä½œè€…
            if author:
                author_count[author] += 1
    
    if author_count:
        print(f"\né«˜äº§ä½œè€… (Top 10):", flush=True)
        for author, count in sorted(author_count.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {author:40s}: {count:2d} ç¯‡", flush=True)
    
    # ä¿å­˜å…ƒæ•°æ®
    try:
        metadata = {
            "fetch_date": datetime.now().isoformat(),
            "total_papers": len(unique_list),
            "date_range": {
                "start": min(p.get('published', '') for p in unique_list if p.get('published')),
                "end": max(p.get('published', '') for p in unique_list if p.get('published'))
            },
            "category_stats": category_stats,
            "year_stats": dict(years)
        }
        
        os.makedirs("Knowledge_Corpus/data/metadata", exist_ok=True)
        with open("Knowledge_Corpus/data/metadata/arxiv_qbio_summary.json", "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜: Knowledge_Corpus/data/metadata/arxiv_qbio_summary.json", flush=True)
    except Exception as e:
        print(f"\nâš ï¸ ä¿å­˜å…ƒæ•°æ®æ—¶å‡ºé”™: {e}", file=sys.stderr, flush=True)
    
    print(f"\nâœ… å®Œæˆï¼", flush=True)