import json
import os
import sys
import re
from datetime import datetime
from collections import defaultdict
import hashlib

def load_json_safe(filepath):
    """å®‰å…¨åŠ è½½JSONæ–‡ä»¶"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # å¦‚æœæ˜¯å­—å…¸ä¸”åŒ…å«papers/repos/pipelinesé”®ï¼Œæå–åˆ—è¡¨
            if isinstance(data, dict):
                if "papers" in data:
                    return data["papers"]
                elif "repos" in data:
                    return data["repos"]
                elif "pipelines" in data:
                    return data["pipelines"]
                elif "collection" in data:
                    return data["collection"]
            return data if isinstance(data, list) else []
    except FileNotFoundError:
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}", file=sys.stderr, flush=True)
        return []
    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSONè§£æé”™è¯¯ {filepath}: {e}", file=sys.stderr, flush=True)
        return []
    except Exception as e:
        print(f"âš ï¸ æ— æ³•åŠ è½½ {filepath}: {e}", file=sys.stderr, flush=True)
        return []


def generate_doc_id(source, identifier):
    """ç”Ÿæˆå”¯ä¸€æ–‡æ¡£ID"""
    if not identifier:
        identifier = "unknown"
    unique_string = f"{source}_{identifier}"
    return hashlib.md5(unique_string.encode()).hexdigest()[:16]


def unify_pubmed(papers):
    """ç»Ÿä¸€PubMedæ ¼å¼"""
    unified = []
    
    for paper in papers:
        if not isinstance(paper, dict):
            continue
        
        pmid = paper.get("pmid", "") or paper.get("pmid", "")
        if not pmid:
            continue  # å¿…é¡»æœ‰PMID
            
        doc = {
            "doc_id": generate_doc_id("pubmed", pmid),
            "source": "PubMed",
            "source_id": pmid,
            "title": paper.get("title", "").strip(),
            "abstract": paper.get("abstract", "").strip(),
            "authors": paper.get("authors", ""),
            "journal": paper.get("journal", ""),
            "date": paper.get("date", ""),
            "doi": paper.get("doi", ""),
            "url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}" if pmid else "",
            "keywords": [],
            "full_text": "",  # PubMedåªæœ‰æ‘˜è¦
            "methods": "",
            "metadata": {
                "pmid": pmid,
                "pmc_id": paper.get("pmc_id", "")
            }
        }
        unified.append(doc)
    
    return unified


def unify_biorxiv(papers):
    """ç»Ÿä¸€bioRxiv/medRxivæ ¼å¼"""
    unified = []
    
    for paper in papers:
        if not isinstance(paper, dict):
            continue
        
        doi = paper.get("doi", "")
        if not doi:
            continue  # å¿…é¡»æœ‰DOI
        
        # åˆ¤æ–­æ¥æº
        doi_lower = doi.lower()
        if "biorxiv" in doi_lower:
            source = "bioRxiv"
        elif "medrxiv" in doi_lower:
            source = "medRxiv"
        else:
            source = "bioRxiv"  # é»˜è®¤
        
        doc = {
            "doc_id": generate_doc_id("biorxiv", doi),
            "source": source,
            "source_id": doi,
            "title": paper.get("title", "").strip(),
            "abstract": paper.get("abstract", "").strip(),
            "authors": paper.get("authors", ""),
            "journal": "",  # é¢„å°æœ¬æ— æœŸåˆŠ
            "date": paper.get("date", ""),
            "doi": doi,
            "url": f"https://doi.org/{doi}" if doi else "",
            "keywords": [],
            "full_text": "",
            "methods": "",
            "metadata": {
                "category": paper.get("category", ""),
                "version": paper.get("version", "")
            }
        }
        unified.append(doc)
    
    return unified


def unify_pmc(papers):
    """ç»Ÿä¸€PMCæ ¼å¼"""
    unified = []
    
    for paper in papers:
        if not isinstance(paper, dict):
            continue
        
        pmc_id = paper.get("pmc_id", "")
        # å¦‚æœæ²¡æœ‰PMC IDï¼Œå°è¯•ä½¿ç”¨DOI
        if not pmc_id or pmc_id == "N/A":
            pmc_id = paper.get("doi", "")
        
        if not pmc_id or pmc_id == "N/A":
            continue  # å¿…é¡»æœ‰æ ‡è¯†ç¬¦
        
        doc = {
            "doc_id": generate_doc_id("pmc", pmc_id),
            "source": "PMC",
            "source_id": pmc_id,
            "title": paper.get("title", "").strip(),
            "abstract": paper.get("abstract", "").strip(),
            "authors": paper.get("authors", ""),
            "journal": paper.get("journal", ""),
            "date": paper.get("date", ""),
            "doi": paper.get("doi", ""),
            "url": f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmc_id}" if pmc_id and pmc_id != "N/A" else "",
            "keywords": [],
            "full_text": "",
            "methods": paper.get("methods", ""),  # PMCå¯èƒ½æœ‰Methodséƒ¨åˆ†
            "metadata": {
                "pmc_id": paper.get("pmc_id", ""),
                "pmid": paper.get("pmid", "")
            }
        }
        unified.append(doc)
    
    return unified


def unify_arxiv(papers):
    """ç»Ÿä¸€arXivæ ¼å¼"""
    unified = []
    
    for paper in papers:
        doc = {
            "doc_id": generate_doc_id("arxiv", paper.get("arxiv_id", "")),
            "source": "arXiv",
            "source_id": paper.get("arxiv_id", ""),
            "title": paper.get("title", ""),
            "abstract": paper.get("abstract", ""),
            "authors": ", ".join(paper.get("authors", [])) if isinstance(paper.get("authors"), list) else paper.get("authors", ""),
            "journal": paper.get("journal_ref", ""),
            "date": paper.get("published", ""),
            "doi": paper.get("doi", ""),
            "url": paper.get("pdf_url", ""),
            "keywords": paper.get("categories", []),
            "full_text": "",
            "methods": "",
            "metadata": {
                "arxiv_id": paper.get("arxiv_id", ""),
                "categories": paper.get("categories", []),
                "primary_category": paper.get("primary_category", ""),
                "comment": paper.get("comment", "")
            }
        }
        unified.append(doc)
    
    return unified


def unify_github(repos):
    """ç»Ÿä¸€GitHubæ ¼å¼"""
    unified = []
    
    for repo in repos:
        doc = {
            "doc_id": generate_doc_id("github", repo.get("full_name", "")),
            "source": "GitHub",
            "source_id": repo.get("full_name", ""),
            "title": repo.get("name", ""),
            "abstract": repo.get("description", ""),
            "authors": "",  # GitHubæ— æ˜ç¡®ä½œè€…åˆ—è¡¨
            "journal": "",
            "date": repo.get("created_at", "")[:10] if repo.get("created_at") else "",
            "doi": "",
            "url": repo.get("url", ""),
            "keywords": repo.get("topics", []),
            "full_text": repo.get("readme", ""),
            "methods": "",
            "metadata": {
                "stars": repo.get("stars", 0),
                "forks": repo.get("forks", 0),
                "language": repo.get("language", ""),
                "license": repo.get("license", ""),
                "category": repo.get("category", ""),
                "updated_at": repo.get("updated_at", "")
            }
        }
        unified.append(doc)
    
    return unified




def parse_pubmed_txt(filepath):
    """è§£ææ–‡æœ¬æ ¼å¼çš„PubMedæ•°æ®"""
    papers = []
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŒ‰è®ºæ–‡åˆ†å‰²ï¼ˆä»¥æ•°å­—ç¼–å·å¼€å¤´ï¼‰
        paper_blocks = re.split(r'\n(?=\d+\.\s)', content)
        
        for block in paper_blocks:
            if not block.strip():
                continue
            
            paper = {}
            lines = block.split('\n')
            
            # æå–PMID
            pmid_match = re.search(r'PMID:\s*(\d+)', block)
            if pmid_match:
                paper['pmid'] = pmid_match.group(1)
            
            # æå–DOI
            doi_match = re.search(r'doi:\s*([^\s\n]+)', block, re.IGNORECASE)
            if doi_match:
                paper['doi'] = doi_match.group(1)
            
            # æå–æ ‡é¢˜ï¼ˆé€šå¸¸åœ¨ç¼–å·åçš„å‡ è¡Œï¼Œä½†å¯èƒ½éš”ç€ç©ºè¡Œï¼‰
            first_num_idx = None
            for i, line in enumerate(lines):
                if re.match(r'^\d+\.', line.strip()):
                    first_num_idx = i
                    break
            if first_num_idx is not None:
                # åœ¨ç¼–å·è¡Œä¹‹åï¼Œå¯»æ‰¾ç¬¬ä¸€ä¸ªéç©ºä¸”ä¸æ˜¯å…ƒæ•°æ®çš„è¡Œä½œä¸ºæ ‡é¢˜
                for j in range(first_num_idx + 1, min(first_num_idx + 6, len(lines))):
                    title_line = lines[j].strip()
                    if title_line and not title_line.startswith(('Author', 'DOI', 'PMID', 'PMCID')):
                        paper['title'] = title_line
                        break
            
            # æå–æ‘˜è¦ï¼ˆä¼˜å…ˆä½¿ç”¨å¸¦BACKGROUNDç»“æ„çš„æ ¼å¼ï¼‰
            abstract_match = re.search(
                r'BACKGROUND:(.*?)(?:CONCLUSIONS?|DOI|PMID|PMCID|$)',
                block,
                re.DOTALL | re.IGNORECASE
            )
            if abstract_match:
                abstract = abstract_match.group(1).strip()
                abstract = re.sub(r'\s+', ' ', abstract)
                paper['abstract'] = abstract
            else:
                # å›é€€ç­–ç•¥ï¼šä» Author information ä¹‹ååˆ° DOI/PMID/PMCID ä¹‹å‰çš„ä¸»è¦æ®µè½è§†ä½œæ‘˜è¦
                lines_stripped = [ln.rstrip() for ln in lines]
                author_idx = None
                for i, ln in enumerate(lines_stripped):
                    if ln.strip().lower().startswith("author information"):
                        author_idx = i
                        break
                
                if author_idx is not None:
                    # æ‰¾åˆ°ä½œè€…ä¿¡æ¯åç¬¬ä¸€ä¸ªâ€œéç©ºä¸”å‰ä¸€è¡Œä¸ºç©ºâ€çš„ä½ç½®ä½œä¸ºæ‘˜è¦èµ·ç‚¹
                    start_idx = None
                    for i in range(author_idx + 1, len(lines_stripped)):
                        if lines_stripped[i].strip() and (i == author_idx + 1 or not lines_stripped[i-1].strip()):
                            start_idx = i
                            break
                    
                    if start_idx is not None:
                        abs_lines = []
                        for j in range(start_idx, len(lines_stripped)):
                            s = lines_stripped[j].strip()
                            # é‡åˆ° DOI/PMID/PMCID ä¹‹ç±»çš„æ ‡ç­¾åˆ™åœæ­¢
                            if re.match(r'^(DOI:|PMID:|PMCID:)', s, re.IGNORECASE):
                                break
                            if not s:
                                # å…è®¸å†…éƒ¨ç©ºè¡Œï¼Œä½†å¦‚æœä¸‹ä¸€è¡Œæ˜¯ DOI/PMID/PMCID å°±æå‰ç»“æŸ
                                if j + 1 < len(lines_stripped) and re.match(
                                    r'^(DOI:|PMID:|PMCID:)', lines_stripped[j+1].strip(), re.IGNORECASE
                                ):
                                    break
                                continue
                            abs_lines.append(s)
                        
                        abstract = re.sub(r'\s+', ' ', " ".join(abs_lines)).strip()
                        if abstract:
                            paper['abstract'] = abstract
            
            # æå–ä½œè€…ï¼ˆAuthor informationéƒ¨åˆ†ï¼‰
            author_match = re.search(r'Author information:(.*?)(?:BACKGROUND|DOI|PMID|PMCID|$)', block, re.DOTALL | re.IGNORECASE)
            if author_match:
                author_text = author_match.group(1)
                # æå–ä½œè€…å§“å
                authors = re.findall(r'([A-Z][a-z]+\s+[A-Z][a-z]+)', author_text)
                if authors:
                    paper['authors'] = ', '.join(authors[:10])  # æœ€å¤šå–å‰10ä¸ªä½œè€…
            
            # æå–æœŸåˆŠå’Œæ—¥æœŸ
            journal_match = re.match(r'^\d+\.\s*([^.]+)\.\s*(\d{4})', block)
            if journal_match:
                paper['journal'] = journal_match.group(1).strip()
                paper['date'] = journal_match.group(2)
            
            if paper.get('pmid') or paper.get('doi'):
                papers.append(paper)
    
    except Exception as e:
        print(f"âš ï¸ è§£æPubMedæ–‡æœ¬æ–‡ä»¶æ—¶å‡ºé”™ {filepath}: {e}", file=sys.stderr, flush=True)
    
    return papers





if __name__ == "__main__":
    
    print("="*60, flush=True)
    print("æ­¥éª¤1: æ•°æ®ç»Ÿä¸€ä¸æ•´åˆ", flush=True)
    print("="*60 + "\n", flush=True)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("Knowledge_Corpus/data/unified", exist_ok=True)
    os.makedirs("Knowledge_Corpus/data/metadata", exist_ok=True)
    
    all_docs = []
    source_stats = defaultdict(int)
    
    # ========== å¤„ç†å„æ•°æ®æº ==========
    
    # 1. PubMed
    print("1ï¸âƒ£ å¤„ç† PubMed æ•°æ®...", flush=True)
    try:
        # å¤„ç†JSONæ ¼å¼
        pubmed_json_files = [f for f in os.listdir("Knowledge_Corpus/data/raw") if f.startswith("pubmed") and f.endswith(".json")]
        if pubmed_json_files:
            print(f"   æ‰¾åˆ° {len(pubmed_json_files)} ä¸ªJSONæ–‡ä»¶", flush=True)
            for i, file in enumerate(pubmed_json_files, 1):
                print(f"   å¤„ç† ({i}/{len(pubmed_json_files)}): {file}...", end=" ", flush=True)
                papers = load_json_safe(f"Knowledge_Corpus/data/raw/{file}")
                if papers:
                    unified = unify_pubmed(papers)
                    all_docs.extend(unified)
                    source_stats["PubMed"] += len(unified)
                    print(f"âœ… {len(unified)} ç¯‡", flush=True)
                else:
                    print(f"âš ï¸ æ— æ•°æ®", flush=True)
        
        
        print(f"   âœ… PubMedæ€»è®¡: {source_stats['PubMed']} ç¯‡\n", flush=True)
    except Exception as e:
        print(f"   âŒ å¤„ç†PubMedæ—¶å‡ºé”™: {e}\n", file=sys.stderr, flush=True)
    
    # 2. bioRxiv/medRxiv
    print("2ï¸âƒ£ å¤„ç† bioRxivæ•°æ®...", flush=True)
    try:
        biorxiv_files = [f for f in os.listdir("Knowledge_Corpus/data/raw") if ("biorxiv" in f) and f.endswith(".json")]
        for file in biorxiv_files:
            papers = load_json_safe(f"Knowledge_Corpus/data/raw/{file}")
            if papers:
                unified = unify_biorxiv(papers)
                all_docs.extend(unified)
                for doc in unified:
                    source_stats[doc["source"]] += 1
        print(f"   âœ… bioRxiv: {source_stats.get('bioRxiv', 0)} ç¯‡", flush=True)
    except Exception as e:
        print(f"   âŒ å¤„ç†bioRxivæ—¶å‡ºé”™: {e}\n", file=sys.stderr, flush=True)
    
    # 3. PMC
    print("3ï¸âƒ£ å¤„ç† PMC æ•°æ®...", flush=True)
    try:
        pmc_files = [f for f in os.listdir("Knowledge_Corpus/data/raw") if "pmc" in f and f.endswith(".json")]
        for file in pmc_files:
            papers = load_json_safe(f"Knowledge_Corpus/data/raw/{file}")
            if papers:
                unified = unify_pmc(papers)
                all_docs.extend(unified)
                source_stats["PMC"] += len(unified)
        print(f"   âœ… PMC: {source_stats['PMC']} ç¯‡\n", flush=True)
    except Exception as e:
        print(f"   âŒ å¤„ç†PMCæ—¶å‡ºé”™: {e}\n", file=sys.stderr, flush=True)
    
    # 4. arXiv
    print("4ï¸âƒ£ å¤„ç† arXiv æ•°æ®...", flush=True)
    try:
        arxiv_files = [f for f in os.listdir("Knowledge_Corpus/data/raw") if "arxiv" in f and f.endswith(".json")]
        for file in arxiv_files:
            papers = load_json_safe(f"Knowledge_Corpus/data/raw/{file}")
            if papers:
                unified = unify_arxiv(papers)
                all_docs.extend(unified)
                source_stats["arXiv"] += len(unified)
        print(f"   âœ… arXiv: {source_stats['arXiv']} ç¯‡\n", flush=True)
    except Exception as e:
        print(f"   âŒ å¤„ç†arXivæ—¶å‡ºé”™: {e}\n", file=sys.stderr, flush=True)
    
    # 5. GitHub
    print("5ï¸âƒ£ å¤„ç† GitHub æ•°æ®...", flush=True)
    try:
        github_files = [f for f in os.listdir("Knowledge_Corpus/data/raw") if "github" in f and f.endswith(".json")]
        for file in github_files:
            repos = load_json_safe(f"Knowledge_Corpus/data/raw/{file}")
            if repos:
                unified = unify_github(repos)
                all_docs.extend(unified)
                source_stats["GitHub"] += len(unified)
        print(f"   âœ… GitHub: {source_stats['GitHub']} ä¸ªä»“åº“\n", flush=True)
    except Exception as e:
        print(f"   âŒ å¤„ç†GitHubæ—¶å‡ºé”™: {e}\n", file=sys.stderr, flush=True)
    
    
    
    
    # ========== ä¿å­˜ç»Ÿä¸€æ ¼å¼æ•°æ® ==========
    
    # å»é‡ï¼ˆåŸºäºdoc_idï¼‰
    print(f"{'='*60}", flush=True)
    print(f"å»é‡å¤„ç†...", flush=True)
    print(f"{'='*60}", flush=True)
    
    seen_ids = set()
    unique_docs = []
    for doc in all_docs:
        doc_id = doc.get("doc_id", "")
        if doc_id and doc_id not in seen_ids:
            seen_ids.add(doc_id)
            unique_docs.append(doc)
    
    print(f"åŸå§‹æ–‡æ¡£æ•°: {len(all_docs)}", flush=True)
    print(f"å»é‡åæ–‡æ¡£æ•°: {len(unique_docs)}", flush=True)
    print(f"é‡å¤æ–‡æ¡£æ•°: {len(all_docs) - len(unique_docs)}\n", flush=True)
    
    print(f"{'='*60}", flush=True)
    print(f"âœ… ç»Ÿä¸€æ ¼å¼å®Œæˆ", flush=True)
    print(f"{'='*60}\n", flush=True)
    
    print(f"æ€»æ–‡æ¡£æ•°: {len(unique_docs)}\n", flush=True)
    
    print("æŒ‰æ¥æºç»Ÿè®¡:", flush=True)
    for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"  {source:20s}: {count:5d} æ¡", flush=True)
    
    # ä¿å­˜
    try:
        with open("Knowledge_Corpus/data/unified/all_documents_raw.json", "w", encoding="utf-8") as f:
            json.dump(unique_docs, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: Knowledge_Corpus/data/unified/all_documents_raw.json", flush=True)
    except Exception as e:
        print(f"\nâŒ ä¿å­˜æ–‡æ¡£æ—¶å‡ºé”™: {e}", file=sys.stderr, flush=True)
    
    # ä¿å­˜ç»Ÿè®¡
    try:
        with open("Knowledge_Corpus/data/metadata/01_unify_stats.json", "w", encoding="utf-8") as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "total_documents": len(unique_docs),
                "original_count": len(all_docs),
                "duplicates_removed": len(all_docs) - len(unique_docs),
                "source_stats": dict(source_stats)
            }, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š ç»Ÿè®¡å·²ä¿å­˜åˆ°: Knowledge_Corpus/data/metadata/01_unify_stats.json", flush=True)
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»Ÿè®¡æ—¶å‡ºé”™: {e}", file=sys.stderr, flush=True)
    
    print(f"\nâœ… å®Œæˆï¼", flush=True)