"""
QA Pairs ç”Ÿæˆè„šæœ¬
ä» documents_deduped.json è¯»å–æ–‡æ¡£ï¼Œä½¿ç”¨ LLM ç”Ÿæˆé«˜è´¨é‡çš„ QA pairs
"""

import json
import asyncio
import sys
import time
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent.parent  # Knowledge_Corpus/scripts -> Knowledge_Corpus -> RE-Agent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from config.settings import get_settings


# é…ç½®å‚æ•°
BATCH_SIZE = 10  # æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡
DELAY_BETWEEN_BATCHES = 2.0  # æ‰¹æ¬¡ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
DELAY_BETWEEN_DOCS = 0.5  # æ–‡æ¡£ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
MAX_QA_PAIRS_PER_DOC = 2  # æ¯ä¸ªæ–‡æ¡£ç”Ÿæˆçš„æœ€å¤§ QA pairs æ•°é‡

# è¾“å…¥è¾“å‡ºè·¯å¾„
INPUT_FILE = Path(__file__).parent.parent / "data" / "cleaned" / "documents_deduped.json"
OUTPUT_FILE = Path(__file__).parent.parent / "data" / "qa_pairs.json"
PROGRESS_FILE = Path(__file__).parent.parent / "data" / "qa_pairs_progress.json"


def load_documents(input_file: Path) -> List[Dict[str, Any]]:
    """åŠ è½½æ–‡æ¡£æ•°æ®"""
    print(f"ğŸ“– æ­£åœ¨åŠ è½½æ–‡æ¡£: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        documents = json.load(f)
    print(f"âœ“ å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    return documents


def load_progress() -> Dict[str, Any]:
    """åŠ è½½è¿›åº¦ä¿¡æ¯"""
    if PROGRESS_FILE.exists():
        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "processed_doc_ids": [],
        "last_processed_index": -1,
        "total_qa_pairs": 0,
        "start_time": datetime.now().isoformat()
    }


def save_progress(progress: Dict[str, Any]):
    """ä¿å­˜è¿›åº¦ä¿¡æ¯"""
    progress["last_update"] = datetime.now().isoformat()
    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)


def load_existing_qa_pairs(output_file: Path) -> List[Dict[str, Any]]:
    """åŠ è½½å·²å­˜åœ¨çš„ QA pairs"""
    if output_file.exists():
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            print(f"  âš ï¸ åŠ è½½å·²æœ‰ QA pairs å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
    return []


def save_qa_pairs(qa_pairs: List[Dict[str, Any]], output_file: Path, append: bool = False):
    """ä¿å­˜ QA pairs"""
    if append:
        existing = load_existing_qa_pairs(output_file)
        qa_pairs = existing + qa_pairs
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)


def create_qa_prompt(doc: Dict[str, Any]) -> str:
    """Create prompt for generating QA pairs"""
    title = doc.get("title", "")
    abstract = doc.get("abstract", "")
    authors = doc.get("authors", "")
    journal = doc.get("journal", "")
    
    prompt = f"""Based on the following scientific literature, generate {MAX_QA_PAIRS_PER_DOC} high-quality question-answer pairs (QA pairs).

**Literature Information:**
- Title: {title}
- Authors: {authors}
- Journal: {journal}
- Abstract: {abstract}

**CRITICAL REQUIREMENTS FOR QUESTIONS:**
Questions MUST be:
1. **Conceptual**: Focus on fundamental concepts, principles, and theoretical frameworks rather than specific experimental details
2. **Generalizable**: Address broad, transferable knowledge that applies beyond this specific study
3. **Universal**: Cover principles and methods that are relevant across different contexts or domains
4. **Methodological**: Emphasize research approaches, techniques, and methodological insights
5. **Factual**: Based on established facts and findings, but at a conceptual level

**AVOID:**
- Questions about specific numerical values, percentages, or exact measurements
- Questions about particular gene names, protein names, or specific biological entities (unless asking about the general concept)
- Questions about specific experimental conditions, sample sizes, or detailed procedures
- Questions about specific dates, locations, or study-specific details
- Questions that require memorizing exact quotes or specific phrases from the abstract

**PREFERRED QUESTION TYPES:**
1. **Conceptual questions**: "What is the fundamental principle/concept of...?"
2. **Methodological questions**: "What approach/method/model is used to...?" or "How does the methodology address...?"
3. **Generalizable questions**: "What are the key factors/mechanisms that...?" or "What general insights can be drawn about...?"
4. **Comparative questions**: "How does this approach compare to other methods in terms of...?" (focus on general principles, not specific comparisons)
5. **Application questions**: "What are the general applications/implications of...?" (at a conceptual level)

**Requirements:**
1. Generate {MAX_QA_PAIRS_PER_DOC} question-answer pairs that focus on conceptual, generalizable, and methodological aspects
2. Questions should be:
   - Conceptually oriented and broadly applicable
   - Focused on principles, methods, and general insights
   - Answerable from the abstract but at a high level of abstraction
   - Avoiding article-specific details

3. Answers should be:
   - Accurate, complete, and based on the literature content
   - Concise and clear (typically 2-4 sentences)
   - Emphasizing conceptual understanding and general principles
   - Including key methodological or theoretical insights

4. The output format must be a strict JSON array, with each element containing "question" and "answer" fields:
```json
[
  {{
    "question": "Question 1",
    "answer": "Answer 1"
  }},
  {{
    "question": "Question 2",
    "answer": "Answer 2"
  }}
]
```

Please output the JSON array directly without any additional text or explanation."""
    
    return prompt


async def generate_qa_pairs_for_doc(
    llm: ChatOpenAI,
    doc: Dict[str, Any],
    doc_index: int,
    total_docs: int,
    max_retries: int = 3
) -> List[Dict[str, Any]]:
    """ä¸ºå•ä¸ªæ–‡æ¡£ç”Ÿæˆ QA pairsï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    prompt = create_qa_prompt(doc)
    
    messages = [
        SystemMessage(content="You are a professional scientific literature analyst expert, skilled at extracting key information from literature and generating high-quality question-answer pairs."),
        HumanMessage(content=prompt)
    ]
    
    print(f"  [{doc_index + 1}/{total_docs}] æ­£åœ¨ä¸ºæ–‡æ¡£ç”Ÿæˆ QA pairs: {doc.get('title', 'N/A')[:60]}...")
    
    # é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        try:
            # API è°ƒç”¨
            response = await llm.ainvoke(messages)
            content = response.content.strip()
            
            # å°è¯•æå– JSON
            # ç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            # è§£æ JSON
            try:
                qa_pairs = json.loads(content)
                if not isinstance(qa_pairs, list):
                    qa_pairs = [qa_pairs]
                
                # éªŒè¯æ ¼å¼å¹¶æ·»åŠ å…ƒæ•°æ®
                validated_pairs = []
                for qa in qa_pairs:
                    if isinstance(qa, dict) and "question" in qa and "answer" in qa:
                        validated_pairs.append({
                            "question": qa["question"],
                            "answer": qa["answer"],
                            "doc_id": doc.get("doc_id", ""),
                            "doc_title": doc.get("title", ""),
                            "doc_source": doc.get("source", ""),
                            "doc_source_id": doc.get("source_id", ""),
                            "generated_at": datetime.now().isoformat()
                        })
                
                if validated_pairs:
                    print(f"    âœ“ æˆåŠŸç”Ÿæˆ {len(validated_pairs)} ä¸ª QA pairs")
                    return validated_pairs
                else:
                    print(f"    âš ï¸ æœªç”Ÿæˆæœ‰æ•ˆçš„ QA pairs")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2)
                        continue
                    return []
                
            except json.JSONDecodeError as e:
                if attempt < max_retries - 1:
                    print(f"    âš ï¸ JSON è§£æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    print(f"    Raw content (first 500 chars): {content[:500]}")
                    await asyncio.sleep(2)  # ç­‰å¾…åé‡è¯•
                    continue
                else:
                    print(f"    âš ï¸ JSON è§£æå¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰: {e}")
                    print(f"    Raw content (first 500 chars): {content[:500]}")
                    return []
            
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"    âš ï¸ API è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                continue
            else:
                print(f"    âŒ ç”Ÿæˆ QA pairs å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰: {e}")
                import traceback
                traceback.print_exc()
                return []
    
    return []  # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥


async def process_batch(
    llm: ChatOpenAI,
    documents: List[Dict[str, Any]],
    batch_start: int,
    batch_end: int,
    progress: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """å¤„ç†ä¸€æ‰¹æ–‡æ¡£"""
    batch = documents[batch_start:batch_end]
    all_qa_pairs = []
    
    for i, doc in enumerate(batch):
        doc_index = batch_start + i
        doc_id = doc.get("doc_id", "")
        
        # è·³è¿‡å·²å¤„ç†çš„æ–‡æ¡£
        if doc_id in progress["processed_doc_ids"]:
            print(f"  [{doc_index + 1}/{len(documents)}] è·³è¿‡å·²å¤„ç†æ–‡æ¡£: {doc_id}")
            continue
        
        # ç”Ÿæˆ QA pairs
        qa_pairs = await generate_qa_pairs_for_doc(llm, doc, doc_index, len(documents))
        
        if qa_pairs:
            all_qa_pairs.extend(qa_pairs)
            progress["processed_doc_ids"].append(doc_id)
            progress["total_qa_pairs"] += len(qa_pairs)
        
        # æ–‡æ¡£é—´å»¶è¿Ÿ
        if i < len(batch) - 1:
            await asyncio.sleep(DELAY_BETWEEN_DOCS)
    
    progress["last_processed_index"] = batch_end - 1
    return all_qa_pairs


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("QA Pairs ç”Ÿæˆè„šæœ¬")
    print("=" * 80)
    print()
    
    # åŠ è½½è®¾ç½®
    settings = get_settings()
    if not settings.openai_api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®")
        return
    
    # åˆå§‹åŒ– LLM
    model_name = "gpt-4o-mini"
    print(f"ğŸ¤– åˆå§‹åŒ– LLM: {model_name}")
    llm = ChatOpenAI(
        model=model_name,
        temperature=0.7,  # ç¨å¾®æé«˜æ¸©åº¦ä»¥å¢åŠ å¤šæ ·æ€§
        openai_api_key=settings.openai_api_key
    )
    print("âœ“ LLM åˆå§‹åŒ–å®Œæˆ")
    print()
    
    # åŠ è½½æ–‡æ¡£
    if not INPUT_FILE.exists():
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {INPUT_FILE}")
        return
    
    documents = load_documents(INPUT_FILE)
    
    # åŠ è½½è¿›åº¦
    progress = load_progress()
    print(f"ğŸ“Š è¿›åº¦ä¿¡æ¯:")
    print(f"  - å·²å¤„ç†æ–‡æ¡£: {len(progress['processed_doc_ids'])}/{len(documents)}")
    print(f"  - å·²ç”Ÿæˆ QA pairs: {progress['total_qa_pairs']}")
    print(f"  - ä¸Šæ¬¡å¤„ç†ç´¢å¼•: {progress.get('last_processed_index', -1)}")
    print()
    
    # ç¡®å®šèµ·å§‹ä½ç½®
    start_index = progress.get("last_processed_index", -1) + 1
    if start_index >= len(documents):
        print("âœ“ æ‰€æœ‰æ–‡æ¡£å·²å¤„ç†å®Œæˆï¼")
        return
    
    print(f"ğŸš€ å¼€å§‹å¤„ç†ï¼Œä»ç´¢å¼• {start_index} å¼€å§‹")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}, æ‰¹æ¬¡å»¶è¿Ÿ: {DELAY_BETWEEN_BATCHES}ç§’, æ–‡æ¡£å»¶è¿Ÿ: {DELAY_BETWEEN_DOCS}ç§’")
    print()
    
    # åˆ†æ‰¹å¤„ç†
    all_qa_pairs = []
    total_batches = (len(documents) - start_index + BATCH_SIZE - 1) // BATCH_SIZE
    
    for batch_num in range(total_batches):
        batch_start = start_index + batch_num * BATCH_SIZE
        batch_end = min(batch_start + BATCH_SIZE, len(documents))
        
        print(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{total_batches} (æ–‡æ¡£ {batch_start + 1}-{batch_end})")
        
        # å¤„ç†æ‰¹æ¬¡
        batch_qa_pairs = await process_batch(llm, documents, batch_start, batch_end, progress)
        all_qa_pairs.extend(batch_qa_pairs)
        
        # ä¿å­˜è¿›åº¦å’Œç»“æœ
        save_progress(progress)
        if batch_qa_pairs:
            # å¢é‡ä¿å­˜ï¼šåŠ è½½å·²æœ‰æ•°æ®ï¼Œè¿½åŠ æ–°æ•°æ®ï¼Œç„¶åä¿å­˜
            existing_qa_pairs = load_existing_qa_pairs(OUTPUT_FILE)
            all_qa_pairs_to_save = existing_qa_pairs + batch_qa_pairs
            save_qa_pairs(all_qa_pairs_to_save, OUTPUT_FILE, append=False)
            print(f"  âœ“ å·²ä¿å­˜ {len(batch_qa_pairs)} ä¸ªæ–° QA pairsï¼ˆæ€»è®¡: {len(all_qa_pairs_to_save)}ï¼‰")
        
        print(f"  ğŸ“Š ç´¯è®¡: {len(all_qa_pairs)} ä¸ª QA pairs, {len(progress['processed_doc_ids'])} ä¸ªæ–‡æ¡£å·²å¤„ç†")
        print()
        
        # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆæœ€åä¸€ä¸ªæ‰¹æ¬¡ä¸éœ€è¦å»¶è¿Ÿï¼‰
        if batch_num < total_batches - 1:
            print(f"  â³ ç­‰å¾… {DELAY_BETWEEN_BATCHES} ç§’...")
            await asyncio.sleep(DELAY_BETWEEN_BATCHES)
    
    # æœ€ç»ˆç»Ÿè®¡
    print("=" * 80)
    print("âœ… å¤„ç†å®Œæˆï¼")
    print("=" * 80)
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  - å¤„ç†æ–‡æ¡£æ•°: {len(progress['processed_doc_ids'])}/{len(documents)}")
    print(f"  - ç”Ÿæˆ QA pairs: {progress['total_qa_pairs']}")
    print(f"  - è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print(f"  - è¿›åº¦æ–‡ä»¶: {PROGRESS_FILE}")
    print()


if __name__ == "__main__":
    asyncio.run(main())

