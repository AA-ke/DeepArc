from Bio import Entrez
import json
import time
import os
import sys
from xml.etree import ElementTree as ET
from http.client import IncompleteRead

# âš ï¸ å¿…é¡»è®¾ç½®ä½ çš„é‚®ç®±ï¼ˆNCBIè¦æ±‚ï¼‰
Entrez.email = "ake0906ake@gmail.com"

def search_pmc_open_access(query, max_results=500):
    """æœç´¢PMCå¼€æ”¾è·å–æ–‡ç« """
    print(f"ğŸ” æœç´¢PMC: {query}", flush=True)
    
    try:
        handle = Entrez.esearch(
            db="pmc",
            term=f"{query} AND open access[filter]",
            retmax=max_results,
            sort="relevance",
            usehistory="y"  # ä½¿ç”¨å†å²è®°å½•ä»¥æ”¯æŒå¤§æ‰¹é‡ä¸‹è½½
        )
        record = Entrez.read(handle)
        handle.close()
        
        return {
            "ids": record["IdList"],
            "count": int(record["Count"]),
            "webenv": record.get("WebEnv"),
            "query_key": record.get("QueryKey")
        }
    except Exception as e:
        print(f"âŒ æœç´¢é”™è¯¯: {e}", file=sys.stderr, flush=True)
        return {
            "ids": [],
            "count": 0,
            "webenv": None,
            "query_key": None
        }


def fetch_pmc_abstracts_batch(id_list, batch_size=100):
    """æ‰¹é‡è·å–PMCæ‘˜è¦ï¼ˆä¸æ˜¯å…¨æ–‡ï¼Œä½†æ›´å¿«ï¼‰"""
    all_papers = []
    
    for i in range(0, len(id_list), batch_size):
        batch = id_list[i:i+batch_size]
        ids_str = ",".join(batch)
        print(f"  è·å– {i+1}-{min(i+batch_size, len(id_list))}/{len(id_list)}...", end=" ", flush=True)

        # å¢åŠ é‡è¯•æœºåˆ¶ï¼Œä¸“é—¨å¤„ç† IncompleteRead ç­‰ç½‘ç»œä¸ç¨³å®šé”™è¯¯
        max_retries = 3
        for attempt in range(1, max_retries + 1):
            try:
                handle = Entrez.efetch(
                    db="pmc",
                    id=ids_str,
                    rettype="abstract",
                    retmode="xml"
                )

                xml_data = handle.read()
                handle.close()
                root = ET.fromstring(xml_data)

                # è§£æXML
                batch_papers = []
                for article in root.findall(".//article"):
                    paper = parse_pmc_article(article)
                    if paper:
                        batch_papers.append(paper)

                all_papers.extend(batch_papers)
                print(f"âœ… +{len(batch_papers)} ç¯‡", flush=True)
                time.sleep(0.4)  # éµå®ˆNCBIé™åˆ¶
                break  # å½“å‰ batch æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯

            except IncompleteRead as e:
                print(f"\n  âš ï¸ IncompleteReadï¼ˆç¬¬ {attempt}/{max_retries} æ¬¡å°è¯•ï¼‰: {e}", file=sys.stderr, flush=True)
                time.sleep(1.0 * attempt)
                if attempt == max_retries:
                    print("  âŒ å¤šæ¬¡ IncompleteReadï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡", file=sys.stderr, flush=True)
            except Exception as e:
                print(f"\n  âŒ é”™è¯¯: {e}", file=sys.stderr, flush=True)
                break  # å…¶ä»–é”™è¯¯æ²¡å¿…è¦é‡è¯•ï¼Œç›´æ¥è·³è¿‡è¯¥æ‰¹æ¬¡
    
    return all_papers


def extract_text_recursive(elem):
    """é€’å½’æå–XMLå…ƒç´ çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹"""
    if elem is None:
        return ""
    
    # è·å–ç›´æ¥æ–‡æœ¬
    text_parts = []
    if elem.text:
        text_parts.append(elem.text.strip())
    
    # é€’å½’å¤„ç†æ‰€æœ‰å­å…ƒç´ 
    for child in elem:
        child_text = extract_text_recursive(child)
        if child_text:
            text_parts.append(child_text)
        # å¤„ç†å­å…ƒç´ åçš„å°¾éšæ–‡æœ¬
        if child.tail:
            text_parts.append(child.tail.strip())
    
    return " ".join(text_parts)


def parse_pmc_article(article):
    """è§£æPMC XMLæ–‡ç« """
    try:
        # æå–æ ‡é¢˜ï¼ˆé€’å½’æå–æ‰€æœ‰æ–‡æœ¬ï¼‰
        title_elem = article.find(".//article-title")
        title = extract_text_recursive(title_elem) if title_elem is not None else "N/A"
        if not title or title.strip() == "":
            title = "N/A"
        
        # æå–æ‘˜è¦ï¼ˆé€’å½’æå–æ‰€æœ‰æ–‡æœ¬ï¼‰
        abstract_elem = article.find(".//abstract")
        abstract = ""
        if abstract_elem is not None:
            # æå–æ‰€æœ‰æ®µè½
            paragraphs = []
            for p in abstract_elem.findall(".//p"):
                p_text = extract_text_recursive(p)
                if p_text:
                    paragraphs.append(p_text)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ®µè½ï¼Œå°è¯•ç›´æ¥æå–abstractçš„æ‰€æœ‰æ–‡æœ¬
            if not paragraphs:
                abstract = extract_text_recursive(abstract_elem)
            else:
                abstract = " ".join(paragraphs)
        
        # æå–ä½œè€…
        authors = []
        for contrib in article.findall(".//contrib[@contrib-type='author']"):
            name_elem = contrib.find(".//name")
            if name_elem is not None:
                surname = name_elem.find("surname")
                given = name_elem.find("given-names")
                if surname is not None:
                    author_name = surname.text
                    if given is not None:
                        author_name = f"{given.text} {author_name}"
                    authors.append(author_name)
        
        # æå–PMC ID
        pmc_id_elem = article.find(".//article-id[@pub-id-type='pmc']")
        pmc_id = pmc_id_elem.text if pmc_id_elem is not None else "N/A"
        
        # æå–DOI
        doi_elem = article.find(".//article-id[@pub-id-type='doi']")
        doi = doi_elem.text if doi_elem is not None else "N/A"
        
        # æå–å‘è¡¨æ—¥æœŸ
        pub_date = article.find(".//pub-date")
        date = "N/A"
        if pub_date is not None:
            year = pub_date.find("year")
            month = pub_date.find("month")
            day = pub_date.find("day")
            if year is not None:
                date = year.text
                if month is not None:
                    date += f"-{month.text.zfill(2)}"
                    if day is not None:
                        date += f"-{day.text.zfill(2)}"
        
        return {
            "pmc_id": pmc_id,
            "doi": doi,
            "title": title,
            "abstract": abstract,
            "authors": ", ".join(authors),
            "date": date,
            "source": "PMC"
        }
    
    except Exception as e:
        print(f"âš ï¸ è§£æé”™è¯¯: {e}", file=sys.stderr, flush=True)
        return None


def save_papers(papers, filename):
    """ä¿å­˜è®ºæ–‡"""
    os.makedirs("Knowledge_Corpus/data/raw", exist_ok=True)
    
    # JSON
    json_path = f"Knowledge_Corpus/data/raw/{filename}.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(papers, f, indent=2, ensure_ascii=False)
    
    # TXT
    txt_path = f"Knowledge_Corpus/data/raw/{filename}.txt"
    with open(txt_path, "w", encoding="utf-8") as f:
        for i, paper in enumerate(papers, 1):
            f.write(f"{'='*100}\n")
            f.write(f"è®ºæ–‡ {i}/{len(papers)}\n")
            f.write(f"{'='*100}\n")
            f.write(f"PMC ID: {paper.get('pmc_id', 'N/A')}\n")
            f.write(f"DOI: {paper.get('doi', 'N/A')}\n")
            f.write(f"Title: {paper.get('title', 'N/A')}\n")
            f.write(f"Authors: {paper.get('authors', 'N/A')}\n")
            f.write(f"Date: {paper.get('date', 'N/A')}\n")
            f.write(f"\nAbstract:\n{paper.get('abstract', 'N/A')}\n\n")
    
    print(f"ğŸ’¾ å·²ä¿å­˜: {json_path}", flush=True)
    print(f"ğŸ’¾ å·²ä¿å­˜: {txt_path}", flush=True)


if __name__ == "__main__":
    

    queries = [
    # åŸºç¡€ç”Ÿç‰©å­¦çŸ¥è¯†
    "cis-regulatory element AND definition AND classification",
    "promoter enhancer silencer insulator AND structure AND function",
    "transcription factor binding site AND motif AND PWM",
    "gene regulatory elements AND transcriptional regulation AND mechanism",
    "chromatin accessibility AND ATAC-seq AND regulatory elements",
    "epigenetic modification AND gene regulatory elements AND histone modification",
    
    # è®¡ç®—æ–¹æ³•ä¸é¢„æµ‹
    "computational prediction AND enhancer activity AND promoter strength",
    "machine learning AND regulatory element prediction AND sequence-to-function",
    "deep learning AND cis-regulatory element AND CNN transformer",
    "MPRA AND massively parallel reporter assay AND regulatory element design",
    "STARR-seq AND enhancer screening AND high-throughput",
    
    # è®¾è®¡æ–¹æ³•
    "synthetic enhancer design AND de novo AND regulatory element",
    "synthetic promoter design AND engineered AND gene expression control",
    "regulatory sequence optimization AND inverse design AND directed evolution",
    "regulatory grammar AND sequence rules AND combinatorial design",
    
    # å®éªŒéªŒè¯ä¸åº”ç”¨
    "designed regulatory elements AND experimental validation AND reporter assay",
    "gene regulatory element design AND gene therapy AND clinical application",
    "cell-type-specific enhancer AND tissue-specific promoter AND design",
    "regulatory element design AND off-target effect AND specificity",
    
    # é«˜çº§ä¸»é¢˜
    "3D genome AND chromatin looping AND enhancer-promoter interaction",
    "gene regulatory network AND cis-regulatory network AND systems biology",
    "evolutionary conservation AND regulatory element AND comparative genomics",
    "single-cell AND regulatory element AND cell-type-specific activity"
    ]
    
    all_papers = []
    all_ids = set()  # å»é‡
    
    print("="*60, flush=True)
    print("å¼€å§‹ä» PMC è·å–å¼€æ”¾è·å–æ–‡ç« ", flush=True)
    print("="*60 + "\n", flush=True)
    
    for i, query in enumerate(queries, 1):
        print(f"\n[{i}/{len(queries)}] æŸ¥è¯¢: {query}", flush=True)
        
        try:
            # æœç´¢ï¼ˆæ¯ä¸ªä¸»é¢˜æœ€å¤š100ç¯‡ï¼‰
            search_result = search_pmc_open_access(query, max_results=100)
            print(f"  æ‰¾åˆ° {search_result['count']} ç¯‡ç›¸å…³æ–‡ç« ", flush=True)
            
            # å»é‡å¹¶é™åˆ¶æ¯ä¸ªä¸»é¢˜æœ€å¤š100ç¯‡
            new_ids = [pid for pid in search_result['ids'] if pid not in all_ids]
            new_ids = new_ids[:100]  # ç¡®ä¿æ¯ä¸ªä¸»é¢˜æœ€å¤š100ç¯‡
            all_ids.update(new_ids)
            
            if new_ids:
                # è·å–æ‘˜è¦
                papers = fetch_pmc_abstracts_batch(new_ids)
                all_papers.extend(papers)
                print(f"  æ–°å¢ {len(papers)} ç¯‡ç‹¬ç‰¹è®ºæ–‡ï¼ˆæœ¬ä¸»é¢˜é™åˆ¶ï¼šæœ€å¤š100ç¯‡ï¼‰", flush=True)
            else:
                print(f"  æ— æ–°è®ºæ–‡ï¼ˆå·²å»é‡ï¼‰", flush=True)
            
            time.sleep(1)  # ç¤¼è²Œå»¶è¿Ÿ
        except Exception as e:
            print(f"  âŒ å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {e}", file=sys.stderr, flush=True)
            continue
    
    # ä¿å­˜æ‰€æœ‰è®ºæ–‡
    if all_papers:
        print(f"\n{'='*60}", flush=True)
        print(f"âœ… æ€»å…±è·å– {len(all_papers)} ç¯‡ç‹¬ç‰¹çš„ PMC è®ºæ–‡", flush=True)
        print(f"{'='*60}", flush=True)
        
        save_papers(all_papers, "pmc_open_access_all")
        
        # ç»Ÿè®¡
        print(f"\nğŸ“Š æŒ‰å¹´ä»½ç»Ÿè®¡:", flush=True)
        years = {}
        for paper in all_papers:
            year = paper.get('date', 'N/A')[:4]
            if year != 'N/A' and year.isdigit():
                years[year] = years.get(year, 0) + 1
        
        if years:
            for year in sorted(years.keys(), reverse=True):
                print(f"  {year}: {years[year]} ç¯‡", flush=True)
        else:
            print("  æ— æœ‰æ•ˆå¹´ä»½æ•°æ®", flush=True)
    else:
        print(f"\nâš ï¸  æœªè·å–åˆ°ä»»ä½•è®ºæ–‡", flush=True)