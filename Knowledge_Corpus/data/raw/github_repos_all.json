[
  {
    "name": "TACO",
    "full_name": "yangzhao1230/TACO",
    "description": "TACO: TFBS-Aware Cis-Regulatory Element Optimization ",
    "stars": 19,
    "forks": 0,
    "language": "Python",
    "url": "https://github.com/yangzhao1230/TACO",
    "topics": [],
    "created_at": "2025-01-23T03:28:41Z",
    "updated_at": "2025-09-24T08:57:58Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# ðŸŒ® TACO: Regulatory DNA Sequence Design with Reinforcement Learning\n\nThis repository provides the official implementation for the ICLR 2025 poster paper:  \n[\"Regulatory DNA Sequence Design with Reinforcement Learning\"](https://openreview.net/pdf?id=F4IMiNhim1).\n\n## Environment Setup\n\nBefore running the code, create and activate a Conda environment:\n\n```bash\nconda create -n taco python=3.9\nconda activate taco\n```\n\nTo install all necessary dependencies, run:\n\n```bash\nbash env_install.sh\n```\n\n\nTo install FlashAttention, run:\n\n```bash\npip install flash-attn --no-build-isolation\ngit clone https://github.com/Dao-AILab/flash-attention.git\n\n# From inside flash-attn/\ncd flash-attention\ncd csrc/layer_norm && pip install .\n```\n\n**Note:** We have recently observed that variations in FlashAttention versions may lead to slight differences in results, potentially due to interactions with HyenaDNA. Specifically, even when using identical model weights, inference can produce slightly different intermediate outputs (for example, we found that a sample had identical output bases until position 131, but diverged from position 132 onward compared to our previous experimental results), resulting in approximately 1-2 point variations in the final evaluation metrics. Unfortunately, we did not document the exact FlashAttention version used during the ICLR submission period (and the development machine from that time has since been recycled). We are actively working to reproduce and investigate this issue. The table below presents our reproduction results from February 2025 as shown in `calculate_metric.ipynb`: \n\n\n|  |                      |  |  |\n|-----------------|----------------------|----------------------|----------------------|\n| SK-N-SH Results                | Top                  | Medium               | Diversity            |\n| alpha = 0.0 (Paper reported)  | 0.67 Â± 0.06          | 0.60 Â± 0.06          | 111.6 Â± 12.86        |\n| alpha = 0.01 (Paper reported)    | 0.68 Â± 0.08          | 0.62 Â± 0.08          | 121.4 Â± 7.86         |\n| alpha = 0.0 (Latest reproduction) | 0.68 Â± 0.07      | 0.62 Â± 0.07          | 120.2 Â± 13.85        |\n| alpha = 0.01 (Latest reproduction)     | 0.7 Â± 0.03           | 0.63 Â± 0.04          | 117.2 Â± 12.64        |\n\n---\n\n## Data Preparation and Reward Model Training\n\nOur data preprocessing scripts and reward model training scripts are mainly adapted from regLM (https://zenodo.org/records/12668907). Specifically, the code repository structure of regLM is as follows:\n\n```\n- human_enhancers\n - 01_data_processing\n - 02_regression_paired\n - 03_regression_separate\n - 04_reglm\n - 05_reglm_interpretation\n - 06_synthetic_enhancer_generation\n - 07_synthetic_enhancer_evaluation\n - 08_synthetic_enhancer_comparison\n```\nYou can find instructions for downloading datasets (and splitting them according to your specific needs) in `01_data_processing`, and learn how to train reward models in `02_regression_paired`.\n\nIn particular, if you want to enable TFBS features, you need to first scan and extract TFBS features from sequence data (also in `01_data_processing`). Briefly, you should save the TFBS feature matrix of interest and the corresponding fitness levels into an `h5ad` format file. Regarding the related JASPAR files needed for scanning TFBS, please refer to https://github.com/yangzhao1230/TACO/issues/2.\n\n## TFBS Reward Inference\n\nYou should have extracted TFBS features and saved them in `h5ad` format (please refer to regLM's notebooks for this step).\n\nThen you can train the lightGBM model with \n```\npython lightGBM_mbo.py\n```\nthen you can infer the TFBS reward through `tfbs_reward_mbo.ipynb`.\n\n## Prepared Data, TFBS Reward and Reward Model\n\nHere we provide pre-processed TFBS rewards, surrogate scoring model weights (note that the policy directly uses regLM weights, and for oracle weights please use the reward model provided by regLM), and datasets partitioned according to the offline MBO strategy described in the paper.\n\n\nðŸ¤—**Model weights:** https://huggingface.co/yangyz1230/TACO/tree/main  \nðŸ¤—**Datasets:** https://huggingface.co/datasets/yangyz1230/TACO/tree/main\n\n## Optimization with RL\n\nWe provide only the optimization script for **offline MBO (Section 4.3)** in the paper.  \nHowever, the implementations of **Section 4.2 and Section 4.3** are **identical**, except for differences in the **reward model, pre-trained model, and dataset**.\n\nTo run inference for offline MBO, use:\n\n```bash\nbash reinforce_mbo.sh\n```\n\n---\n\n## Acknowledgements\n\nOur implementation builds upon several open-source projects:\n- **[regLM](https://github.com/Genentech/regLM)**: Provided the implementation of our policy, reward model, and data processing related code.\n- **[LatProtRL](https://github.com/haewonc/LatProtRL)**: Contributed baseline implementations and evaluation code\n- **[RL4Chem](https://github.com/montrealrobotics/RL4Chem)**: Supplied the reinforcement learning algorithmic framework\n\nWe sincerely appreciate their valuable contributions to this work.\n\n---\n\n## TODO List\n- [x] Provide environment configuration instructions.\n- [x] Provide core algorithm code implementation.\n- [x] Provide data and checkpoints for offline MBO settings.\n- [x] Provide the training scripts for the LightGBM model.\n- [x] Provide code for tfbs reward inference.\n\n---\n\n## Citation\nIf you use our code or find our work inspiring, please cite our paper:\n\n```bibtex\n@inproceedings{yang2025regulatory,\n  title={Regulatory DNA Sequence Design with Reinforcement Learning},\n  author={Zhao Yang and Bing Su and Chuan Cao and Ji-Rong Wen},\n  booktitle={The Thirteenth International Conference on Learning Representations},\n  year={2025},\n  url={https://openreview.net/forum?id=F4IMiNhim1}\n}\n",
    "readme_length": 5735
  },
  {
    "name": "Widespread-Long-range-Cis-Regulatory-Elements-in-the-Maize-Genome",
    "full_name": "schmitzlab/Widespread-Long-range-Cis-Regulatory-Elements-in-the-Maize-Genome",
    "description": "scripts used for processing and analyzing data in the article. ",
    "stars": 13,
    "forks": 11,
    "language": "Shell",
    "url": "https://github.com/schmitzlab/Widespread-Long-range-Cis-Regulatory-Elements-in-the-Maize-Genome",
    "topics": [],
    "created_at": "2019-10-29T16:38:00Z",
    "updated_at": "2024-03-29T02:37:28Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# Widespread-Long-range-Cis-Regulatory-Elements-in-the-Maize-Genome\n\nScripts used for processing and analyzing data in the article. \n",
    "readme_length": 133
  },
  {
    "name": "Canidae-CREF-analysis",
    "full_name": "JianhuiShi/Canidae-CREF-analysis",
    "description": "A comprehensive and comparative analysis of genome-wide cis-regulatory element frequency (CREF) for five canids: dog, dingo, red fox, dhole, and wolf.",
    "stars": 11,
    "forks": 0,
    "language": "MATLAB",
    "url": "https://github.com/JianhuiShi/Canidae-CREF-analysis",
    "topics": [],
    "created_at": "2024-04-27T07:44:21Z",
    "updated_at": "2025-10-23T10:08:22Z",
    "homepage": "",
    "license": "GNU General Public License v3.0",
    "readme": "# Canidae-CREF-analysis\n\n## 1. Introduction\n\nCanidae-CREF-analysis is a comprehensive and comparative analysis of genome-wide cis-regulatory element frequency (CREF) for five canids: dog, dingo, red fox, dhole, and wolf.\n\n### 1.1 Pipeline description\n\n1. Input the species-specific CREF matrices of five canids.\n2. Perform the robust singular value decomposition (SVD) to stratify each CREF matrix into multiple dual eigen-modules at frequency levels from high to low. Each module is comprised of the singular value and the pair of gene- and motif-eigenvectors.\n3. Polarize the gene- and motif-eigenvectors by sorting their loadings.\n4. Evaluate the correlation between the motif-eigenvectors.\n5. Analyze the rotation between motif-eigenvectors by projecting the fourth and fifth motif-eigenvectors of the other four canids onto the 2-D eigenspaces of dogs.\n6. Compute the rank of all motifs according to their loadings in polarized motif-eigenvectors.\n7. Perform the enrichment analysis on the polarized gene-eigenvectors to identify the biological pathways significantly enriched at the two poles.\n\n### 1.2 Major results\n\n1. The top three eigen-modules are highly conserved while a phase transition occurred between the fourth and fifth ones in dogs.\n\n2. The dogs underwent a rotation in its motif-eigenvectors with reference to the wolf. The motif-eigenvectors of dingo and dhole are highly correlated with that of the dog.\n\n3. Long-term memory, myelination, and cochlear development are significantly enhanced at level four in dogs.\n\n4. The red fox is closest to the singularity or fusion point that characterizes the phase transition onset.\n\n**Note**: This pipeline provides a systematic framework for analyzing the genome-wide CREF and is applicable to the comparison between other species.\n\n## 2. Install\n\nYou can download the codes and data by the command:\n```bash\ngit clone https://github.com/JianhuiShi/Canidae-CREF-analysis.git\n```\nor click the **Download Zip** button and decompress the package.\n\n## 3. Dependencies\n\n1. `R` with the following packages: `tidyverse`, `rlist`, `openxlsx`, and `snowfall`.\n\n2. `MATLAB`\n\n## 4. Usage\n\n```bash\ncd code\nbash run.sh\n```\nAll computational results can be found in the `results` directory generated.\n\n## 5. Directory description\n\n- ### `data` directory\n\n  * **`CREF_matrix`** directory\n  contains the CREF matrices of five canids: dog, dingo, red fox, dhole, and wolf.\n\n  * **`geneName`** directory\n  contains the gene names files of five canids. Gene names represent the row names of CREF matrix.\n\n  * **`motif_symbol.txt`**\n  contains all symbols of 1403 motifs used in this study. Motif symbols represent the column names of CREF matirx.\n\n  * **`pathway`** directory\n  contains the pathway data of [GO](https://geneontology.org/) (GO.BP, GO.CC, and GO.MF), [KEGG](https://www.kegg.jp/), and [Reactome](https://reactome.org/).\n\n- ### `code` directory\n\n  * **`run.sh`**\n  This is the script used for running other scripts all in sequence.\n\n  * **`config.yaml`**\n  This is the configuration file that contains the basic information of the species used in computation.\n\n  * **`motifRank.R`**\n  This script computes the rank of all motifs according to their loadings in polarized motif-eigenvectors. The results are saved in the file `results/motifRank.xlsx`.\n\n  * **`PCC.R`**\n  This script computes the Pearson correlation coefficients between motif-eigenvectors across speices. The results are saved in the file `results/PCC.xlsx`.\n\n  * **`projection.R`**\n  This script computes the projections of the 4th and 5th motif-eigenvectors of one species onto\nthe 2-D eigen space spanned by the 4th and 5th motif-eigenvectors of another species. The results are saved in the file `results/projection_4-5.xlsx`.\n\n  * #### `SVD` directory\n\n    * **`robustSVD.m`**\n    This script performs the robust singular value decomposition (robust SVD) on the CREF matrices of five canids. The singular values, gene-eigenvectors, and motif-eigenvectors are saved in the directory `results/SVD`. The polarized eigenvectors are saved in the directory `results/loadings`.\n\n    * **`scripts`** directory\n    contains matlab functions we defined for robust SVD.\n\n    * **`yamlmatlab`** directory\n    is the [yamlmatlab](https://github.com/ewiger/yamlmatlab) package.\n\n    * **`inexact_alm_rpca`** directory\n    is the matlab package for the [inexact augmented Lagrange multipliers (IALM)](https://arxiv.org/abs/1009.5055) method.\n\n  * #### `enrichmentAnalysis` directory\n\n    * **`gene_enrichment_analysis.R`**\n    This script performs the gene enrichment analysis by the Wilcoxon scoring method on the polarized gene eigenvectors. The results are saved in the directory `results/enrichment/RData`.\n\n    * **`format_enrichment_result.R`**\n    This script formats the raw enrichment results (.RData) in `results/enrichment/RData` into readable files (.xlsx). The results are saved in the directory `results/enrichment/xlsx`.\n\n    * **`enrichment_utils.R`**\n    This script contains functions we defined for enrichment analysis.\n\n## 6. Contact\n\nPlease contact shi.jianhui@foxmail.com for any questions.\n\n## 7. License\n\n**GNU General Public License v3.0 only**\n\nFor details, please read `Canidae-CREF-analysis/LICENSE`.\n",
    "readme_length": 5243
  },
  {
    "name": "craTEs",
    "full_name": "PulverCyril/craTEs",
    "description": "Estimate the cis-regulatory activity of transposable element (TEs) subfamilies",
    "stars": 10,
    "forks": 0,
    "language": "R",
    "url": "https://github.com/PulverCyril/craTEs",
    "topics": [],
    "created_at": "2023-04-03T09:18:11Z",
    "updated_at": "2025-02-10T13:40:11Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# *craTEs* (*cis*-regulatory activities of Transposable Element subfamilies)\n*craTEs* is a statistical framework that estimates the *cis*-regulatory activity of transposable element (TE) subfamilies from standardly processed RNA-seq data. It is provided as an R package.\n\n## Why should I care?\nAre you looking for *cis*-regulatory sequences that may explain your transcriptomic data? Then read on. \n\nTransposable elements (TEs) are virus-like selfish genetic parasites of eukaryote genomes. More than half of the human genome is TE-derived, owing to the copy-and-paste mechanisms that the majority of human TEs leverage to replicate. To do so, TEs must first attract the host transcriptional machinery and have therefore evolved embedded cis-regulatory sequences to attract host transcription factors and epigenetic modifiers. In short, as TEs spread, they litter genomes with hundreds to thousands copies of ready-to-use *cis*-regulatory platforms. It is now widely believed that waves of TE invasion contribute to enhancer turnover. \n\nHere is what you will benefit from using *craTEs*:\n- Interpretable quantitative estimates of TE-mediated *cis*-regulatory activities from standardly processed RNA-seq counts\n- Leveraging replicates to maximize statistical power\n- A lightweight method that easily scales to screening thousands of RNA-seq experiments for TE-mediated *cis*-regulation\n\nHere are the pain points you will avoid thanks to *craTEs*: \n- Remapping your RNA-seq data to TE sequences, which are masked in standard mapping pipelines. In addition, we have shown that TE-derived transcription is not necessarily a good proxy for TE-mediated *cis*-regulation.\n- Having to generate epigenomics data - e.g. ChIP-seq, ATAC-seq or DNase-seq - to get insights into the role of specific TE subfamilies in gene regulation.\n\n## Why this acronym?\nTEs were long discarded as non-functional and uninteresting \"junk DNA\". *craTEs* thus alludes to the crates found in vinyl record shops, in particular \"dollar bins\" in second hand shops where \"diggers\" go looking for forgotten gems.\n\n## Try it!\n\nKickstart your explorations with *craTEs* using [this repository](https://renkulab.io/projects/cyril.pulver/crates-basics/sessions/new) on the reproducible data science platform `renku`, powered by the Swiss Federal Instituted of Technology. We suggest using 2 CPUs and 8GB RAM, and using juyterlab, although you may also use RStudio if you prefer, but will have to convert the notebook to rmarkdown.\n\n## Usage\n\n```\nlibrary(craTEs)\n\n# genes as rows, samples as columns\ncountTable = readRDS('path_to_countTable')\n\n# cis-regulatory susceptibility matrix\nN = craTEs::N_from_tsv('path_to_N_matrix') #e.g. downloaded from https://doi.org/10.5281/zenodo.8117257\n\npreprocessed = craTEs::preprocess_E_N_for_activities(countTable, N, log_tpm_plot_path = 'path_to_qc_file.pdf')\n\n# estimating TE-dependent cis-regulatory activities from RNA-seq data\nactivities = craTEs::getSignifActivities(preprocessed$E_centered, preprocessed$N, treatment_group = c(\"treatment_s1\", \"treatment_s2\"), control_group = c(\"control_s1\", \"control_s2\"))\n\n# plotting cis-regulatory subfamilies (with example output)\ncraTEs::plot_signif_subfams(activities, 0.05, 4, \"Epigenetic repression of LTR5-Hs/SVA g#1\")\n```\n![image](https://github.com/bopekno/craTEs/assets/44056089/02c2017a-819a-4dd1-a9fd-3706b70d7538)\n\nSee [this jupyter notebook](https://renkulab.io/gitlab/crates/klf4-znf611-sva-crispri/-/blob/master/notebooks/TE_subfamily_diff_activity_poc.md) for examples of more complex use cases.\n\n## hg38 version\n\nWhile the Trono lab typically sticks to hg19 for all TE-related matters, we have created hg38-compatible N matrices for use with hg38 ensembl gene IDs. Get them from ZENODO at [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10390247.svg)](https://doi.org/10.5281/zenodo.10390247) and [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10390262.svg)](https://doi.org/10.5281/zenodo.10390262).\n\n\n\n## Installation\nDepends on the following Bioconductor packages: `Biobase`, `GenomicFeatures`, `GenomicRanges`, `RMariaDB`, `ensembldb` and `EnsDb.Hsapiens.v86` that must be installed before `craTEs`, as per the following R commands:\n\n```\ninstall.packages(\"devtools\", quietly = TRUE)\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(version = \"3.16\")\nBiocManager::install(c(\"Biobase\", \"GenomicFeatures\", \"GenomicRanges\", \"RMariaDB\", \"ensembldb\", \"EnsDb.Hsapiens.v86\"))\nlibrary(\"devtools\")\ndevtools::install_github(\"pulvercyril/craTEs\")\n```\n\n## Publications\n[Pulver et al., Statistical learning quantifies transposable element-mediated cis-regulation, Genome Biology 2023](https://doi.org/10.1186/s13059-023-03085-7)\n\n[Pontis et al., Primate-specific transposable elements shape transcriptional networks during human development, Nature Communications 2022](https://www.nature.com/articles/s41467-022-34800-w)\n\n[Martins et al., KRAB zinc finger proteins ZNF587/ZNF417 protect lymphoma cells from replicative stress-induced inflammation](https://www.biorxiv.org/content/10.1101/2023.03.08.531722v1)\n\n## Attributions\nConcept: Cyril Pulver, Didier Trono\n\nMethod development: Cyril Pulver, RaphaÃ«l de Fondeville, Julien Pontis\n\nImplementation: Cyril Pulver\n\nFunding: Didier Trono\n",
    "readme_length": 5297
  },
  {
    "name": "pyslam",
    "full_name": "luigifreda/pyslam",
    "description": "pySLAM is a Python-based Visual SLAM pipeline that supports monocular, stereo, and RGB-D cameras. It offers a wide range of modern local and global features, multiple loop-closing strategies, a volumetric reconstruction pipeline, integration of depth prediction models, and semantic segmentation for enhanced scene understanding.",
    "stars": 2738,
    "forks": 450,
    "language": "Python",
    "url": "https://github.com/luigifreda/pyslam",
    "topics": [],
    "created_at": "2019-04-06T16:32:09Z",
    "updated_at": "2025-11-28T02:31:16Z",
    "homepage": "",
    "license": "GNU General Public License v3.0",
    "readme": "<p align=\"center\"><img src=\"./images/pyslam-logo.png\" style=\"height: 160px; width: auto\"></p>\n\n# pySLAM v2.8.10\n\nAuthor: **[Luigi Freda](https://www.luigifreda.com)**\n\n \n**pySLAM** is a python implementation of a *Visual SLAM* pipeline that supports **monocular**, **stereo** and **RGBD** cameras. It provides the following features in a **single python environment**:\n- A wide range of classical and modern **[local features](#supported-local-features)** with a convenient interface for their integration.\n- Multiple loop closing methods, including **[descriptor aggregators](#supported-global-descriptors-and-local-descriptor-aggregation-methods)** such as visual Bag of Words (BoW, iBow), Vector of Locally Aggregated Descriptors (VLAD) and modern **[global descriptors](#supported-global-descriptors-and-local-descriptor-aggregation-methods)** (image-wise descriptors).\n- A **[volumetric reconstruction pipeline](#volumetric-reconstruction)** that processes depth and color images using volumetric integration to produce dense reconstructions. It supports **TSDF** with voxel hashing and incremental **Gaussian Splatting**. \n- Integration of **[depth prediction models](#depth-prediction)** within the SLAM pipeline. These include DepthPro, DepthAnythingV2, RAFT-Stereo, CREStereo, etc.  \n- A suite of segmentation models for **[semantic understanding](#semantic-mapping)** of the scene, such as DeepLabv3, Segformer, and dense CLIP.\n- Additional tools for VO (Visual Odometry) and SLAM, with built-in support for both **g2o** and **GTSAM**, along with custom Python bindings for features not available in the original libraries.\n- Built-in support for over [10 dataset types](#datasets).\n\npySLAM serves as a flexible baseline framework to experiment with VO/SLAM techniques, *[local features](#supported-local-features)*, *[descriptor aggregators](#supported-global-descriptors-and-local-descriptor-aggregation-methods)*, *[global descriptors](#supported-global-descriptors-and-local-descriptor-aggregation-methods)*, *[volumetric integration](#volumetric-reconstruction-pipeline)*, *[depth prediction](#depth-prediction)* and *[semantic mapping](#semantic-mapping)*. It allows to explore, prototype and develop VO/SLAM pipelines. pySLAM is a research framework and a work in progress. It is not optimized for real-time performance.   \n\n**Enjoy it!**\n\n<p align=\"center\">\n  <img src=\"./images/feature-matching.png\" style=\"height: 160px; width: auto;\"/> <img src=\"./images/main-feature-matching.png\" style=\"height: 160px; width: auto;\" /> <img src=\"./images/main-vo-rerun.png\" style=\"height: 160px; width: auto;\" /> <img src=\"./images/STEREO.png\" style=\"height: 160px; width: auto;\" /> <img src=\"./images/RGBD2.png\" style=\"height: 160px; width: auto;\" /> <img src=\"./images/kitti-stereo.png\" style=\"height: 160px; width: auto;\" /> <img src=\"./images/loop-detection2.png\" style=\"height: 160px; width: auto;\" /> <img src=\"./images/depth-prediction.png\" style=\"height: 160px; width: auto;\" /> <img src=\"./images/dense-reconstruction-with-depth-prediction.png\" style=\"height: 160px; width: auto;\" /> <img src=\"./images/dense-reconstruction2.png\" style=\"height: 160px; width: auto;\" /> <img src=\"./images/dense-reconstruction-composition.gif\" style=\"height: 300px; width: auto;\" />\n</p>\n\n## Table of contents\n\n<!-- TOC -->\n\n- [pySLAM v2.8.10](#pyslam-v2810)\n  - [Table of contents](#table-of-contents)\n  - [Overview](#overview)\n    - [Main Scripts](#main-scripts)\n    - [System overview](#system-overview)\n  - [Install](#install)\n    - [Main requirements](#main-requirements)\n    - [Ubuntu](#ubuntu)\n    - [MacOS](#macos)\n    - [Docker](#docker)\n    - [How to install non-free OpenCV modules](#how-to-install-non-free-opencv-modules)\n    - [Troubleshooting and performance issues](#troubleshooting-and-performance-issues)\n  - [Usage](#usage)\n    - [Visual odometry](#visual-odometry)\n    - [Full SLAM](#full-slam)\n    - [Selecting a dataset and different configuration parameters](#selecting-a-dataset-and-different-configuration-parameters)\n    - [Feature tracking](#feature-tracking)\n    - [Loop closing and relocalization](#loop-closing-and-relocalization)\n      - [Vocabulary management](#vocabulary-management)\n      - [Vocabulary-free loop closing](#vocabulary-free-loop-closing)\n      - [Verify your loop detection configuration and verify vocabulary compability](#verify-your-loop-detection-configuration-and-verify-vocabulary-compability)\n        - [Loop detection method based on a pre-trained vocabulary](#loop-detection-method-based-on-a-pre-trained-vocabulary)\n        - [Missing vocabulary for the selected front-end descriptor type](#missing-vocabulary-for-the-selected-front-end-descriptor-type)\n    - [Volumetric reconstruction](#volumetric-reconstruction)\n      - [Dense reconstruction while running SLAM](#dense-reconstruction-while-running-slam)\n      - [Reload a saved sparse map and perform dense reconstruction](#reload-a-saved-sparse-map-and-perform-dense-reconstruction)\n      - [Reload and check your dense reconstruction](#reload-and-check-your-dense-reconstruction)\n      - [Controlling the spatial distribution of keyframe FOV centers](#controlling-the-spatial-distribution-of-keyframe-fov-centers)\n    - [Depth prediction](#depth-prediction)\n    - [Semantic mapping](#semantic-mapping)\n    - [Saving and reloading](#saving-and-reloading)\n      - [Save the a map](#save-the-a-map)\n      - [Reload a saved map and relocalize in it](#reload-a-saved-map-and-relocalize-in-it)\n      - [Trajectory saving](#trajectory-saving)\n    - [Graph optimization engines](#graph-optimization-engines)\n    - [SLAM GUI](#slam-gui)\n    - [Monitor the logs of tracking, local mapping, loop closing and volumetric mapping simultaneously](#monitor-the-logs-of-tracking-local-mapping-loop-closing-and-volumetric-mapping-simultaneously)\n    - [Evaluating SLAM](#evaluating-slam)\n      - [Run a SLAM evaluation](#run-a-slam-evaluation)\n      - [pySLAM performances and comparative evaluations](#pyslam-performances-and-comparative-evaluations)\n  - [Supported components and models](#supported-components-and-models)\n    - [Supported local features](#supported-local-features)\n    - [Supported matchers](#supported-matchers)\n    - [Supported global descriptors and local descriptor aggregation methods](#supported-global-descriptors-and-local-descriptor-aggregation-methods)\n        - [Local descriptor aggregation methods](#local-descriptor-aggregation-methods)\n        - [Global descriptors](#global-descriptors)\n    - [Supported depth prediction models](#supported-depth-prediction-models)\n    - [Supported volumetric mapping methods](#supported-volumetric-mapping-methods)\n    - [Supported semantic segmentation methods](#supported-semantic-segmentation-methods)\n  - [Configuration](#configuration)\n    - [Main configuration file](#main-configuration-file)\n    - [Datasets](#datasets)\n      - [KITTI Datasets](#kitti-datasets)\n      - [TUM Datasets](#tum-datasets)\n      - [ICL-NUIM Datasets](#icl-nuim-datasets)\n      - [EuRoC Datasets](#euroc-datasets)\n      - [Replica Datasets](#replica-datasets)\n      - [Tartanair Datasets](#tartanair-datasets)\n      - [ScanNet Datasets](#scannet-datasets)\n      - [ROS1 bags](#ros1-bags)\n      - [ROS2 bags](#ros2-bags)\n      - [Video and Folder Datasets](#video-and-folder-datasets)\n    - [Camera Settings](#camera-settings)\n  - [References](#references)\n  - [Credits](#credits)\n  - [License](#license)\n  - [Contributing to pySLAM](#contributing-to-pyslam)\n  - [Roadmap](#roadmap)\n\n<!-- /TOC -->\n\n## Overview\n\n```bash\nâ”œâ”€â”€ cpp         # Pybind11 C++ bindings\nâ”œâ”€â”€ data        # Sample input/output data\nâ”œâ”€â”€ docs        # Documentation files\nâ”œâ”€â”€ pyslam      # Core Python package\nâ”‚   â”œâ”€â”€ dense\nâ”‚   â”œâ”€â”€ depth_estimation\nâ”‚   â”œâ”€â”€ evaluation\nâ”‚   â”œâ”€â”€ io\nâ”‚   â”œâ”€â”€ local_features\nâ”‚   â”œâ”€â”€ loop_closing\nâ”‚   â”œâ”€â”€ semantics\nâ”‚   â”œâ”€â”€ slam\nâ”‚   â”œâ”€â”€ utilities\nâ”‚   â”œâ”€â”€ viz\nâ”œâ”€â”€ scripts     # Shell utility scripts\nâ”œâ”€â”€ settings    # Dataset/configuration files\nâ”œâ”€â”€ test        # Tests and usage examples\nâ”œâ”€â”€ thirdparty  # External dependencies\n```\n\n### Main Scripts\n\n* `main_vo.py` combines the simplest VO ingredients without performing any image point triangulation or windowed bundle adjustment. At each step $k$, `main_vo.py` estimates the current camera pose $C_k$ with respect to the previous one $C_{k-1}$. The inter-frame pose estimation returns $[R_{k-1,k},t_{k-1,k}]$ with $\\Vert t_{k-1,k} \\Vert=1$. With this very basic approach, you need to use a ground truth in order to recover a correct inter-frame scale $s$ and estimate a valid trajectory by composing $C_k = C_{k-1} [R_{k-1,k}, s t_{k-1,k}]$. This script is a first start to understand the basics of inter-frame feature tracking and camera pose estimation.\n\n* `main_slam.py` adds feature tracking along multiple frames, point triangulation, keyframe management, bundle adjustment, loop closing, dense mapping and depth inference in order to estimate the camera trajectory and build both a sparse and dense map. It's a full SLAM pipeline and includes all the basic and advanced blocks which are necessary to develop a real visual SLAM pipeline.\n\n* `main_feature_matching.py` shows how to use the basic feature tracker capabilities (*feature detector* + *feature descriptor* + *feature matcher*) and allows to test the different available local features. \n\n* `main_depth_prediction.py` shows how to use the available depth inference models to get depth estimations from input color images.\n  \n* `main_map_viewer.py` reloads a saved map and visualizes it. Further details on how to save a map [here](#reload-a-saved-map-and-relocalize-in-it).\n\n* `main_map_dense_reconstruction.py` reloads a saved map and uses a configured volumetric integrator to obtain a dense reconstruction (see [here](#volumetric-reconstruction)). \n\n* `main_slam_evaluation.py` enables automated SLAM evaluation by executing `main_slam.py` across a collection of datasets and configuration presets (see [here](#evaluating-slam)).\n\nOther *test/example scripts* are provided in the `test` folder.\n\n### System overview      \n\n[This page](./docs/system_overview.md) provides a high-level system overview, including diagrams that illustrate the main **workflow**, key **components**, and **class** relationships or dependencies.\n\n**paper**: [\"pySLAM: An Open-Source, Modular, and Extensible Framework for SLAM\"](https://arxiv.org/abs/2502.11955), *Luigi Freda*      \nYou may find an updated version of the paper [here](./docs/tex/document.pdf).\n\n**presentation**: [_\"pySLAM and slamplay: Modular, Extensible SLAM Tools for Rapid Prototyping and Integration\"_](https://docs.google.com/presentation/d/e/2PACX-1vSHoOR5-oiL7yDkowOe3mCbPvq4-qZzmWWZFswvCEiSMLkyUQoXgoODiG4GZL8pMpKTqqJUZ3auk0T-/pub?start=false&loop=false&delayms=3000), *Luigi Freda*    \nRSS 2025 Workshop: _Unifying Visual SLAM_. The recorded talk is available [here](https://www.youtube.com/watch?v=OsS4XzVDSj4).\n\n--- \n## Install \n\nFirst, clone this repo and its submodules by running \n```bash\ngit clone --recursive https://github.com/luigifreda/pyslam.git\ncd pyslam \n```\n\nThen, under **Ubuntu** and **MacOs** you can simply run:\n```bash\n#pixi shell      # If you want to use pixi, this is the first step that prepares the installation. \n./install_all.sh   # Unified install procedure \n``` \n\nThis install scripts creates a **single python environment** `pyslam` that hosts all the [supported components and models](#supported-components-and-models). If `conda` is available, it automatically uses it, otherwise it installs and uses `venv`. An internet connection is required.\n\nRefer to these links for further details about the specific install procedures that are supported.\n- **Ubuntu**  [=>](#ubuntu)\n- **MacOs** [=>](#macos)  \n- **Windows+WSL2** [=>](https://github.com/luigifreda/pyslam/issues/51)\n- **Docker** [=>](#docker)\n\nOnce you completed the install procedure you can jump the [usage section](#usage).\n\n### Main requirements\n\n* Python **3.11.9**\n* OpenCV >=4.10 (see [below](#how-to-install-non-free-opencv-modules))\n* PyTorch >=2.3.1\n* Tensorflow >=2.13.1\n* Kornia >=0.7.3\n* Rerun\n* You need **CUDA** in order to run Gaussian splatting and dust3r-based methods. Check you have installed a suitable version of **cuda toolkit** by running `./cuda_config.sh` \n\nThe internal pySLAM libraries are imported by using a `Config` instance (from [pyslam/config.py](./pyslam/config.py)) in the main or test scripts. If you encounter any issues or performance problems, please refer to the [TROUBLESHOOTING](./docs/TROUBLESHOOTING.md) file for assistance.\n\n\n### Ubuntu \n\nThe install procedure was tested under *Ubuntu 20.04*, *22.04* and *24.04*. \n\n- With **venv**: Follow the instructions reported [here](./docs/PYTHON-VIRTUAL-ENVS.md).  \n- With **conda**: Run the procedure described in this other [file](./docs/CONDA.md).\n- With **pixi**: Run `pixi shell` in the root folder of the repo before launching `./install_all.sh` (see this [file](./docs/PIXI.md) for further details).\n\nThe install process creates a new python virtual environment `pyslam`.\n\n### MacOS\n\nFollow the instructions in this [file](./docs/MAC.md). The reported procedure was tested under *Sequoia 15.1.1* and *Xcode 16.1*.\n\n\n### Docker\n\nIf you prefer docker or you have an OS that is not supported yet, you can use [rosdocker](https://github.com/luigifreda/rosdocker): \n- With its custom `pyslam` / `pyslam_cuda` docker files (follow the instructions [here](https://github.com/luigifreda/rosdocker#pyslam)). \n- With one of the suggested docker images (*ubuntu\\*_cuda* or *ubuntu\\**), where you can clone, build and run pyslam. \n\n\n### How to install non-free OpenCV modules\n\nThe provided install scripts take care of installing a recent opencv version (>=**4.10**) with non-free modules enabled (see [scripts/install_opencv_python.sh](./scripts/install_opencv_python.sh)). To quickly verify your installed opencv version run:\n```bash       \n#pixi shell           # If you use pixi, this activates the pyslam environment. \n. pyenv-activate.sh   # Activate `pyslam` python environment. Only needed once in a new terminal. Not needed with pixi.\n./scripts/opencv_check.py\n```\n<!-- Otherwise, run the following commands: \n```bash       \npython3 -c \"import cv2; print(cv2.__version__)\" # check opencv version               \npython3 -c \"import cv2; detector = cv2.xfeatures2d.SURF_create()\"  # check if you have non-free OpenCV module support (no errors imply success)\n``` -->\n\n### Troubleshooting and performance issues\n\nIf you run into issues or errors during the installation process or at run-time, please, check the [docs/TROUBLESHOOTING.md](./docs/TROUBLESHOOTING.md) file. Before submitting a new git issue please read [here](docs/TROUBLESHOOTING.md#submitting-a-git-issue).\n\n--- \n\n## Usage \n\nOpen a new terminal and start experimenting with the scripts. In each new terminal, you are supposed to start with this command:\n```bash\n#pixi shell           # If you use pixi, this activates the pyslam environment. \n. pyenv-activate.sh   # Activate `pyslam` python environment. Only needed once in a new terminal. Not needed with pixi.\n```\nIf you are using `pixi` then just run `pixi shell` to activate the `pyslam` environment.\nThe file [config.yaml](./config.yaml) serves as a single entry point to configure the system and its global configuration parameters contained in [pyslam/config_parameters.py](./pyslam/config_parameters.py). Further information on how to configure pySLAM are provided [here](#selecting-a-dataset-and-different-configuration-parameters).\n\n \n\n### Visual odometry\n\nThe basic **Visual Odometry** (VO) can be run with the following commands:\n```bash\n#pixi shell           # If you use pixi, this activates the pyslam environment. \n. pyenv-activate.sh   # Activate `pyslam` python environment. Only needed once in a new terminal. Not needed with pixi.\n./main_vo.py\n```\nBy default, the script processes a [KITTI](http://www.cvlibs.net/datasets/kitti/eval_odometry.php) video (available in the folder `data/videos`) by using its corresponding camera calibration file (available in the folder `settings`), and its groundtruth (available in the same `data/videos` folder). If matplotlib windows are used, you can stop `main_vo.py` by clicking on one of them and pressing the key 'Q'. As explained above, this very *basic* script `main_vo.py` **strictly requires a ground truth**. \nNow, with RGBD datasets, you can also test the **RGBD odometry** with the classes `VisualOdometryRgbd` or `VisualOdometryRgbdTensor` (ground truth is not required here). \n \n\n### Full SLAM\n\nSimilarly, you can test the **full SLAM** by running `main_slam.py`:\n```bash\n#pixi shell           # If you use pixi, this activates the pyslam environment. \n. pyenv-activate.sh   # Activate `pyslam` python environment. Only needed once in a new terminal. Not needed with pixi.\n./main_slam.py\n```\n\nThis will process the same default [KITTI]((http://www.cvlibs.net/datasets/kitti/eval_odometry.php)) video (available in the folder `data/videos`) by using its corresponding camera calibration file (available in the folder `settings`). You can stop it by clicking on one of the opened windows and pressing the key 'Q' or closing the 3D pangolin GUI. \n\n--- \n\n### Selecting a dataset and different configuration parameters\n\nThe file [config.yaml](./config.yaml) serves as a single entry point to configure the system, the target dataset and its global configuration parameters set in [pyslam/config_parameters.py](./pyslam/config_parameters.py). \n\nTo process a different **dataset** with both VO and SLAM scripts, you need to update the file [config.yaml](./config.yaml):\n* Select your dataset `type` in the section `DATASET` (further details in the section *[Datasets](#datasets)* below for further details). This identifies a corresponding dataset section (e.g. `KITTI_DATASET`, `TUM_DATASET`, etc). \n* Select the `sensor_type` (`mono`, `stereo`, `rgbd`) in the chosen dataset section.  \n* Select the camera `settings` file in the dataset section (further details in the section *[Camera Settings](#camera-settings)* below).\n* Set the `groudtruth_file` accordingly. Further details in the section *[Datasets](#datasets)* below  (see also the files `io/ground_truth.py`, `io/convert_groundtruth_to_simple.py`).\n\nYou can use the section `GLOBAL_PARAMETERS` of the file [config.yaml](./config.yaml) to override the global configuration parameters set in [pyslam/config_parameters.py](./pyslam/config_parameters.py). This is particularly useful when running a [SLAM evaluation](#evaluating-slam).\n\n---\n\n### Feature tracking\n\nIf you just want to test the basic feature tracking capabilities (*feature detector* + *feature descriptor* + *feature matcher*) and get a taste of the different available local features, run\n```bash\n#pixi shell           # If you use pixi, this activates the pyslam environment. \n. pyenv-activate.sh   # Activate `pyslam` python environment. Only needed once in a new terminal. Not needed with pixi.\n./main_feature_matching.py\n```\n\nIn any of the above scripts, you can choose any detector/descriptor among *ORB*, *SIFT*, *SURF*, *BRISK*, *AKAZE*, *SuperPoint*, etc. (see the section *[Supported Local Features](#supported-local-features)* below for further information). \n\nSome basic examples are available in the subfolder `test/cv`. In particular, as for feature detection/description, you may want to take a look at [test/cv/test_feature_manager.py](./test/cv/test_feature_manager.py) too.\n\n---\n\n### Loop closing and relocalization\n\nMany [loop closing methods](#loop-closing) are available, combining different [aggregation methods](#local-descriptor-aggregation-methods) and [global descriptors](#global-descriptors).\n\nWhile running full SLAM, loop closing is enabled by default and can be disabled by setting `kUseLoopClosing=False` in `pyslam/config_parameters.py`. Different configuration options `LoopDetectorConfigs` can be found in [pyslam/loop_closing/loop_detector_configs.py](./pyslam/loop_closing/loop_detector_configs.py): Code comments provide additional useful details.\n\nOne can start experimenting with loop closing methods by using the examples in `test/loopclosing`. The example [test/loopclosing/test_loop_detector.py](./test/loopclosing/test_loop_detector.py) is the recommended entry point.\n\n\n#### Vocabulary management \n\n`DBoW2`, `DBoW3`, and `VLAD` require **pre-trained vocabularies**. ORB-based vocabularies are automatically downloaded into the `data` folder (see [pyslam/loop_closing/loop_detector_configs.py](pyslam/loop_closing/loop_detector_configs.py)).\n\nTo create a new vocabulary, follow these steps:\n\n1. **Generate an array of descriptors**: Use the script `test/loopclosing/test_gen_des_array_from_imgs.py` to generate the array of descriptors that will be used to train the new vocabulary. Select your desired descriptor type via the tracker configuration. \n\n2.  **DBOW vocabulary generation**: Train your target DBOW vocabulary by using the script `test/loopclosing/test_gen_dbow_voc_from_des_array.py`.\n\n3. **VLAD vocabulary generation**: Train your target VLAD \"vocabulary\" by using the script `test/loopclosing/test_gen_vlad_voc_from_des_array.py`.\n\nOnce you have trained the vocabulary, you can add it in [pyslam/loop_closing/loop_detector_vocabulary.py](./pyslam/loop_closing/loop_detector_vocabulary.py) and correspondingly create a new loop detector configuration in [pyslam/loop_closing/loop_detector_configs.py](./pyslam/loop_closing/loop_detector_configs.py) that uses it.\n\n#### Vocabulary-free loop closing\n\nMost methods do not require pre-trained vocabularies. Specifically:\n- `iBoW` and `OBindex2`: These methods incrementally build bags of binary words and, if needed, convert (front-end) non-binary descriptors into binary ones. \n- Others: Methods like `HDC_DELF`, `SAD`, `AlexNet`, `NetVLAD`, `CosPlace`, `EigenPlaces`, and `Megaloc` directly extract their specific **global descriptors** and process them using dedicated aggregators, independently from the used front-end descriptors.\n\nAs mentioned above, only `DBoW2`, `DBoW3`, and `VLAD` require pre-trained vocabularies.\n\n#### Verify your loop detection configuration and verify vocabulary compability\n\n##### Loop detection method based on a pre-trained vocabulary\n\nWhen selecting a **loop detection method based on a pre-trained vocabulary** (such as `DBoW2`, `DBoW3`, and `VLAD`), ensure the following:\n1. The back-end and the front-end are using the same descriptor type (this is also automatically checked for consistency) or their descriptor managers are independent (see further details in the configuration options `LoopDetectorConfigs` available in [pyslam/loop_closing/loop_detector_configs.py](pyslam/loop_closing/loop_detector_configs.py)).\n2. A corresponding pre-trained vocubulary is available. For more details, refer to the [vocabulary management section](#vocabulary-management).\n\n##### Missing vocabulary for the selected front-end descriptor type\n\nIf you lack a compatible vocabulary for the selected front-end descriptor type, you can follow one of these options:     \n1. Create and load the vocabulary (refer to the [vocabulary management section](#vocabulary-management)).     \n2. Choose an `*_INDEPENDENT` loop detector method, which works with an independent local_feature_manager.     \n3. Select a vocabulary-free loop closing method.      \n   \nSee the file [pyslam/loop_closing/loop_detector_configs.py](./pyslam/loop_closing/loop_detector_configs.py) for further details.\n\n---\n\n### Volumetric reconstruction\n\n#### Dense reconstruction while running SLAM \n\nThe SLAM back-end hosts a volumetric reconstruction pipeline. This is disabled by default. You can enable it by setting `kUseVolumetricIntegration=True` and selecting your preferred method `kVolumetricIntegrationType` in `pyslam/config_parameters.py`. At present, two methods are available: `TSDF` and `GAUSSIAN_SPLATTING` (see [pyslam/dense/volumetric_integrator_factory.py](pyslam/dense/volumetric_integrator_factory.py)). Note that you need CUDA in order to run `GAUSSIAN_SPLATTING` method.\n\nAt present, the volumetric reconstruction pipeline works with:\n- RGBD datasets \n- When a [depth estimator](#depth-prediction) is used\n  * in the back-end with STEREO datasets (you can't use depth prediction in the back-end with MONOCULAR datasets, further details [here](#depth-prediction))\n  * in the front-end (to emulate an RGBD sensor) and a depth prediction/estimation gets available for each processed keyframe. \n\nTo obtain a mesh as output, set `kVolumetricIntegrationExtractMesh=True` in `pyslam/config_parameters.py`.\n\n#### Reload a saved sparse map and perform dense reconstruction \n\nUse the script `main_map_dense_reconstruction.py` to reload a saved sparse map and perform dense reconstruction by using its posed keyframes as input. You can select your preferred dense reconstruction method directly in the script. \n\n- To check what the volumetric integrator is doing, run in another shell `tail -f logs/volumetric_integrator.log` (from repository root folder).\n- To save the obtained dense and sparse maps, press the `Save` button on the GUI. \n\n#### Reload and check your dense reconstruction \n\nYou can check the output pointcloud/mesh by using [CloudCompare](https://www.cloudcompare.org/). \n\nIn the case of a saved Gaussian splatting model, you can visualize it by:\n1. Using the [superslat editor](https://playcanvas.com/supersplat/editor) (drag and drop the saved Gaussian splatting `.ply` pointcloud in the editor interface). \n2. Getting into the folder `test/gaussian_splatting` and running:      \n    `python test_gsm.py --load <gs_checkpoint_path>`      \n    The directory ` <gs_checkpoint_path>` is expected to have the following structure:      \n    ```bash\n    â”œâ”€â”€ gs_checkpoint_path\n        â”œâ”€â”€ pointcloud   # folder containing different subfolders, each one with a saved .ply econding the Gaussian splatting model at a specific iteration/checkpoint\n        â”œâ”€â”€ last_camera.json\n        â”œâ”€â”€ config.yml\n    ```\n\n#### Controlling the spatial distribution of keyframe FOV centers\n\nIf you are targeting volumetric reconstruction while running SLAM, you can enable a **keyframe generation policy** designed to manage the spatial distribution of keyframe field-of-view (FOV) centers. The *FOV center of a camera* is defined as the backprojection of its image center, calculated using the median depth of the frame. With this policy, a new keyframe is generated only if its FOV center lies beyond a predefined distance from the nearest existing keyframe's FOV center. You can enable this policy by setting the following parameters in the yaml setting:\n```yaml\nKeyFrame.useFovCentersBasedGeneration: 1  # compute 3D fov centers of camera frames by using median depth and use their distances to control keyframe generation\nKeyFrame.maxFovCentersDistance: 0.2       # max distance between fov centers in order to generate a keyframe\n```\n\n---\n\n### Depth prediction\n\nThe available depth prediction models can be utilized both in the SLAM back-end and front-end. \n- **Back-end**: Depth prediction can be enabled in the [volumetric reconstruction](#volumetric-reconstruction) pipeline by setting the parameter `kVolumetricIntegrationUseDepthEstimator=True` and selecting your preferred `kVolumetricIntegrationDepthEstimatorType` in `pyslam/config_parameters.py`. \n- **Front-end**: Depth prediction can be enabled in the front-end by setting the parameter `kUseDepthEstimatorInFrontEnd` in `pyslam/config_parameters.py`. This feature estimates depth images from input color images to emulate a RGBD camera. Please, note this functionality is still *experimental* at present time [WIP].   \n\n**Notes**: \n* In the case of a **monocular SLAM**, do NOT use depth prediction in the back-end volumetric integration: The SLAM (fake) scale will conflict with the absolute metric scale of depth predictions. With monocular datasets, you can enable depth prediction to run in the front-end (to emulate an RGBD sensor).\n- Depth inference may be very slow (for instance, with DepthPro it takes ~1s per image on a typical machine). Therefore, the resulting volumetric reconstruction pipeline may be very slow.\n\nRefer to the file `depth_estimation/depth_estimator_factory.py` for further details. Both stereo and monocular prediction approaches are supported. You can test depth prediction/estimation by using the script `main_depth_prediction.py`.\n\n---\n\n### Semantic mapping\n\n\n<p style=\"text-align: center;\">\n  <img src=\"./images/semantic_mapping_from_david.jpeg\" alt=\"Semantic Mapping\" style=\"height: 300px; width: auto;\"/>\n</p>\n\nThe semantic mapping pipeline can be enabled by setting the parameter `kDoSemanticMapping=True` in `pyslam/config_parameters.py`. The best way of configuring the semantic mapping module used is to modify it in `pyslam/semantics/semantic_mapping_configs.py`.\n\nDifferent semantic mapping methods are available (see [here](./docs/semantics.md) for furthere details). Currently, we support semantic mapping using **dense semantic segmentation**.\n  - `DEEPLABV3`: from `torchvision`, pre-trained on COCO/VOC.\n  - `SEGFORMER`: from `transformers`, pre-trained on Cityscapes or ADE20k.\n  - `CLIP`: from `f3rm` package for open-vocabulary support.\n\n**Semantic features** are assigned to **keypoints** on the image and fused into map points. The semantic features can be:\n- *Labels*: categorical labels as numbers.\n- *Probability vectors*: probability vectors for each class.\n- *Feature vectors*: feature vectors obtained from an encoder. This is generally used for open vocabulary mapping.\n\nThe simplest way to test the available segmentation models is to run: `test/semantics/test_semantic_segmentation.py`.\n\n---\n\n### Saving and reloading\n\n#### Save the a map\n\nWhen you run the script `main_slam.py` (`main_map_dense_reconstruction.py`):\n- You can save the current map state by pressing the button `Save` on the GUI. This saves the current map along with front-end, and backend configurations into the default folder `results/slam_state` (`results/slam_state_dense_reconstruction`). \n- To change the default saving path, open `config.yaml` and update target `folder_path` in the section: \n  ```bash\n  SYSTEM_STATE:\n    folder_path: results/slam_state   # default folder path (relative to repository root) where the system state is saved or reloaded\n  ```\n\n#### Reload a saved map and relocalize in it \n\n- A saved map can be loaded and visualized in the GUI by running: \n  ```bash\n  . pyenv-activate.sh   #  Activate pyslam python virtual environment. This is only needed once in a new terminal.\n  ./main_map_viewer.py  #  Use the --path options to change the input path\n  ```\n  \n- To enable map reloading and relocalization when running `main_slam.py`, open `config.yaml` and set \n  ```bash\n  SYSTEM_STATE:\n    load_state: True                  # Flag to enable SLAM state reloading (map state + loop closing state)\n    folder_path: results/slam_state   # Default folder path (relative to repository root) where the system state is saved or reloaded\n  ```\n\nNote that pressing the `Save` button saves the current map, front-end, and backend configurations. Reloading a saved map replaces the current system configurations to ensure descriptor compatibility.  \n\n\n#### Trajectory saving\n\nEstimated trajectories can be saved in three **formats**: *TUM* (The Open Mapping format), *KITTI* (KITTI Odometry format), and *EuRoC* (EuRoC MAV format). pySLAM saves two **types** of trajectory estimates:\n\n- **Online**: In *online* trajectories, each pose estimate depends only on past poses. A pose estimate is saved at the end of each front-end iteration for the current frame.\n- **Final**: In *final* trajectories, each pose estimate depends on both past and future poses. A pose estimate is refined multiple times by LBA windows that include it, as well asby PGO and GBA during loop closures.\n\n\nTo enable trajectory saving, open `config.yaml` and search for the `SAVE_TRAJECTORY`: set `save_trajectory: True`, select your `format_type` (`tum`, `kitti`, `euroc`), and the output filename. For instance for a `kitti` format output:   \n```bash\nSAVE_TRAJECTORY:\n  save_trajectory: True\n  format_type: kitti             # Supported formats: `tum`, `kitti`, `euroc`\n  output_folder: results/metrics # Relative to pyslam root folder \n  basename: trajectory           # Basename of the trajectory saving output\n```\n\n---\n\n### Graph optimization engines\n\nCurrently, pySLAM supports both `g2o` and `gtsam` for graph optimization, with `g2o` set as the default engine. You can enable `gtsam` by setting to `True` the following parameters in `pyslam/config_parameters.py`:\n```python\n  # Optimization engine \n  kOptimizationFrontEndUseGtsam = True    \n  kOptimizationBundleAdjustUseGtsam = True \n  kOptimizationLoopClosingUseGtsam = True \n```\n\nAdditionally, the `gtsam_factors` package provides custom Python bindings for features not available in the original gtsam framework. See [here](./thirdparty/gtsam_factors/README.md) for further details.\n\n---\n\n### SLAM GUI \n\nSome quick information about the non-trivial GUI buttons of `main_slam.py`: \n- `Step`: Enter in the *Step by step mode*. Press the button `Step` a first time to pause. Then, press it again to make the pipeline process a single new frame.\n- `Save`: Save the map into the file `map.json`. You can visualize it back by using the script `/main_map_viewer.py` (as explained above). \n- `Reset`: Reset SLAM system. \n- `Draw Ground Truth`:  If a ground truth dataset (e.g., KITTI, TUM, EUROC, or REPLICA) is loaded, you can visualize it by pressing this button. The ground truth trajectory will be displayed in 3D and will be progressively aligned with the estimated trajectory, updating approximately every 10-30 frames. As more frames are processed, the alignment between the ground truth and estimated trajectory becomes more accurate. After about 20 frames, if the button is pressed, a window will appear showing the Cartesian alignment errors along the main axes (i.e., $e_x$, $e_y$, $e_z$) and the history of the total $RMSE$ between the ground truth and the aligned estimated trajectories.\n\n---\n\n### Monitor the logs of tracking, local mapping, loop closing and volumetric mapping simultaneously\n\nThe logs generated by the modules `local_mapping.py`, `loop_closing.py`, `loop_detecting_process.py`, `global_bundle_adjustments.py`, and `volumetric integrator_<X>.py` are collected in the files `local_mapping.log`, `loop_closing.log`, `loop_detecting.log`, `gba.log`, and `volumetric_integrator.log`, respectively. These logs files are all stored in the folder `logs`. At runtime, for debugging purposes, you can individually monitor any of the log files by running the following command:    \n`tail -f logs/<log file name>`     \nOtherwise, to check all logs at the same time, run this `tmux`-based script:          \n`./scripts/launch_tmux_logs.sh`           \nTo launch slam and check all logs, run:     \n`./scripts/launch_tmux_slam.sh`      \nPress `CTRL+A` and then `CTRL+Q` to exit from `tmux` environment.\n\n--- \n\n### Evaluating SLAM\n\n#### Run a SLAM evaluation \n\nThe `main_slam_evaluation.py` script enables automated SLAM evaluation by executing `main_slam.py` across a collection of **datasets** and configuration **presets**. The main input to the script is an evaluation configuration file (e.g., `evaluation/configs/evaluation.json`) that specifies which datasets and presets to be used. For convenience, sample configurations for the datasets `TUM`, `EUROC` and `KITTI` datasets are already provided in the `evaluation/configs/` directory.\n\nFor each evaluation run, results are stored in a dedicated subfolder within the `results` directory, containing all the computed metrics. These metrics are then processed and compared. The final output is a report, available in `PDF`, `LaTeX`, and `HTML` formats, that includes comparison tables summarizing the *Absolute Trajectory Error* (ATE), the maximum deviation from the ground truth trajectory and other metrics. \n\nYou can find some obtained evaluation results [here](./docs/evaluations/evaluations.md).\n\n#### pySLAM performances and comparative evaluations \n\nFor a comparative evaluation of the \"**online**\" trajectory estimated by pySLAM versus the \"**final**\" trajectory estimated by ORB-SLAM3, check out this nice [notebook](https://github.com/anathonic/Trajectory-Comparison-ORB-SLAM3-pySLAM/blob/main/trajectories_comparison.ipynb). For more details about \"*online*\" and \"*final*\" trajectories, refer to this [section](#trajectory-saving).\n\n**Note**: Unlike ORB-SLAM3, which only saves the final pose estimates (recorded after the entire dataset has been processed), pySLAM saves both online and final pose estimates. For details on how to save trajectories in pySLAM, refer to this [section](#trajectory-saving).\nWhen you click the `Draw Ground Truth` button in the GUI (see [here](#slam-gui)), you can visualize the *Absolute Trajectory Error* (ATE or *RMSE*) history and evaluate both online and final errors up to the current time.\n\n---\n\n## Supported components and models\n### Supported local features\n\nAt present time, the following feature **detectors** are supported: \n* *[FAST](https://www.edwardrosten.com/work/fast.html)*  \n* *[Good features to track](https://ieeexplore.ieee.org/document/323794)* \n* *[ORB](http://www.willowgarage.com/sites/default/files/orb_final.pdf)*  \n* *[ORB2](https://github.com/raulmur/ORB_SLAM2)* (improvements of ORB-SLAM2 to ORB detector) \n* *[SIFT](https://www.cs.ubc.ca/~lowe/papers/iccv99.pdf)*   \n* *[SURF](http://people.ee.ethz.ch/~surf/eccv06.pdf)*   \n* *[KAZE](https://www.doc.ic.ac.uk/~ajd/Publications/alcantarilla_etal_eccv2012.pdf)*\n* *[AKAZE](http://www.bmva.org/bmvc/2013/Papers/paper0013/paper0013.pdf)* \n* *[BRISK](http://www.margaritachli.com/papers/ICCV2011paper.pdf)*  \n* *[AGAST](http://www.i6.in.tum.de/Main/ResearchAgast)*\n* *[MSER](http://cmp.felk.cvut.cz/~matas/papers/matas-bmvc02.pdf)*\n* *[StarDector/CenSurE](https://link.springer.com/content/pdf/10.1007%2F978-3-540-88693-8_8.pdf)*\n* *[Harris-Laplace](https://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/mikolajczyk_ijcv2004.pdf)* \n* *[SuperPoint](https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork)*\n* *[D2-Net](https://github.com/mihaidusmanu/d2-net)*\n* *[DELF](https://github.com/tensorflow/models/tree/master/research/delf)*\n* *[Contextdesc](https://github.com/lzx551402/contextdesc)*\n* *[LFNet](https://github.com/vcg-uvic/lf-net-release)*\n* *[R2D2](https://github.com/naver/r2d2)*\n* *[Key.Net](https://github.com/axelBarroso/Key.Net)*\n* *[DISK](https://arxiv.org/abs/2006.13566)*\n* *[ALIKED](https://arxiv.org/abs/2304.03608)*\n* *[Xfeat](https://arxiv.org/abs/2404.19174)*\n* *[KeyNetAffNetHardNet](https://github.com/axelBarroso/Key.Net)* (KeyNet detector + AffNet + HardNet descriptor).\n\nThe following feature **descriptors** are supported: \n* *[ORB](http://www.willowgarage.com/sites/default/files/orb_final.pdf)*  \n* *[SIFT](https://www.cs.ubc.ca/~lowe/papers/iccv99.pdf)*\n* *[ROOT SIFT](https://www.robots.ox.ac.uk/~vgg/publications/2012/Arandjelovic12/arandjelovic12.pdf)*\n* *[SURF](http://people.ee.ethz.ch/~surf/eccv06.pdf)*    \n* *[AKAZE](http://www.bmva.org/bmvc/2013/Papers/paper0013/paper0013.pdf)* \n* *[BRISK](http://www.margaritachli.com/papers/ICCV2011paper.pdf)*     \n* *[FREAK](https://www.researchgate.net/publication/258848394_FREAK_Fast_retina_keypoint)* \n* *[SuperPoint](https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork)*\n* *[Tfeat](https://github.com/vbalnt/tfeat)*\n* *[BOOST_DESC](https://www.labri.fr/perso/vlepetit/pubs/trzcinski_pami15.pdf)*\n* *[DAISY](https://ieeexplore.ieee.org/document/4815264)*\n* *[LATCH](https://arxiv.org/abs/1501.03719)*\n* *[LUCID](https://pdfs.semanticscholar.org/85bd/560cdcbd4f3c24a43678284f485eb2d712d7.pdf)*\n* *[VGG](https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/simonyan14learning.pdf)*\n* *[Hardnet](https://github.com/DagnyT/hardnet.git)*\n* *[GeoDesc](https://github.com/lzx551402/geodesc.git)*\n* *[SOSNet](https://github.com/yuruntian/SOSNet.git)*\n* *[L2Net](https://github.com/yuruntian/L2-Net)*\n* *[Log-polar descriptor](https://github.com/cvlab-epfl/log-polar-descriptors)*\n* *[D2-Net](https://github.com/mihaidusmanu/d2-net)*\n* *[DELF](https://github.com/tensorflow/models/tree/master/research/delf)*\n* *[Contextdesc](https://github.com/lzx551402/contextdesc)*\n* *[LFNet](https://github.com/vcg-uvic/lf-net-release)*\n* *[R2D2](https://github.com/naver/r2d2)*\n* *[BEBLID](https://raw.githubusercontent.com/iago-suarez/BEBLID/master/BEBLID_Boosted_Efficient_Binary_Local_Image_Descriptor.pdf)*\n* *[DISK](https://arxiv.org/abs/2006.13566)*\n* *[ALIKED](https://arxiv.org/abs/2304.03608)*\n* *[Xfeat](https://arxiv.org/abs/2404.19174)*\n* *[KeyNetAffNetHardNet](https://github.com/axelBarroso/Key.Net)* (KeyNet detector + AffNet + HardNet descriptor).\n  \nFor more information, refer to [pyslam/local_features/feature_types.py](pyslam/local_features/feature_types.py) file. Some of the local features consist of a *joint detector-descriptor*. You can start playing with the supported local features by taking a look at `test/cv/test_feature_manager.py` and `main_feature_matching.py`.\n\nIn both the scripts `main_vo.py` and `main_slam.py`, you can create your preferred detector-descritor configuration and feed it to the function `feature_tracker_factory()`. Some ready-to-use configurations are already available in the file [local_features/feature_tracker.configs.py](local_features/feature_tracker_configs.py)\n\nThe function `feature_tracker_factory()` can be found in the file `pyslam/local_features/feature_tracker.py`. Take a look at the file `pyslam/local_features/feature_manager.py` for further details.\n\n**N.B.**: You just need a *single* python environment to be able to work with all the [supported local features](#supported-local-features)!\n\n\n### Supported matchers \n\n* *BF*: Brute force matcher on descriptors (with KNN).\n* *[FLANN](https://www.semanticscholar.org/paper/Fast-Approximate-Nearest-Neighbors-with-Automatic-Muja-Lowe/35d81066cb1369acf4b6c5117fcbb862be2af350)* \n* *[XFeat](https://arxiv.org/abs/2404.19174)*      \n* *[LightGlue](https://arxiv.org/abs/2306.13643)*\n* *[LoFTR](https://arxiv.org/abs/2104.00680)*\n* *[MASt3R](https://arxiv.org/abs/2406.09756)*\n  \nSee the file `local_features/feature_matcher.py` for further details.\n\n\n### Supported global descriptors and local descriptor aggregation methods\n\n##### Local descriptor aggregation methods\n\n* Bag of Words (BoW): [DBoW2](https://github.com/dorian3d/DBoW2), [DBoW3](https://github.com/rmsalinas/DBow3).  [[paper](https://doi.org/10.1109/TRO.2012.2197158)]\n* Vector of Locally Aggregated Descriptors: [VLAD](https://www.vlfeat.org/api/vlad.html).  [[paper](https://doi.org/10.1109/CVPR.2010.5540039)] \n* Incremental Bags of Binary Words (iBoW) via Online Binary Image Index: [iBoW](https://github.com/emiliofidalgo/ibow-lcd), [OBIndex2](https://github.com/emiliofidalgo/obindex2).  [[paper](https://doi.org/10.1109/LRA.2018.2849609)]\n* Hyperdimensional Computing: [HDC](https://www.tu-chemnitz.de/etit/proaut/hdc_desc).  [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Neubert_Hyperdimensional_Computing_as_a_Framework_for_Systematic_Aggregation_of_Image_CVPR_2021_paper.html)]\n\n\n**NOTE**: *iBoW* and *OBIndex2* incrementally build a binary image index and do not need a prebuilt vocabulary. In the implemented classes, when needed, the input non-binary local descriptors are transparently transformed into binary descriptors.\n\n##### Global descriptors\n\nAlso referred to as *holistic descriptors*:\n\n* [SAD](https://ieeexplore.ieee.org/document/6224623)\n* [AlexNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet)\n* [NetVLAD](https://www.di.ens.fr/willow/research/netvlad/)\n* [HDC-DELF](https://www.tu-chemnitz.de/etit/proaut/hdc_desc)\n* [CosPlace](https://github.com/gmberton/CosPlace)\n* [EigenPlaces](https://github.com/gmberton/EigenPlaces)\n* [Megaloc](https://github.com/gmberton/MegaLoc)\n\n\nDifferent [loop closing methods](#loop-closing) are available. These combines the above aggregation methods and global descriptors.\nSee the file [pyslam/loop_closing/loop_detector_configs.py](pyslam/loop_closing/loop_detector_configs.py) for further details.\n\n\n### Supported depth prediction models\n\nBoth monocular and stereo depth prediction models are available. SGBM algorithm has been included as a classic reference approach. \n\n* [SGBM](https://ieeexplore.ieee.org/document/4359315): Depth SGBM from OpenCV (Stereo, classic approach)\n* [Depth-Pro](https://arxiv.org/abs/2410.02073) (Monocular)\n* [DepthAnythingV2](https://arxiv.org/abs/2406.09414) (Monocular)\n* [RAFT-Stereo](https://arxiv.org/abs/2109.07547) (Stereo)\n* [CREStereo](https://arxiv.org/abs/2203.11483) (Stereo)\n* [MASt3R](https://arxiv.org/abs/2406.09756) (Stereo/Monocular)\n* [MV-DUSt3R](https://arxiv.org/abs/2412.06974) (Stereo/Monocular)\n\n### Supported volumetric mapping methods\n\n* [TSDF](https://arxiv.org/pdf/2110.00511) with voxel block grid (parallel spatial hashing)\n* Incremental 3D Gaussian Splatting. See [here](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/) and [MonoGS](https://arxiv.org/abs/2312.06741) for a description of its backend.\n\n\n### Supported semantic segmentation methods\n\n- [DeepLabv3](https://arxiv.org/abs/1706.05587): from `torchvision`, pre-trained on COCO/VOC.\n- [Segformer](https://arxiv.org/abs/2105.15203): from `transformers`, pre-trained on Cityscapes or ADE20k.\n- [CLIP](https://arxiv.org/abs/2212.09506): from `f3rm` package for open-vocabulary support.\n\n--- \n\n## Configuration \n\n### Main configuration file\n\nRefer to [this section](#selecting-a-dataset-and-different-configuration-parameters) for how to update the main configuration file [config.yaml](./config.yaml) and affect the configuration parameters in [pyslam/config_parameters.py](./pyslam/config_parameters.py).\n\n### Datasets\n\nThe following datasets are supported:\n\nDataset | type in `config.yaml`\n--- | --- \n[KITTI odometry data set (grayscale, 22 GB)](http://www.cvlibs.net/datasets/kitti/eval_odometry.php)  | `type: KITTI_DATASET` \n[TUM dataset](https://vision.in.tum.de/data/datasets/rgbd-dataset/download)                           | `type: TUM_DATASET` \n[ICL-NUIM dataset](https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html)                              | `type: ICL_NUIM_DATASET` \n[EUROC dataset](http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets)          | `type: EUROC_DATASET` \n[REPLICA dataset](https://github.com/facebookresearch/Replica-Dataset)                                | `type: REPLICA_DATASET` \n[TARTANAIR dataset](https://theairlab.org/tartanair-dataset/)                                         | `type: TARTANAIR_DATASET` \n[ScanNet dataset](http://www.scan-net.org/)                                                                                                  | `type: SCANNET_DATASET`\n[ROS1  bags](https://wiki.ros.org/Bags)                                                                                                      | `type: ROS1BAG_DATASET` \n[ROS2  bags](https://docs.ros.org/en/foxy/Tutorials/Beginner-CLI-Tools/Recording-And-Playing-Back-Data/Recording-And-Playing-Back-Data.html) | `type: ROS2BAG_DATASET` \nVideo file                                                                                                                                   | `type: VIDEO_DATASET` \nFolder of images                                                                                                                             | `type: FOLDER_DATASET` \n\n\n\nUse the download scripts available in the folder `scripts` to download some of the following datasets.\n\n#### KITTI Datasets\n\npySLAM code expects the following structure in the specified KITTI path folder (specified in the section `KITTI_DATASET` of the file `config.yaml`). : \n```bash\nâ”œâ”€â”€ sequences\n    â”œâ”€â”€ 00\n    ...\n    â”œâ”€â”€ 21\nâ”œâ”€â”€ poses\n    â”œâ”€â”€ 00.txt\n        ...\n    â”œâ”€â”€ 10.txt\n```\n1. Download the dataset (grayscale images) from http://www.cvlibs.net/datasets/kitti/eval_odometry.php and prepare the KITTI folder as specified above\n\n2. Select the corresponding calibration settings file (section `KITTI_DATASET: settings:` in the file `config.yaml`)\n\n\n#### TUM Datasets \n\npySLAM code expects a file `associations.txt` in each TUM dataset folder (specified in the section `TUM_DATASET:` of the file `config.yaml`). \n\n1. Download a sequence from http://vision.in.tum.de/data/datasets/rgbd-dataset/download and uncompress it.\n2. Associate RGB images and depth images using the python script [associate.py](http://vision.in.tum.de/data/datasets/rgbd-dataset/tools). You can generate your `associations.txt` file by executing:\n    ```bash\n    python associate.py PATH_TO_SEQUENCE/rgb.txt PATH_TO_SEQUENCE/depth.txt > associations.txt      # pay attention to the order!\n    ```\n3. Select the corresponding calibration settings file (section `TUM_DATASET: settings:` in the file `config.yaml`).\n\n\n#### ICL-NUIM Datasets \n\nFollow the same instructions provided for the TUM datasets. \n\n\n#### EuRoC Datasets\n\n1. Download a sequence (ASL format) from http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets (check this direct [link](http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/))\n2. Use the script `io/generate_euroc_groundtruths_as_tum.sh` to generate the TUM-like groundtruth files `path + '/' + name + '/mav0/state_groundtruth_estimate0/data.tum'` that are required by the `EurocGroundTruth` class.\n3. Select the corresponding calibration settings file (section `EUROC_DATASET: settings:` in the file `config.yaml`).\n\n\n#### Replica Datasets\n\n1. You can download the zip file containing all the sequences by running:    \n   `wget https://cvg-data.inf.ethz.ch/nice-slam/data/Replica.zip`    \n2. Then, uncompress it and deploy the files as you wish.\n3. Select the corresponding calibration settings file (section `REPLICA_DATASET: settings:` in the file `config.yaml`).\n\n\n#### Tartanair Datasets\n\n1. You can download the datasets from https://theairlab.org/tartanair-dataset/     \n2. Then, uncompress them and deploy the files as you wish.\n3. Select the corresponding calibration settings file (section `TARTANAIR_DATASET: settings:` in the file `config.yaml`).\n\n#### ScanNet Datasets\n\n1. You can download the datasets following instructions in http://www.scan-net.org/. You will need to request the dataset from the authors.\n2. There are two versions you can download: \n- A subset of pre-processed data termed as `tasks/scannet_frames_2k`: this version is smaller, and more generally available for training neural networks. However, it only includes one frame out of each 100, which makes it unusable for SLAM. The labels are processed by mapping them from the original Scannet label annotations to NYU40.\n- The raw data: this version is the one used for SLAM. You can download the whole dataset (TBs of data) or specific scenes. A common approach for evaluation of semantic mapping is to use the `scannetv2_val.txt` scenes. For downloading and processing the data, you can use the following [repository](https://github.com/dvdmc/scannet-processing) as the original Scannet repository is tested under Python 2.7 and does't support batch downloading of scenes.\n2. Once you have the `color`, `depth`, `pose`, and (optional for semantic mapping) `label` folders, you should place them following `{path_to_scannet}/scans/{scene_name}/[color, depth, pose, label]`. Then, configure the `base_path` and `name` in the file `config.yaml`.\n3.  Select the corresponding calibration settings file (section `SCANNET_DATASET: settings:` in the file `config.yaml`). NOTE: the RGB images are rescaled to match the depth image. The current intrinsic parametes in the existing calibration file reflect that.\n\n#### ROS1 bags\n\n1. Source the main ROS1 `setup.bash` after you have sourced the `pyslam` python environment.\n2. Set the paths and `ROS1BAG_DATASET: ros_parameters` in the file `config.yaml`.\n3. Select/prepare the correspoding calibration settings file (section `ROS1BAG_DATASET: settings:` in the file `config.yaml`). See the available yaml files in the folder `Settings` as an example.\n\n\n#### ROS2 bags\n\n1. Source the main ROS2 `setup.bash` after you have sourced the `pyslam` python environment.\n2. Set the paths and `ROS2BAG_DATASET: ros_parameters` in the file `config.yaml`.\n3. Select/prepare the correspoding calibration settings file (section `ROS2BAG_DATASET: settings:` in the file `config.yaml`). See the available yaml files in the folder `Settings` as an example.\n\n\n#### Video and Folder Datasets\n\nYou can use the `VIDEO_DATASET` and `FOLDER_DATASET` types to read generic video files and image folders (specifying a glob pattern), respectively. A companion ground truth file can be set in the simple format type: Refer to the class `SimpleGroundTruth` in `io/ground_truth.py` and check the script `io/convert_groundtruth_to_simple.py`.  \n\n--- \n### Camera Settings\n\nThe folder `settings` contains the camera settings files which can be used for testing the code. These are the same used in the framework [ORB-SLAM2](https://github.com/raulmur/ORB_SLAM2). You can easily modify one of those files for creating your own new calibration file (for your new datasets).\n\nIn order to calibrate your camera, you can use the scripts in the folder `calibration`. In particular: \n1. Use the script `grab_chessboard_images.py` to collect a sequence of images where the chessboard can be detected (set the chessboard size therein, you can use the calibration pattern `calib_pattern.pdf` in the same folder) \n2. Use the script `calibrate.py` to process the collected images and compute the calibration parameters (set the chessboard size therein)\n\nFor more information on the calibration process, see this [tutorial](https://learnopencv.com/camera-calibration-using-opencv/) or this other [link](https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html). \n\nIf you want to **use your camera**, you have to:\n* Calibrate it and configure [WEBCAM.yaml](./settings/WEBCAM.yaml) accordingly\n* Record a video (for instance, by using `save_video.py` in the folder `calibration`)\n* Configure the `VIDEO_DATASET` section of `config.yaml` in order to point to your recorded video.\n\n--- \n## References\n\n\n\n- [\"pySLAM: An Open-Source, Modular, and Extensible Framework for SLAM\"](./docs/tex/document.pdf), *Luigi Freda*\n- [_\"pySLAM and slamplay: Modular, Extensible SLAM Tools for Rapid Prototyping and Integration\"_](https://docs.google.com/presentation/d/e/2PACX-1vSHoOR5-oiL7yDkowOe3mCbPvq4-qZzmWWZFswvCEiSMLkyUQoXgoODiG4GZL8pMpKTqqJUZ3auk0T-/pub?start=false&loop=false&delayms=3000), *Luigi Freda*\nRSS 2025 Workshop: _Unifying Visual SLAM_\n- [\"Semantic pySLAM: Unifying semantic mapping approaches under the same framework\"](./docs/pyslam-semantic.pdf), *David Morilla-Cabello*, *Eduardo Montijano*  \nRSS 2025 Workshop: _Unifying Visual SLAM_\n\n\nSuggested books:\n* *[Multiple View Geometry in Computer Vision](https://www.robots.ox.ac.uk/~vgg/hzbook/)* by Richard Hartley and Andrew Zisserman\n* *[An Invitation to 3-D Vision](https://link.springer.com/book/10.1007/978-0-387-21779-6)* by Yi-Ma, Stefano Soatto, Jana Kosecka, S. Shankar Sastry \n* *[State Estimation for Robotics](http://asrl.utias.utoronto.ca/~tdb/)* by Timothy D. Barfoot\n* *[Computer Vision: Algorithms and Applications](http://szeliski.org/Book/)*, by Richard Szeliski \n* *[Introduction to Visual SLAM](https://link.springer.com/book/10.1007/978-981-16-4939-4)* by Xiang Gao, Tao Zhang\n* *[Deep Learning](http://www.deeplearningbook.org/lecture_slides.html)*, by Ian Goodfellow, Yoshua Bengio and Aaron Courville\n* *[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)*, By Michael Nielsen \n\nSuggested material:\n* *[Vision Algorithms for Mobile Robotics](http://rpg.ifi.uzh.ch/teaching.html)* by Davide Scaramuzza \n* *[CS 682 Computer Vision](http://cs.gmu.edu/~kosecka/cs682.html)* by Jana Kosecka   \n* *[ORB-SLAM: a Versatile and Accurate Monocular SLAM System](http://webdiis.unizar.es/~raulmur/MurMontielTardosTRO15.pdf)* by R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos\n* *[Double Window Optimisation for Constant Time Visual SLAM](http://hauke.strasdat.net/files/strasdat2011iccv.pdf)* by H. Strasdat, A. J. Davison, J.M.M. Montiel, K. Konolige\n* *[The Role of Wide Baseline Stereo in the Deep Learning World](https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/03/27/intro.html)* by Dmytro Mishkin\n* *[To Learn or Not to Learn: Visual Localization from Essential Matrices](https://arxiv.org/abs/1908.01293)* by Qunjie Zhou, Torsten Sattler, Marc Pollefeys, Laura Leal-Taixe\n* *[Awesome local-global descriptors](https://github.com/shamangary/awesome-local-global-descriptor)* repository \n* *[Introduction to Feature Matching Using Neural Networks](https://learnopencv.com/feature-matching/)*\n* *[Visual Place Recognition: A Tutorial](https://arxiv.org/pdf/2303.03281)*\n* *[Bags of Binary Words for Fast Place Recognition in Image Sequences](http://doriangalvez.com/papers/GalvezTRO12.pdf)*\n\nMoreover, you may want to have a look at the OpenCV [guide](https://docs.opencv.org/4.x/index.html) or [tutorials](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html).  \n\n---\n## Credits \n\n* [Pangolin](https://github.com/stevenlovegrove/Pangolin) \n* [g2opy](https://github.com/uoip/g2opy)\n* [ORBSLAM2](https://github.com/raulmur/ORB_SLAM2)\n* [SuperPointPretrainedNetwork](https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork)\n* [Tfeat](https://github.com/vbalnt/tfeat)\n* [Image Matching Benchmark Baselines](https://github.com/vcg-uvic/image-matching-benchmark-baselines)\n* [Hardnet](https://github.com/DagnyT/hardnet.git)\n* [GeoDesc](https://github.com/lzx551402/geodesc.git)\n* [SOSNet](https://github.com/yuruntian/SOSNet.git)\n* [L2Net](https://github.com/yuruntian/L2-Net)\n* [Log-polar descriptor](https://github.com/cvlab-epfl/log-polar-descriptors)\n* [D2-Net](https://github.com/mihaidusmanu/d2-net)\n* [DELF](https://github.com/tensorflow/models/blob/master/research/delf/INSTALL_INSTRUCTIONS.md)\n* [Contextdesc](https://github.com/lzx551402/contextdesc)\n* [LFNet](https://github.com/vcg-uvic/lf-net-release)\n* [R2D2](https://github.com/naver/r2d2)\n* [BEBLID](https://raw.githubusercontent.com/iago-suarez/BEBLID/master/BEBLID_Boosted_Efficient_Binary_Local_Image_Descriptor.pdf)\n* [DISK](https://arxiv.org/abs/2006.13566)\n* [Xfeat](https://arxiv.org/abs/2404.19174)\n* [LightGlue](https://arxiv.org/abs/2306.13643)\n* [Key.Net](https://github.com/axelBarroso/Key.Net)\n* [Twitchslam](https://github.com/geohot/twitchslam)\n* [MonoVO](https://github.com/uoip/monoVO-python)\n* [VPR_Tutorial](https://github.com/stschubert/VPR_Tutorial.git)\n* [DepthAnythingV2](https://github.com/DepthAnything/Depth-Anything-V2)\n* [DepthPro](https://github.com/apple/ml-depth-pro)\n* [RAFT-Stereo](https://github.com/princeton-vl/RAFT-Stereo)\n* [CREStereo](https://github.com/megvii-research/CREStereo) and [CREStereo-Pytorch](https://github.com/ibaiGorordo/CREStereo-Pytorch)\n* [MonoGS](https://github.com/muskie82/MonoGS)\n* [mast3r](https://github.com/naver/mast3r)\n* [mvdust3r](https://github.com/facebookresearch/mvdust3r)\n* [MegaLoc](https://github.com/gmberton/MegaLoc)\n* Many thanks to [Anathonic](https://github.com/anathonic) for adding the trajectory-saving feature and for the comparison notebook: [pySLAM vs ORB-SLAM3](https://github.com/anathonic/Trajectory-Comparison-ORB-SLAM3-pySLAM/blob/main/trajectories_comparison.ipynb).\n* Many thanks to [David Morilla Cabello](https://github.com/dvdmc) for his great work on integrating [semantic predictions](./docs/semantics.md) into pySLAM.\n\n---\n## License \n\npySLAM is released under [GPLv3 license](./LICENSE). pySLAM contains some modified libraries, each one coming with its license. Where nothing is specified, a GPLv3 license applies to the software.\n\nIf you use pySLAM in your projects, please cite this document:\n[\"pySLAM: An Open-Source, Modular, and Extensible Framework for SLAM\"](https://arxiv.org/abs/2502.11955), *Luigi Freda*      \nYou may find an updated version of this document [here](./docs/tex/document.pdf).\n\n--- \n## Contributing to pySLAM\n\nIf you like pySLAM and would like to contribute to the code base, you can report bugs, leave comments and proposing new features through issues and pull requests on github. Feel free to get in touch at *luigifreda(at)gmail[dot]com*. Thank you!\n\n---\n## Roadmap\n\nMany improvements and additional features are currently under development: \n\n- [x] Loop closing\n- [x] Relocalization \n- [x] Stereo and RGBD support\n- [x] Map saving/loading \n- [x] Modern DL matching algorithms \n- [ ] Object detection\n  - [ ] Open vocabulary segment (object) detection\n- [X] Semantic segmentation [by @dvdmc]\n  - [X] Dense closed-set labels\n  - [X] Dense closed-set probability vectors\n  - [X] Dense open vocabulary feature vectors\n- [x] 3D dense reconstruction \n- [x] Unified install procedure (single branch) for all OSs \n- [x] Trajectory saving \n- [x] Depth prediction integration, more models: VGGT, MoGE [WIP]\n- [x] ROS support [WIP]\n- [x] Gaussian splatting integration\n- [x] Documentation [WIP]\n- [x] GTSAM integration [WIP]\n- [ ] IMU integration\n- [ ] LIDAR integration\n- [x] XSt3r-based methods integration [WIP]\n- [x] Evaluation scripts \n- [ ] More camera models\n",
    "readme_length": 61432
  },
  {
    "name": "PyTorch-Transformer-for-RUL-Prediction",
    "full_name": "jiaxiang-cheng/PyTorch-Transformer-for-RUL-Prediction",
    "description": "Transformer implementation with PyTorch for remaining useful life prediction on turbofan engine with NASA CMAPSS data set. Inspired by Mo, Y., Wu, Q., Li, X., & Huang, B. (2021). Remaining useful life estimation via transformer encoder enhanced by a gated convolutional unit. Journal of Intelligent Manufacturing, 1-10.",
    "stars": 282,
    "forks": 51,
    "language": "Python",
    "url": "https://github.com/jiaxiang-cheng/PyTorch-Transformer-for-RUL-Prediction",
    "topics": [
      "cmapss",
      "prediction",
      "predictive-maintenance",
      "pytorch",
      "pytorch-implementation",
      "remaining-useful-life",
      "transformer",
      "turbofan-engine"
    ],
    "created_at": "2021-05-10T03:03:00Z",
    "updated_at": "2025-11-26T07:13:03Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "# PyTorch Transformer for RUL Prediction\nAn implementation with Transformer encoder and convolution layers with PyTorch for remaining useful life prediction.   \n_Author: Jiaxiang Cheng, Nanyang Technological University, Singapore_\n\n<img alt=\"Python\" src=\"https://img.shields.io/badge/python-%2314354C.svg?style=for-the-badge&logo=python&logoColor=white\"/> <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white\" />\n\n## Quick Run\nSimply run `python train.py --dataset FD001`. And you will get the training loss and testing result for each epoch where the RMSE is from the test set:\n```\nEpoch: 0, loss: 9474.43470, RMSE: 61.11946\nEpoch: 1, loss: 5858.27227, RMSE: 46.03318\nEpoch: 2, loss: 3208.53410, RMSE: 29.78244\nEpoch: 3, loss: 1310.71390, RMSE: 22.94705\n...\n```\nThe testing is conducted for each epoch as the data set is not large so it's no big deal but you may remove them and only do the evaluation after finishing the training epochs.\n\n## Environment Details\n```\npython==3.8.8\nnumpy==1.20.1\npandas==1.2.4\nmatplotlib==3.3.4\npytorch==1.8.1\n```\n\n## Credit\nThis work is inpired by Mo, Y., Wu, Q., Li, X., & Huang, B. (2021). Remaining useful life estimation via transformer encoder enhanced by a gated convolutional unit. Journal of Intelligent Manufacturing, 1-10.\n\n## Citation\n[![DOI](https://zenodo.org/badge/365902211.svg)](https://zenodo.org/badge/latestdoi/365902211)\n\n## License\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n",
    "readme_length": 1575
  },
  {
    "name": "PL-BERT",
    "full_name": "yl4579/PL-BERT",
    "description": "Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions",
    "stars": 263,
    "forks": 56,
    "language": "Python",
    "url": "https://github.com/yl4579/PL-BERT",
    "topics": [
      "bert",
      "bert-model",
      "text-to-speech",
      "tts"
    ],
    "created_at": "2023-01-24T02:27:11Z",
    "updated_at": "2025-11-11T08:15:53Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions\n\n### Yinghao Aaron Li, Cong Han, Xilin Jiang, Nima Mesgarani\n\n> Large-scale pre-trained language models have been shown to be helpful in improving the naturalness of text-to-speech (TTS) models by enabling them to produce more naturalistic prosodic patterns. However, these models are usually word-level or sup-phoneme-level and jointly trained with phonemes, making them inefficient for the downstream TTS task where only phonemes are needed. In this work, we propose a phoneme-level BERT (PL-BERT) with a pretext task of predicting the corresponding graphemes along with the regular masked phoneme predictions. Subjective evaluations show that our phoneme-level BERT encoder has significantly improved the mean opinion scores (MOS) of rated naturalness of synthesized speech compared with the state-of-the-art (SOTA) StyleTTS baseline on out-of-distribution (OOD) texts.\n\nPaper: [https://arxiv.org/abs/2301.08810](https://arxiv.org/abs/2301.08810)\n\nAudio samples: [https://pl-bert.github.io/](https://pl-bert.github.io/)\n\n## Pre-requisites\n1. Python >= 3.7\n2. Clone this repository:\n```bash\ngit clone https://github.com/yl4579/PL-BERT.git\ncd PL-BERT\n```\n3. Create a new environment (recommended):\n```bash\nconda create --name BERT python=3.8\nconda activate BERT\npython -m ipykernel install --user --name BERT --display-name \"BERT\"\n```\n4. Install python requirements: \n```bash\npip install pandas singleton-decorator datasets \"transformers<4.33.3\" accelerate nltk phonemizer sacremoses pebble\n```\n\n## Preprocessing\nPlease refer to the notebook [preprocess.ipynb](https://github.com/yl4579/PL-BERT/blob/main/preprocess.ipynb) for more details. The preprocessing is for English Wikipedia dataset only. I will make a new branch for Japanese if I have extra time to demostrate training on other languages. You may also refer to [#6](https://github.com/yl4579/PL-BERT/issues/6#issuecomment-1797869275) for preprocessing in other languages like Japanese. \n\n## Trianing\nPlease run each cell in the notebook [train.ipynb](https://github.com/yl4579/PL-BERT/blob/main/train.ipynb). You will need to change the line\n`config_path = \"Configs/config.yml\"` in cell 2 if you wish to use a different config file. The training code is in Jupyter notebook primarily because the initial epxeriment was conducted in Jupyter notebook, but you can easily make it a Python script if you want to. \n\n## Finetuning\nHere is an example of how to use it for StyleTTS finetuning. You can use it for other TTS models by replacing the text encoder with the pre-trained PL-BERT.\n1. Modify line 683 in [models.py](https://github.com/yl4579/StyleTTS/blob/main/models.py#L683) with the following code to load BERT model in to StyleTTS:\n```python\nfrom transformers import AlbertConfig, AlbertModel\n\nlog_dir = \"YOUR PL-BERT CHECKPOINT PATH\"\nconfig_path = os.path.join(log_dir, \"config.yml\")\nplbert_config = yaml.safe_load(open(config_path))\n\nalbert_base_configuration = AlbertConfig(**plbert_config['model_params'])\nbert = AlbertModel(albert_base_configuration)\n\nfiles = os.listdir(log_dir)\nckpts = []\nfor f in os.listdir(log_dir):\n    if f.startswith(\"step_\"): ckpts.append(f)\n\niters = [int(f.split('_')[-1].split('.')[0]) for f in ckpts if os.path.isfile(os.path.join(log_dir, f))]\niters = sorted(iters)[-1]\n        \ncheckpoint = torch.load(log_dir + \"/step_\" + str(iters) + \".t7\", map_location='cpu')\nstate_dict = checkpoint['net']\nfrom collections import OrderedDict\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    name = k[7:] # remove `module.`\n    if name.startswith('encoder.'):\n        name = name[8:] # remove `encoder.`\n        new_state_dict[name] = v\nbert.load_state_dict(new_state_dict)\n\nnets = Munch(bert=bert,\n  # linear projection to match the hidden size (BERT 768, StyleTTS 512)\n  bert_encoder=nn.Linear(plbert_config['model_params']['hidden_size'], args.hidden_dim),\n  predictor=predictor,\n    decoder=decoder,\n             pitch_extractor=pitch_extractor,\n                 text_encoder=text_encoder,\n                 style_encoder=style_encoder,\n             text_aligner = text_aligner,\n            discriminator=discriminator)\n```\n2. Modify line 126 in [train_second.py](https://github.com/yl4579/StyleTTS/blob/main/train_second.py#L126) with the following code to adjust the learning rate of BERT model:\n```python\n# for stability\nfor g in optimizer.optimizers['bert'].param_groups:\n    g['betas'] = (0.9, 0.99)\n    g['lr'] = 1e-5\n    g['initial_lr'] = 1e-5\n    g['min_lr'] = 0\n    g['weight_decay'] = 0.01\n```\n3. Modify line 211 in [train_second.py](https://github.com/yl4579/StyleTTS/blob/main/train_second.py#L211) with the following code to replace text encoder with BERT encoder:\n```python\n            bert_dur = model.bert(texts, attention_mask=(~text_mask).int()).last_hidden_state\n            d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n            d, _ = model.predictor(d_en, s, \n                                                    input_lengths, \n                                                    s2s_attn_mono, \n                                                    m)\n```\n[line 257](https://github.com/yl4579/StyleTTS/blob/main/train_second.py#L257):\n```python\n            _, p = model.predictor(d_en, s, \n                                                    input_lengths, \n                                                    s2s_attn_mono, \n                                                    m)\n```\nand [line 415](https://github.com/yl4579/StyleTTS/blob/main/train_second.py#L415):\n```python\n                bert_dur = model.bert(texts, attention_mask=(~text_mask).int()).last_hidden_state\n                d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n                d, p = model.predictor(d_en, s, \n                                                    input_lengths, \n                                                    s2s_attn_mono, \n                                                    m)\n```\n\n4. Modify line 347 in [train_second.py](https://github.com/yl4579/StyleTTS/blob/main/train_second.py#L347) with the following code to make sure parameters of BERT model are updated:\n```python\n            optimizer.step('bert_encoder')\n            optimizer.step('bert')\n```\n\nThe pre-trained PL-BERT on Wikipedia for 1M steps can be downloaded at: [PL-BERT link](https://github.com/yl4579/StyleTTS2/tree/main/Utils/PLBERT).\n\nThe demo on LJSpeech dataset along with the pre-modified StyleTTS repo and pre-trained models can be downloaded here: [StyleTTS Link](https://huggingface.co/yl4579/StyleTTS/blob/main/LJSpeech_PLBERT/Models.zip). This zip file contains the code modification above, the pre-trained PL-BERT model listed above, pre-trained StyleTTS w/ PL-BERT, pre-trained StyleTTS w/o PL-BERT and pre-trained HifiGAN on LJSpeech from the StyleTTS repo.\n\n## References\n- [NVIDIA/NeMo-text-processing](https://github.com/NVIDIA/NeMo-text-processing)\n- [tomaarsen/TTSTextNormalization](https://github.com/tomaarsen/TTSTextNormalization)\n",
    "readme_length": 7057
  },
  {
    "name": "ABC-Enhancer-Gene-Prediction",
    "full_name": "broadinstitute/ABC-Enhancer-Gene-Prediction",
    "description": "Cell type specific enhancer-gene predictions using ABC model (Fulco, Nasser et al, Nature Genetics 2019)",
    "stars": 241,
    "forks": 73,
    "language": "Python",
    "url": "https://github.com/broadinstitute/ABC-Enhancer-Gene-Prediction",
    "topics": [],
    "created_at": "2019-04-17T14:11:42Z",
    "updated_at": "2025-11-28T01:32:49Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "CircleCI [![CircleCI](https://dl.circleci.com/status-badge/img/gh/broadinstitute/ABC-Enhancer-Gene-Prediction.svg?style=svg)](https://app.circleci.com/pipelines/github/broadinstitute/ABC-Enhancer-Gene-Prediction)\n\n> :memo: **Note:** This repository is the version of ABC presented in [Gschwind _et al._ (_BioRxiv_ 2023)](https://doi.org/10.1101/2023.11.09.563812) and is a revamp of the original ABC codebase. To access any previous version of ABC, please visit https://github.com/EngreitzLab/ABC-Enhancer-Gene-Prediction-20250314-archive.\n> - For the version of ABC presented in Fulco _et al._ (_Nat. Genet._ 2019)[1], use the [NG2019 branch](https://github.com/EngreitzLab/ABC-Enhancer-Gene-Prediction-20250314-archive/tree/NG2019) of the archived repo\n> - For the version of ABC presented in Nasser _et al._ (_Nature_ 2021), use the [master branch](https://github.com/EngreitzLab/ABC-Enhancer-Gene-Prediction-20250314-archive/tree/master) of the archived repo\n\nWe have documented the codebase and usage in [Read The Docs](https://abc-enhancer-gene-prediction.readthedocs.io/en/latest/) \n\n# Activity by Contact Model of Enhancer-Gene Specificity\n\nThe Activity-by-Contact (ABC) model predicts which enhancers regulate which genes on a cell type specific basis. This repository contains the code needed to run the ABC model as well as small sample data files, example commands, and some general tips and suggestions.\n\nIf you use the ABC model in published research, please cite:\n\n[1] Fulco CP, Nasser J, Jones TR, Munson G, Bergman DT, Subramanian V, Grossman SR, Anyoha R, Doughty BR, Patwardhan TA, Nguyen TH, Kane M, Perez EM, Durand NC, Lareau CA, Stamenova EK, Aiden EL, Lander ES & Engreitz JM. Activity-by-contact model of enhancerâ€“promoter regulation from thousands of CRISPR perturbations. Nat. Genet. 51, 1664â€“1669 (2019). https://www.nature.com/articles/s41588-019-0538-0\n\n## Contact\nPlease submit a GitHub issue with any questions or if you experience any issues/bugs.\n",
    "readme_length": 1981
  },
  {
    "name": "HoP",
    "full_name": "Sense-X/HoP",
    "description": "[ICCV 2023] Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction",
    "stars": 194,
    "forks": 14,
    "language": "Python",
    "url": "https://github.com/Sense-X/HoP",
    "topics": [],
    "created_at": "2023-07-14T13:20:02Z",
    "updated_at": "2025-10-30T15:13:27Z",
    "homepage": null,
    "license": "Apache License 2.0",
    "readme": "# Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/temporal-enhanced-training-of-multi-view-3d/3d-object-detection-on-nuscenes-camera-only)](https://paperswithcode.com/sota/3d-object-detection-on-nuscenes-camera-only?p=temporal-enhanced-training-of-multi-view-3d)\n\nThis repo is the official implementation of [\"Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction\"](https://arxiv.org/abs/2304.00967) by Zhuofan Zong, Dongzhi Jiang, Guanglu Song, Zeyue Xue, Jingyong Su, Hongsheng Li, and Yu Liu.\n\n\n## News\n* ***[07/25/2023]*** Code for HoP on BEVDet is released!\n* ***[07/14/2023]*** HoP is accepted to ICCV 2023!\n* ***[04/05/2023]*** HoP achieves new SOTA performance on [nuScenes 3D detection leaderboard](https://www.nuscenes.org/object-detection?externalData=all&mapData=all&modalities=Camera) with **68.5 NDS** and **62.4 mAP**.\n\n## Model Zoo\n\n### Result on BEVDet4D-Depth\n\n|          model           | backbone |   pretrain   | img size | Epoch |  NDS   |  mAP   |                            config                            |                             ckpt                             |                             log                              |\n| :----------------------: | :------: | :----------: | :------: | :---: | :----: | :----: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |\n| BEVDet4D-Depth(Baseline) |  Res50   | [ImageNet]() | 256x704  |  24   | 0.4930 | 0.3848 | [cfg](https://github.com/Sense-X/HoP/blob/main/configs/hop_bevdet/bevdet4d-r50-depth.py) | [ckpt](https://github.com/Sense-X/HoP/releases/download/Release/BEVDet_ep24_ema.pth) | [log](https://github.com/Sense-X/HoP/releases/download/Release/BEVDet.log) |\n|    HoP_BEVDet4D-Depth    |  Res50   | [ImageNet]() | 256x704  |  24   | 0.5099 | 0.3990 | [cfg](https://github.com/Sense-X/HoP/blob/main/configs/hop_bevdet/hop_bevdet4d-r50-depth.py) | [ckpt](https://github.com/Sense-X/HoP/releases/download/Release/HoP_BEVDet_ep24_ema.pth) | [log](https://github.com/Sense-X/HoP/releases/download/Release/HoP_BEVDet.log) |\n\n## Get Started\n\n### Install\n\nWe train our models under the following environment: \n\n```\npython=3.6.9\npytorch=1.8.1\ntorchvision=0.9.1\ncuda=11.2\n```\n\nOther versions may possibly be imcompatible.\n\nWe use [MMDetection3D V1.0.0rc4](https://github.com/open-mmlab/mmdetection3d/tree/v1.0.0rc4), [MMDetection V2.24.0](https://github.com/open-mmlab/mmdetection/releases/tag/v2.25.3) and [MMCV V1.5.0](https://github.com/open-mmlab/mmcv/releases/tag/v1.5.0). The source code of MMDetection3D has been included in this repo.\n\nYou can take the following steps to install packages above: \n\n1. Build MMCV following [official instructions](https://github.com/open-mmlab/mmcv/tree/v1.5.2#installation). \n\n2. Install MMDetection by \n\n   ```bash\n   pip install mmdet==2.24.0\n   ```\n\n3. Copy HoP repo and install MMDetection3D.\n\n   ```bash\n   git clone git@github.com:Sense-X/HoP.git\n   cd HoP\n   pip install -e .\n   ```\n\n### Data Preparation\n\nFollow the steps to prepare nuScenes Dataset introduced in [nuscenes_det.md](https://github.com/HuangJunJie2017/BEVDet/blob/dev2.1/docs/en/datasets/nuscenes_det.md) and create the pkl by running:\n\n```bash\npython tools/create_data_bevdet.py\n```\n\n### Train HoP\n\n```bash\n# single gpu\npython tools/train.py configs/hop_bevdet/hop_bevdet4d-r50-depth.py\n# multiple gpu\n./tools/dist_train.sh configs/hop_bevdet/hop_bevdet4d-r50-depth.py $num_gpu\n```\n\n### Eval HoP\n\n```bash\n# single gpu\npython tools/test.py configs/hop_bevdet/hop_bevdet4d-r50-depth.py $checkpoint --eval bbox\n# multiple gpu\n./tools/dist_test.sh configs/hop_bevdet/hop_bevdet4d-r50-depth.py $checkpoint $num_gpu --eval bbox\n```\n\n## Method\n\n<img src=\"resources/HoP_framework.png\" width=\"1000\" >\n\n## TODO\n\n- [ ] Release code for HoP on BEVFormer.\n\n\n## Cite HoP\n\nIf you find this repository useful, please use the following BibTeX entry for citation.\n\n```latex\n@misc{hop2023,\n      title={Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction},\n      author={Zhuofan Zong and Dongzhi Jiang and Guanglu Song and Zeyue Xue and Jingyong Su and Hongsheng Li and Yu Liu},\n      year={2023},\n      eprint={2304.00967},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n## License\n\nThis project is released under the MIT license. Please see the [LICENSE](LICENSE) file for more information.",
    "readme_length": 4630
  },
  {
    "name": "GraphCare",
    "full_name": "pat-jj/GraphCare",
    "description": "[ICLR'24] Enhancing Healthcare Predictions with Personalized Knowledge Graphs",
    "stars": 189,
    "forks": 37,
    "language": "Python",
    "url": "https://github.com/pat-jj/GraphCare",
    "topics": [
      "data-mining",
      "graph-neural-networks",
      "healthcare-ai",
      "knowledge-graph"
    ],
    "created_at": "2023-05-08T15:35:07Z",
    "updated_at": "2025-11-26T13:23:05Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# GraphCare\nCode for the paper: [GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs](https://openreview.net/pdf?id=tVTN7Zs0ml) in ICLR'24.\n\n\n## Requirements:\n``` bash\npip install torch==1.12.0\npip install torch-geometric==2.3.0\npip install pyhealth==1.1.2\npip install scikit-learn==1.2.1\npip install openai==0.27.4\n```\n\n---\n\n**We follow the flow of methodology section (Section 3) to explain our implementation.**\n\n## 1. Concept-specific Knowledge Graph (KG) Generation\n### 1.1 LLM-based KG extraction via prompting\nThe jupyter notebook to prompt KG for EHR medical code:\n\n``` bash\n/graphcare_/graph_generation/graph_gen.ipynb\n```\nWe place sample KGs generated by GPT-4 as \n``` bash\n/graphs/{condition/CCSCM,procedure/CCSPROC,drug/ATC3}/{code_id}.txt\n```\n\n### 1.2 Subgraph sampling from existing KGs\nThe script for subgraph sampling from UMLS:\n``` bash\n/KG_mapping/umls_sampling.py\n```\nWe place 2-hop sample KGs randomly subsampled from UMLS as \n``` bash\n/graphs/umls_2hop.csv\n```\n\n### 1.3 Word Embedding Retrieval for Nodes & Edges\nThe jupyter notebooks for word embedding retrieval:\n``` bash\n/graphcare_/graph_generation/{cond,proc,drug}_emb_ret.ipynb\n```\nDue to the large size of word embedding, we do not include them in the repo. You can use our script to retrieve it and store it in either \n``` bash\n/graphs/cond_proc/{entity_embedding.pkl, relation_embedding.pkl}\nor\n/graphs/cond_proc_drug/{entity_embedding.pkl, relation_embedding.pkl}\n```\ndepending on the features used for the prediction tasks.\n\n### 1.4 Node & Edge Clustering\nThe function for node & edge clustering:\n``` bash\nclustering() in data_prepare.py\n```\nWe place some clustering results (only \"_inv\" as cluster embedding has large size) in \n``` bash\n/clustering/\n```\n\n## 2. Personalized Knowledge Graph Composition\n``` bash\nprocess_sample_dataset() and process_graph() in data_prepare.py\n&\nget_subgraph() in graphcare.py\n```\n\n## 3. Bi-attention Augmented (BAT) Graph Neural Network\nThe implementation of our proposed BAT model is in\n``` bash\n/graphcare_/model.py\n```\n\n## 4. Training and Prediction\nThe creation of task-specific datasets (using PyHealth) is in \n``` bash\ndata_prepare.py\n```\nThe training and prediction details are in\n``` bash\ngraphcare.py\n```\n\n## Run Baseline Models\nThe scripts running baseline models are placed in \n``` bash\nehr_models.py\n```\n\n## Cite **GraphCare**\n``` bash\n@inproceedings{jiang2024graphcare,\n  title = {GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs},\n  author = {Jiang, Pengcheng and Xiao, Cao and Cross, Adam and Sun, Jimeng},\n  booktitle = {The Twelfth International Conference on Learning Representations},\n  year = {2024},\n}\n```\n\nThanks for your interest in our work! ðŸ˜Š\n\n---\n",
    "readme_length": 2745
  },
  {
    "name": "Adv-ALSTM",
    "full_name": "fulifeng/Adv-ALSTM",
    "description": "Code for paper \"Enhancing Stock Movement Prediction with Adversarial Training\" IJCAI 2019",
    "stars": 181,
    "forks": 70,
    "language": "Python",
    "url": "https://github.com/fulifeng/Adv-ALSTM",
    "topics": [],
    "created_at": "2019-06-01T08:21:11Z",
    "updated_at": "2025-10-28T07:29:19Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# Adv-ALSTM\nCode for paper \"Enhancing Stock Movement Prediction with Adversarial Training\" IJCAI 2019\n\n# Requirements\nPython 3.6.1\n\nTensorflow 1.8.0\n\nNumpy 1.14.5\n\nTo run the proposed Adv-ALSTM, ALSTM, and LSTM on the ACL18 and KDD17 dataset, run the commands in the hyperparameter file.\n\n# Reference\nFor usage of this code, please cite our paper\n```\n@article{feng2019enhancing,\n  title={Enhancing Stock Movement Prediction with Adversarial Training},\n  author={Feng, Fuli and Chen, Huimin and He, Xiangnan and Ding, Ji and Sun, Maosong and Chua, Tat-Seng},\n  journal={IJCAI},\n  year={2019}\n}\n```\n",
    "readme_length": 597
  },
  {
    "name": "Alzhimers-Disease-Prediction-Using-Deep-learning",
    "full_name": "himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning",
    "description": "# AD-Prediction  Convolutional Neural Networks for Alzheimer's Disease Prediction Using Brain MRI Image  ## Abstract Alzheimers disease (AD) is characterized by severe memory loss and cognitive impairment. It associates with significant brain structure changes, which can be measured by magnetic resonance imaging (MRI) scan. The observable preclinical structure changes provides an opportunity for AD early detection using image classification tools, like convolutional neural network (CNN). However, currently most AD related studies were limited by sample size. Finding an efficient way to train image classifier on limited data is critical. In our project, we explored different transfer-learning methods based on CNN for AD prediction brain structure MRI image. We find that both pretrained 2D AlexNet with 2D-representation method and simple neural network with pretrained 3D autoencoder improved the prediction performance comparing to a deep CNN trained from scratch. The pretrained 2D AlexNet performed even better (**86%**) than the 3D CNN with autoencoder (**77%**).  ## Method #### 1. Data In this project, we used public brain MRI data from **Alzheimers Disease Neuroimaging Initiative (ADNI)** Study. ADNI is an ongoing, multicenter cohort study, started from 2004. It focuses on understanding the diagnostic and predictive value of Alzheimers disease specific biomarkers. The ADNI study has three phases: ADNI1, ADNI-GO, and ADNI2. Both ADNI1 and ADNI2 recruited new AD patients and normal control as research participants. Our data included a total of 686 structure MRI scans from both ADNI1 and ADNI2 phases, with 310 AD cases and 376 normal controls. We randomly derived the total sample into training dataset (n = 519), validation dataset (n = 100), and testing dataset (n = 67).  #### 2. Image preprocessing Image preprocessing were conducted using Statistical Parametric Mapping (SPM) software, version 12. The original MRI scans were first skull-stripped and segmented using segmentation algorithm based on 6-tissue probability mapping and then normalized to the International Consortium for Brain Mapping template of European brains using affine registration. Other configuration includes: bias, noise, and global intensity normalization. The standard preprocessing process output 3D image files with an uniform size of 121x145x121. Skull-stripping and normalization ensured the comparability between images by transforming the original brain image into a standard image space, so that same brain substructures can be aligned at same image coordinates for different participants. Diluted or enhanced intensity was used to compensate the structure changes. the In our project, we used both whole brain (including both grey matter and white matter) and grey matter only.  #### 3. AlexNet and Transfer Learning Convolutional Neural Networks (CNN) are very similar to ordinary Neural Networks. A CNN consists of an input and an output layer, as well as multiple hidden layers. The hidden layers are either convolutional, pooling or fully connected. ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.  #### 3.1. AlexNet The net contains eight layers with weights; the first five are convolutional and the remaining three are fully connected. The overall architecture is shown in Figure 1. The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. AlexNet maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution. The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU (as shown in Figure1). The kernels of the third convolutional layer are connected to all kernel maps in the second layer. The neurons in the fully connected layers are connected to all neurons in the previous layer. Response-normalization layers follow the first and second convolutional layers. Max-pooling layers follow both response-normalization layers as well as the fifth convolutional layer. The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer. ![](images/f1.png)  The first convolutional layer filters the 224x224x3 input image with 96 kernels of size 11x11x3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map). The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5x5x48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3x3x256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3x3x192 , and the fifth convolutional layer has 256 kernels of size 3x3x192. The fully-connected layers have 4096 neurons each.  #### 3.2. Transfer Learning Training an entire Convolutional Network from scratch (with random initialization) is impractical[14] because it is relatively rare to have a dataset of sufficient size. An alternative is to pretrain a Conv-Net on a very large dataset (e.g. ImageNet), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. Typically, there are three major transfer learning scenarios:   **ConvNet as fixed feature extractor:** We can take a ConvNet pretrained on ImageNet, and remove the last fully-connected layer, then treat the rest structure as a fixed feature extractor for the target dataset. In AlexNet, this would be a 4096-D vector. Usually, we call these features as CNN codes. Once we get these features, we can train a linear classifier (e.g. linear SVM or Softmax classifier) for our target dataset.   **Fine-tuning the ConvNet:** Another idea is not only replace the last fully-connected layer in the classifier, but to also fine-tune the parameters of the pretrained network. Due to overfitting concerns, we can only fine-tune some higher-level part of the network. This suggestion is motivated by the observation that earlier features in a ConvNet contains more generic features (e.g. edge detectors or color blob detectors) that can be useful for many kind of tasks. But the later layer of the network becomes progressively more specific to the details of the classes contained in the original dataset.   **Pretrained models:** The released pretrained model is usually the final ConvNet checkpoint. So it is common to see people use the network for fine-tuning.  #### 4. 3D Autoencoder and Convolutional Neural Network We take a two-stage approach where we first train a 3D sparse autoencoder to learn filters for convolution operations, and then build a convolutional neural network whose first layer uses the filters learned with the autoencoder. ![](images/f2.png)  #### 4.1. Sparse Autoencoder An autoencoder is a 3-layer neural network that is used to extract features from an input such as an image. Sparse representations can provide a simple interpretation of the input data in terms of a small number of \\parts by extracting the structure hidden in the data. The autoencoder has an input layer, a hidden layer and an output layer, and the input and output layers have same number of units, while the hidden layer contains more units for a sparse and overcomplete representation. The encoder function maps input x to representation h, and the decoder function maps the representation h to the output x. In our problem, we extract 3D patches from scans as the input to the network. The decoder function aims to reconstruct the input form the hidden representation h.  #### 4.2. 3D Convolutional Neural Network Training the 3D convolutional neural network(CNN) is the second stage. The CNN we use in this project has one convolutional layer, one pooling layer, two linear layers, and finally a log softmax layer. After training the sparse autoencoder, we take the weights and biases of the encoder from trained model, and use them a 3D filter of a 3D convolutional layer of the 1-layer convolutional neural network. Figure 2 shows the architecture of the network.  #### 5. Tools In this project, we used Nibabel for MRI image processing and PyTorch Neural Networks implementation.",
    "stars": 170,
    "forks": 25,
    "language": "Python",
    "url": "https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning",
    "topics": [],
    "created_at": "2020-11-17T04:10:06Z",
    "updated_at": "2025-11-27T04:09:16Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# Alzhimers-Disease-Prediction-Using-Deep-learning\n# AD-Prediction  Convolutional Neural Networks for Alzheimer's Disease Prediction Using Brain MRI Image \nAlzheimers disease (AD) is characterized by severe memory loss and cognitive impairment. It associates with significant brain structure changes, which can be measured by magnetic resonance imaging (MRI) scan. The observable preclinical structure changes provides an opportunity for AD early detection using image classification tools, like convolutional neural network (CNN). However, currently most AD related studies were limited by sample size. Finding an efficient way to train image classifier on limited data is critical. In our project, we explored different transfer-learning methods based on CNN for AD prediction brain structure MRI image. We find that both pretrained 2D AlexNet with 2D-representation method and simple neural network with pretrained 3D autoencoder improved the prediction performance comparing to a deep CNN trained from scratch. The pretrained 2D AlexNet performed even better (**86%**) than the 3D CNN with autoencoder (**77%**).  ## Method \n1. Data In this project, we used public brain MRI data from **Alzheimers Disease Neuroimaging Initiative (ADNI)** Study. ADNI is an ongoing, multicenter cohort study, started from 2004. It focuses on understanding the diagnostic and predictive value of Alzheimers disease specific biomarkers. The ADNI study has three phases: ADNI1, ADNI-GO, and ADNI2. Both ADNI1 and ADNI2 recruited new AD patients and normal control as research participants. Our data included a total of 686 structure MRI scans from both ADNI1 and ADNI2 phases, with 310 AD cases and 376 normal controls. We randomly derived the total sample into training dataset (n = 519), validation dataset (n = 100), and testing dataset (n = 67).  #### 2. Image preprocessing Image preprocessing were conducted using Statistical Parametric Mapping (SPM) software, version 12. The original MRI scans were first skull-stripped and segmented using segmentation algorithm based on 6-tissue probability mapping and then normalized to the International Consortium for Brain Mapping template of European brains using affine registration. Other configuration includes: bias, noise, and global intensity normalization. The standard preprocessing process output 3D image files with an uniform size of 121x145x121. Skull-stripping and normalization ensured the comparability between images by transforming the original brain image into a standard image space, so that same brain substructures can be aligned at same image coordinates for different participants. Diluted or enhanced intensity was used to compensate the structure changes. the In our project, we used both whole brain (including both grey matter and white matter) and grey matter only.  \n3. AlexNet and Transfer Learning Convolutional Neural Networks (CNN) are very similar to ordinary Neural Networks. A CNN consists of an input and an output layer, as well as multiple hidden layers. The hidden layers are either convolutional, pooling or fully connected. ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.  #### 3.1. AlexNet The net contains eight layers with weights; the first five are convolutional and the remaining three are fully connected. The overall architecture is shown in Figure 1. The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. AlexNet maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution. The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU (as shown in Figure1). The kernels of the third convolutional layer are connected to all kernel maps in the second layer. The neurons in the fully connected layers are connected to all neurons in the previous layer. Response-normalization layers follow the first and second convolutional layers. Max-pooling layers follow both response-normalization layers as well as the fifth convolutional layer. The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer. ![](images/f1.png)  The first convolutional layer filters the 224x224x3 input image with 96 kernels of size 11x11x3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map). The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5x5x48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3x3x256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3x3x192 , and the fifth convolutional layer has 256 kernels of size 3x3x192. The fully-connected layers have 4096 neurons each.  #### 3.2. Transfer Learning Training an entire Convolutional Network from scratch (with random initialization) is impractical[14] because it is relatively rare to have a dataset of sufficient size. An alternative is to pretrain a Conv-Net on a very large dataset (e.g. ImageNet), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. Typically, there are three major transfer learning scenarios:   **ConvNet as fixed feature extractor:** We can take a ConvNet pretrained on ImageNet, and remove the last fully-connected layer, then treat the rest structure as a fixed feature extractor for the target dataset. In AlexNet, this would be a 4096-D vector. Usually, we call these features as CNN codes. Once we get these features, we can train a linear classifier (e.g. linear SVM or Softmax classifier) for our target dataset.   **Fine-tuning the ConvNet:** Another idea is not only replace the last fully-connected layer in the classifier, but to also fine-tune the parameters of the pretrained network. Due to overfitting concerns, we can only fine-tune some higher-level part of the network. This suggestion is motivated by the observation that earlier features in a ConvNet contains more generic features (e.g. edge detectors or color blob detectors) that can be useful for many kind of tasks. But the later layer of the network becomes progressively more specific to the details of the classes contained in the original dataset.   **Pretrained models:** The released pretrained model is usually the final ConvNet checkpoint. So it is common to see people use the network for fine-tuning.  \n4. 3D Autoencoder and Convolutional Neural Network We take a two-stage approach where we first train a 3D sparse autoencoder to learn filters for convolution operations, and then build a convolutional neural network whose first layer uses the filters learned with the autoencoder. ![](images/f2.png)  #### 4.1. Sparse Autoencoder An autoencoder is a 3-layer neural network that is used to extract features from an input such as an image. Sparse representations can provide a simple interpretation of the input data in terms of a small number of \\parts by extracting the structure hidden in the data. The autoencoder has an input layer, a hidden layer and an output layer, and the input and output layers have same number of units, while the hidden layer contains more units for a sparse and overcomplete representation. The encoder function maps input x to representation h, and the decoder function maps the representation h to the output x. In our problem, we extract 3D patches from scans as the input to the network. The decoder function aims to reconstruct the input form the hidden representation h.  #### 4.2. 3D Convolutional Neural Network Training the 3D convolutional neural network(CNN) is the second stage. The CNN we use in this project has one convolutional layer, one pooling layer, two linear layers, and finally a log softmax layer. After training the sparse autoencoder, we take the weights and biases of the encoder from trained model, and use them a 3D filter of a 3D convolutional layer of the 1-layer convolutional neural network. Figure 2 shows the architecture of the network.  \n5. Tools In this project, we used Nibabel for MRI image processing and PyTorch Neural Networks implementation.\n",
    "readme_length": 8792
  },
  {
    "name": "SWIFT",
    "full_name": "William-Liwei/SWIFT",
    "description": "This is an official implementation of \"SWIFT: State-space Wavelet Integrated Forecasting Technology for Enhanced Time Series Prediction\".",
    "stars": 145,
    "forks": 4,
    "language": "Python",
    "url": "https://github.com/William-Liwei/SWIFT",
    "topics": [],
    "created_at": "2025-03-08T08:18:06Z",
    "updated_at": "2025-12-02T01:59:14Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "# SWIFT: State-space Wavelet Integrated Forecasting Technology for Enhanced Time Series Prediction\n\nThis is an official implementation of [SWIFT: State-space Wavelet Integrated Forecasting Technology for Enhanced Time Series Prediction].\n\n![license](https://img.shields.io/badge/License-Apache_2.0-blue.svg) [![Issues](https://badgen.net/github/issues/William-Liwei/SWIFT?color=red)](https://github.com/William-Liwei/SWIFT/issues) [![PRs](https://badgen.net/github/prs/William-Liwei/SWIFT?color=yellow)](https://github.com/William-Liwei/SWIFT/pulls) ![GitHub repo size](https://img.shields.io/github/repo-size/william-liwei/SWIFT?color=green) ![GitHub Repo stars](https://badgen.net/github/stars/William-Liwei/SWIFT?color=orange) ![GitHub forks](https://badgen.net/github/forks/William-Liwei/SWIFT?color=pink)\n\n## Author(s)\n\nWei Li, Shanghai University;\n\n\n## Prerequisites\n\nEnsure you are using Python 3.9 and install the necessary dependencies by running:\n\n```\npip install -r requirements.txt\n```\n\n## Data set preparation\n\nDue to the storage space limitations of GitHub, some data sets may need to be downloaded separately. However, some small data sets have been included in the repository for demonstration purposes. If you encounter any issues with the data sets, please contact us via email.\n\n## Citation\n\nIf you find this repo useful, please cite it as follows:\n\n```\n@inproceedings{li2025swift,\n  title={SWIFT: State-space Wavelet Integrated Forecasting Technology for Enhanced Time Series Prediction},\n  author={Li, Wei},\n  booktitle={International Conference on Artificial Neural Networks},\n  year={2025},\n  note={to be published}\n}\n```\n## Contact\n\nIf you have any questions, please contact <liwei008009@163.com> or submit an issue. \nDue to time constraints, business agreements and intellectual property protection, the implementation of this model is partial. If you are interested in our model, please contact me by email. \nThis repo model is a simplified model and is allowed for commercial use, but please cite. If you want to use the complete implementation model, you can further negotiate business cooperation, thank you very much for your attention.\n\n## Acknowledgement\n\nWe extend our gratitude to the following repositories for their valuable code and datasets:\n- https://github.com/thuml/Time-Series-Library\n- https://github.com/thuml/Autoformer\n- https://github.com/Hank0626/WFTNet\n- https://github.com/William-Liwei/EnergyPatchTST\n\n## Star History\n\n<a href=\"https://www.star-history.com/#William-Liwei/SWIFT&Date\">\n\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=William-Liwei/SWIFT&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=William-Liwei/SWIFT&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=William-Liwei/SWIFT&type=Date\" />\n </picture>\n\n</a>",
    "readme_length": 2946
  },
  {
    "name": "UI-R1",
    "full_name": "lll6gg/UI-R1",
    "description": "[AAAI-2026] Code for \"UI-R1: Enhancing Efficient Action Prediction of GUI Agents by Reinforcement Learning\"",
    "stars": 143,
    "forks": 11,
    "language": "Python",
    "url": "https://github.com/lll6gg/UI-R1",
    "topics": [
      "efficient-reasoning",
      "gui-agent",
      "multimodal-large-language-models",
      "multimodal-learning",
      "r1",
      "reinforcement-learning"
    ],
    "created_at": "2025-03-28T06:51:58Z",
    "updated_at": "2025-11-28T02:06:12Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "# UI-R1: Enhancing **Efficient** Action Prediction of GUI Agents by Reinforcement Learning\n\n<font size=4><div align='center' > [[ðŸ“– Paper](https://arxiv.org/abs/2503.21620)] [[ðŸ¤— UI-R1-3B](https://huggingface.co/LZXzju/Qwen2.5-VL-3B-UI-R1)] [[ðŸ¤— UI-R1-E-3B](https://huggingface.co/LZXzju/Qwen2.5-VL-3B-UI-R1-E)][[ðŸ¤— Datasets](https://huggingface.co/datasets/LZXzju/UI-R1-3B-Train)] [[ðŸ¤— DailyÂ Paper](https://huggingface.co/papers/2503.21620)]</div></font>\n\n## ðŸ”¥ Overview\n\nWe propose **UI-R1**, the first framework to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for GUI action prediction tasks.\n<div align=\"center\">\n  <img src=\"assets/method.png\" alt=\"Logo\" style=\"width:75%;\">\n</div>\n\n\nExperimental results demonstrate that our proposed **UI-R1-3B** achieves significant improvements over the base model (i.e. Qwen2.5-VL-3B) on both in-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of **22.1%** on ScreenSpot, **6.0%** on ScreenSpot-Pro, and **12.7%** on AndroidControl. Furthermore, UI-R1-3B delivers competitive performance compared to larger models (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K samples.\n\n<div align=\"center\">\n  <img src=\"assets/radar.png\" alt=\"Logo\" style=\"width:70%;\">\n</div>\n\n## Grounding Leaderboard: [UI-I2E-Bench](https://colmon46.github.io/i2e-bench-leaderboard/)\n|     Model      | ScreenSpot | UI-I2E-Bench Avg | ScreenSpot-Pro | Average  |\n| :------------: | :--------: | :--------------: | :------------: | :--: |\n| UI-TARS-1.5-7B |    88.1    |       73.2       |      42.2      | 67.8 |\n| Uground-V1-72B |    89.7    |       76.3       |      34.3      | 66.8 |\n|  UI-TARS-72B   |    88.4    |       73.7       |      38.1      | 66.7 |\n|   **UI-R1-E-3B**   |    89.2    |       69.1       |      33.5      | 63.9 |\n| Uground-V1-7B  |    87.1    |       70.3       |      31.1      | 62.8 |\n|   InfiGUI-R1   |    87.5    |       69.7       |      29.6      | 62.3 |\n|   UI-TARS-7B   |    89.5    |       61.4       |      35.7      | 62.2 |\n| Qwen2.5-VL-72B |    87.1    |       51.4       |      43.6      | 60.7 |\n| UI-I2E-VLM-7B  |    82.5    |       69.5       |      23.6      | 58.5 |\n|   UI-TARS-2B   |    82.3    |        62        |      27.7      | 57.3 |\n| Qwen2.5-VL-7B  |    84.7    |       53.8       |       29       | 55.8 |\n| OmniParser-V2  |     72     |       54.8       |      39.6      | 55.5 |\n| Uground-V1-2B  |    78.8    |       57.4       |      26.6      | 54.3 |\n|  OS-Atlas-7B   |    82.5    |       58.6       |      18.9      | 53.3 |\n|     **UI-R1-3B**      |    83.3    |       58.5       |      17.8      | 53.2 |\n|   UGround-7B   |    74.1    |       54.2       |      16.5      | 48.3 |\n| UI-I2E-VLM-4B  |    70.4    |       53.4       |      12.2      | 45.3 |\n|   OmniParser   |    73.9    |       53.1       |      8.3       | 45.1 |\n|   ShowUI-2B    |    76.8    |       41.5       |      7.7       |  42  |\n| Qwen2.5-VL-3B  |    55.5    |       41.7       |      23.9      | 41.3 |\n|   Aguvis-7B    |    84.4    |       53.2       |      22.9      | 40.4 |\n|  OS-Atlas-4B   |    70.1    |       44.3       |      3.7       | 39.4 |\n|  Qwen2-VL-7B   |    42.6    |       48.7       |      1.6       |  31  |\n|    Seeclick    |    55.8    |       26.4       |      1.1       | 27.8 |\n|  InternVL2-4B  |    4.2     |       0.9        |      0.3       | 1.8  |\n\n## ðŸ”¥Insight 1 : Fast Grounding\n\n> **Thinking is not needed for GUI grounding.**\n\nInspired by concurrent works studying efficient LRM, we realize efficient reasoning by RFT training. UI-R1-3B-E's training consists of two steps:\n\n1. DAST (Difficulty-Adaptive Slow-Thinking): Add difficulty-adaptive length reward to make reasoning from slow to fast.\n2. Nothinking: Not output reasoning process.\n\nNote: UI-R1-3B (v2) and UI-R1-3B-E both train on larger dataset (2K grounding data in [GUI-R1-3K](https://huggingface.co/datasets/ritzzai/GUI-R1)) compared to UI-R1-3B (v1).\n\n#### Benchmark 1: ScreenSpotV2\n\n| ScreenSpotV2  | inference mode | Mobile-T | Mobile-I | Desktop-T | Desktop-I | Web-T    | Web-I    | Avgâ†‘ / Lenâ†“        |\n| ------------- | -------------- | -------- | -------- | --------- | --------- | -------- | -------- | ----------------- |\n| OS-ATLAS-7B   | w/o thinking   | 95.2     | 75.8     | 90.7      | 63.6      | 90.6     | 77.3     | 84.1 /            |\n| UI-TARS-7B    | w/o thinking   | 95.2     | 79.1     | 90.7      | 68.6      | 90.6     | 78.3     | 84.7 /            |\n| UI-R1-3B (v1) | w/ thinking    | 96.2     | **84.3** | 92.3      | 63.6      | 89.2     | 75.4     | 85.4 / 67         |\n| GUI-R1-3B     | w/ thinking    | 97.6     | 78.2     | 94.3      | 64.3      | 91.0     | 72.4     | 85.0 / 80         |\n| UI-R1-3B (v2) | w/ thinking    | 97.6     | 79.6     | 92.3      | 67.9      | 88.9     | 77.8     | 85.8 / 60         |\n| UI-R1-E-3B    | w/o thinking   | **98.2** | 83.9     | **94.8**  | **75.0**  | **93.2** | **83.7** | **89.5** / **28** |\n\n#### Benchmark 2: ScreenSpot-Pro\n\n| ScreenSpot-Pro | inference mode | Average Lengthâ†“ | Average Accuracyâ†‘ |\n| -------------- | -------------- | --------------- | ---------------- |\n| UGround-7B     | w/o thinking   | -               | 16.5             |\n| OS-ATLAS-7B    | w/o thinking   | -               | 18.9             |\n| UI-R1-3B (v1)  | w/ thinking    | 102             | 17.8             |\n| GUI-R1-3B      | w/ thinking    | 114             | 26.6             |\n| UI-R1-3B (v2)  | w/ thinking    | 129             | 29.8             |\n| UI-R1-E-3B     | w/o thinking   | **28**          | **33.5**         |\n\n##### Analysis\n\n1. Our UI-R1-3B-E achieves **SOTA** with **least** answer tokens in 3B/7B Open-source methods, demonstrating GUI grounding needs no reasoning.\n\n##### Todo\n\n- [ ] Performance on 7B may be opposite.\n- [ ] Performance on Planning may be opposite. The author predicts that Fast Grounding, Slow Planning.\n- [X] The checkpoints of UI-R1-3B-E will be released soon.\n- [X] The updated paper will come soon.\n- [X] The efficient training code will come soon. (in src/script/train_e.sh)\n## Setup\n\n```shell\nconda create -n ui-r1 python=3.10\nconda activate ui-r1\nbash setup.sh\n```\n\n## Data\n\nOur training mobile data is a subset from AndroidControl and ScreenSpot.\n\nYou can also prepare your training or inference data like:\n\n```\nimages/:\n\timage1.png\n\timage2.png\n```\n\n```\ntest.json:\n[\n\t{\n\t\"img_filename\": \"image1.png\",\n        \"bbox\": [\n            825,\n            72,\n            1673,\n            149\n        ],\n        \"instruction\": \"search bar\"\n     },\n     {\n\t\"img_filename\": \"image2.png\",\n        \"bbox\": [\n            123,\n            732,\n            334,\n            812\n        ],\n        \"instruction\": \"check weather\"\n     }\n]\n```\n\nwhere bbox : [x1,y1,x2,y2] is the coordinate of the left top and the right bottom of the ground truth bbox\n\n## Inference\n\nWe provide an example here\n\n```shell\ncd evaluation/\nbash test.sh\n```\n\nPlease fill the MODEL_PATH, IMG_PATH, TEST_JSON with your real checkpoint path and data path.\n## Training\n\n```shell\ncd src/script/\nbash train.sh\n# efficient training\nbash train_e.sh\n```\n\n\n\n\n## ðŸ—žï¸ News\n- **`2025-11-08`**: Our paper was accepted by **AAAI-2026**.\n- **`2025-05-14`**: We update the [paper](https://arxiv.org/abs/2503.21620) with UI-R1-E-3B.\n- **`2025-05-12`**: We release the [checkpoints](https://huggingface.co/LZXzju/Qwen2.5-VL-3B-UI-R1-E) of the UI-R1-E-3B model.\n- **`2025-05-12`**: We fix the bug of scales when batch_size > 1.\n- **`2025-05-11`**: We release the efficient training code of the UI-R1-E-3B model.\n- **`2025-04-02`**: We release the [datasets](https://huggingface.co/datasets/LZXzju/UI-R1-3B-Train) of the UI-R1-3B (v1) model.\n- **`2025-03-30`**: We release the [checkpoints](https://huggingface.co/LZXzju/Qwen2.5-VL-3B-UI-R1) of the UI-R1-3B (v1) model.\n- **`2025-03-30`**: We release the UI-R1 repository.\n- **`2025-03-27`**: We release our [paper](https://arxiv.org/abs/2503.21620).\n\n\n\n\n\n## â­ï¸ Citation\n\nIf you find this project useful, welcome to cite us.\n\n```bit\n@article{lu2025ui,\n  title={UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning},\n  author={Lu, Zhengxi and Chai, Yuxiang and Guo, Yaxuan and Yin, Xi and Liu, Liang and Wang, Hao and Xiong, Guanjing and Li, Hongsheng},\n  journal={arXiv preprint arXiv:2503.21620},\n  year={2025}\n}\n```\n\n\n\n## ðŸ¤ Acknowledgements\n\nWe sincerely thank projects [R1-V](https://github.com/Deep-Agent/R1-V), [Open-R1](https://github.com/huggingface/open-r1), and [Open-r1-multimodal](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal), [VLM-R1](https://github.com/om-ai-lab/VLM-R1) for providing their open-source resources.\n",
    "readme_length": 8714
  },
  {
    "name": "UUKG",
    "full_name": "usail-hkust/UUKG",
    "description": "UUKG: Unified Urban Knowledge Graph Dataset for Knowledge-Enhanced Urban Spatiotemporal Prediction",
    "stars": 113,
    "forks": 14,
    "language": "Python",
    "url": "https://github.com/usail-hkust/UUKG",
    "topics": [],
    "created_at": "2023-02-02T13:48:09Z",
    "updated_at": "2025-11-30T11:50:24Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "<div align=\"center\">\n    <img src=\"https://github.com/usail-hkust/UUKG/blob/main/title.png\" width=\"440px\">\n    <p> \n    \t<b>\n        The Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal Prediction. <a href=\"https://arxiv.org/pdf/2306.11443.pdf\" title=\"PDF\">PDF</a>\n        </b>\n    </p>\n\n------\n\n<p align=\"center\">\n  <a href=\"## Overview\">Overview</a> â€¢\n  <a href=\"## Installation\">Installation</a> â€¢\n  <a href=\"## Dataset\">Dataset</a> â€¢\n  <a href=\"## How to Run\">How to Run </a> â€¢\n  <a href=\"## Directory Structure\">Directory Structure</a> â€¢\n  <a href=\"## Citation\">Citation</a> \n</p>\n</div>\n\nOfficial repository of NeurIPS 2023 Dataset and Benchmark Track paper [\"UUKG: The Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal Prediction\"](https://arxiv.org/pdf/2306.11443.pdf). Please star, watch and fork our repo for the active updates!\n\n## 1. Overview\n<div style=\"display: flex; justify-content: center;\">\n  <img src=\"https://github.com/usail-hkust/UUKG/blob/main/workflow.png\" width=\"400\">\n  <img src=\"https://github.com/usail-hkust/UUKG/blob/main/UrbanKG.png\" width=\"300\">\n</div>\n\nUUKG is an open-sourced and multifaceted urban knowledge graph dataset compatible with various USTP tasks. The above left-figure illustrates the workflow of UUKG construction. For a given city, we first construct an Urban Knowledge Graph (UrbanKG) from multi-sourced urban data. As shown in the above right-figure, by extracting and organizing entities (e.g., POIs, road segments, etc.) into a multi-relational heterogeneous graph, UrbanKG encodes various high order structural patterns in a unified configuration (i.e., a multi-scale spatial hierarchy), which facilitates joint processing for various downstream USTP tasks.\n## 2. Installation\nStep 1: Create a python 3.7 environment and install dependencies:\n\n```\nconda create -n python3.7 UUKG\nsource activate UUKG\n```\n\nStep 2: Install library\n\n```bash\npip install -r ./UrbanKG_Embedding_Model/requirements.txt\npip install -r ./USTP_Model/requirements.txt\n```\n\nYou can also follow the **'./USTP_Model/readme.md'**  and  **'./UrbanKG_Embedding_Model/readme.md'** files to install related packages.\n\n## 3. Dataset\nWe opensource two large-scale Urban Knowledge Graph (UrbanKG) datasets in New York and Chicago compatible with five Urban SpatioTemporal Prediction (USTP) tasks. As the original dataset is quite large, we have included example data, data processing code, and model code to assist researchers in understanding our work. The complete data sources can be found on [Google Drive](https://drive.google.com/drive/folders/1egTmnKRzTQuyW_hsbFURUonGC-bJmBHW?usp=sharing).\n\nWe provide very detailed explanation for our data and pre-processing module in both [UrbanKG construction](https://github.com/usail-hkust/UUKG/tree/main/UrbanKG_data) and [USTP dataset construction](https://github.com/usail-hkust/UUKG/tree/main/USTP_data). The above dataset construction scheme is highly reusable, one can prepare their own urban data and use our code to build their personalized UrbanKG and USTP dataset easily. \n\n#### 3.1 UrbanKG Data\n\n| Dataset | Entity  | Relation | Triplet | Train   | Valid  | Test   |\n| ------- | ------- | -------- | ------- | ------- | ------ | ------ |\n| NYC     | 236,287 | 13       | 930,240 | 837,216 | 46,512 | 46,512 |\n| CHI     | 140,602 | 13       | 564,400 | 507,960 | 28,220 | 28,220 |\n\n##### 3.1.1 Guidance on data usage and processing\n\nWe store the original unprocessed files in the **'./Meta_data'** directory. To preprocess, align, and filter these files, we utilize either the **`preprocess_meta_data_nyc.py`** or **`preprocess_meta_data_chi.py`** script. The processed data is then saved in the **'./Processed_data'** directory. Finally, we execute the **`construct_UrbanKG_NYC.py`** or **`construct_UrbanKG_CHI.py`** script to obtain the constructed urban knowledge graphs, which are stored in the **'./UrbanKG'** directory.\n\nWe divide the training set, verification set and test set by **`train_val_test_ent2id_rel2id.py`**\n\nThe file information in each directory is as follows:\n\n```\n./Meta_data    Raw data set: administrative division data, POI and road network data\n./Processed_data   Aligned datasets: administrative region entity, POI entity, road network entity\n./UrbanKG    City fact triples obtained from 8 entities and 13 relationships\n```\n\nThe following types of atomic files are defined:\n\n| filename            | content                                 | example              |\n| ------------------- | --------------------------------------- | -------------------- |\n| entity2id_XXX.txt   | entity_name, entity_id                  | Road/106710 11       |\n| relation2id_XXX.txt | relation_name, relation_id              | BNB 2                |\n| train               | entity_id, relation_id, entity_id       | 196632  12 85987     |\n| valid               | entity_id, relation_id, entity_id       | 43982   10 233474    |\n| test                | entity_id, relation_id, entity_id       | 167134  6  75149     |\n| triplet.txt         | entity_id, relation_id, entity_id       | 48034   12 168303    |\n| UrbanKG_XXX.txt     | entity_name, relation_name, entity_name | POI/663 PLA Area/230 |\n\n##### 3.1.2 To create your urban knowledge graph dataset\n\nOur urban knowledge graph construction scheme is highly reusable. You can prepare your urban data following either the file format in **'./Meta_data'** or **'./Processed_data'**, and then run scripts **`construct_UrbanKG_XXX.py`** to build your personalized urban knowledge graph. This flexibility allows you to adapt the construction process to various cities and datasets easily.\n\n##### 3.1.3 Visualization\n<img src=\"https://github.com/usail-hkust/UUKG/blob/main/UrbanKG_data/visualization.png\" width=\"440px\">\n\nWe offer comprehensive visualization solutions for all types of urban knowledge. By leveraging the powerful visualization capabilities of **Folium**, we provide an intuitive understanding of the urban entities and relationships encoded in the constructed urban knowledge map. This allows users to interact with and explore the urban knowledge graph in a user-friendly manner, facilitating better insights and analysis of the urban data.\n\nYou can run **`UrbanKG_visulization_XXX.py`** to get the overall visualization of urban entities like borough, area, POI and road segment. You can also develop other visualization function according to your preferences.\n\n#### 3.2 USTP Data\n\n| Type | USTP flow prediction  | USTP event prediction  |\n| ------- | -------  | ------ |\n| Dataset  | taxi, bike, human Mobility | crime, 311 service |\n| Sensor | region-level, road-level, POI-level | region-level |\n\n##### 3.2.1 Guidance on data usage and processing\n\nWe store the original unprocessed files in the **'./Meta_data'** directory. To preprocess, align, and filter these files, we utilize either the **`preprocess_meta_data_nyc.py`** or **`preprocess_meta_data_chi.py`** script. The processed data is then saved in the **'./Processed_data'** directory. \n\nFinally, we execute the **`construct_USTP_Pointflow_XXX.py`** script to obtain the spatiotemporal flow prediction dataset and derive **`construct_USTP_Event_XXX.py`** script to obtain the constructed urban event prediction dataset. \n\nWe storage them in the  **'./USTP'** directory with the special format mentioned in [here](https://github.com/usail-hkust/UUKG/blob/main/USTP_Model/readme.md).\n\nThe file information in each directory is as follows:\n\n```\n./Meta_data    Raw data set: taxi, bike, crime and 311 service event data.\n./Processed_data   Aligned datasets: taxi, bike, human, crime and 311 service spatiotemporal dataset which are aligned with area, road and POI.\n./USTP    The reformatted USTP dataset is now ready for use with downstream USTP models. \n```\nThe following types of atomic files are defined:\n\n| filename    | content                                  | example                                  |\n| ----------- | ---------------------------------------- | ---------------------------------------- |\n| xxx.geo     | Store geographic entity attribute information. | geo_id, type, coordinates                |\n| xxx.rel     | Store the relationship information between entities, such as areas. | rel_id, type, origin_id, destination_id  |\n| xxx.dyna    | Store traffic condition information.     | dyna_id, type, time, entity_id, location_id |\n| config.json | Used to supplement the description of the above table information. |                                          |\n\nwe explain the above four atomic files as follows:\n\n**xxx.geo**: An element in the Geo table consists of the following four parts:\n\n**geo_id, type, coordinates.**\n\n```\ngeo_id: The primary key uniquely determines a geo entity.\ntype: The type of geo. These three values are consistent with the points, lines and planes in Geojson.\ncoordinates: Array or nested array composed of float type. Describe the location information of the geo entity, using the coordinates format of Geojson.\n```\n\n**xxx.rel**: An element in the Rel table consists of the following four parts:\n\n**rel_id, type, origin_id, destination_id.**\n\n```\nrel_id: The primary key uniquely determines the relationship between entities.\ntype: The type of rel. Range in [usr, geo], which indicates whether the relationship is based on geo or usr.\norigin_id: The ID of the origin of the relationship, which is either in the Geo table or in the Usr table.\ndestination_id: The ID of the destination of the relationship, which is one of the Geo table or the Usr table.\n```\n\n**xxx.dyna**: An element in the Dyna table consists of the following five parts:\n\n**dyna_id, type, time, entity_id(multiple columns**.\n\n```\ndyna_id: The primary key uniquely determines a record in the Dyna table.\ntype: The type of dyna. There are two values: label (for event-based task) and state (for traffic state prediction task).\ntime: Time information, using the date and time combination notation in ISO-8601 standard, such as: 2020-12-07T02:59:46Z.\nentity_id: Describe which entity the record is based on, which is the ID of geo or usr.\n```\n\n**xxx.config**: The config file is used to supplement the information describing the above five tables themselves. It is stored in `json` format and consists of six keys: `geo`, `usr`, `rel`, `dyna`, `ext`, and `info`.\n\n\n##### 3.2.2 To create your USTP dataset\nOur urban spatiotemporal prediction dataset construction scheme is highly reusable. You can prepare your urban downstream task data following either the file format in **'./Meta_data'** or **'./Processed_data'**, and then run scripts **`construct_USTP_Pointflow_XXX.py`** or **`construct_USTP_Event_XXX.py`** to build your personalized USTP dataset. This flexibility allows you to adapt the construction process to various cities and datasets easily.\n\n##### 3.2.3 Visualization\n<img src=\"https://github.com/usail-hkust/UUKG/blob/main/USTP_data/bike_start_end.png\" width=\"650px\">\n\nWe offer spatial and temporal visualization implement for all types of USTP dataset. By leveraging the powerful visualization capabilities of **Folium**, we provide an intuitive understanding of different USTP tasks. \n\nYou can run **`visualize_USTP.py`** to get the overall spatial and temporal distribution of USTP dataset. You can also develop other visualization function according to your preferences.\n\n## 4. How to Run\n\n#### 4.1 Structure-aware UrbanKG Embedding\n\nTo train and evaluate a UrbanKG embedding model for the link prediction task, use the run.py script:\n\n```bash\npython ./UrbanKG_Embedding_Model/run.py \n\t\t\t [-h] [--dataset {NYC, CHI}]\n              [--model {TransE, RotH, ...}]\n              [--regularizer {N3,N2}] [--reg REG]\n              [--optimizer {Adagrad,Adam,SGD,SparseAdam,RSGD,RAdam}]\n              [--max_epochs MAX_EPOCHS] [--patience PATIENCE] [--valid VALID]\n              [--rank RANK] [--batch_size BATCH_SIZE]\n              [--neg_sample_size NEG_SAMPLE_SIZE] [--dropout DROPOUT]\n              [--init_size INIT_SIZE] [--learning_rate LEARNING_RATE]\n              [--gamma GAMMA] [--bias {constant,learn,none}]\n              [--dtype {single,double}] [--double_neg] [--debug] [--multi_c]\n\n```\n**How to get the embedding?**\n\nWe build the index between entities and learned embeddings and storage the index file in **./data/entity_idx_embedding.csv**. To obtain the learned UrbanKG embedding, run **`get_embedding.py`**.\n\n#### 4.2 Knowledge-enhanced Urban SpatioTemporal Prediction\n\nTo train and evaluate a USTP model for the link prediction task, use the run.py script:\n\n```bash\npython ./USTP_Model/run.py --task traffic_state_pred --model STGCN --dataset NYCTaxi20200406\n```\nThis script will run the STGCN model on the NYCTaxi20200406 dataset for traffic state prediction task under the default configuration.\n\n**How to fuse UrbanKG embedding?**\n\nTo fuse UrbanKG embedding, we directly concatenate the embedding with USTP feature for input. You can mannualy modify it in the **`./data/dataset/traffic_state_dataset.py`**.\n\nThe **\"readme.md\"** file in [USTP_Model](https://github.com/usail-hkust/UUKG/tree/main/USTP_Model) and [UrbanKG_Embedding_Model](https://github.com/usail-hkust/UUKG/tree/main/UrbanKG_Embedding_Model) provide more details about models.\n\n## 5 Directory Structure\n\nThe expected structure of files is:\n```\nUUKG\n |-- UrbanKG_data  # UrbanKG_data\n |    |-- Meta_data\n |    |    |-- NYC  # meta data for New York\n |    |    |    |-- Administrative_data    \n |    |    |    |-- POI     \n |    |    |    |-- RoadNetwork     \n |    |    |-- CHI  # meta data for Chicago\n |    |    |    |-- Administrative_data    \n |    |    |    |-- POI     \n |    |    |    |-- RoadNetwork     \n |    |-- Processed_data  # \n |    |    |-- NYC\n |    |    |-- CHI \n |    |-- UrbanKG  # constructed urban knowledge graph\n |    |    |-- NYC\n |    |    |    |-- entity2id_NYC.txt   \n |    |    |    |-- relation2id_NYC.txt     \n |    |    |    |-- UrbanKG_NYC.txt\n |    |    |    |-- triplets_NYC.txt   \n |    |    |    |-- train_NYC.txt  \n |    |    |    |-- valid_NYC.txt\n |    |    |    |-- test_NYC.txt\n |    |    |-- CHI \n |    |-- construct_UrbanKG_NYC.py # UrbanKG constructuion\n |    |-- preprocess_meta_data_nyc.py # data preprocessing\n |-- UrbanKG_Embedding_Model  # KG embedding\n |    |-- data\n |    |    |-- NYC\n |    |    |-- CHI \n |    |-- dataset\n |    |-- models\n |    |-- optimizer\n |    |-- utils\n |    |-- run.py # KG embedding \n |    |-- requirements.txt\n |-- USTP_data  # USTP_data\n |    |-- Meta_data\n |    |    |-- NYC  # meta data for New York\n |    |    |    |-- Flow_taxi    \n |    |    |    |-- Flow_bike     \n |    |    |    |-- Flow_human     \n |    |    |    |-- Event_crime     \n |    |    |    |-- Event_311  \n |    |    |-- CHI  # meta data for Chicago\n |    |-- Processed_data  # \n |    |    |-- NYC\n |    |    |-- CHI \n |    |-- USTP  # constructed urban spatiotemporal prediction dataset\n |    |    |-- NYC\n |    |    |    |-- NYCTaxi20200406  \n |    |    |    |-- NYCBike20200406     \n |    |    |    |-- NYCHuman20200406\n |    |    |    |-- NYCCrime20210112   \n |    |    |    |-- NYC311Service20210112  \n |    |    |-- CHI \n |    |-- utils  # constructed urban spatiotemporal prediction dataset\n |    |-- preprocess_meta_data_nyc # USTP data preprocessing\n |    |-- construct_USTP_Pointflow_NYC.py # USTP flow dataset construction\n |    |-- construct_USTP_Event_NYC.py # USTP event dataset construction\n |-- USTP_Model  # USTP_model\n |    |-- libcity\n |    |-- log\n |    |-- raw_data\n |    |-- run.py # urban spatiotemporal prediction \n |    |-- requirements.txt\n |-- README.md\n\n```\n\n## 6 Citation\nIf you find our work is useful for your research, please consider citing:\n```bash\n@article{ning2024uukg,\n  title={UUKG: unified urban knowledge graph dataset for urban spatiotemporal prediction},\n  author={Ning, Yansong and Liu, Hao and Wang, Hao and Zeng, Zhenyu and Xiong, Hui},\n  journal={Advances in Neural Information Processing Systems},\n  volume={36},\n  year={2024}\n}\n```\n",
    "readme_length": 15979
  },
  {
    "name": "NASA_Li-ion_Battery_SOH_Prediction_with_MVIP-Trans",
    "full_name": "Tianyou-Bai/NASA_Li-ion_Battery_SOH_Prediction_with_MVIP-Trans",
    "description": "This research provides a prognostic framework for off-line SOH estimation of Li-ion battery. With a CNN-Transformer architecture, this program is capable of modeling the temporal correlations of battery signals from both local and global views. In this way, the learning ability of both local features and long period contexts will be enhanced. ",
    "stars": 107,
    "forks": 14,
    "language": "Python",
    "url": "https://github.com/Tianyou-Bai/NASA_Li-ion_Battery_SOH_Prediction_with_MVIP-Trans",
    "topics": [],
    "created_at": "2023-03-11T12:17:06Z",
    "updated_at": "2025-12-02T02:38:09Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# NASA_Li-ion_Battery_SOH_Prediction_with_MVIP-Trans\nThis research provides a prognostic framework for off-line SOH estimation of Li-ion battery. With a CNN-Transformer architecture, this program is capable of modeling the temporal correlations of battery signals from both local and global views. In this way, the learning ability of both local features and long period contexts will be enhanced. \n\nSee our paper on: https://ieeexplore.ieee.org/document/10198842\n",
    "readme_length": 464
  },
  {
    "name": "clamp",
    "full_name": "ml-jku/clamp",
    "description": "Code for the paper Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language",
    "stars": 106,
    "forks": 11,
    "language": "Python",
    "url": "https://github.com/ml-jku/clamp",
    "topics": [
      "assay-modeling",
      "cheminformatics",
      "contrastive-learning",
      "drug-discovery",
      "machine-learning",
      "qsar",
      "zero-shot"
    ],
    "created_at": "2023-01-22T15:27:29Z",
    "updated_at": "2025-11-19T04:53:08Z",
    "homepage": "https://arxiv.org/abs/2303.03363",
    "license": "Other",
    "readme": "# :clamp: CLAMP\n\n[![arXiv](https://img.shields.io/badge/arXiv-2303.03363-b31b1b.svg)](https://arxiv.org/abs/2303.03363)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ml-jku/clamp/blob/main/notebooks/CLAMP_colab_demo.ipynb)\n\nCLAMP (Contrastive Language-Assay Molecule Pre-Training) is trained on molecule-bioassay pairs. It can be instructed in natural language to predict the most relevant molecule, given a textual description of a bioassay, without training samples. In extensive experiments, our method yields improved predictive performance on few-shot learning benchmarks and zero-shot problems in drug discovery. \n\n## Approach\n\n![CLAMP](./data/figs/clamp.png)\n\n## :rocket: Updates\n\n- 04/24: create augmentations for assay-descriptions using [assay_augment.py](./clamp/dataset/assay_augment.py) \n- 11/23: Pretrained Model weights for Frequent Hitter (FH), a strong baseline for few- and zero-shot drug discovery. Use it running `fh_model = clamp.FH(device='cpu')`.\n- 10/23: PubChem23, a new version of the PubChem-dataset with >~500k assays, in a preprocessed form is available (see [./data/pubchem.md](./data/pubchem.md))\n\n## :gear: Setup Environment\n\nWhen using `conda`, an environment can be set up using\n```bash\nconda env create -f env.yml\nconda activate clamp_env\n```\nTo activate the environment call ```conda activate clamp_env```.\nYou may need to adjust the CUDA version.\n\nAnother option is:\n```bash\npip install -e git+https://github.com/ml-jku/clamp.git\n```\n\n## :fire: Use a pretrained CLAMP model\n\nWarning: Currently only one version is available. We will update this repo with new pretrained models.\n\n```python\nimport torch\nimport clamp\n\nmodel = clamp.CLAMP(device='cpu')\nmodel.eval()\n\nmolecules = [\n    'CCOP(=O)(Nc1cccc(Cl)c1)OCC', #inactive\n    'O=C(O)c1ccccc1O', #inactive\n    'NNP(=S)(NN)c1ccccc1', #active\n    'CC(=O)OC1=CC=CC=C1C(=O)O', # Aspirin\n    ]\nassay_descriptions = [\n    'HIV: Experimentally measured abilities to inhibit HIV replication.',\n    ]\n\nwith torch.no_grad():\n    logits = model.forward_dense(molecules, assay_descriptions)\n    probs = logits.softmax(dim=0).cpu().numpy() # probs for molecules\n\nprint(\"Mol probs for assay:\", probs[:,0]) # res: [0.258 0.235 0.269  0.236]\n```\n\n\n## :lab_coat: Reproduce\n\n### Setup FS-Mol\nFor the [preprocessed FS-Mol dataset](https://cloud.ml.jku.at/s/dCjrt9c4arbz6rF/download) used in the paper run the following commands, which downloads, unzips and deletes the zip-file from your clamp directory:\n```bash\nwget -N -r https://cloud.ml.jku.at/s/dCjrt9c4arbz6rF/download -O fsmol.zip\nunzip fsmol.zip; rm fsmol.zip\n```\n\nTo download an preprocess from the original source:\n```python clamp/dataset/prep_fsmol.py --data_dir=./data/fsmol/```\n\nTo compute the compound encodings as input for your model run\n```bash\npython clamp/dataset/encode_compound.py \\\n--compounds=./data/fsmol/compound_names.parquet \\\n--compound2smiles=./data/fsmol/compound_smiles.parquet \\\n--fp_type=morganc+rdkc --fp_size=8096\n```\n\nTo compute the assay encodings as input for your model run\n```bash\npython clamp/dataset/encode_assay.py --assay_path=./data/fsmol/assay_names.parquet --encoding=clip --gpu=0 --columns \\\nassay_type_description description assay_category assay_cell_type assay_chembl_id assay_classifications assay_organism assay_parameters assay_strain assay_subcellular_fraction assay_tax_id assay_test_type assay_tissue assay_type bao_format bao_label cell_chembl_id confidence_description confidence_score document_chembl_id relationship_description relationship_type src_assay_id src_id target_chembl_id tissue_chembl_id variant_sequence \\\n--suffix=all\n```\nor use ```--encoding=lsa```.\n\n### Setup PubChem\n\nfor the [version used in the paper](https://cloud.ml.jku.at/s/2ybfLRXWSYb4DZN/download) as well as to generate an up-to-date version see ```./data/pubchem.md```\n\n## :fire: Train your own model\n\nRun (adjust hparams by adding it as command or in the file ```./hparams/default.json```)\n```bash\npython clamp/train.py --dataset=./data/fsmol --assay_mode=clip --split=FSMOL_split\n```\n\nThis should result in a model with a zero-shot $\\text{AUROC}$ of $0.70$ and $\\Delta \\text{AP}$ of $0.19$ on the test-set.\n\n## Evaluate a pretrained CLAMP model\n\nNote alterations in the exact split, as well as in the pretraining (droped MoleculeNet molecules)\n\nto compute the clip assay-features run:\n```\npython clamp/dataset/encode_assay.py --assay_path=./data/pubchem18/assay_names.parquet --encoding=clip --gpu=0 --columns title\n```\nand for the compound-features:\n```\npython clamp/dataset/encode_compound.py --compound2smiles=./data/pubchem18/compound_smiles.parquet --compounds=./data/pubchem18/compound_names.parquet --fp_type=morganc+rdkc --fp_size=8192\n```\n\nNow you can use the pretrained CLAMP model:\n```bash\npython clamp/train.py --model=PretrainedCLAMP --dataset=./data/pubchem18 --assay_mode=clip --split=time_a --epoch_max=0\n```\n(Warning about checkpoint can be ignored) \nThis should return $\\Delta \\text{AP}$ of $0.13$ on the test-set.\n\n## Downstream Evaluation:\n### Setup MoleculeNet\n\nTo download the preprocessed downstream datasets call\n```bash\nwget -N -r https://cloud.ml.jku.at/s/pyJMm4yQeWFM2gG/download -O downstream.zip\nunzip downstream.zip; rm downstream.zip\n```\n\nTo download an preprocess the downstream datasets from the source call.\n```python clamp/dataset/prep_moleculenet.py```\n(Doesn't include Tox21-10k)\n\n## :test_tube: Linear Probing\nGet a clamp-encoding\n```\npython clamp/dataset/encode_compound.py --compound2smiles=./data/moleculenet/tox21/compound_smiles.parquet --fp_type=clamp\n```\nRun linear probing on this encoding\n```\npython clamp/linear_probe.py ./data/moleculenet/tox21/ --split=scaffold_split --compound_mode=clamp\n```\n\nYou can also use the clamp-encoding of a pretrained model by providing an mlflow run-directory:\nYou have to specify the correct compound_features_size as well as the assay_features_size of the model.\n```\npython clamp/linear_probe.py ./data/moleculenet/hiv/ --split=scaffold_split --run_dir=./mlruns/711448512597702417/c00af103806c4243b816ecf2aed7387a/ --compound_features_size=8192 --assay_features_size=867\n```\n\nA further example can be found in the [colab-demo](https://colab.research.google.com/github/ml-jku/clamp/blob/main/notebooks/CLAMP_colab_demo.ipynb).\n\n\n## :books: Cite\nIf you find this work helpful, please cite\n```bibtex\n@article{seidl2023clamp,\n   author = {Seidl, Philipp and Vall, Andreu and Hochreiter, Sepp and Klambauer, G{\\\"u}nter},\n   title = {Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language},\n   journal = {Proceedings of the 40th International Conference on Machine Learning (ICML)},\n   institution = {Institute for Machine Learning, Johannes Kepler University, Linz},\n   year = {2023},\n   month = {July},\n   eprint={2303.03363},\n   doi = {}\n}\n```\n\n## Keywords\nDrug Discovery, Machine Learning, Zero-shot, NLP, LLM, Scientific Language Model\n",
    "readme_length": 7104
  },
  {
    "name": "Liquid-Neural-Networks-in-Stock-Market-Prediction",
    "full_name": "HusseinJammal/Liquid-Neural-Networks-in-Stock-Market-Prediction",
    "description": "This repository hosts a stock market prediction model for Tesla and Apple using Liquid Neural Networks. It showcases data-driven forecasting techniques, feature engineering, and machine learning to enhance the accuracy of financial predictions.",
    "stars": 88,
    "forks": 21,
    "language": "Python",
    "url": "https://github.com/HusseinJammal/Liquid-Neural-Networks-in-Stock-Market-Prediction",
    "topics": [
      "apple-stock-data",
      "deep-learning",
      "liquid-neural-networks",
      "machine-learning",
      "neural-networks",
      "stock-market",
      "stock-market-prediction",
      "tesla-stock-analysis",
      "time-series",
      "time-series-analysis"
    ],
    "created_at": "2024-05-17T06:42:26Z",
    "updated_at": "2025-11-23T18:15:25Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "# Liquid-Neural-Networks-in-Stock-Market-Prediction",
    "readme_length": 51
  },
  {
    "name": "ANANSE",
    "full_name": "vanheeringen-lab/ANANSE",
    "description": "Prediction of key transcription factors in cell fate determination using enhancer networks. See full ANANSE documentation for detailed installation instructions and usage examples.",
    "stars": 87,
    "forks": 15,
    "language": "Python",
    "url": "https://github.com/vanheeringen-lab/ANANSE",
    "topics": [
      "bioinformatics",
      "cell-fate-determination",
      "enhancer-database",
      "grn",
      "key-transcription-factors"
    ],
    "created_at": "2019-05-22T13:37:38Z",
    "updated_at": "2025-11-29T09:49:26Z",
    "homepage": "http://anansepy.readthedocs.io",
    "license": "MIT License",
    "readme": "# ANANSE: ANalysis Algorithm for Networks Specified by Enhancers\n[![bioconda-badge](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat)](http://bioconda.github.io)\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/ananse/badges/version.svg)](https://anaconda.org/bioconda/ananse)\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/ananse/badges/downloads.svg)](https://anaconda.org/bioconda/ananse)\n\n[![Documentation Status](https://readthedocs.org/projects/anansepy/badge/?version=master)](https://anansepy.readthedocs.io/en/master/?badge=master)\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/ananse/badges/license.svg)](https://anaconda.org/bioconda/ananse)\n[![DOI:10.1093/nar/gkab598](http://img.shields.io/badge/DOI-10.1093/nar/gkab598-B31B1B.svg)](https://doi.org/10.1093/nar/gkab598)\n\n[![Maintainability](https://api.codeclimate.com/v1/badges/875df8c40fec66d68b1f/maintainability)](https://codeclimate.com/github/vanheeringen-lab/ANANSE/maintainability)\n[![Test Coverage](https://api.codeclimate.com/v1/badges/875df8c40fec66d68b1f/test_coverage)](https://codeclimate.com/github/vanheeringen-lab/ANANSE/test_coverage)\n\n## Prediction of key transcription factors in cell fate determination using enhancer networks\nANANSE is a computational approach to infer enhancer-based gene regulatory networks (GRNs) and to identify key transcription factors between two GRNs. You can use it to study transcription regulation during development and differentiation, or to generate a shortlist of transcription factors for trans-differentiation experiments. \n\nANANSE is written in Python and comes with a command-line interface that includes 3 main commands: `ananse binding`, `ananse network`, and `ananse influence`. A graphical overview of the tools is shown below.\n\n![](docs/img/Fig2.png)\n\nCheck out the **[ANANSE documentation](https://anansepy.readthedocs.io/en/master/)** for \n* [installation instructions](https://anansepy.readthedocs.io/en/master/installation/)\n* [command explanations](https://anansepy.readthedocs.io/en/master/command-line_reference/)\n* [input explanations and examples](https://anansepy.readthedocs.io/en/master/input_data/)\n* [usage examples](https://anansepy.readthedocs.io/en/master/examples/)\n* [FAQ](https://anansepy.readthedocs.io/en/master/faq/)\n* and more!\n \nFor documentation on the **development version** see [here](https://anansepy.readthedocs.io/en/develop/).\n\n\n## Citation\n\n> ANANSE: an enhancer network-based computational approach for predicting key transcription factors in cell fate determination \n> Quan Xu, Georgios Georgiou, Siebren FrÃ¶lich, Maarten van der Sande, Gert Jan C Veenstra, Huiqing Zhou, Simon J van Heeringen\n> Nucleic Acids Research, gkab598, https://doi.org/10.1093/nar/gkab598\n\n\n## scANANSE: Gene regulatory network and motif analysis of single-cell clusters\n\nscANANSE is a pipeline developed for single-cell RNA-sequencing data and single-cell ATAC-sequencing data. It can export single-cell cluster data from both Seurat or Scanpy objects, and runs the clusters through ANANSE using a snakemake workflow to significantly simplify the process. Afterwards, results can be imported back into your single-cell object.\n\nFor more info on this implementation check out the\n* [scANANSE workflow](https://doi.org/10.12688/f1000research.130530.1)\n* [Python package for Scanpy objects](https://github.com/Arts-of-coding/AnanseScanpy)\n* [R package for Seurat objects](https://github.com/JGASmits/AnanseSeurat)\n* [anansnake package for automating multiple ANANSE analyses](https://github.com/vanheeringen-lab/anansnake)\n\n\n## Help and Support\n\n* The preferred way to get support is through the [Github issues page](https://github.com/vanheeringen-lab/ANANSE/issues).\n\n\n## License\n\n  - **[MIT license](http://opensource.org/licenses/mit-license.php)** [![Anaconda-Server Badge](https://anaconda.org/qxuchn/ananse/badges/license.svg)](https://anaconda.org/qxuchn/ananse)\n  - Copyright 2020 Â© <a href=\"https://github.com/vanheeringen-lab\" target=\"_blank\">vanheeringen-lab</a>.\n",
    "readme_length": 4069
  },
  {
    "name": "MATP-with-HEAT",
    "full_name": "Xiaoyu006/MATP-with-HEAT",
    "description": "This repo contains the code for our paper entitled \"Multi-Agent Trajectory Prediction with Heterogeneous Edge-Enhanced Graph Attention Network\".",
    "stars": 75,
    "forks": 4,
    "language": "Python",
    "url": "https://github.com/Xiaoyu006/MATP-with-HEAT",
    "topics": [],
    "created_at": "2022-01-20T06:25:36Z",
    "updated_at": "2025-11-24T17:50:46Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# MATP-with-HEAT\nThis repo contains the code for our paper entitled \"Multi-Agent Trajectory Prediction with Heterogeneous Edge-Enhanced Graph Attention Network\", IEEE T-ITS, 2022.\n\n## Install dependencies via pip.\n`pip install -r requirements.txt`\n\n## Data preprocessing\nThe strucutre of the raw INTERACTION Dataset can be found in `INTERACTION Dataset Tree.txt`.\n\nTo obtain the sorted dataset, please refer to \n[INTERPRET_challenge_regular_generalizability_track](https://github.com/interaction-dataset/INTERPRET_challenge_regular_generalizability_track). \n\nRun `bash datapre_run.sh` to process all the scenarios provided by the INTERACTION dataset.\n\n## Models\nBase model -> Heat model -> HeatIR model.\n\n## Traning\nRun `python it_all_train.py -m Heat` to train the one-channel HEAT-based trajectory predictor.\n\n## Validation\n\n## Citation\nIf you have found this work to be useful, please consider citing our paper:\n```\n@article{mo2022multi,\n  title={Multi-agent trajectory prediction with heterogeneous edge-enhanced graph attention network},\n  author={Mo, Xiaoyu and Huang, Zhiyu and Xing, Yang and Lv, Chen},\n  journal={IEEE Transactions on Intelligent Transportation Systems},\n  year={2022},\n  publisher={IEEE}\n}\n```\n",
    "readme_length": 1220
  },
  {
    "name": "DGFNet",
    "full_name": "XinGP/DGFNet",
    "description": "[RAL 2025]Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network",
    "stars": 73,
    "forks": 7,
    "language": "Python",
    "url": "https://github.com/XinGP/DGFNet",
    "topics": [
      "argoverse",
      "autonomous-driving",
      "behavior-prediction",
      "deep-learning",
      "motion-forecasting",
      "multi-agent",
      "trajectory-prediction"
    ],
    "created_at": "2024-01-19T06:38:08Z",
    "updated_at": "2025-11-27T03:16:14Z",
    "homepage": "https://arxiv.org/abs/2407.18551",
    "license": "Apache License 2.0",
    "readme": "# DGFNet: Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network\n### [Paper](https://arxiv.org/abs/2407.18551) | [Webpage](https://github.com/XinGP/DGFNet)\nThis is the official implementation of the paper *Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network*.\n\nThis paper is accepted for publication in the IEEE Robotics and Automation Letters (RA-L), 2025.\n\n## Introduction\n\nTraditional methods usually perform holistic inference on the trajectories of agents, neglecting differences in prediction difficulty among agents. This paper proposes a novel Difficulty-Guided Feature Enhancement Network (**DGFNet**), which leverages the prediction difficulty differences among agents for multiagent trajectory prediction. \n\n<p align=\"center\">\n  <img src=\"files/Intro.jpg\" style=\"width: 50%; height: 50%;\">\n</p>\n\nFirstly, we employ Spatio-temporal Feature Extraction to capture rich spatio-temporal features. Secondly, a Difficulty-Guided Decoder controls the flow of future trajectories into subsequent modules, obtaining **reliable future trajectories**. Then, feature interaction and fusion are performed through the Future Feature Interaction module. Finally, the fused actor features are fed into the Final Decoder to generate the predicted trajectory distributions for multiple participants.\n\n<p align=\"center\">\n  <img src=\"files/DGFNet.jpg\">\n</p>\n\nCompared to the SOTA methods, our method balances trajectory prediction accuracy and real-time inference speed. \n\n<p align=\"center\">\n  <img src=\"files/Param.png\" style=\"width: 50%; height: 50%;\">\n</p>\n\n### Argoverse 1(Single model)\n- **Performance Metrics:**\n\n| Split | brier-minFDE | minFDE | MR | minADE | Param |\n|-------|:------------:|:------:|:--:|:------:|:------:|\n| Val   | 1.499       | 0.897 | 0.073 | 0.634 | 4.53 |\n| Test  | 1.742       | 1.117 | 0.108 | 0.763 | - |\n\n### Argoverse 1(Ensemble model--Five models trained from different random seeds)\n- **Performance Metrics:**\n\n| Split | brier-minFDE | minFDE | MR | minADE |\n|-------|:------------:|:------:|:--:|:------:|\n| Test  | 1.693       | 1.110 | 0.107 | 0.752 | \n\n## Qualitative Results\n\n* On Argoverse 1 motion forecasting dataset\n<p align=\"center\">\n  <img src=\"files/AV1-1.png\" width = \"250\"/>\n  <img src=\"files/AV1-2.png\" width = \"250\"/>\n  <img src=\"files/AV1-3.png\" width = \"250\"/>\n</p>\n\n* On Argoverse 2 motion forecasting dataset\n<p align=\"center\">\n  <img src=\"files/AV2-1.png\" width = \"250\"/>\n  <img src=\"files/AV2-2.png\" width = \"250\"/>\n  <img src=\"files/AV2-3.png\" width = \"250\"/>\n</p>\n\n----\n\n\n## Gettting Started\n\n### Install dependencies\n- Create a new conda virtual env\n```\nconda create --name DGFNet python=3.8\nconda activate DGFNet\n```\n\n- Install PyTorch according to your CUDA version. We recommend CUDA >= 11.1, PyTorch >= 1.8.0.\n```\nconda install pytorch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 cudatoolkit=11.6 -c pytorch -c conda-forge\n```\n\n- Install Argoverse 1 APIs, please follow [argoverse-api](https://github.com/argoai/argoverse-api).\n\n- Install other dependencies\n```\npip install scikit-image IPython tqdm ipdb tensorboard\n```\n\n### Train from scratch\n\n- Preprocess full Argoverse 1 motion forecasting dataset using the script:\n```\nsh scripts/argo_preproc.sh\n```\n\n- Launch training using the script:\n```\nsh scripts/DGFNet_train.sh\n```\n\n- For model evaluation, please refer to the following scripts:\n```\nsh scripts/DGFNet_eval.sh\n```\n\n### Test from scratch\n\n- Generate files that can be submitted on the [EvalAI](https://eval.ai/web/challenges/challenge-page/454/submission):\n```\nsh scripts/DGFNet_test.sh\n```\n\n## Contact\nIf you have any questions, please contact [Guipeng Xin](https://github.com/XinGP) via email (xinguipeng@whut.edu.cn).\n\n## Citation\nIf you find DGFNet is useful in your research or applications, please consider giving us a star ðŸŒŸ and citing it by the following BibTeX entry.\n```bibtex\n@article{xin2025multi,\n  title={Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network},\n  author={Xin, Guipeng and Chu, Duanfeng and Lu, Liping and Deng, Zejian and Lu, Yuang and Wu, Xigang},\n  journal={IEEE Robotics and Automation Letters},\n  year={2025},\n  publisher={IEEE}\n}\n\n@article{xin2024multi,\n  title={Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network},\n  author={Xin, Guipeng and Chu, Duanfeng and Lu, Liping and Deng, Zejian and Lu, Yuang and Wu, Xigang},\n  journal={arXiv preprint arXiv:2407.18551},\n  year={2024}}\n```\n\n## Acknowledgment\nWe would like to express sincere thanks to the authors of the following packages and tools:\n- [Simpl](https://github.com/HKUST-Aerial-Robotics/SIMPL)\n- [ADAPT](https://github.com/gorkaydemir/ADAPT)\n- [argoverse](https://github.com/argoverse)\n\n## License\nThis repository is licensed under [MIT license](https://github.com/XinGP/DGFNet/blob/main/LICENSE).\n",
    "readme_length": 4891
  },
  {
    "name": "otter-knowledge",
    "full_name": "IBM/otter-knowledge",
    "description": "Knowledge-enhanced learned representation enriches protein sequence and SMILES drug databases with a large Knowledge Graph fused from different sources. This improves results on TDC drug target binding affinity prediction benchmarks.",
    "stars": 61,
    "forks": 7,
    "language": "Python",
    "url": "https://github.com/IBM/otter-knowledge",
    "topics": [],
    "created_at": "2023-05-19T06:46:55Z",
    "updated_at": "2025-07-28T10:42:14Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# Otter Knowledge\nThe link to the preprint of our work: [Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery](https://arxiv.org/abs/2306.12802)\n\nAAAI 2024: [Knowledge Enhance Representation Learning for Drug Discovery](https://ojs.aaai.org/index.php/AAAI/article/view/28924/)\n\nKnowledge-enhanced learned representation enriches protein sequence and SMILES drug databases with a large multi-modal Knowledge Graph fused from different sources. This improves results on TDC drug target binding affinity prediction benchmarks.\n\nRecent research in representation learning utilizes large databases of proteins or molecules to acquire knowledge of drug and protein structures through unsupervised learning techniques. These pre-trained representations have proven to significantly enhance the accuracy of subsequent tasks, such as predicting the affinity between drugs and target proteins. In this study, we demonstrate that by incorporating knowledge graphs from diverse sources and modalities into the sequences or SMILES representation, we can further enrich the representation and achieve state-of-the-art results on established benchmark datasets. We provide preprocessed and integrated data obtained from 7 public sources, which encompass over 30M triples. Our multimodal knowledge graphs are directed labeled graphs where each node (entities and attributes of an antity) has a modality, a particular mode that qualifies its type (text, image, numerical value, protein, molecule, etc.) and edges have labels with well-defined meanings. Additionally, we make available the pre-trained models based on this data, along with the reported outcomes of their performance on three widely-used benchmark datasets for drug-target binding affinity prediction found in the Therapeutic Data Commons (TDC) benchmarks. Additionally, we make the source code for training models on benchmark datasets publicly available. Our objective in releasing these pre-trained models, accompanied by clean data for model pretraining and benchmark results, is to encourage research in knowledge-enhanced representation learning.\n\n### Datasets\nWe release 4 different Knowledge Graphs built from various existent datasets: [**Otter UBC**](https://huggingface.co/datasets/ibm/otter_uniprot_bindingdb_chembl), [**Otter PrimeKG**](https://huggingface.co/datasets/ibm/otter_primekg), [**Otter DUDe**](https://huggingface.co/datasets/ibm/otter_dude) and [**Otter STITCH**](https://huggingface.co/datasets/ibm/otter_stitch).\n\nTo build these Knowledge graphs, the framework takes as input a schema file (specified in JSON) defined manually by an expert user, with good knowledge of input data sources (for example CSV or XML files, or datasets stored in a document platform such as [Deep Search](https://ds4sd.github.io/)). The schema declaratively describes how to build the desired graph from a set of data sources. The schema defines entities (nodes in the KG) with their relevant data attributes and corresponding modality, as well as relations to other entities; these values are taken from fields present in the input data sources. Typically, one field value will be used to generate a unique identifier (UID) for the entity node, together with a namespace that indicates the origin of the entity within the original data source. Additionally, the JSON schema used as input to our framework permits user to define same_as relations among entity nodes in the graph, to represent that two nodes (potentially originating from two different input sources) are actually the same entity even if they have different UIDs. As an illustrative [example of such schema](https://github.com/IBM/otter-knowledge/blob/main/schemas/ubc_morgan_non_overlapping.json), the schema used to build Otter UBC Knowledge Graph, described below, can be found in the 'schemas' folder. The framework that takes these schemas to construct the KGs will be released in the future.\n\n#### Otter UBC\n[UBC](https://huggingface.co/datasets/ibm/otter_uniprot_bindingdb_chembl) is a KG dataset comprising entities (Proteins/Drugs) from Uniprot (U), BindingDB (B) and. ChemBL (C). It contains 6,207,654 triples.\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/IBM/otter-knowledge/main/assets/neurips_ubc.png\" alt=\"Overview of the creation of UBC\"/>\n</div>\n\n- **Uniprot** comprises of 573,227 proteins from SwissProt, which is the subset of manually curated entries within UniProt, including attributes with different modalities like the sequence (567,483 of them), full name, organism, protein family, description of its function, catalytics activity, pathways and its length. The number of edges are 38,665 of type *target_of* from Uniprot ids to both ChEMBL and Drugbank ids, and 196,133 interactants between Uniprot protein ids.\n- **BindingDB** consists of 2,656,221 data points, involving 1.2 million compounds and 9,000 targets. Instead of utilizing the affinity score, we generate a triple for each combination of drugs and proteins. In order to prevent any data leakage, we eliminate overlapping triples with the TDC DTI dataset. As a result, the dataset concludes with a total of 2,232,392 triples.\n- **ChemBL** comprises of drug-like bioactive molecules, 10,261 ChEMBL ids with their corresponding SMILES were downloaded from OpenTargets \\cite{opentargets}, from which 7,610 have a *sameAs* link to drugbank id molecules. \n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/IBM/otter-knowledge/main/assets/ubckg_example.jpg\" alt=\"Example of UBC\"/>\n</div>\n\n\n\n#### Otter PrimeKG\n[PrimeKg](https://huggingface.co/datasets/ibm/otter_primekg) (the Precision Medicine Knowledge Graph) integrates 20 biomedical resources, it describes 17,080 diseases with 4 million relationships. PrimeKG includes nodes describing Gene/Proteins (29,786) and Drugs (7,957 nodes). The MKG that we built from PrimeKG contains 13 modalities, 12,757,300 edges (154,130 data properties, and 12,603,170 object properties), including 642,150 edges describing interactions between proteins, 25,653 edges describing drug-protein interactions, and 2,672,628 describing interactions between drugs.  Our multimodal KG extends the original PrimeKG graph by adding SMILES for listed drugs, and sequences for listed Uniprot proteins\n\n#### Otter DUDe \n[DUDe](https://huggingface.co/datasets/ibm/otter_dude) comprises a collection of 22,886 active compounds and their corresponding affinities towards 102 targets. For our study, we utilized a preprocessed version of the DUDe, which includes 1,452,568 instances of drug-target interactions. To prevent any data leakage, we eliminated the negative interactions and the overlapping triples with the TDC DTI dataset. As a result, we were left with a total of 40,216 drug-target interaction pairs.\n\n#### Otter STITCH\n[STITCH](https://huggingface.co/datasets/ibm/otter_stitch) (Search Tool for Interacting Chemicals) is a database of known and predicted interactions between chemicals represented by SMILES strings and proteins whose sequences are taken from STRING database. Those interactions are obtained from computational prediction, from knowledge transfer between organisms, and from interactions aggregated from other (primary) databases. For the MKG curation we filtered only the interaction with highest confidence, i.e., the one which is higher 0.9. This resulted into 10,717,791 triples for 17,572 different chemicals and 1,886,496 different proteins. Furthermore, the graph was split into 5 roughly same size subgraphs and GNN was trained sequentially on each of them by upgrading the model trained using the previous subgraph.\n\n\nMerging datasets into a single source is complex due to the difficulty of aligning their structures automatically. For example, in the STITCH knowledge graph, the connection labeled \"interaction with\", denoting the relationship between chemicals and proteins, is established by analyzing their co-occurrence within a Pubmed abstract. This form of association might bear resemblance to the \"target of\" relation found in Uniprot, however confirming their equivalence presents a challenging task because co-orcurrence in a Pubmed abstract does not mean a protein is a target of a drug. Moreover, creating a comprehensive graph by combining multiple ones demands significant computational resources. \n\nTo address these issues, we propose an approach based on ensemble methods, and we show below that it can effectively capture information from these subset of our multimodal knowledge graphs without merging them.  \n\n\n### Models\nWe release 12 models, 3 per each dataset. Otter models are based on Graph Neural Networks (GNN) that propagates initial embeddings through a set of layers that upgrade input embedding according to the node neighbours. \nThe architecture of GNN consists of two main blocks: encoder and decoder. \n- For encoder we first define a projection layer which consists of a set of linear transformations for each node modality and projects nodes into common dimensionality, then we apply several multi-relational graph convolutional layers (R-GCN) which distinguish between different types of edges between source and target nodes by having a set of trainable parameters for each edge type. \n- For decoder we consider link prediction task, which consists of a scoring function that maps each triple of source and target nodes and the corresponding edge and maps that to a scalar number defined over interval [0; 1].\n\nFor link prediction, we consider three choices of scoring functions: DistMult, TransE and a Binary Classifier that are commonly used in the literature. The outcomes of scoring of each triple are then compared against actual labels using negative log likelihood loss function.\n\n- Flow control: One crucial aspect of pretraining the GNN involves addressing the disparity between the data accessible during pretraining and the data accessible during subsequent tasks. Specifically, during pretraining, there are numerous attributes associated with proteins or drugs, whereas during downstream fine-tuning, only amino acid sequences and SMILES are available. Consequently, during pretraining, we explore two scenarios: one which controls the information propagated to the Drug/Protein entities and one without such control. In our experiments, we present results for both cases to provide an insight on the impact of restricting information flow during pretraining on the subsequent tasks. \n- Noisy Links: An additional significant consideration is the presence of noisy links within the up-stream data and how they affect the downstream tasks. To investigate the potential impact on these tasks, we manually handpick a subset of links from each database that are relevant to drug discovery (see details in the Appendix). We then compare the outcomes when training the GNN using only these restricted links versus using all possible links present in the graphs. \n- Regression: Certain pretraining datasets, like Uniprot, contain numerical data properties. Hence, we incorporate an extra regression objective aimed at minimizing the root mean square error (MSE) of the predicted numerical data properties. In the learning process, we combine the regression objective and the link prediction objective to create a single objective function.  \n\n#### Models released:\n| Model Name                                                                      | Dataset | Scoring Type    | Noisy Links | Flow Control | Regression |\n|---------------------------------------------------------------------------------|:-------:|-----------------|:-----------:|:------------:|:----------:|\n| [otter_ubc_distmult](https://huggingface.co/ibm/otter_ubc_distmult)             |   UBC   | DistMult        |     No      |     Yes      |     No     |\n| [otter_ubc_transe](https://huggingface.co/ibm/otter_ubc_transe)                 |   UBC   |     TransE      |     No      |     Yes      |     No     |\n| [otter_ubc_classifier](https://huggingface.co/ibm/otter_ubc_classifier)         |   UBC   | Classifier Head |     No      |     Yes      |     No     |\n| [otter_primekg_distmult](https://huggingface.co/ibm/otter_primekg_distmult)     | PrimeKG | DistMult        |     No      |     Yes      |     No     |\n| [otter_primekg_transe](https://huggingface.co/ibm/otter_primekg_transe)         | PrimeKG |     TransE      |     No      |     Yes      |     No     |\n| [otter_primekg_classifier](https://huggingface.co/ibm/otter_primekg_classifier) | PrimeKG | Classifier Head |     No      |     Yes      |     No     |\n| [otter_dude_distmult](https://huggingface.co/ibm/otter_dude_distmult)           |  DUDe   | DistMult        |     No      |     Yes      |     No     |\n| [otter_dude_transe](https://huggingface.co/ibm/otter_dude_transe)               |  DUDe   |     TransE      |     No      |     Yes      |     No     |\n| [otter_dude_classifier](https://huggingface.co/ibm/otter_dude_classifier)       |  DUDe   | Classifier Head |     No      |     Yes      |     No     |\n\n#### Models results:\n<div align=\"left\">\n    <table class=\"tg\">\n    <thead>\n      <tr>\n        <th class=\"tg-0pky\">Dataset</th>\n        <th class=\"tg-c3ow\">DTI DG</th>\n        <th class=\"tg-c3ow\" colspan=\"3\">DAVIS</th>\n        <th class=\"tg-c3ow\" colspan=\"3\">KIBA</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td class=\"tg-0pky\">Splits</td>\n        <td class=\"tg-c3ow\">Temporal</td>\n        <td class=\"tg-c3ow\">Random</td>\n        <td class=\"tg-c3ow\">Target</td>\n        <td class=\"tg-c3ow\">Drug</td>\n        <td class=\"tg-c3ow\">Random</td>\n        <td class=\"tg-c3ow\">Target</td>\n        <td class=\"tg-c3ow\">Drug</td>\n      </tr>\n      <tr>\n        <td class=\"tg-0pky\"><a href=\"https://tdcommons.ai/benchmark/dti_dg_group/bindingdb_patent/\">TDC LeaderBoard</a></td>\n        <td class=\"tg-c3ow\">0.538</td>\n        <td class=\"tg-c3ow\">NA</td>\n        <td class=\"tg-c3ow\">NA</td>\n        <td class=\"tg-c3ow\">NA</td>\n        <td class=\"tg-c3ow\">NA</td>\n        <td class=\"tg-c3ow\">NA</td>\n        <td class=\"tg-c3ow\">NA</td>\n      </tr>\n\t <tr>\n        <td class=\"tg-0pky\"><a>ESM+Morgan-Fingerprint</a></td>\n        <td class=\"tg-c3ow\">0.569</td>\n        <td class=\"tg-c3ow\">0.805</td>\n        <td class=\"tg-c3ow\">0.554</td>\n        <td class=\"tg-c3ow\">0.264</td>\n        <td class=\"tg-c3ow\">0.852</td>\n        <td class=\"tg-c3ow\">0.630</td>\n        <td class=\"tg-c3ow\">0.576</td>\n      </tr>\n      <tr>\n        <td class=\"tg-0pky\"><a href=\"https://huggingface.co/ibm/otter_ubc_distmult\">otter_ubc_distmult</a></td>\n        <td class=\"tg-c3ow\">0.578</td>\n        <td class=\"tg-c3ow\">0.808</td>\n        <td class=\"tg-c3ow\">0.572</td>\n        <td class=\"tg-c3ow\">0.152</td>\n        <td class=\"tg-c3ow\">0.859</td>\n        <td class=\"tg-c3ow\">0.627</td>\n        <td class=\"tg-c3ow\">0.593</td>\n      </tr>\n      <tr>\n        <td class=\"tg-0pky\"><a href=\"https://huggingface.co/ibm/otter_ubc_transe\">otter_ubc_transe</a></td>\n        <td class=\"tg-c3ow\">0.577</td>\n        <td class=\"tg-c3ow\">0.807</td>\n        <td class=\"tg-c3ow\">0.571</td>\n        <td class=\"tg-c3ow\">0.130</td>\n        <td class=\"tg-c3ow\">0.858</td>\n        <td class=\"tg-c3ow\">0.644</td>\n        <td class=\"tg-c3ow\">0.583</td>\n      </tr>\n      <tr>\n        <td class=\"tg-0pky\"><a href=\"https://huggingface.co/ibm/otter_ubc_classifier\">otter_ubc_classifier</a></td>\n        <td class=\"tg-c3ow\">0.580</td>\n        <td class=\"tg-c3ow\">0.810</td>\n        <td class=\"tg-c3ow\">0.573</td>\n        <td class=\"tg-c3ow\">0.104</td>\n        <td class=\"tg-c3ow\">0.861</td>\n        <td class=\"tg-c3ow\">0.631</td>\n        <td class=\"tg-c3ow\">0.616</td>\n      </tr>\n      <tr>\n        <td class=\"tg-0pky\"><a href=\"https://huggingface.co/ibm/otter_primekg_distmult\">otter_primekg_distmult</a></td>\n        <td class=\"tg-c3ow\">0.575</td>\n        <td class=\"tg-c3ow\">0.806</td>\n        <td class=\"tg-c3ow\">0.571</td>\n        <td class=\"tg-c3ow\">0.162</td>\n        <td class=\"tg-c3ow\">0.856</td>\n        <td class=\"tg-c3ow\">0.611</td>\n        <td class=\"tg-c3ow\">0.617</td>\n      </tr>\n      <tr>\n        <td class=\"tg-0pky\"><a href=\"https://huggingface.co/ibm/otter_primekg_transe\">otter_primekg_transe</a></td>\n        <td class=\"tg-c3ow\">0.573</td>\n        <td class=\"tg-c3ow\">0.807</td>\n        <td class=\"tg-c3ow\">0.568</td>\n        <td class=\"tg-c3ow\">0.186</td>\n        <td class=\"tg-c3ow\">0.858</td>\n        <td class=\"tg-c3ow\">0.642</td>\n        <td class=\"tg-c3ow\">0.607</td>\n      </tr>\n      <tr>\n        <td class=\"tg-0pky\"><a href=\"https://huggingface.co/ibm/otter_primekg_classifier\">otter_primekg_classifier</a></td>\n        <td class=\"tg-c3ow\">0.576</td>\n        <td class=\"tg-c3ow\">0.813</td>\n        <td class=\"tg-c3ow\">0.576</td>\n        <td class=\"tg-c3ow\">0.133</td>\n        <td class=\"tg-c3ow\">0.861</td>\n        <td class=\"tg-c3ow\">0.630</td>\n        <td class=\"tg-c3ow\">0.635</td>\n      </tr>\n      <tr>\n        <td class=\"tg-0pky\"><a href=\"https://huggingface.co/ibm/otter_dude_distmult\">otter_dude_distmult</a></td>\n        <td class=\"tg-c3ow\">0.577</td>\n        <td class=\"tg-c3ow\">0.805</td>\n        <td class=\"tg-c3ow\">0.573</td>\n        <td class=\"tg-c3ow\">0.132</td>\n        <td class=\"tg-c3ow\">0.857</td>\n        <td class=\"tg-c3ow\">0.650</td>\n        <td class=\"tg-c3ow\">0.607</td>\n      </tr>\n      <tr>\n        <td class=\"tg-0pky\"><a href=\"https://huggingface.co/ibm/otter_dude_transe\">otter_dude_transe</a></td>\n        <td class=\"tg-c3ow\">0.576</td>\n        <td class=\"tg-c3ow\">0.807</td>\n        <td class=\"tg-c3ow\">0.570</td>\n        <td class=\"tg-c3ow\">0.170</td>\n        <td class=\"tg-c3ow\">0.856</td>\n        <td class=\"tg-c3ow\">0.653</td>\n        <td class=\"tg-c3ow\">0.604</td>\n      </tr>\n      <tr>\n        <td class=\"tg-0pky\"><a href=\"https://huggingface.co/ibm/otter_dude_classifier\">otter_dude_classifier</a></td>\n        <td class=\"tg-c3ow\">0.579</td>\n        <td class=\"tg-c3ow\">0.808</td>\n        <td class=\"tg-c3ow\">0.574</td>\n        <td class=\"tg-c3ow\">0.167</td>\n        <td class=\"tg-c3ow\">0.860</td>\n        <td class=\"tg-c3ow\">0.641</td>\n        <td class=\"tg-c3ow\">0.630</td>\n      </tr>\n      <tr>\n        <td class=\"tg-0pky\"><a>otter-knowledge-ensemble</a></td>\n        <td class=\"tg-c3ow\">0.588</td>\n        <td class=\"tg-c3ow\">0.839</td>\n        <td class=\"tg-c3ow\">0.578</td>\n        <td class=\"tg-c3ow\">0.168</td>\n        <td class=\"tg-c3ow\">0.886</td>\n        <td class=\"tg-c3ow\">0.678</td>\n        <td class=\"tg-c3ow\">0.638</td>\n      </tr>\n    </tbody>\n    </table>\n</div>\n\n## How to use it\n\n### Installation\nClone the repo:\n```\ngit clone https://github.com/IBM/otter-knowledge.git\ncd otter-knowledge\n```\n\nInstall the requirements:\n```\npip install -r requirements.txt\n```\n\n### Run inference\n```\nusage: inference.py [-h] --input_path INPUT_PATH [--sequence_column SEQUENCE_COLUMN] [--input_type INPUT_TYPE] [--model_path MODEL_PATH] --output_path OUTPUT_PATH [--batch_size BATCH_SIZE] [--no_cuda]\n\nInference\n\noptions:\n  -h, --help            show this help message and exit\n  --input_path INPUT_PATH\n                        Path to the csv file with the sequence/smiles\n  --sequence_column SEQUENCE_COLUMN\n                        Name of the column with sequence/smiles information for proteins or molecules\n  --input_type INPUT_TYPE\n                        Type of the sequences. Options: Drug; Protein\n  --model_path MODEL_PATH\n                        Path to the model or name of the model in the HuggingfaceHub\n  --output_path OUTPUT_PATH\n                        Path to the output embedding file.\n  --batch_size BATCH_SIZE\n                        Batch size to use.\n  --no_cuda             If set to True, CUDA won't be used even if available.\n\n```\n- Run the inference for Proteins:\n  \n  *Replace test_data with the path to a CSV file containing the protein sequences, name_of_the_column with the name of the column of the protein sequence in the CSV and output_path with the filename of the JSON file to be created with the embeddings.*\n```python\npython inference.py --input_path test_data --sequence_column name_of_the_column --model_path ibm/otter_dude_distmult --output_path output_path\n```\n- Run the inference for Drugs:\n\n  *Replace test_data with the path to a CSV file containing the Drug SMILES, name_of_the_column with the name of the column of the SMILES in the CSV and output_path with the filename of the JSON file to be created with the embeddings.*.*\n```python\npython inference.py --input_path test_data --sequence_column name_of_the_column input_type Drug --relation_name smiles --model_path ibm/otter_dude_distmult --output_path output_path\n```\n\n### Benchmarks\n#### Training benchmark models\n\nWe assume that you have used the [inference script](#run-inference) to generate embeddings for training and test proteins/drugs. The embeddings of training and test proteins/drugs should be combined into files with the following format that keep computed embeddings of drugs/proteins. It is important to notice that the inference only generates embeddings for either drugs or proteins so you need to combine and convert them into the following format so that they can be used as input to the model benchmark training as explained below.\n```json\n{\n\t\"Drug\": {\n\t\t\"CN(C)CC(=O)NC(COc1cncc(-c2ccc3cnccc3c2)c1)Cc1c[nH]c2ccccc12\": [\n            -1.2718517780303955,\n            0.6045345664024353,\n            -0.03671235218644142,\n            0.9915799498558044,\n            -0.7146453857421875],\n      \"Cc1sc2ncnc(N)c2c1-c1ccc(NC(=O)Nc2cc(C(F)(F)F)ccc2F)cc1\": [\n            -0.6596673130989075,\n            0.2838267683982849,\n            -0.042177166789770126,\n            0.7447476387023926,\n            -0.27911311388015747]\n\t},\n\t\"Target\": {\n        \"MTLDVGPEDELPDWAAAKEFYQKYDPKDVIGRGVSSVVRRCVHRATGHE\": [\n            -0.46595990657806396,\n            -0.297667533159256,\n            -0.048857495188713074]\n\t}\n}\n```\n\nTraining benchmark models can be done with the following example command:\n\n```\npython -m benchmarks.dti.train --train train_val.csv --test test.csv --train_embeddings train_val_embeddings.json --test_embeddings test_embeddings.json\n```\nWhere the input to the script are:\n\n - train_val.csv the path to the csv file that keep the training data from [TDC benchmarks](https://tdcommons.ai/multi_pred_tasks/dti/)\n -  test.csv the path to the csv file that keep the test data from [TDC benchmarks](https://tdcommons.ai/multi_pred_tasks/dti/)\n - the input files train_val_embeddings.json and test_embeddings.json keeps the computed embeddings of train/test protein/drugs respectively in the format that we have discussed above.\n\nThere are other optional hyperparameter you can set such as the learning rate, the number of training steps etc as below\n```\nusage: train.py [-h] [--train TRAIN] [--test TEST] [--train_embeddings TRAIN_EMBEDDINGS] [--test_embeddings TEST_EMBEDDINGS] [--lr LR] [--steps STEPS] [--seeds SEEDS] [--batch_size BATCH_SIZE] [--is_initial_embeddings IS_INITIAL_EMBEDDINGS]\n                [--gnn_embedding_dim GNN_EMBEDDING_DIM]\n\nTDC DG training\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --train TRAIN         Root directory with the training data\n  --test TEST           Root directory with the test data\n  --train_embeddings TRAIN_EMBEDDINGS\n                        Root directory with the embeddings of training drugs and proteins.\n  --test_embeddings TEST_EMBEDDINGS\n                        Root directory with the embeddings of test drugs and proteins.\n  --lr LR               Learning rate.\n  --steps STEPS         Maximum number of training steps\n  --seeds SEEDS         Random seeds.\n  --batch_size BATCH_SIZE\n                        Mini batch size.\n  --is_initial_embeddings IS_INITIAL_EMBEDDINGS\n                        Set this value to yes if want to train with initial embeddings without GNN embeddings.\n  --gnn_embedding_dim GNN_EMBEDDING_DIM\n                        Size of the GNN embeddings.\n```\n\n#### Ensemble learning\nEnsemble method combines the predictions trained on different GNN embeddings provided by different pretrained models. The following example command run ensemble learning:\n\n\n```\npython -m benchmarks.dti.train_ensemble_model --train train_val.csv --test test.csv --train_embeddings train_embeddings.txt --test_embeddings test_embeddings.txt\n```\n\nWhere the input to the script are:\n\n - train_val.csv the path to the csv file that keep the training data from [TDC benchmarks](https://tdcommons.ai/multi_pred_tasks/dti/)\n -  test.csv the path to the csv file that keep the test data from [TDC benchmarks](https://tdcommons.ai/multi_pred_tasks/dti/)\n - the input files train_embeddings.txt and test_embeddings.txt keeps keep a list of train/test embedding files (each line in the file is the path to the computed embeddings files).\n\nFor example, the content of the train_embeddings.txt may look like follows:\n```\ntrain_val_embeddings_1.json\ntrain_val_embeddings_2.json\ntrain_val_embeddings_3.json\n```\n\nAnd the content of the test_embeddings.txt may look like follows:\n\n```\ntest_embeddings_1.json\ntest_embeddings_2.json\ntest_embeddings_3.json\n```\n\nWhere the train_val_embeddings_1.json and test_embeddings_1.json are the computed GNN embeddings of train/test drugs/proteins respectively using a pretrained models.\n",
    "readme_length": 25328
  },
  {
    "name": "DIPS-Plus",
    "full_name": "BioinfoMachineLearning/DIPS-Plus",
    "description": "The Enhanced Database of Interacting Protein Structures for Interface Prediction",
    "stars": 50,
    "forks": 8,
    "language": "Python",
    "url": "https://github.com/BioinfoMachineLearning/DIPS-Plus",
    "topics": [
      "bioinformatics",
      "datasets",
      "deep-learning",
      "machine-learning",
      "proteins"
    ],
    "created_at": "2021-05-27T03:01:46Z",
    "updated_at": "2025-08-29T18:39:32Z",
    "homepage": "https://zenodo.org/record/5134732",
    "license": "GNU General Public License v3.0",
    "readme": "<div align=\"center\">\n\n# DIPS-Plus\n\nThe Enhanced Database of Interacting Protein Structures for Interface Prediction\n\n[![Paper](http://img.shields.io/badge/paper-arxiv.2106.04362-B31B1B.svg)](https://www.nature.com/articles/s41597-023-02409-3)  [![CC BY 4.0][cc-by-shield]][cc-by] [![Primary Data DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5134732.svg)](https://doi.org/10.5281/zenodo.5134732) [![Supplementary Data DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8140981.svg)](https://doi.org/10.5281/zenodo.8140981)\n\n[cc-by]: http://creativecommons.org/licenses/by/4.0/\n[cc-by-image]: https://i.creativecommons.org/l/by/4.0/88x31.png\n[cc-by-shield]: https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg\n\n[comment]: <> ([![Conference]&#40;http://img.shields.io/badge/NeurIPS-2021-4b44ce.svg&#41;]&#40;https://papers.nips.cc/book/advances-in-neural-information-processing-systems-35-2021&#41;)\n\n[![OMol25 on Hugging Face](https://img.shields.io/badge/-OMol25-FDEE21?style=for-the-badge&logo=HuggingFace&logoColor=black)](https://huggingface.co/facebook/OMol25)\n\n[<img src=\"https://twixes.gallerycdn.vsassets.io/extensions/twixes/pypi-assistant/1.0.3/1589834023190/Microsoft.VisualStudio.Services.Icons.Default\" width=\"50\"/>](https://pypi.org/project/DIPS-Plus/)\n\n</div>\n\n## Versioning\n\n* Version 1.0.0: Initial release of DIPS-Plus and DB5-Plus (DOI: 10.5281/zenodo.4815267)\n* Version 1.1.0: Minor updates to DIPS-Plus and DB5-Plus' tar archives (DOI: 10.5281/zenodo.5134732)\n  * DIPS-Plus' final 'raw' tar archive now includes standardized 80%-20% lists of filenames for training and validation, respectively\n  * DB5-Plus' final 'raw' tar archive now includes (optional) standardized lists of filenames for training and validation, respectively\n  * DB5-Plus' final 'raw' tar archive now also includes a corrected (i.e. de-duplicated) list of filenames for its 55 test complexes\n    * Benchmark results included in our paper were run after this issue was resolved\n    * However, if you ran experiments using DB5-Plus' filename list for its test complexes, please re-run them using the latest list\n* Version 1.2.0: Minor additions to DIPS-Plus tar archives, including new residue-level intrinsic disorder region annotations and raw Jackhmmer-small BFD MSAs (Supplementary Data DOI: 10.5281/zenodo.8071136)\n* Version 1.3.0: Minor additions to DIPS-Plus tar archives, including new FoldSeek-based structure-focused training and validation splits, residue-level (scalar) disorder propensities, and a Graphein-based featurization pipeline (Supplementary Data DOI: 10.5281/zenodo.8140981)\n\n## How to set up\n\nFirst, download Mamba (if not already downloaded):\n```bash\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\nbash Mambaforge-$(uname)-$(uname -m).sh  # Accept all terms and install to the default location\nrm Mambaforge-$(uname)-$(uname -m).sh  # (Optionally) Remove installer after using it\nsource ~/.bashrc  # Alternatively, one can restart their shell session to achieve the same result\n```\n\nThen, create and configure Mamba environment:\n\n```bash\n# Clone project:\ngit clone https://github.com/BioinfoMachineLearning/DIPS-Plus\ncd DIPS-Plus\n\n# Create Conda environment using local 'environment.yml' file:\nmamba env create -f environment.yml\nconda activate DIPS-Plus  # Note: One still needs to use `conda` to (de)activate environments\n\n# Install local project as package:\npip3 install -e .\n```\n\nTo install PSAIA for feature generation, install GCC 10 for PSAIA:\n\n```bash\n# Install GCC 10 for Ubuntu 20.04:\nsudo apt install software-properties-common\nsudo add-apt-repository ppa:ubuntu-toolchain-r/ppa\nsudo apt update\nsudo apt install gcc-10 g++-10\n\n# Or install GCC 10 for Arch Linux/Manjaro:\nyay -S gcc10\n```\n\nThen install QT4 for PSAIA:\n\n```bash\n# Install QT4 for Ubuntu 20.04:\nsudo add-apt-repository ppa:rock-core/qt4\nsudo apt update\nsudo apt install libqt4* libqtcore4 libqtgui4 libqtwebkit4 qt4* libxext-dev\n\n# Or install QT4 for Arch Linux/Manjaro:\nyay -S qt4\n```\n\nConclude by compiling PSAIA from source:\n\n```bash\n# Select the location to install the software:\nMY_LOCAL=~/Programs\n\n# Download and extract PSAIA's source code:\nmkdir \"$MY_LOCAL\"\ncd \"$MY_LOCAL\"\nwget http://complex.zesoi.fer.hr/data/PSAIA-1.0-source.tar.gz\ntar -xvzf PSAIA-1.0-source.tar.gz\n\n# Compile PSAIA (i.e., a GUI for PSA):\ncd PSAIA_1.0_source/make/linux/psaia/\nqmake-qt4 psaia.pro\nmake\n\n# Compile PSA (i.e., the protein structure analysis (PSA) program):\ncd ../psa/\nqmake-qt4 psa.pro\nmake\n\n# Compile PIA (i.e., the protein interaction analysis (PIA) program):\ncd ../pia/\nqmake-qt4 pia.pro\nmake\n\n# Test run any of the above-compiled programs:\ncd \"$MY_LOCAL\"/PSAIA_1.0_source/bin/linux\n# Test run PSA inside a GUI:\n./psaia/psaia\n# Test run PIA through a terminal:\n./pia/pia\n# Test run PSA through a terminal:\n./psa/psa\n```\n\nLastly, install Docker following the instructions from https://docs.docker.com/engine/install/\n\n## How to generate protein feature inputs\nIn our [feature generation notebook](notebooks/feature_generation.ipynb), we provide examples of how users can generate the protein features described in our [accompanying manuscript](https://arxiv.org/abs/2106.04362) for individual protein inputs.\n\n## How to use data\nIn our [data usage notebook](notebooks/data_usage.ipynb), we provide examples of how users might use DIPS-Plus (or DB5-Plus) for downstream analysis or prediction tasks. For example, to train a new NeiA model with DB5-Plus as its cross-validation dataset, first download DB5-Plus' raw files and process them via the `data_usage` notebook:\n\n```bash\nmkdir -p project/datasets/DB5/final\nwget https://zenodo.org/record/5134732/files/final_raw_db5.tar.gz -O project/datasets/DB5/final/final_raw_db5.tar.gz\ntar -xzf project/datasets/DB5/final/final_raw_db5.tar.gz -C project/datasets/DB5/final/\n\n# To process these raw files for training and subsequently train a model:\npython3 notebooks/data_usage.py\n```\n\n## How to split data using FoldSeek\nWe provide users with the [ability](https://github.com/BioinfoMachineLearning/DIPS-Plus/blob/75775d98f0923faf11fb50639eb58cd510a10ffd/project/datasets/builder/partition_dataset_filenames.py#L486) to perform structure-based splits of the complexes in DIPS-Plus using FoldSeek. This script is designed to allow users to customize how stringent one would like FoldSeek's searches to be for structure-based splitting. Moreover, we provide standardized structure-based splits of DIPS-Plus' complexes in the corresponding [supplementary Zenodo data record](https://zenodo.org/record/8140981).\n\n## How to featurize DIPS-Plus complexes using Graphein\nIn the new [graph featurization script](https://github.com/BioinfoMachineLearning/DIPS-Plus/blob/main/project/datasets/builder/add_new_feature.py), we provide an example of how users may install new Expasy protein scale features using the Graphein library. The script is designed to be amenable to simple user customization such that users can use this script to insert arbitrary new Graphein-based features into each DIPS-Plus complex's pair file, for downstream tasks.\n\n## Standard DIPS-Plus directory structure\n\n```\nDIPS-Plus\nâ”‚\nâ””â”€â”€â”€project\n     â”‚\n     â””â”€â”€â”€datasets\n         â”‚\n         â””â”€â”€â”€DB5\n         â”‚   â”‚\n         â”‚   â””â”€â”€â”€final\n         â”‚   â”‚   â”‚\n         â”‚   â”‚   â””â”€â”€â”€processed  # task-ready features for each dataset example\n         â”‚   â”‚   â”‚\n         â”‚   â”‚   â””â”€â”€â”€raw  # generic features for each dataset example\n         â”‚   â”‚\n         â”‚   â””â”€â”€â”€interim\n         â”‚   â”‚   â”‚\n         â”‚   â”‚   â””â”€â”€â”€complexes  # metadata for each dataset example\n         â”‚   â”‚   â”‚\n         â”‚   â”‚   â””â”€â”€â”€external_feats  # features curated for each dataset example using external tools\n         â”‚   â”‚   â”‚\n         â”‚   â”‚   â””â”€â”€â”€pairs  # pair-wise features for each dataset example\n         â”‚   â”‚\n         â”‚   â””â”€â”€â”€raw  # raw PDB data downloads for each dataset example\n         â”‚\n         â””â”€â”€â”€DIPS\n             â”‚\n             â””â”€â”€â”€filters  # filters to apply to each (un-pruned) dataset example\n             â”‚\n             â””â”€â”€â”€final\n             â”‚   â”‚\n             â”‚   â””â”€â”€â”€processed  # task-ready features for each dataset example\n             â”‚   â”‚\n             â”‚   â””â”€â”€â”€raw  # generic features for each dataset example\n             â”‚\n             â””â”€â”€â”€interim\n             â”‚   â”‚\n             â”‚   â””â”€â”€â”€complexes  # metadata for each dataset example\n             â”‚   â”‚\n             â”‚   â””â”€â”€â”€external_feats  # features curated for each dataset example using external tools\n             â”‚   â”‚\n             â”‚   â””â”€â”€â”€pairs-pruned  # filtered pair-wise features for each dataset example\n             â”‚   â”‚\n             â”‚   â””â”€â”€â”€parsed  # pair-wise features for each dataset example after initial parsing\n             â”‚\n             â””â”€â”€â”€raw\n                 â”‚\n                 â””â”€â”€â”€pdb  # raw PDB data downloads for each dataset example\n```\n\n## How to compile DIPS-Plus from scratch\n\nRetrieve protein complexes from the RCSB PDB and build out directory structure:\n\n```bash\n# Remove all existing training/testing sample lists\nrm project/datasets/DIPS/final/raw/pairs-postprocessed.txt project/datasets/DIPS/final/raw/pairs-postprocessed-train.txt project/datasets/DIPS/final/raw/pairs-postprocessed-val.txt project/datasets/DIPS/final/raw/pairs-postprocessed-test.txt\n\n# Create data directories (if not already created):\nmkdir project/datasets/DIPS/raw project/datasets/DIPS/raw/pdb project/datasets/DIPS/interim project/datasets/DIPS/interim/pairs-pruned project/datasets/DIPS/interim/external_feats project/datasets/DIPS/final project/datasets/DIPS/final/raw project/datasets/DIPS/final/processed\n\n# Download the raw PDB files:\nrsync -rlpt -v -z --delete --port=33444 --include='*.gz' --include='*.xz' --include='*/' --exclude '*' \\\nrsync.rcsb.org::ftp_data/biounit/coordinates/divided/ project/datasets/DIPS/raw/pdb\n\n# Extract the raw PDB files:\npython3 project/datasets/builder/extract_raw_pdb_gz_archives.py project/datasets/DIPS/raw/pdb\n\n# Process the raw PDB data into associated pair files:\npython3 project/datasets/builder/make_dataset.py project/datasets/DIPS/raw/pdb project/datasets/DIPS/interim --num_cpus 28 --source_type rcsb --bound\n\n# Apply additional filtering criteria:\npython3 project/datasets/builder/prune_pairs.py project/datasets/DIPS/interim/pairs project/datasets/DIPS/filters project/datasets/DIPS/interim/pairs-pruned --num_cpus 28\n\n# Generate externally-sourced features:\npython3 project/datasets/builder/generate_psaia_features.py \"$PSAIADIR\" \"$PROJDIR\"/project/datasets/builder/psaia_config_file_dips.txt \"$PROJDIR\"/project/datasets/DIPS/raw/pdb \"$PROJDIR\"/project/datasets/DIPS/interim/parsed \"$PROJDIR\"/project/datasets/DIPS/interim/pairs-pruned \"$PROJDIR\"/project/datasets/DIPS/interim/external_feats --source_type rcsb\npython3 project/datasets/builder/generate_hhsuite_features.py \"$PROJDIR\"/project/datasets/DIPS/interim/parsed \"$PROJDIR\"/project/datasets/DIPS/interim/pairs-pruned \"$HHSUITE_DB\" \"$PROJDIR\"/project/datasets/DIPS/interim/external_feats --num_cpu_jobs 4 --num_cpus_per_job 8 --num_iter 2 --source_type rcsb --write_file  # Note: After this, one needs to re-run this command with `--read_file` instead\n\n# Generate multiple sequence alignments (MSAs) using a smaller sequence database (if not already created using the standard BFD):\nDOWNLOAD_DIR=\"$HHSUITE_DB_DIR\" && ROOT_DIR=\"${DOWNLOAD_DIR}/small_bfd\" && SOURCE_URL=\"https://storage.googleapis.com/alphafold-databases/reduced_dbs/bfd-first_non_consensus_sequences.fasta.gz\" && BASENAME=$(basename \"${SOURCE_URL}\") && mkdir --parents \"${ROOT_DIR}\" && aria2c \"${SOURCE_URL}\" --dir=\"${ROOT_DIR}\" && pushd \"${ROOT_DIR}\" && gunzip \"${ROOT_DIR}/${BASENAME}\" && popd  # e.g., Download the small BFD\npython3 project/datasets/builder/generate_hhsuite_features.py \"$PROJDIR\"/project/datasets/DIPS/interim/parsed \"$PROJDIR\"/project/datasets/DIPS/interim/pairs-pruned \"$HHSUITE_DB_DIR\"/small_bfd \"$PROJDIR\"/project/datasets/DIPS/interim/external_feats --num_cpu_jobs 4 --num_cpus_per_job 8 --num_iter 2 --source_type rcsb --generate_msa_only --write_file  # Note: After this, one needs to re-run this command with `--read_file` instead\n\n# Identify interfaces within intrinsically disordered regions (IDRs) #\n# (1) Pull down the Docker image for `flDPnn`\ndocker pull docker.io/sinaghadermarzi/fldpnn\n# (2) For all sequences in the dataset, predict which interface residues reside within IDRs\npython3 project/datasets/builder/annotate_idr_interfaces.py \"$PROJDIR\"/project/datasets/DIPS/final/raw --num_cpus 16\n\n# Add new features to the filtered pairs, ensuring that the pruned pairs' original PDB files are stored locally for DSSP:\npython3 project/datasets/builder/download_missing_pruned_pair_pdbs.py \"$PROJDIR\"/project/datasets/DIPS/raw/pdb \"$PROJDIR\"/project/datasets/DIPS/interim/pairs-pruned --num_cpus 32 --rank \"$1\" --size \"$2\"\npython3 project/datasets/builder/postprocess_pruned_pairs.py \"$PROJDIR\"/project/datasets/DIPS/raw/pdb \"$PROJDIR\"/project/datasets/DIPS/interim/pairs-pruned \"$PROJDIR\"/project/datasets/DIPS/interim/external_feats \"$PROJDIR\"/project/datasets/DIPS/final/raw --num_cpus 32\n\n# Partition dataset filenames, aggregate statistics, and impute missing features\npython3 project/datasets/builder/partition_dataset_filenames.py \"$PROJDIR\"/project/datasets/DIPS/final/raw --source_type rcsb --filter_by_atom_count True --max_atom_count 17500 --rank \"$1\" --size \"$2\"\npython3 project/datasets/builder/collect_dataset_statistics.py \"$PROJDIR\"/project/datasets/DIPS/final/raw --rank \"$1\" --size \"$2\"\npython3 project/datasets/builder/log_dataset_statistics.py \"$PROJDIR\"/project/datasets/DIPS/final/raw --rank \"$1\" --size \"$2\"\npython3 project/datasets/builder/impute_missing_feature_values.py \"$PROJDIR\"/project/datasets/DIPS/final/raw --impute_atom_features False --advanced_logging False --num_cpus 32 --rank \"$1\" --size \"$2\"\n\n# Optionally convert each postprocessed (final 'raw') complex into a pair of DGL graphs (final 'processed') with labels\npython3 project/datasets/builder/convert_complexes_to_graphs.py \"$PROJDIR\"/project/datasets/DIPS/final/raw \"$PROJDIR\"/project/datasets/DIPS/final/processed --num_cpus 32 --edge_dist_cutoff 15.0 --edge_limit 5000 --self_loops True --rank \"$1\" --size \"$2\"\n```\n\n## How to assemble DB5-Plus\n\nFetch prepared protein complexes from Dataverse:\n\n```bash\n# Download the prepared DB5 files:\nwget -O project/datasets/DB5.tar.gz https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/H93ZKK/BXXQCG\n\n# Extract downloaded DB5 archive:\ntar -xzf project/datasets/DB5.tar.gz --directory project/datasets/\n\n# Remove (now) redundant DB5 archive and other miscellaneous files:\nrm project/datasets/DB5.tar.gz project/datasets/DB5/.README.swp\nrm -rf project/datasets/DB5/interim project/datasets/DB5/processed\n\n# Create relevant interim and final data directories:\nmkdir project/datasets/DB5/interim project/datasets/DB5/interim/external_feats\nmkdir project/datasets/DB5/final project/datasets/DB5/final/raw project/datasets/DB5/final/processed\n\n# Construct DB5 dataset pairs:\npython3 project/datasets/builder/make_dataset.py \"$PROJDIR\"/project/datasets/DB5/raw \"$PROJDIR\"/project/datasets/DB5/interim --num_cpus 32 --source_type db5 --unbound\n\n# Generate externally-sourced features:\npython3 project/datasets/builder/generate_psaia_features.py \"$PSAIADIR\" \"$PROJDIR\"/project/datasets/builder/psaia_config_file_db5.txt \"$PROJDIR\"/project/datasets/DB5/raw \"$PROJDIR\"/project/datasets/DB5/interim/parsed \"$PROJDIR\"/project/datasets/DB5/interim/parsed \"$PROJDIR\"/project/datasets/DB5/interim/external_feats --source_type db5\npython3 project/datasets/builder/generate_hhsuite_features.py \"$PROJDIR\"/project/datasets/DB5/interim/parsed \"$PROJDIR\"/project/datasets/DB5/interim/parsed \"$HHSUITE_DB\" \"$PROJDIR\"/project/datasets/DB5/interim/external_feats --num_cpu_jobs 4 --num_cpus_per_job 8 --num_iter 2 --source_type db5 --write_file\n\n# Add new features to the filtered pairs:\npython3 project/datasets/builder/postprocess_pruned_pairs.py \"$PROJDIR\"/project/datasets/DB5/raw \"$PROJDIR\"/project/datasets/DB5/interim/pairs \"$PROJDIR\"/project/datasets/DB5/interim/external_feats \"$PROJDIR\"/project/datasets/DB5/final/raw --num_cpus 32 --source_type db5\n\n# Partition dataset filenames, aggregate statistics, and impute missing features\npython3 project/datasets/builder/partition_dataset_filenames.py \"$PROJDIR\"/project/datasets/DB5/final/raw --source_type db5 --rank \"$1\" --size \"$2\"\npython3 project/datasets/builder/collect_dataset_statistics.py \"$PROJDIR\"/project/datasets/DB5/final/raw --rank \"$1\" --size \"$2\"\npython3 project/datasets/builder/log_dataset_statistics.py \"$PROJDIR\"/project/datasets/DB5/final/raw --rank \"$1\" --size \"$2\"\npython3 project/datasets/builder/impute_missing_feature_values.py \"$PROJDIR\"/project/datasets/DB5/final/raw --impute_atom_features False --advanced_logging False --num_cpus 32 --rank \"$1\" --size \"$2\"\n\n# Optionally convert each postprocessed (final 'raw') complex into a pair of DGL graphs (final 'processed') with labels\npython3 project/datasets/builder/convert_complexes_to_graphs.py \"$PROJDIR\"/project/datasets/DB5/final/raw \"$PROJDIR\"/project/datasets/DB5/final/processed --num_cpus 32 --edge_dist_cutoff 15.0 --edge_limit 5000 --self_loops True --rank \"$1\" --size \"$2\"\n```\n\n## How to reassemble DIPS-Plus' \"interim\" external features\n\nWe split the (tar.gz) archive into eight separate parts with\n'split -b 4096M interim_external_feats_dips.tar.gz \"interim_external_feats_dips.tar.gz.part\"'\nto upload it to the dataset's primary Zenodo record, so to recover the original archive:\n\n```bash\n# Reassemble external features archive with 'cat'\ncat interim_external_feats_dips.tar.gz.parta* >interim_external_feats_dips.tar.gz\n```\n\n## Python 2 to 3 pickle file solution\n\nWhile using Python 3 in this project, you may encounter the following error if you try to postprocess '.dill' pruned\npairs that were created using Python 2.\n\nModuleNotFoundError: No module named 'dill.dill'\n\n1. To resolve it, ensure that the 'dill' package's version is greater than 0.3.2.\n2. If the problem persists, edit the pickle.py file corresponding to your Conda environment's Python 3 installation (\n   e.g. ~/DIPS-Plus/venv/lib/python3.8/pickle.py) and add the statement\n\n```python\nif module == 'dill.dill': module = 'dill._dill'\n```\n\nto the end of the\n\n```python\nif self.proto < 3 and self.fix_imports:\n```\n\nblock in the Unpickler class' find_class() function\n(e.g. line 1577 of Python 3.8.5's pickle.py).\n\n## Citation\nIf you find DIPS-Plus useful in your research, please cite:\n\n```bibtex\n@article{morehead2023dips,\n  title={DIPS-Plus: The enhanced database of interacting protein structures for interface prediction},\n  author={Morehead, Alex and Chen, Chen and Sedova, Ada and Cheng, Jianlin},\n  journal={Scientific Data},\n  volume={10},\n  number={1},\n  pages={509},\n  year={2023},\n  publisher={Nature Publishing Group UK London}\n}\n```\n",
    "readme_length": 18947
  },
  {
    "name": "clarity_CC",
    "full_name": "claritychallenge/clarity_CC",
    "description": "Support for Clarity Enhancement and Prediction Challenges (obsolete - see README) ",
    "stars": 48,
    "forks": 11,
    "language": "Python",
    "url": "https://github.com/claritychallenge/clarity_CC",
    "topics": [
      "hearing-aids",
      "machine-learning",
      "speech-intelligibility",
      "speech-processing"
    ],
    "created_at": "2021-01-11T22:07:24Z",
    "updated_at": "2024-12-31T06:55:33Z",
    "homepage": "http://claritychallenge.org/the-team",
    "license": "Creative Commons Attribution Share Alike 4.0 International",
    "readme": "# The Clarity Challenges\n\n\n:bangbang:\nThis repository has been replaced with a newer release of the Clarity code that supports all Clarity Challenges: CEC1, CPC1 and the current challenge CEC2. Please redirect to [https://github.com/claritychallenge/clarity](https://github.com/claritychallenge/clarity) . \n:bangbang:\n\n\nIn this repository, you will find code to support all Clarity Challenges\n\n- clartiy_CEC1 - [The first Clarity Enhancement Challenge](https://claritychallenge.github.io/clarity_CEC1_doc/) (Closed 1st June 2021)\n- clarity_CPC1 - [The first Clarity Prediction Challenge](https://claritychallenge.github.io/clarity_CPC1_doc/)  (Launched 16th Nov 2021)\n\nData for the challenges is available separately. See specific instructions in each challenge sub-directory, or visit the challenge website.\n\nFor further details of the Clarity Project visit, [claritychallenge.org](https://claritychallenge.org/).\n",
    "readme_length": 915
  },
  {
    "name": "KARE",
    "full_name": "pat-jj/KARE",
    "description": "[ICLR'25] Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval",
    "stars": 48,
    "forks": 7,
    "language": "Python",
    "url": "https://github.com/pat-jj/KARE",
    "topics": [
      "graph-community",
      "healthcare-ai",
      "knowledge-graph",
      "large-language-models"
    ],
    "created_at": "2024-09-29T03:24:18Z",
    "updated_at": "2025-11-12T08:17:20Z",
    "homepage": "https://arxiv.org/pdf/2410.04585",
    "license": "N/A",
    "readme": "# Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval\n\n\n### 1. Prepare EHR data\n```bash\ncd ehr_prepare\npython ehr_data_prepare.py\npython sample_prepare.py\n```\n#### (Baseline Models Evaluation)\n```bash\nbaselines/baseline_play.ipynb\n```\n\n### 2. Knowledge Graph (KG) Construction\n\n**General Preparation:**\n```bash\ncd kg_construct\npython query_data_prepare.py\n```\n\n**Extract KG from PubMed:**\n\nPreparation:\n```bash\ncd kg_construct/pubmed_index\npython download_pubmed.py\npython embed_pubmed.py\npython convert_dat.py\n```\n\nConstruct KG:\n```bash\ncd kg_construct\npython pubmed_source.py\n```\n\n\n**Extract KG from UMLS:**\n\nSource data files: [graph.txt](https://storage.googleapis.com/pyhealth/umls/graph.txt) (UMLS KG) and [UMLS.csv](https://storage.googleapis.com/pyhealth/umls/UMLS.csv) (mapping file)\n\n```bash\ncd kg_construct\npython umls_source.py\n```\n\nAlternatively, you can use our processed UMLS KG: [Google Drive](https://drive.google.com/file/d/1Zs4hXUiXs_ikkHjHbqp9ZEoH4l6WEP5H/view?usp=sharing)\n\n**Extract KG from LLM:**\n```bash\ncd kg_construct\npython llm_source.py\n```\n\n**Combine KGs**\n```bash\ncd kg_construct\npython combine.py\n```\n\n**Semantic Clustering:**\n\nAfter combining all the KGs into kg_raw.txt under \"graph\" folder (in project root), run:\n```bash\ncd kg_construct\npython refine_kg.py\n```\n\n### 3. KG Community Detection and Indexing (Summarization)\n```bash\ncd kg_index\npython structure_partition_leiden.py\n```\n\n\n### 4. Patient Context Construction and Augmentation\n```bash\ncd patient_context\npython base_context.py\npython get_emb.py\npython sim_patient_ret_faiss.py\npython augment_context.py\n```\n\n### 5. Reasoning Chain Generation \n```bash\ncd prediction\npython data_prepare.py\npython split_task.py\n\n```\n\n\n### 6. LLM Fine-tuning\nPlease follow https://github.com/huggingface/alignment-handbook to build the environment for fine-tuning.\nStart the fine-tuning for the specific task (mortality/readmission):\n```bash\nsh finetune/sft_{task}.sh\n```\n\n### 7. Prediction & Evaluation\n```bash\n# For the prediction\ncd prediction\ncd llm_inference\npython generate.py\n\n# For the evaluation\ncd prediction\npython eval.py\n```\n\n### * A Cost-Effective/Naive Approach (Skipping Step 1-3) to Validate Our Results\n---\nThis approach will directly retrieve the knowledge summaries from an LLM, and use them to construct the input and output for LLM fine-tuning. However, the result would not be as good as the original method, but can still be used to validate the philosophy underlying our method.\n\n    (This approach is suitable for those who do not want to spend money on building their own context-aware and concept-specific KG.)\n\n    **Major Advantange** over our method: \n    (1) Much lower cost than our original implementation.\n    (2) No need to tune the hyperparameters for the context augmentation.\n\n    **Major Disadvantage**: \n    (1) Relatively lower performance as it only uses the knowledge from LLM. \n    (2) For real-world application, you will need to prepare the knowledge from the **same LLM** for the every new sample during the inference -> higher cost if the application is long-term.\n    \n```bash\ncd prediction\npython dp_new.py\n```\n\nOthers\n---\nTo call LLM APIs in this work, you need to \n\nEnter AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION in ``apis_example/claude_api.py`` to call Claude APIs.\n\nEnter your OpenAI API key in ``apis_example/openai.key`` to call OpenAI APIs.\n\nThen, you need to rename ``apis_example`` to ``apis``, and put it under each folder where you need to call APIs.\n\n---\n\n## Cite KARE\n```bibtex\n@misc{jiang2024reasoningenhancedhealthcarepredictionsknowledge,\n      title={Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval}, \n      author={Pengcheng Jiang and Cao Xiao and Minhao Jiang and Parminder Bhatia and Taha Kass-Hout and Jimeng Sun and Jiawei Han},\n      year={2024},\n      eprint={2410.04585},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2410.04585}, \n}\n```\n\nThank you for your interest in our work!",
    "readme_length": 4061
  },
  {
    "name": "MS-HGAT",
    "full_name": "slingling/MS-HGAT",
    "description": "Implementation of AAAI22 paper: MS-HGAT: Memory-enhanced Sequential Hypergraph Attention Network for Information Diffusion Prediction",
    "stars": 43,
    "forks": 2,
    "language": "Python",
    "url": "https://github.com/slingling/MS-HGAT",
    "topics": [],
    "created_at": "2021-12-13T02:35:02Z",
    "updated_at": "2025-11-13T10:02:23Z",
    "homepage": null,
    "license": "N/A",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "FreeDyG",
    "full_name": "Tianxzzz/FreeDyG",
    "description": "Codes for ICLR 2024 paper \"FreeDyG: Frequency Enhanced Continuous-Time Dynamic Graph Model for Link Prediction\"",
    "stars": 35,
    "forks": 8,
    "language": "Python",
    "url": "https://github.com/Tianxzzz/FreeDyG",
    "topics": [],
    "created_at": "2024-03-07T09:26:09Z",
    "updated_at": "2025-11-16T12:29:26Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# FreeDyG\nCodes for ICLR 2024 paper \"FreeDyG: Frequency Enhanced Continuous-Time Dynamic Graph Model for Link Prediction\"\n\n# Acknowledgments\nWe are grateful to the authors of [DyGFormer](https://github.com/yule-BUAA/DyGLib) for making their project codes publicly available.\n\n# Citation\nPlease consider citing our paper when using this project.\n```{bibtex}\n@inproceedings{\ntian2024freedyg,\ntitle={FreeDyG: Frequency Enhanced Continuous-Time Dynamic Graph Model for Link Prediction},\nauthor={Yuxing Tian and Yiyan Qi and Fan Guo},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=82Mc5ilInM}\n}\n```\n",
    "readme_length": 676
  },
  {
    "name": "8-Ball-Pool-Analysis",
    "full_name": "brandonabela/8-Ball-Pool-Analysis",
    "description": "This project employs computer vision techniques to analyse 8-ball game footage and provide accurate shot predictions to enhance performance.",
    "stars": 35,
    "forks": 10,
    "language": "Python",
    "url": "https://github.com/brandonabela/8-Ball-Pool-Analysis",
    "topics": [
      "8-ball-pool",
      "8-ball-pool-analysis",
      "computer-vision",
      "computer-vision-opencv",
      "python"
    ],
    "created_at": "2019-07-10T18:05:48Z",
    "updated_at": "2025-10-12T22:16:20Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# 8 Ball Pool Analysis\n\nThis is an Open Source project that analyses game footage from **[Miniclip's 8 ball pool](https://miniclip.com/games/8-ball-pool-multiplayer/en/)**.\n\nIt determines paths, which show the user a possibility of how the balls could be potted.\n\n![animated.gif](../master/Assets/animated.gif)\n\n## Installation\n\nThis tool uses standard image processing techniques through OpenCV, together with some vector algebra and graph logic.\n\n```\npip install opencv-python\n```\n\n## Usage\n\nThis tool is run via the command-line, and contains various fine-tuning options, depending on the video input.\n\nThe current default values for ball and hole sizes were determined after rigorous testing, on a video from a 1080p display, with zoom and scaling set to 100%. As a result, videos which have been captured on displays with a different resolution, zoom and scaling might need further tweaking to obtain adequate results.\n\n```\nusage: start.py [-br N] [-hr N] [-bd N] [-tb type] [-ip file] [-op file] [-sf N] [-show] [-save] [-h]\n\nThis project analyses in game footage that indicates the optimal shot predictions using computer vision.\n\noptional arguments:\n  -br N, --ball_radius N         Radius of the pool balls (dependent on resolution, zooming and scaling).\n  -hr N, --hole_radius N         Radius of the table holes (dependent on resolution, zooming and scaling).\n  -bd N, --border_distance N     Distance from the centre of the holes to the outermost edge of the table.\n  -tb type, --target_balls type  Choose ball type for path calculation.\n  -ip file, --input_video file   File path containing the game footage to be analysed (*.MP4).\n  -op file, --output_video file  File path for the output video (*.MP4).\n  -sf N, --skip_frame N          Process a frame every N frame when analysing the video.\n  -show, --show_video            Show the video while processing is being done.\n  -save, --save_video            Save the video after the processing has finished.\n  -h, --help                     Show this help message and exit.\n```\n",
    "readme_length": 2040
  },
  {
    "name": "ST-LLM-Plus",
    "full_name": "kethmih/ST-LLM-Plus",
    "description": "Official implementation of the paper \"ST-LLM+ :  Graph Enhanced Spatio-Temporal Large Language Models for Traffic Prediction\"",
    "stars": 35,
    "forks": 5,
    "language": "Python",
    "url": "https://github.com/kethmih/ST-LLM-Plus",
    "topics": [],
    "created_at": "2024-07-11T08:00:04Z",
    "updated_at": "2025-11-13T12:49:46Z",
    "homepage": "",
    "license": "Other",
    "readme": "# ST-LLM+ : Graph Enhanced Spatio-Temporal Large Language Models for Traffic Prediction\nThis repository provides the official Pytorch implementation of our manuscript titled \"[ST-LLM+: Graph Enhanced Spatio-Temporal Large Language Models for Traffic Prediction](https://ieeexplore.ieee.org/document/11005661)\". This work is an extension of the original [ST-LLM](https://github.com/ChenxiLiu-HNU/ST-LLM/blob/main/ST-LLM.pdf) model. The foundational training framework is derived from the open-source codebase developed by [ChenxiLiu-HNU](https://github.com/ChenxiLiu-HNU/ST-LLM/tree/main).\n\n## Abstract\n> *Traffic prediction is a crucial component of data management systems, leveraging historical data to learn spatio-temporal dynamics for forecasting future traffic and enabling efficient decision-making and resource allocation. Despite efforts to develop increasingly complex architectures, existing traffic prediction models often struggle to generalize across diverse datasets and contexts, limiting their adaptability in real-world applications. In contrast to existing traffic prediction models, large language models (LLMs) progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose ST-LLM+, the graph enhanced spatio-temporal large language models for traffic prediction. Through incorporating a proximity-based adjacency matrix derived from the traffic network into the calibrated LLMs, ST-LLM+ captures complex spatio-temporal dependencies within the traffic network. The Partially Frozen Graph Attention (PFGA) module is designed to retain global dependencies learned during LLMs pre-training while modeling localized dependencies specific to the traffic domain. To reduce computational overhead, ST-LLM+ adopts the LoRA-augmented training strategy, allowing attention layers to be fine-tuned with fewer learnable parameters. Comprehensive experiments on real-world traffic datasets demonstrate that ST-LLM+ outperforms state-of-the-art models. In particular, ST-LLM+ also exhibits robust performance in both few-shot and zero-shot prediction scenarios. Additionally, our case study demonstrates that ST-LLM+ captures global and localized dependencies between stations, verifying its effectiveness for traffic prediction tasks.*\n\n![Image](https://github.com/kethmih/ST-LLM-Plus/blob/main/assets/Architecture_Diagram.png)\n\n## Dependencies\n\n* Python 3.8.19\n* PyTorch 2.4.1\n* cuda 11.7\n* torchvision 0.19.1\n\n```bash\n> conda env create -f env_ubuntu.yaml\n```\n\n## Datasets\nWe provide preprocessed datasets, which you can access [here](https://drive.google.com/drive/folders/1iif59LObrPu-QrpL8Y6lWeajbn_gRf7v?usp=drive_link).   \nIf you need the original datasets, please refer to the [ESG](https://github.com/LiuZH-19/ESG).\n\n## Training\n\n```bash\nCUDA_VISIBLE_DEVICES=0\nnohup python train_plus.py --data taxi_pick > your_log_name.log &\n```\n\n## BibTex\nIf you find our work useful in your research. Please consider giving a star â­ and citation ðŸ“š:\n```bibtex\n@ARTICLE{11005661,\n  author={Liu, Chenxi and Hettige, Kethmi Hirushini and Xu, Qianxiong and Long, Cheng and Xiang, Shili and Cong, Gao and Li, Ziyue and Zhao, Rui},\n  journal={IEEE Transactions on Knowledge and Data Engineering}, \n  title={ST-LLM+: Graph Enhanced Spatio-Temporal Large Language Models for Traffic Prediction}, \n  year={2025},\n  volume={37},\n  number={8},\n  pages={4846-4859},\n  keywords={Time series analysis;Predictive models;Forecasting;Large language models;Adaptation models;Data models;Computational modeling;Training;Electronic mail;Attention mechanisms;Traffic prediction;large language models;spatio-temporal data},\n  doi={10.1109/TKDE.2025.3570705}}\n```\n\n## Further Reading\n[**Spatial-Temporal Large Language Model for Traffic Prediction**](https://arxiv.org/abs/2401.10134), in *MDM* 2024.\n[\\[GitHub Repo\\]](https://github.com/ChenxiLiu-HNU/ST-LLM/tree/main)\n```bibtex\n@inproceedings{liu2024spatial,\n  title={Spatial-Temporal Large Language Model for Traffic Prediction},\n  author={Liu, Chenxi and Yang, Sun and Xu, Qianxiong and Li, Zhishuai and Long, Cheng and Li, Ziyue and Zhao, Rui},\n  booktitle={MDM},\n  year={2024}\n}\n```\n\n## Contact Us\nFor inquiries or further assistance, contact us at [kethmihi001@e.ntu.edu.sg](mailto:kethmihi001@e.ntu.edu.sg) and [chenxi.liu@ntu.edu.sg](mailto:chenxi.liu@ntu.edu.sg), or open an issue on this repository.\n",
    "readme_length": 4424
  },
  {
    "name": "Agile-MLOps-Deployment-Docker-AWS-CI-CD-Pipeline",
    "full_name": "Abhi0323/Agile-MLOps-Deployment-Docker-AWS-CI-CD-Pipeline",
    "description": "An end-to-end predictive maintenance application using machine learning to enhance industrial efficiency. This project employs robust modular architecture and advanced MLOps practices, including Docker and AWS for scalable, real-time maintenance predictions.",
    "stars": 34,
    "forks": 9,
    "language": "Python",
    "url": "https://github.com/Abhi0323/Agile-MLOps-Deployment-Docker-AWS-CI-CD-Pipeline",
    "topics": [
      "aws",
      "aws-ec2",
      "deployment",
      "docker",
      "ecr-repositories",
      "github-actions",
      "machine-learning",
      "mlops",
      "model"
    ],
    "created_at": "2024-04-13T03:06:08Z",
    "updated_at": "2025-11-18T17:50:23Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# Predictive Maintenance Application: Agile MLOps Deployment: Docker-AWS-CI/CD Pipeline\n\n\n\n![ezgif com-animated-gif-maker (3)](https://github.com/Abhi0323/Agile-MLOps-Deployment-Docker-AWS-CI-CD-Pipeline/assets/112967999/c215775e-0e0c-41ee-9d88-102c3a284267)\nYou can view my blog, which details the end-to-end steps for this project, here: https://medium.com/@abhishekgoud1212/building-an-end-to-end-predictive-maintenance-machine-learning-application-mlops-ecff82f5d103\n\n## 1. Introduction\n\nThis project is an exemplary demonstration of applying advanced machine learning (ML) techniques and best practices in a real-world predictive maintenance system. It features a sophisticated end-to-end ML pipeline that integrates MLOps practices to ensure scalability, maintainability, and efficient deployment in production environments.\n\n\n## 2. Project Setup\n\n* **Version Control and Collaboration:** Leveraged GitHub for robust version control, enabling seamless collaboration and code management.\n\n* **Environment Consistency:** Established a controlled Python environment with virtualenv, detailed in a requirements.txt file for replicability across development and production stages.\n\n\n## 3. Advanced Architectural Design\n\n* **Modular Architecture:** Adopted a modular design, segregating functionalities into distinct modulesâ€”data ingestion, transformation, model training, and predictionâ€”to manage complexities effectively and enhance scalability.\n\n* **Exception Handling and Systematic Logging:** Implemented comprehensive exception handling and a logging system to ensure high application reliability and operational transparency for real-time monitoring.\n\n\n## 4. Technical Implementation Phases\n\n* **Robust Data Handling:** Engineered a highly efficient data ingestion and transformation framework using Pythonâ€™s dataclasses and scikit-learn pipelines, ensuring data integrity and consistency.\n\n* **Exploratory Data Analysis (EDA) and Feature Engineering:** Performed deep exploratory analysis and innovative feature engineering to inform and optimize model selection and hyperparameter tuning.\n\n* **Advanced Model Training Techniques:** Deployed multiple machine learning models, utilizing cross-validation and grid search for hyperparameter optimization. Evaluation metrics such as accuracy, precision, recall, and F1-score were used to select the optimal model.\n\n* **Predictive Pipeline:** Constructed a sophisticated prediction pipeline capable of processing real-time data inputs and generating predictions with high accuracy and speed.\n\n\n## 5. MLOps and CI/CD Integration\n\n* **Continuous Integration/Continuous Deployment (CI/CD):** Established a CI/CD pipeline using GitHub Actions to automate testing, building, and deployment phases, significantly accelerating the development cycle and ensuring high-quality releases.\n\n* **Docker and AWS Deployment:** Utilized Docker for application containerization, achieving consistency across various environments. Integrated with AWS services, including EC2 and ECR, to facilitate a scalable and secure deployment.\n\n![ezgif com-animated-gif-maker (4)](https://github.com/Abhi0323/Agile-MLOps-Deployment-Docker-AWS-CI-CD-Pipeline/assets/112967999/8d683ee0-cf52-4c2b-9243-1bfcf6936fce)\n\n\n## 6. Web Application and User Interaction\n\n* **Flask Application:** Developed a dynamic Flask web application to serve the predictive maintenance system, integrating the backend ML model with a frontend interface.\n\n* **User-Friendly Design:** Crafted responsive HTML templates and CSS styling to provide a seamless and engaging user experience, enabling easy interaction with the predictive system.\n\n\n## 7. Conclusion\n\nThis project not only demonstrates the effective deployment of a high-performance predictive maintenance application but also showcases the integration of cutting-edge MLOps practices and technologies. It stands as a testament to the potential of combining machine learning innovation with robust software engineering and operational best practices to solve complex real-world problems.\n",
    "readme_length": 4045
  },
  {
    "name": "PCPNet",
    "full_name": "Blurryface0814/PCPNet",
    "description": "PCPNet: An Efficient and Semantic-Enhanced Transformer Network for Point Cloud Prediction.",
    "stars": 33,
    "forks": 4,
    "language": "Python",
    "url": "https://github.com/Blurryface0814/PCPNet",
    "topics": [],
    "created_at": "2023-01-29T07:53:33Z",
    "updated_at": "2025-11-09T13:58:59Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# PCPNet: An Efficient and Semantic-Enhanced Transformer Network for Point Cloud Prediction\n\nAccepted by IEEE RA-L.\n\nDeveloped by [Zhen Luo](https://github.com/Blurryface0814) and [Junyi Ma](https://github.com/BIT-MJY).\n\n<img src=\"figs/motivation.png\" width=\"70%\" />\n\n*PCPNet predicts future range images based on past range image sequences. The semantic information of sequential range images is extracted for auxiliary training, making the outputs of PCPNet closer to the ground truth in semantics.*\n\n<img src=\"figs/predictions.gif\" width=\"100%\" />\n\n*Ground truth and future range images predicted by PCPNet.*\n\n## Contents\n1. [Publication](#Publication)\n2. [Dataset](#Dataset)\n3. [Installation](#Installation)\n4. [Training](#Training)\n5. [Semantic-Auxiliary-Training](#Semantic-Auxiliary-Training)\n6. [Testing](#Testing)\n7. [Visualization](#Visualization)\n8. [Download](#Dwnload)\n9. [Acknowledgment](#Acknowledgment)\n10. [License](#License)\n\n![](figs/overall_architecture.png)\n*Overall architecture of our proposed PCPNet. The input range images are first downsampled and compressed along the height and width dimensions respectively to generate the sentence-like features for the following Transformer blocks. The enhanced features are then combined and upsampled to the predicted range images and mask images. Semantic auxiliary training is used to improve the practical value of predicted point clouds.*\n\n## Publication\nIf you use our code in your academic work, please cite the corresponding [paper](https://ieeexplore.ieee.org/abstract/document/10141631?casa_token=VCXSYRIkT88AAAAA:-LLz-ZSNJVSLCYSjXjzpV_DrwtBggRvOKW_1dWxUDNa1IE4VzREdHovp-PyD1zA9rVlRZblXpQu1qfQ):\n    \n```latex\n@ARTICLE{10141631,\n  author={Luo, Zhen and Ma, Junyi and Zhou, Zijie and Xiong, Guangming},\n  journal={IEEE Robotics and Automation Letters}, \n  title={PCPNet: An Efficient and Semantic-Enhanced Transformer Network for Point Cloud Prediction}, \n  year={2023},\n  volume={8},\n  number={7},\n  pages={4267-4274},\n  doi={10.1109/LRA.2023.3281937}}\n```\n\n## Dataset\nWe use the KITTI Odometry dataset to train and evaluate PCPNet in this repo, which you can download [here](http://www.cvlibs.net/datasets/kitti/eval_odometry.php).\n\n\nBesides, we use SemanticKITTI for semantic auxiliary training, which you can download [here](http://semantic-kitti.org/dataset.html#download).\n\n## Installation\n\n### Source Code\nClone this repository and run \n```bash\ncd PCPNet\ngit submodule update --init\n```\nto install the Chamfer distance submodule. The Chamfer distance submodule is originally taken from [here](https://github.com/chrdiller/pyTorchChamferDistance) with some modifications to use it as a submodule.\n\nAll parameters are stored in ```config/parameters.yaml```.\n\n### Dependencies\nIn this project, we use CUDA 11.4, pytorch 1.8.0 and pytorch-lightning 1.5.0. All other dependencies are managed with Python Poetry and can be found in the ```poetry.lock``` file. If you want to use Python Poetry, install it with:\n```bash\ncurl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py | python -\n```\n\nInstall Python dependencies with Python Poetry\n```bash\npoetry install\n```\n\nand activate the virtual environment in the shell with\n```bash\npoetry shell\n```\n\n## Training\nWe process the data in advance to speed up training. The preprocessing is automatically done if ```GENERATE_FILES``` is set to True in ```config/parameters.yaml```.\n\nIf you have not pre-processed the data yet, please set ```GENERATE_FILES: True``` in ```config/parameters.yaml```, and run the training script by\n```bash\npython train.py --rawdata /PATH/TO/RAW/KITTI/dataset/sequences --dataset /PATH/TO/PROCESSED/dataset/\n```\nin which ```--rawdata``` points to the directory containing the train/val/test sequences specified in the config file and  ```--dataset``` points to the directory containing the processed train/val/test sequences\n\nIf you have already pre-processed the data, please set ```GENERATE_FILES: False``` to skip this step, and run the training script by\n```bash\npython train.py --dataset /PATH/TO/PROCESSED/dataset/\n```\n\nTo resume from a checkpoint, please run the training script by\n```bash\npython train.py --dataset /PATH/TO/PROCESSED/dataset/ --resume /PATH/TO/YOUR/MODEL/\n```\nYou can also use the flag```--weights``` to initialize the weights from a pre-trained model. Pass the flag ```--help``` if you want to see more options.\n\nA directory will be created in ```runs``` which saves everything like the model files, used config, logs, and checkpoint.\n\n## Semantic-Auxiliary-Training\nIf you want to use our proposed semantic auxiliary training strategy, you need to first pre-train a semantic segmentation model. We provide codes for semantic auxiliary training using RangeNet++ in ```semantic_net/rangenet```. To use these codes, please first clone the [official codes](https://github.com/PRBonn/lidar-bonnetal) of RangeNet++ and train a semantic segmentation model. \n\n*Note that we recommend using squeezesegV2 backbone without CRF and only use ```range``` in the ```input_depth``` option while training RangeNet++, according to the codes we are currently providing.* If you want to use other backbones, please make corresponding modifications to ```class loss_semantic```  in ```pcpnet/models/loss.py```.\n\nOnce you have completed the pre-training, you need to copy the folder containing the pre-trained model to ```semantic_net/rangenet/model/``` and modify ```SEMANTIC_PRETRAINED_MODEL``` in ```config/parameters.yaml```  to the folder name.\n\nAfter completing the above steps, you can start to use semantic auxiliary training by running the training script by\n```bash\npython train.py --dataset /PATH/TO/PROCESSED/dataset/\n```\n*Note* that you need to set ```LOSS_WEIGHT_SEMANTIC``` in ```config/parameters.yaml``` to the weight you want (we recommend 1.0) instead of 0.0 before running the training script.\n\n## Testing\nYou can evaluate the performance of the model by running\n```bash\npython test.py --dataset /PATH/TO/PROCESSED/dataset/ --model /PATH/TO/YOUR/MODEL/\n```\n*Note*: Please use the flag ```-s``` if you want to save the predicted point clouds for visualization, and ```-l``` if you want to test the model on a smaller amount of data. By using the flag ```-o```, you can only save the predicted point clouds without computing loss to accelerate the speed of saving.\n\n## Visualization\nAfter passing the ```-s``` flag or the ```-o```flag to the testing script, the predicted range images will be saved as .png files in ```runs/MODEL_NAME/test_TIME/range_view_predictions```. The predicted point clouds are saved to ```runs/MODEL_NAME/test_TIME/point_clouds```. You can visualize the predicted point clouds by running\n```bash\npython visualize.py --path runs/MODEL_NAME/test_TIME/point_clouds\n```\nPlease download the car model from [here](https://drive.google.com/drive/folders/1bmBMdfJaN2ptJVh7gHv1Gy8L1aLMOslr?usp=share_link) and put it into ```./car_ model/``` to display the car during the visualization process.\n\n\n## Download\nYou can download our pre-trained model from this [link](https://drive.google.com/drive/folders/1p9q_SoXOsigi8vB_bXxWNZbU-ypSm19J?usp=share_link). Just extract the zip file into ```runs```.\n\n## Acknowledgment\nWe would like to thank Benedikt Mersch, Andres Milioto and Christian Diller et al. for their significant contributions in the field of point cloud processing. Some of the code in this repo is borrowed from [TCNet](https://github.com/PRBonn/point-cloud-prediction), [RangeNet++](https://github.com/PRBonn/lidar-bonnetal), and [pyTorchChamferDistance](https://github.com/chrdiller/pyTorchChamferDistance). Thank all the authors for their awesome projects.\n\n## License\nThis project is free software made available under the MIT License. For details see the LICENSE file.\n",
    "readme_length": 7806
  },
  {
    "name": "MEAP",
    "full_name": "scitix/MEAP",
    "description": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More",
    "stars": 33,
    "forks": 3,
    "language": "Python",
    "url": "https://github.com/scitix/MEAP",
    "topics": [],
    "created_at": "2025-01-24T01:48:06Z",
    "updated_at": "2025-10-24T11:30:19Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# MEAP\n\nThis repository contains the official implementation of \"[Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More](https://arxiv.org/abs/2502.07490)\".\n\n**2025-05-01: MEAP is accepted to ICML 2025! ([Poster](https://icml.cc/virtual/2025/poster/46344))**\n\n*Xialie Zhuang, Zhikai Jia, Jianjin Li, Zhenyu Zhang, Li Shen, Zheng Cao, Shiwei Liu*\n\n## ðŸ“‹ Table of Contents\n- [MEAP-Pretrain](#MEAP-Pretrain)\n- [MEAP-Sft](#MEAP-Sft)\n\n## Overview\n\nMEAP (Mask-Enhanced Autoregressive Prediction) is a novel training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) using a decoder-only Transformer. By masking a small fraction of input tokens during standard autoregressive training, MEAP enhances model performance on key information retrieval tasks while maintaining strong reasoning capabilities.\n\nKey Features:\n- Seamless integration of MLM into NTP\n- No additional computational overhead\n- Compatible with decoder-only architectures\n- Improved performance on information retrieval tasks\n\n## MEAP-Pretrain\n\n### Model Architecture\n\nThe MEAP architecture extends standard decoder-only transformers by:\n1. Randomly masking a portion of input tokens\n2. Training the model to predict both masked tokens and next tokens\n3. Maintaining the autoregressive property during inference\n\n### Installation\n#### Install env\n\n```bash\nconda create -n meap python=3.10\nconda activate meap\n```\n\n#### Install Pytorch.\n```bash\npip install torch==2.5.0  --index-url https://download.pytorch.org/whl/cu121\n```\n\n#### Install lightning\n```bash\npip install lightning==2.1.2\npip install lightning-app\npip install lightning-cloud==0.5.52\n```\n\n#### Install Flash-Attention 2 and other fused operators:\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention\npip install flash-attn\ncd csrc/rotary && pip install .\ncd ../layer_norm && pip install .\ncd ../xentropy && pip install .\ncd ../.. && rm -rf flash-attention\n```\n\n#### Build XFormers from Source\n\n```bash\npip3 install xformers --no-deps\n```\n#### Install Remaining Dependencies\n```\npip install -r requirements.txt tokenizers sentencepiece transformers\n```\nto install other dependencies.\nIt may take >= 5 minutes to build xformers/flash-attention. Do not worry if the process seemly stagnant or the terminal print out many warnings.\n\nThen you are ready to go ðŸŽ‰!\n\n### Data Preparation\n\n#### Download Datasets\nDownload the c4 dataset to your chosen directory.\n```bash\nmkdir original_data\ncd original_data\ngit lfs install\ngit clone https://huggingface.co/datasets/allenai/c4/tree/main\ncd ..\n```\n\nExtract the downloaded c4 file and move it to the json_c4 folder.\n```bash\npython data_process/gz_unzip_v1.py\nmkdir json_c4\nmv original_data \nmv *.json ../json_c4/\n```\n\n\n\n#### Tokenize data\nUse the provided scripts to tokenize the datasets and divide them into chunks.\n\n\n```bash\nmkdir c4_bin \npython3 prepare_c4.py --source_path ../  --destination_path  ../c4_bin --checkpoint_dir   ../tokenizer\ncd ..\n```\nWe have placed some sample data in the 'c4_bin' folder. Please note that this is only for testing the program, and these data are not the complete training data.\n\n###  Train\n\n\nIf your setup comprises two nodes, each with 1 GPUs, you can initiate pretraining with the following commands:\n\n```bash\ncd MEAP-Pretrain\nsh run_one_node.sh  ../pretrained/meap_1b.py \n```\nIf you want to modify the number of GPUs to be used, please simultaneously modify the `--devices` parameter in `run_one_node.sh`, the `num_of_devices` parameter  and the default parameter of `devices` in the `setup` function in `meap_1b.py`.\n\nThe default path for saving the model weights is `out_mask_1b_mask0.15`. If you want to modify the save path, please change the `out_dir` parameter in `meap_1b.py`.\n\nThe default value of the `n_chunks` parameter in `meap_1b.py` is 1. Increasing its value can increase the throughput of data reading.\n\nMore training hyperparameters can also be modified in `meap_1b.py`.\n\n\n### convert to huggingface\n\nConvert the trained model to the HF format.\n\n```bash\ncd convert\n\npython3 convert_lit_checkpoint.py --checkpoint_name  xxxx.pth  --out_dir your_save_dir --model_name  trained_model_name,such as tiny_LLaMA_1b_mask  --model_only false\n```\n\nAfter running the script, a bin file will be stored in the 'out_dir' folder.\n\nFinally, run convert_safetensors.py to convert the bin file to the safetensors format, where checkpoint_path is the path of the bin file and out_dir is the save path for the safetensors file.\n\n```bash\npython3 convert_safetensors.py\n```\n## MEAP-SFT\n\n### Model Architecture\n\nThe MEAP architecture extends standard decoder-only transformers by:\n\n1. **Randomly Mask Target Text**: Randomly select positions in the target text to mask based on the given `mask_ratio`.\n2. **Align Input and Labels**: Ensure input sequences and labels are aligned in length, and truncate sequences that exceed the maximum length.\n3. **Dynamically Generate Masks**: Dynamically select mask positions in each training step to improve the model's generalization ability.\n\n### Installation\n\n```\nconda create -n MEAP-SFT python=3.10 -y\nconda activate MEAP-SFT\npip install -r ./MEAP-SFT/requirements.txt\n```\n\n### Train\n\n- IF there is no LLAMA3-8B weight,  need to download\n\n```\nbash ./script/MEAP-SFT.sh\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Cite as\n```\n@article{zhuang2025mask,\n  title={Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More},\n  author={Zhuang, Xialie and Jia, Zhikai and Li, Jianjin and Zhang, Zhenyu and Shen, Li and Cao, Zheng and Liu, Shiwei},\n  journal={arXiv preprint arXiv:2502.07490},\n  year={2025}\n}\n```\n\n## Acknowledgments\n\nWe would like to acknowledge and thank the following projects and platforms that helped make this work possible:\n\n- [Siflow](https://scitix.ai/) - The entire development process relies on the Siflow platform, provided by SCITIX (SGP) TECH PTE. LTD.\n\n- [TinyLlama](https://github.com/jzhang38/TinyLlama) - Our work builds upon insights and implementations from the TinyLlama project.\n\n\n\n\n\n\n\n\n",
    "readme_length": 6145
  },
  {
    "name": "Attention-Mechanism-Enhanced-KPN",
    "full_name": "z-bingo/Attention-Mechanism-Enhanced-KPN",
    "description": "The Implementation of Attention Mechanism Enhanced Kernel Prediction Networks in PyTorch.",
    "stars": 30,
    "forks": 4,
    "language": "Python",
    "url": "https://github.com/z-bingo/Attention-Mechanism-Enhanced-KPN",
    "topics": [],
    "created_at": "2019-08-16T11:06:54Z",
    "updated_at": "2023-12-25T10:37:25Z",
    "homepage": null,
    "license": "N/A",
    "readme": "## Attention Mechanism Enhanced Kernel Prediction Networks (AME-KPNs)\n\nThe official implementation of AME-KPNs in PyTorch, and our paper is accepted by ICASSP 2020 (oral), it is available at [http://arxiv.org/abs/1910.08313](http://arxiv.org/abs/1910.08313).\n\n### News\n- Support KPN (Kernel Prediction Networks), MKPN (Multi-Kernel Prediction Networks) by modifing the config file.\n- The current version supports training on color images.\n- The noise can be generated in a simple way as the paper descirbed, and a complex way as [Jaroensri's work](https://github.com/12dmodel/camera_sim) but replacing the Halide with OpenCV and scikit-image.\n\n### TODO\nWrite the documents.\n\n### Requirements\n- Python3\n- PyTorch >= 1.0.0\n- Scikit-image\n- Numpy\n- TensorboardX (needed tensorflow support)\n\n### How to use it?\nThis repo. supports training on multiple GPUs and the default setting is also multi-GPU.  \n\nIf you want to restart the train process using KPN, the command you can type as\n```\nCUDA_VISIBLE_DEVICES=x,x train_eval_syn.py --cuda --mGPU -nw 4 --config_file ./kpn_specs/kpn_config.conf --restart\n```\nIf no `--restart`, the train process would be resumed.\n\n### Citation\n```\n@article{zhang2019attention,\n    title={Attention Mechanism Enhanced Kernel Prediction Networks for Denoising of Burst Images},\n    author={Bin Zhang and Shenyao Jin and Yili Xia and Yongming Huang and Zixiang Xiong},\n    year={2019},\n    journal={arXiv preprint arXiv:1910.08313}\n}\n```\n",
    "readme_length": 1462
  },
  {
    "name": "PreAct",
    "full_name": "Fu-Dayuan/PreAct",
    "description": "PreAct: Prediction Enhances Agent's Planning Ability (Coling2025)",
    "stars": 30,
    "forks": 0,
    "language": "Python",
    "url": "https://github.com/Fu-Dayuan/PreAct",
    "topics": [],
    "created_at": "2024-02-18T07:44:47Z",
    "updated_at": "2025-10-29T21:29:06Z",
    "homepage": "",
    "license": "N/A",
    "readme": "\n # <div align=\"center\"> PreAct: Prediction Enhances Agent's Planning Ability<div>\n\n\n<div align=\"center\">\n<a><img alt=\"Static Badge\" src=\"https://img.shields.io/badge/made_with-Python-blue\"></a>\n  <a href=\"https://arxiv.org/pdf/2402.11534\" target=\"_blank\"><img src=https://img.shields.io/badge/arXiv-b5212f.svg?logo=arxiv></a>\n  <a href=\"https://github.com/dongguanting/IF-RAG/blob/main/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\"></a>\n</div>\n\n## ðŸ’¥ News\n- [12/2024] ðŸ”¥ Our paper has been accepted by **COLING 2025**. \n\n- [12/2024] ðŸ”¥ We release the code for inference HotpotQA task. You can inference PreAct+TOT here. \n\n- [12/2024] ðŸ”¥ We introduced ***PreAct***. Check out the [paper](https://arxiv.org/pdf/2402.11534). \n\n---\n\n\n\n## Introduction\nAddressing the disparity between forecasts and actual results can enable individuals to expand their thought processes and stimulate self-reflection, thus promoting accurate planning.\nIn this research, we present **PreAct**, an agent framework that integrates **pre**diction, **rea**soning, and **act**ion. By utilizing the information derived from predictions, the large language model (LLM) agent can provide a wider range and more strategically focused reasoning. This leads to more efficient actions that aid the agent in accomplishing intricate tasks. Our experimental results show that PreAct surpasses the ReAct method in completing complex tasks and that PreAct's performance can be further improved when paired with other memory or selection strategy techniques. We presented the model with varying quantities of historical predictions and discovered that these predictions consistently enhance LLM planning.\nThe variances in single-step reasoning between PreAct and ReAct indicate that PreAct indeed has benefits in terms of diversity and strategic orientation over ReAct.\n\n## PreAct with TOT\n\nWe use [LanguageAgentTreeSearch](https://github.com/lapisrocks/LanguageAgentTreeSearch) to run the HotpotQA task. We release the code for inference. You can inference PreAct+TOT with the following commands:\n\n```bash \n  cd LanguageAgentTreeSearch/hotpot\n  bash tot.sh\n```\n\nâ­ **We will also provide the AgentBench experiment code soon! Thanks for waiting!**\n\n## Citation\nPlease kindly cite our paper if it helps your research:\n```bibtex\n@article{fu2024preact,\n  author       = {Dayuan Fu and\n                  Jianzhao Huang and\n                  Siyuan Lu and\n                  Guanting Dong and\n                  Yejie Wang and\n                  Keqing He and\n                  Weiran Xu},\n  title        = {PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability},\n  journal      = {CoRR},\n  volume       = {abs/2402.11534},\n  year         = {2024},\n  url          = {https://doi.org/10.48550/arXiv.2402.11534},\n  doi          = {10.48550/ARXIV.2402.11534},\n  eprinttype    = {arXiv},\n  eprint       = {2402.11534},\n  timestamp    = {Wed, 19 Jun 2024 17:14:13 +0200},\n  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-11534.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}\n```\n",
    "readme_length": 3117
  },
  {
    "name": "crysgnn",
    "full_name": "kdmsit/crysgnn",
    "description": "CrysGNN : Distilling pre-trained knowledge to enhance property prediction for crystalline materials. (AAAI-2023)",
    "stars": 29,
    "forks": 0,
    "language": "Python",
    "url": "https://github.com/kdmsit/crysgnn",
    "topics": [
      "crystal-materials",
      "gnn-pretraining",
      "graph-neural-networks",
      "knowledge-distillation",
      "materials-property-prediction"
    ],
    "created_at": "2022-11-28T13:17:34Z",
    "updated_at": "2025-09-03T14:50:51Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# CrysGNN : Crystal Graph Neural Network\n\nThis is software package for Crsytal Graph Neural Network(CrysGNN), a new pre-trained GNN framework for crystalline\nmaterials, which captures both node and graph level structural information of crystal graphs using a huge amount of unlabelled material data. CrysGNN takes as input\nany arbitary crystal structure in .cif file format and return node embeddings of the crystal graph structure. Using these embeddings, we extract distilled knowledge\nfrom CrysGNN and inject into different state of the art property predictors to enhance their property prediction accuracy.\nThe following paper describes the details of the CrysGNN framework: <br/>\n[CrysGNN: Distilling pre-trained knowledge to enhance property prediction for crystalline materials (AAAI-2023)](https://kdmsit.github.io/assets/pdf/CrysGNN_Full.pdf) \n\n- Pre-training CrysGNN : \n    ![CrysGNN diagram](images/CrysGNN.png)\n    <div align='center'><strong>Figure 1. Overview of both node and graph-level decoding methods for CrysGNN.</strong></div>\n    \n- Property Prediction using distilled knowledge from CrysGNN\n    ![Property Prediction diagram](images/Distillation-Property-Prediction.png)\n    <div align='center'><strong>Figure 2. Overview of Property Prediction using Knowledge Distillation from CrysGNN.</strong></div>\n    \n## Table of Contents\n- [Requirements](#requirements)\n- [Usage](#usage)\n  - [Dataset](#define-a-customized-dataset)\n  - [Train CrysGNN Model](#train-crysgnn-model)\n  - [How to use CrysGNN in property predictor](#how-to-use-crysgnn)\n- [How to cite](#how-to-cite)\n\n##  Requirements\n\nThe package requirements are listed in requirements.txt file. Create a virtual environment and run the following command to install dependencies in your virtual environment:\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage\n### Dataset\n#### Pre-training Dataset\nWe curated around 800K untagged crystal graph data from two popular materials databases, Materials Project (MP) and OQMD. \nYou can download the data from the following github link :<br/>\n[crystal_untagged_800K](https://github.com/kdmsit/crystal_untagged_800K)\n#### Downstream Dataset\nWe evaluate the performance of different SOTA models with distilled knowledge from CrysGNN using two popular dataset :\n\n- <b>Materials Project Dataset(MP 2018.6.1)</b> : MP 2018.6.1 consists of 69,239 materials with two properties namely bandgap and formation energy. We follow the training setup mentioned\nby ALIGNN paper and use 60000, 5000, and 4239 crystals for raining, validation, and test set.\n- <b>JARVIS Dataset</b> : JARVIS-DFT(2021.8.18) consists of 55,722 materials and we consider 19 properties for the downstream tasks. We follow the training setup mentioned\nby ALIGNN paper and use 80%,10% and 10% raining, validation, and test splits\n### Train CrysGNN Model\nWe have already trained crysgnn with 800K [Dataset](#define-a-customized-dataset) and a pretrained model (crysgnn_v1.pth) will be provided into the '../model' directory.\n\nYet, if You want to to train CrysGNN model from scratch by some other dataset, use the following procedure :\n\n- Go to \"crysgnn\" directory\n- Run the following command : \n\n```bash\npython -W ignore pretrain.py --data-path <Data_Path> --epochs <num_epochs>\n```\nPlease keep the materails in .cif file format. <br/>\nDuring training, state of model at each epoch will be saved at \"../model/\" path.\n\n### How to use CrysGNN in property predictor\n\nIn order to distill knowledge from CrysGNN and improve property prediction performance of your model, \nyou need to follow the following procedure during training of your model : <br/>\n- copy \"crysgnn\" folder into your model codebase\n- Include following Code snippet into your training script <br/><br/>\n```\n    ## Import CrysGNN class\n    from crysgnn.model import *\n    \n    ##Loading the CrysGNN model from checkpoints\n    \n    crysgnn_model = CrysGNN(64, nbr_fea_len, atom_fea_len, n_conv=5)\n    checkpoint_file_path = \"../model/crysgnn_state_checkpoint.pth.tar\"\n    checkpoint = torch.load(checkpoint_file_path)\n    crysgnn_model.load_state_dict(checkpoint['state_dict'])\n    criterion = nn.MSELoss()\n    \n    ##Computing Embedding Loss with your loss\n    \n    for <cif_file> in root_dir:\n        loss,atom_emb = your_model(your parameters)\n        \n        ## CrysGNN takes input atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx, which is similar to CGCNN model.\n        _, _, _, _, crysgnn_atom_emb = crysgnn_model(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx,args.cuda)\n        \n        # Embedding Loss\n        loss_emb = criterion(atom_emb,crysgnn_atom_emb)\n        \n        # Overall Loss\n        loss = 0.5*loss + 0.5*loss_emb \n```\n- Keep the hidden embedding dimension same for your_model and crysgnn_model(64).\nWe provide distilled version of cgcnn in the \"distilled baslines\" directory. You can follow it. \nFor any further query, feel free to contact [Kishalay Das](kishalaydas@kgpian.iitkgp.ac.in)\n\n## How to cite\n\nIf you have any query about CrysGNN, please contact me at [kishalaydas@kgpian.iitkgp.ac.in](kishalaydas@kgpian.iitkgp.ac.in)\n\n## How to cite\n\nIf you are using CrysGNN, please cite our work as follow :\n\n```\n@article{Das_Samanta_Goyal_Lee_Bhattacharjee_Ganguly_2023, \ntitle={CrysGNN: Distilling Pre-trained Knowledge to Enhance Property Prediction for Crystalline Materials}, \nvolume={37}, \nurl={https://ojs.aaai.org/index.php/AAAI/article/view/25892}, \nDOI={10.1609/aaai.v37i6.25892}, number={6}, \njournal={Proceedings of the AAAI Conference on Artificial Intelligence}, \nauthor={Das, Kishalay and Samanta, Bidisha and Goyal, Pawan and Lee, Seung-Cheol and Bhattacharjee, Satadeep and Ganguly, Niloy}, \nyear={2023}, month={Jun.}, pages={7323-7331} \n}\n```\n\n",
    "readme_length": 5734
  },
  {
    "name": "ESAN-VSP",
    "full_name": "cj4L/ESAN-VSP",
    "description": "[Pattern Recognition]Video Saliency Prediction using Enhanced Spatiotemporal Alignment Network",
    "stars": 25,
    "forks": 4,
    "language": "Python",
    "url": "https://github.com/cj4L/ESAN-VSP",
    "topics": [
      "convlstm",
      "dcnv2",
      "video-saliency-prediction"
    ],
    "created_at": "2019-12-29T08:16:42Z",
    "updated_at": "2024-12-17T13:34:05Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# Video Saliency Prediction Using Enhanced Spatiotemporal Alignment Network\n* PDF: [arXiv](https://arxiv.org/abs/2001.00292) or [sciencedirect](https://www.sciencedirect.com/science/article/pii/S0031320320304180)\n\n## Overview of our method\n![](https://github.com/cj4L/ESAN-VSP/raw/master/pic/network.png)\n\n## Results download\n&emsp;Our results on DHF1K, HollyWood-2, UCF and DIEM can be downloaded by Google Drive or Baidu wangpan. Which consists of 4 training settings with the training set(s) from (i)DHF1K, (ii)HollyWood-2, (iii)UCF sports, (iv)DHF1K + HollyWood-2 + UCF sports.\n\n  settings/datasets  |DHF1K | HollyWood-2|UCF|DIEM|\n  ---| ---  | ---   | ---   | ---   | \n  setting(i)|<a href=\"https://drive.google.com/open?id=1TRheAJrYT4KxZSeO7NCFLW0KgRXwv4vg\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a> | <a href=\"https://drive.google.com/open?id=1L0hbBpC9OoFGXCg-OG24l-DthKqnBIKA\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a> | <a href=\"https://drive.google.com/open?id=1mpwCQdQRX0ZTqoJzMdDNvAfJElPFCfwp\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a> | <a href=\"https://drive.google.com/open?id=1qCZ2gsiC085datnSKIKMvNhZr-pNv69v\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a>\n  setting(ii)|<a href=\"https://drive.google.com/open?id=1-zyWZhhmPvG8oo_z7nD-qznr2keNRgCh\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a> | <a href=\"https://drive.google.com/open?id=10zDWXK-ng4BaBNjbIeS6LefF8QYGj_Rt\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a> | <a href=\"https://drive.google.com/open?id=1XlKBv7oukaUM2BDQKrjg9UTYjFEObOqX\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a> | <a href=\"https://drive.google.com/open?id=1DCYzK1SQ9AWYq0arNRt0BEHnc3aLhs5k\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a>\n  setting(iii)|<a href=\"https://drive.google.com/open?id=13CxZXPatYP2O7KR2hQA9NPqp-Dc63nJy\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a> | <a href=\"https://drive.google.com/open?id=1XrAogBffOsEdh3x7aB-Vb5cPjnGy_f7k\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a> | <a href=\"https://drive.google.com/open?id=1KN6enpI3P8LvQtN7CNYe21uYTNZaMqJF\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a> | <a href=\"https://drive.google.com/open?id=1Sd1kFHA7NRUVI-hf_X66VW7Yx2V08wvs\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a>\n  setting(iv)|<a href=\"https://drive.google.com/open?id=10zYqjO2KyEe0tZ-K4iFrtcyVt0Q0Irc3\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a> | <a href=\"https://drive.google.com/open?id=1AS7Zhz7shui2EHeL1srEhGgpLfJfeo2u\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a> | <a href=\"https://drive.google.com/open?id=1XkLKAlUuCl8tgdXFfF_9JA6AqXpEb4nw\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a> | <a href=\"https://drive.google.com/open?id=12ktmGBcjb2EkYMEfCKfA21T7Mn1IYoJs\"><img src=\"https://github.com/cj4L/ESAN-VSP/raw/master/pic/googledrive.png\" width=\"35\" height=\"30\"></a>\n  \n## Preparation\n### Datasets download\n&emsp;How to get million pictures is the first barrier in video saliency prediction task. Thanks to [@wenguanwang](https://github.com/wenguanwang) proposed, pre-processed and published some pictures. DHF1K, HollyWood-2, UCF can be downloaded by Google Drive in his [repo](https://github.com/wenguanwang/DHF1K#dhf1k).\n\n&emsp;For convenience, we clone and upload the duplicate of HollyWood-2 to Baidu wangpan to download. All data belongs to the original author, sharing the duplicate is only for academic development, if there is any infringement, please contact me.\n\n&emsp;We adopt [here](https://github.com/wenguanwang/DHF1K/blob/master/make_gauss_masks.m) to pre-processed the DIEM datasets. Following\n[STRA-Net](https://github.com/ashleylqx/STRA-Net), the testing sets contain 20 selected videos which including first 300 frames per video, and some frames without labels are eliminated. Click [here](https://drive.google.com/open?id=1rCvtBQxMdqoy9gmisZzhOi_LLBddFArk) to download in Google Drive.\n\n### Models download\n&emsp;We use the VGG16 pretrained weights from PyTorch official version [here](https://pytorch.org/docs/stable/_modules/torchvision/models/vgg.html#vgg16), and we remove the last few layers. Click [here](https://drive.google.com/open?id=1Ar3pF4bzNWX-CSXaWcQqSoTRS_46KSLl) to download in Google Drive, click [here](https://pan.baidu.com/s/1uJFG2O3_Vc6qdFhlsGrDhQ) to download in Baidu wangpan.\n\n&emsp;The trained model in setting(iv): click [here](https://drive.google.com/open?id=1sJHoD-2ypsLzyKSn3JiHl2pdSmwpakpc) to download in Google Drive, click [here](https://pan.baidu.com/s/1Lqwu-LYIrO1JgoxQkcnaBw) to download in Baidu wangpan.\n\n### Experiments platform\n&emsp;OS: ubuntu 16.04\n\n&emsp;RAM: 64G\n\n&emsp;CPU: Intel i7-8700\n\n&emsp;GPU: Nvidia RTX 2080Ti * 2\n\n&emsp;Language: Python 3\n\n### Enviroment dependencies\n&emsp;Due to the compilation of DCN need earlier version PyTorch and torch.trapz() function need newer, so our dependencies are listed:\n\n&emsp;Training and testing phase: PyTorch 1.0.1.post2, torchvision, Pillow, numpy, scipy and other dependencies.\n\n&emsp;Eval phase: PyTorch 1.2 or newer, Pillow, numpy, scipy, tkinter and other dependencies.\n\n## First\n* We use [DCN-V2](https://github.com/CharlesShang/DCNv2) and modify something in dcn_v2.py, you need replace file and re-compile it. \n\n## Test\n* Get or download the dataset.\n* Download pretrained model in [Google Drive](https://drive.google.com/file/d/1sJHoD-2ypsLzyKSn3JiHl2pdSmwpakpc/view?usp=sharing).\n* Modify the config.py and run test.py.\n\n## Val\n* Modify the config.py and run eval.py.\n\n## Train\n* Get or download the dataset.\n* Download VGG16 pretrained weights in [Google Drive](https://drive.google.com/file/d/1KIWIspVxLRwv8bzOuMn6lY8kStoedToV/view?usp=sharing). Actually is from PyTorch offical model weights, expect for deleting the last serveral layers.\n* Modify the config.py and run main.py.\n\n### Notes\n* There is something wrong about the share of BaiduPan, contact me if want.\n\n#### Schedule\n- [x] Create github repo (2019.12.29)\n- [x] Release arXiv pdf (2020.01.05)\n- [x] Release all results (2020.01.09)\n- [x] Add preparation (2020.01.13)\n- [x] Test and Train code (2021.06.04)\n",
    "readme_length": 6863
  },
  {
    "name": "STanHop",
    "full_name": "MAGICS-LAB/STanHop",
    "description": "[ICLR 2024] STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction",
    "stars": 25,
    "forks": 2,
    "language": "Python",
    "url": "https://github.com/MAGICS-LAB/STanHop",
    "topics": [
      "attention-mechanism",
      "hopfield",
      "modern-hopfield-model",
      "modern-hopfield-networks",
      "time-series-forecasting",
      "time-series-prediction",
      "transformer"
    ],
    "created_at": "2024-01-18T15:26:55Z",
    "updated_at": "2025-11-13T12:41:32Z",
    "homepage": "https://arxiv.org/abs/2312.17346",
    "license": "MIT License",
    "readme": "# STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction\n\nThis is the code for the paper: **STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction**\n\nto run the experiment, \nFirst create a json file inside folder `config/`, and name it `<data>_<out_len>.json`.\nFor more, please see example file.\nthen use\netc: `python3 run.py --data <data> --out_len <out_len>`.\n\n## If you find our code useful, please consider citing our work\n```\n@inproceedings{\nwu2023stanhop,\ntitle={STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction},\nauthor={Wu, Dennis and Hu, Jerry Yao-Chieh and Li, Weijian and Chen, Bo-Yu and Liu, Han},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://arxiv.org/abs/2312.17346}\n}\n```\n",
    "readme_length": 830
  },
  {
    "name": "DIB-PEB-Sequential-RS",
    "full_name": "ouououououou/DIB-PEB-Sequential-RS",
    "description": "A tensorflow implementation of the  paper \"Dynamic Item Block and Prediction Enhancing Block for Sequential Recommendation\" in IJCAI 2019",
    "stars": 24,
    "forks": 6,
    "language": "Python",
    "url": "https://github.com/ouououououou/DIB-PEB-Sequential-RS",
    "topics": [],
    "created_at": "2019-05-17T10:01:12Z",
    "updated_at": "2024-07-01T14:32:17Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# DIB-PEB-Sequential-RS\nA tensorflow implementation of the  paper \"Dynamic Item Block and Prediction Enhancing Block for Sequential Recommendation\" in IJCAI 2019\n\néœ€æ±‚çŽ¯å¢ƒï¼š\n---------\npython3.6ï¼Œtensorflow\n\n1.paperåŽŸæ–‡\n------\n  åœ¨Githubä¸­ç›´æŽ¥å¯ä»¥ä¸‹è½½ï¼Œæ–‡ä»¶åä¸ºDynamic Item Block and Prediction Enhancing Block for Sequential Recommendation.pdf\n  \n2.å¦‚ä½•èŽ·å–è®­ç»ƒæ•°æ®é›†ï¼Ÿ\n------\n  è‡ªå¸¦äº†ml-100kæ•°æ®é›†ï¼Œå­˜æ”¾åœ¨dataset/processed_datasets/ml-100k<br>\n  è¿˜å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼èŽ·å¾—å…¶ä½™ä¸‰ä¸ªæ•°æ®é›†<br>\n  ç™¾åº¦ç½‘ç›˜åœ°å€ï¼šhttps://pan.baidu.com/s/1X7G3dVEJ0JN7yppcKLY4pA<br>\n  ä¸‹è½½åŽè§£åŽ‹åœ¨dataset/processed_datasetsç›®å½•ä¸‹å³å¯<br>\n\n\n3.å¦‚ä½•è®­ç»ƒæ¨¡åž‹\n------\n  ç›´æŽ¥æ‰§è¡ŒRUM-Ksoft-mulcha-test.pyå³å¯<br>\n  ä»¥ä¸‹æ–‡ä»¶ï¼š<br>\n  caser-test.py <br>\n  fpmc-test.py <br>\n  gru4rec-test.py <br>\n  rum-i-test.py<br>\n  ä¸ºpaperä¸­æ‰€é€‰baselineï¼Œä¹Ÿå¯ä»¥ç›´æŽ¥è¿è¡Œ<br>\n  \n4å‚æ•°è¯´æ˜Ž\n------\n'learnRate': 0.002,             å­¦ä¹ é€ŸçŽ‡<br>\n'maxIter': 2000,                è¿­ä»£æ¬¡æ•°<br>\n'trainBatchSize': 512,          è®­ç»ƒbatch_size<br>\n'testBatchSize': 512,           æµ‹è¯•batch_size <br>\n'numFactor': 128,               itemå’Œuser embeddingçš„size<br>\n'cell_numbers': 128,            å¦‚æžœé€‰ç”¨GRUä¸ºä¸­é—´ç½‘ç»œç»“æž„ï¼Œcellçš„æ•°é‡<br>\n'topN': 10,                     testæ—¶é€‰TopNè¿›è¡Œè¯„æµ‹<br>\n'gru_model': False,             è®¾ç½®ä¸ºTrueåˆ™ä¸­é—´ç½‘ç»œç»“æž„ä¸ºGRUï¼Œè®¾ç½®ä¸ºFalseåˆ™ä¸ºmemory network<br>\n'decrease soft': True,          è®¾ç½®ä¸ºFalseï¼ŒæŸå¤±å‡½æ•°ä¸ºsoftmax + cross entropyï¼Œè®¾ç½®ä¸ºTrueï¼Œåˆ™é‡‡ç”¨å…¶ä»–loss<br>\n'loss_type': 'PEB',             åœ¨decrease softä¸ºTrueæƒ…å†µä¸‹æœ‰æ•ˆï¼Œå¯é€‰æ‹©top1ï¼Œbprï¼Œnegï¼ŒPEB<br>\n'negative_numbers': 25,         è´Ÿæ ·æœ¬æ•°é‡<br>\n'eval_item_num': 1000,          trainé˜¶æ®µæŠ½å–å¤šå°‘itemä½œä¸ºä¸€ä¸ªè¯„ä»·å­é›†ï¼Œå½“æ•°æ®é›†ä¸ºml-100kæ—¶ï¼Œè®¾ç½®ä¸º500å·¦å³æ¯”è¾ƒåˆé€‚<br>\n'numK': 15,                     PEBæ€»å…±è®¡ç®—äº†Kæ¬¡æ¦‚çŽ‡åˆ†å¸ƒ<br>\n'save_path': 'saved_model',<br>\n'save_model': True,<br>\n'load_model': False,<br>\n\nfor fileName in ['newkin-seq']:   æ‹¬å·ä¸­ä¸ºè®­ç»ƒæ‰€ç”¨æ•°æ®é›†ï¼Œå¯è®¾ç½®ä¸º'newkin-seq','ml-100k','cd-seq','movies_tv-seq'\n",
    "readme_length": 1698
  },
  {
    "name": "survml-transformer-rul-prediction",
    "full_name": "survml/survml-transformer-rul-prediction",
    "description": "Transformer implementation with PyTorch for remaining useful life prediction on turbofan engine with NASA CMAPSS data set. Inspired by Mo, Y., Wu, Q., Li, X., & Huang, B. (2021). Remaining useful life estimation via transformer encoder enhanced by a gated convolutional unit. Journal of Intelligent Manufacturing, 1-10.",
    "stars": 24,
    "forks": 1,
    "language": "Python",
    "url": "https://github.com/survml/survml-transformer-rul-prediction",
    "topics": [],
    "created_at": "2023-01-07T03:37:53Z",
    "updated_at": "2025-09-21T11:55:19Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "# PyTorch Transformer for RUL Prediction       \nTransferred from https://github.com/jiaxiang-cheng/PyTorch-Transformer-for-RUL-Prediction        \nAn implementation with Transformer encoder and convolution layers with PyTorch for remaining useful life prediction.   \n_Author: Jiaxiang Cheng, Nanyang Technological University, Singapore_\n\n<img alt=\"Python\" src=\"https://img.shields.io/badge/python-%2314354C.svg?style=for-the-badge&logo=python&logoColor=white\"/> <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white\" />\n\n## Quick Run\nSimply run `python train.py --dataset FD001`. And you will get the training loss and testing result for each epoch where the RMSE is from the test set:\n```\nEpoch: 0, loss: 9474.43470, RMSE: 61.11946\nEpoch: 1, loss: 5858.27227, RMSE: 46.03318\nEpoch: 2, loss: 3208.53410, RMSE: 29.78244\nEpoch: 3, loss: 1310.71390, RMSE: 22.94705\n...\n```\nThe testing is conducted for each epoch as the data set is not large so it's no big deal but you may remove them and only do the evaluation after finishing the training epochs.\n\n## Environment Details\n```\npython==3.8.8\nnumpy==1.20.1\npandas==1.2.4\nmatplotlib==3.3.4\npytorch==1.8.1\n```\n\n## Credit\nThis work is inpired by Mo, Y., Wu, Q., Li, X., & Huang, B. (2021). Remaining useful life estimation via transformer encoder enhanced by a gated convolutional unit. Journal of Intelligent Manufacturing, 1-10.\n\n## Citation\n[![DOI](https://zenodo.org/badge/365902211.svg)](https://zenodo.org/badge/latestdoi/365902211)\n\n## License\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n",
    "readme_length": 1680
  },
  {
    "name": "Acropole",
    "full_name": "DGAC/Acropole",
    "description": "This repository contains the Acropole model for aircraft fuel flow prediction and Python packages for aircraft trajectory processing and fuel flow enhancement.",
    "stars": 23,
    "forks": 5,
    "language": "Python",
    "url": "https://github.com/DGAC/Acropole",
    "topics": [
      "air-traffic-management",
      "aircraft-performance",
      "artificial-intelligence",
      "environment",
      "machine-learning"
    ],
    "created_at": "2023-04-04T09:25:21Z",
    "updated_at": "2025-11-14T05:07:48Z",
    "homepage": "",
    "license": "GNU Affero General Public License v3.0",
    "readme": "# Acropole <img src=\"https://github.com/DGAC/Acropole/blob/main/logo.png\" width=\"30\">\n\nThis repository contains the Acropole model for aircraft fuel flow prediction and Python packages for aircraft trajectory processing and fuel flow enhancement.\n\n## Easy Install\n\nFor a trouble-free installation, creating a dedicated anaconda environment is recommended :\n\n```sh\nconda create -n acropole python=3.12 -c conda-forge\n```\n\nActivate the conda environment :\n\n```sh\nconda activate acropole\n```\n\nInstall this library:\n\n```sh\ngit clone https://github.com/DGAC/Acropole.git\ncd Acropole\npip install .\n\n```\n\n## Example of use\n\nHere is a minimal working example:\n\n```python\nimport pandas as pd\nfrom acropole import FuelEstimator\n\nfe = FuelEstimator()\n\nflight = pd.DataFrame({\n    \"typecode\": [\"A320\", \"A320\", \"A320\", \"A320\"],\n    \"groundspeed\": [400, 410, 420, 430],\n    \"altitude\": [10000, 11000, 12000, 13000],\n    \"vertical_rate\": [2000, 1500, 1000, 500],\n\n    # optional features:\n    \"second\": [0.0, 1.0, 2.0, 3.0],\n    \"airspeed\": [400, 410, 420, 430],\n    \"mass\": [60000, 60000, 60000, 60000]\n})\n\nflight_fuel = fe.estimate(flight) #flight.data if traffic flight\n```\n\nNote:\n\n- When the `second` column is provided, the fuel estimation is more accurate,\n  especially due to **derivatives of speeds** (acceleration) used in the estimation.\n- `airspeed` is optional. If not provided, it is assumed to be equal\n  to groundspeed. However, accurate airspeed is recommended for better estimation.\n- Expected sampling rate is 4 seconds, higher or lower sampling rate might induce noisier fuel flow. Resampling data before estimating fuel flow is recommanded.\n\nFor a more complete example, refer to `examples/fuel_estimation.ipynb`\n\n## Aircraft data and estimation models\n\nAircraft parameters from open data to feed the model are available in `data/aircraft_params.csv` and loaded by default. Model data is available in `models/` and also loaded by default.\n\nYou can specify your own data and model file with the following initialization of `FuelEstimator`. You need to make sure the same column names are in your aircraft CSV file.\n\n```python\nfe = FuelEstimator(\n    aircraft_params_path=\"path/to/your/data.csv\",\n    model_path=\"path/to/your/SavedModel/\",\n)\n```\n\n## Model training and evaluation\n\nThe Acropole model is a neural network built using data from Quick Access Recorder (QAR) from different aircraft types. Evaluation of the model and list of aircraft is available at https://github.com/DGAC/Acropole/tree/main/evaluation/Dense_Acropole_FuelFlow_Scaling.\n\n\n## Comparison of Different Model Performances\n\nComparison of different model performances per phase for 1000 test flights of A320-214 aircraft using real mass and true airspeed.\n\n| Phase | Samples \\# | Metric | ACROPOLE | OpenAP | OpenAP V2 | BADA  | Poll-Schumann |\n|-------|------------|--------|--------------|--------------|------------|---------------|----------------|\n|       |            | **MAPE (%)**  | 2.13    | 30.35                     | 8.84       | 6.53                       | 6.85                                       |\n| CLIMB | 1,403,850   | **MAE (kg/min)** | 1.66          | 25.81                     | 6.92       | 5.53                       | 5.65                                       |\n|       |            | **ME (kg/min)**  | 0.85     | -25.66                    | -2.48      | -5.27                      | -4.62                                      |\n||||||||||\n|       |            | **MAPE (%)**  | 4.41   | 18.59                     | 10.69      | 7.01                       | 4.84                                       |\n| LEVEL | 4,017,801   | **MAE (kg/min)** | 1.82      | 7.82                      | 3.48       | 2.65                       | 2.03                                       |\n|       |            | **ME (kg/min)**  | 1.22     | -7.47                     | 2.64       | -1.43                      | -0.73                                      |\n||||||||||\n|       |            | **MAPE (%)**  | 12.63      | 51.69                     | 32.4       | 21.50                      | 21.55                                      |\n| DESCENT| 1,684,117  | **MAE (kg/min)** | 2.71         | 8.62                      | 5.58       | 3.71                       | 4.71                                       |\n|       |            | **ME (kg/min)**  | 1.88         | -1.75                     | -1.16      | -0.64                      | -3.67                                      |\n||||||||||\n|       |            | **MAPE (%)**  | 5.91       | 27.60                     | 14.71      | 9.84                       | 8.61                                       |\n| ALL   | 7,105,768   | **MAE (kg/min)** | 1.99        | 11.55                     | 4.58       | 3.44                       | 3.29                                       |\n|       |            | **ME (kg/min)**  | 1.30       | -9.92                     | 0.84       | -2.03                      | -2.09                                      |\n||||||||||\n|       |            | **Processing time (s)** | 3          | 284                      | 255        | 474                        | 28                                         |\n\n\n\n## Credits\n\n```bibtex\n@inproceedings{jarry2024towards,\n  title={Towards aircraft generic Quick Access Recorder fuel flow regression models for ADS-B data},\n  author={Jarry, Gabriel and Delahaye, Daniel and Hurter, Christophe},\n  booktitle={International Conference on Research in Air Transportation},\n  year={2024}\n}\n\n```\n",
    "readme_length": 5502
  },
  {
    "name": "REMed",
    "full_name": "starmpcc/REMed",
    "description": "REMed: Retrieval-Enhanced Medical prediction model",
    "stars": 22,
    "forks": 5,
    "language": "Python",
    "url": "https://github.com/starmpcc/REMed",
    "topics": [
      "amsterdamumcdb",
      "eicu",
      "hirid-dataset",
      "medical",
      "mimic-iv",
      "mlhc",
      "prediction",
      "remed",
      "retrieval"
    ],
    "created_at": "2023-10-10T05:30:02Z",
    "updated_at": "2025-11-20T12:24:38Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# REMed: Retrieval-Enhanced Medical Prediction Model\nOfficial implementation for [General-purpose Retrieval-Enhanced Medical Prediction Model using Near-Infinite History](https://arxiv.org/abs/2310.20204)\n\n> Developing medical prediction models based on EHRs typically relies on expert opinion for feature selection and adjusting the observation window size.\nTo address these issues, we propose **R**etrieval-**E**nhanced **Med**ical prediction model (**REMed**), which can essentially evaluate an unlimited number of medical events, retrieve the relevant ones, and make predictions.\n\n![model_architecture](resources/model.jpg)\n\n## Update\n- 2024.07.05: Our paper has been accepted on Machine Learning for Healthcare Conference (MLHC) 2024!\n- 2024.03.25: REMed now support [UMCdb](https://amsterdammedicaldatascience.nl/amsterdamumcdb/) and [HIRID](https://hirid.intensivecare.ai/)!\n\n\n## Standalone REMed\n- For enhanced accessibility, we offer a simplified, standalone REMed model available in `standalone_remed.py`.\n- This model takes a list of event vectors and their corresponding timestamps as input, and performs a binary classification.\n- For multi-task or multi-class support, please refer to the original code.\n- Dependencies: `pytorch` and `transformers`.\n\n```python\nimport torch\nfrom standalone_remed import REMed\n\nmodel = REMed(\n    pred_dim=512,  # Model hidden dimension size (E)\n    n_heads=8,  # Number of heads for Transformer Predictor\n    n_layers=2,  # Number of layers for Transformer Predictor\n    dropout=0.2,  # Dropout rate\n    max_retrieve_len=128,  # Maximum number of retrieved events (k of Top-k)\n    pred_time=48,  # Prediction time. Set to maximum of the input timestamp (h)\n)\n\nreprs = torch.randn(2, 1000, 512) # Batch of list of event vectors (B, L, E)\ntimes = torch.randint(0, 48*60, (2, 1000)) # Batch of list of event times (B, L) (unit=Minute)\n\nmodel(reprs, times) # Return probability between [0,1] (B, 1)\n```\n\n\n## Reproducing Guide (Paper)\n\n> [!NOTE]\n> For the MEDS-formatted dataset, please follow the instructions in the section below.\n\n<details>\n<summary>Requirements</summary>\n\n- For preprocessing: `python>=3.8, Java>=8`\n```bash\npip install numpy pandas tqdm treelib transformers pyspark polars\n```\n\n- For training & test\n```bash\nexport PATH=/usr/local/cuda/bin:$PATH\nconda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia\nconda install numpy pandas einops h5pickle tqdm scikit-learn -y\npip install performer_pytorch recurrent_memory_transformer_pytorch==0.2.2 transformers==4.30.1 accelerate==0.20.3 \ncd src/models/kernels/\npython setup.py install\n```\n\n</details>\n\n<details>\n<summary> Data Preprocessing </summary>\n\n- We use [Integrated-EHR-Pipeline](https://github.com/Jwoo5/integrated-ehr-pipeline) for MIMIC-IV and eICU database. \n- NOTE: This process requires high RAM. If you meet out-of-memory, please lower the `--num_threads`\n\n```bash\ngit clone https://github.com/Jwoo5/integrated-ehr-pipeline\ngit checkout snub\n```\n\n```bash\n# MIMIC-IV, 48h Prediction time\npython main.py --ehr mimiciv --data {MIMIC-IV Path} --obs_size 48 --pred_size 48 --max_patient_token_len 2147483647 --max_event_size 2147483647 --use_more_tables --dest {DATA_PATH}/48h --num_threads 32 --readmission --diagnosis --min_event_size 0 --seed \"2020, 2021, 2022, 2023, 2024\" --use_ed\n\n# MIMIC-IV, 24h Prediction time\npython main.py --ehr mimiciv --data {MIMIC-IV Path} --obs_size 48 --pred_size 24 --max_patient_token_len 2147483647 --max_event_size 2147483647 --use_more_tables --dest {DATA_PATH}/24h --num_threads 32 --readmission --diagnosis --min_event_size 0 --seed \"2020, 2021, 2022, 2023, 2024\" --use_ed\n\n# eICU, 48h Prediction time\npython main.py --ehr eicu --data {eICU Path} --obs_size 48 --pred_size 48 --max_patient_token_len 2147483647 --max_event_size 2147483647 --use_more_tables --dest {DATA_PATH}/48h --num_threads 32 --readmission --diagnosis --min_event_size 0 --seed \"2020, 2021, 2022, 2023, 2024\"\n\n# eICU, 24h Prediction time\npython main.py --ehr eicu --data {eICU Path} --obs_size 48 --pred_size 24 --max_patient_token_len 2147483647 --max_event_size 2147483647 --use_more_tables --dest {DATA_PATH}/24h --num_threads 32 --readmission --diagnosis --min_event_size 0 --seed \"2020, 2021, 2022, 2023, 2024\"\n```\n\n</details>\n\n<details>\n<summary>Pretrain Encoder & Caching</summary>\n\n- We used NVIDIA RTX A6000 (48GB) for pretraining & Encoding\n- If you meet CUDA OOM, please adjust the numbers in `src/main.py:270-271`\n- This requires large empty disk space (>200G)\n\n```bash\naccelerate launch \\\n    --config_file config/single.json \\\n    --num_processes 1 \\\n    --gpu_ids ${GPU_ID} \\\n    main.py \\\n    --src ${SRC_DATA} \\\n    --input ${DATA_PATH} \\\n    --save_dir ${SAVE_PATH} \\\n    --train_type short \\\n    --time -99999 \\\n    --pred_time ${PRED_TIME} \\\n    --lr 5e-5 \\\n    --random_sample \\\n    --encode_events \\\n    # if you want to log using wandb\n    --wandb \\\n    --wandb_project_name ${PROJECT_NAME} \\\n    --wandb_entity_name ${ENTITY_NAME} \\\n```\n- As a result, you can get `${SRC_DATA}_encoded.h5` at `${SAVE_PATH}/${EXPERIMENT_NAME}`.\n\n\n</details>\n\n<details>\n<summary>Train REMed</summary>\n\n- Note that the `${EXPERIMENT_NAME}` refers to the name of the pre-training experiment.\n- If you want to run an experiment with infinite observation window, set time=-99999\n- Otherwise, the time should be {PRED_TIME} - {OBS_SIZE} (e.g. pred time 48h, obs 12h -> time 36)\n```bash\naccelerate launch \\\n    --config_file config/single.json \\\n    --num_processes 1 \\\n    --gpu_ids ${GPU_ID} \\\n    main.py \\\n    --src ${SRC_DATA} \\\n    --input ${DATA_PATH} \\\n    --save_dir ${SAVE_PATH} \\\n    --train_type remed \\\n    --time ${TIME} \\\n    --pred_time ${PRED_TIME} \\\n    --lr 1e-5 \\\n    --scorer \\\n    --scorer_use_time \\\n    --pretrained ${EXPERIMENT_NAME} \\\n    --no_pretrained_checkpoint \\\n    # if you want to log using wandb\n    --wandb \\\n    --wandb_project_name ${PROJECT_NAME} \\\n    --wandb_entity_name ${ENTITY_NAME}\n```\n\n</details>\n\n## Support for MEDS dataset\nWe officially support to process [MEDS](https://github.com/Medical-Event-Data-Standard/meds/releases/tag/0.3.0) dataset (currently, MEDS v0.3) with a cohort defined by [ACES](https://github.com/justin13601/ACES), only for the REMed model.\nIt consists of 4 steps in total, each of which can be run by Python or shell scripts that are prepared in [`scripts/meds/`](scripts/meds/) directory.\nFor more detailed information, please follow the instructions below.  \nNote that all the following commands should be run in the root directory of the repository, not in `scripts/meds/` or any other sub-directories.  \nAdditionally, the following scripts assume your dataset is split into `\"train\"`, `\"tuning\"`, and `\"held_out\"` subsets for training, validation, and test, respecitvely. If it doesn't apply to your case, you can modify them by adding these command line arguments: `--train_subset`, `--valid_subset`, and `--test_subset`. For example, if you need to process only the train subset, you can specify it by adding `--train_subset=\"train\" --valid_subset=\"\" --test_subset=\"\"`.\n\n### Processing MEDS dataset\n<details>\n<summary>Preprocessing MEDS dataset</summary>\n\n* We provide a script to preprocess MEDS dataset with a cohort defined by [ACES](https://github.com/justin13601/ACES) to meet the input format for REMed.\n    ```shell script\n    $ python scripts/meds/process_meds.py $MEDS_PATH \\\n        --cohort $ACES_COHORT_PATH \\\n        --output_dir $PROCESSED_MEDS_DIR \\\n        --rebase \\\n        --workers $NUM_WORKERS\n    ```\n    * `$MEDS_PATH`: path to MEDS dataset to be processed. It can be a directory or the exact file path with the file exenstion (only `.csv` or `.parquet` allowed). If provided with directory, it tries to scan all `*.csv` or `*.parquet` files contained in the directory recursively.\n    * `$ACES_COHORT_PATH`: path to the defined cohort, which must be a result of [ACES](https://github.com/justin13601/ACES). It can be a directory or the exact file path that has the same file extension with the MEDS dataset to be processed. The file structure of this cohort directory should be the same with the provided MEDS dataset directory (`$MEDS_PATH`) to match each cohort to its corresponding shard data.\n    * `$PROCESSED_MEDS_DIR`: directory to save processed outputs.\n    * `$NUM_WORKERS`: number of parallel workes to multi-process the script.\n    * **NOTE: If you encounter this error:** _\"polars' maximum length reached. consider installing 'polars-u64-idx'\"_, **please consider using more workers or doing `pip install polars-u64-idx`.**\n* As a result of this script, you will have .h5 and .tsv files that has a following respective structure:\n    * *.h5\n        ```\n        *.h5\n        â””â”€â”€ ${cohort_id}\n            â””â”€â”€ \"ehr\"\n                â”œâ”€â”€ â€œhiâ€\n                â”‚\tâ””â”€â”€ np.ndarray with a shape of (num_events, 3, max_length)\n                â”œâ”€â”€ â€œtimeâ€\n                â”‚\tâ””â”€â”€ np.ndarray with a shape of (num_events, )\n                â””â”€â”€ â€œlabelâ€\n                    â””â”€â”€ binary label (0 or 1) for ${cohort_id} given the defined task\n        ```\n        * `${cohort_id}`: `\"${patient_id}_${cohort_number}\"`, standing for \"N-th cohort in the patient\"\n        * Numpy array under `\"hi\"`\n            * `[:, 0, :]`: token input ids for the tokenized events with a maximum length of `max_length`\n            * `[:, 1, :]`: token type ids to distinguish where each input token comes from (special tokens such as `[CLS]` or `[SEP]`, column keys, or column values), which was firstly used in GenHPF. Can be set to all zeros.\n            * `[:, 2, :]`: ids for digit place embedding, which also originated from GenHPF. It assigns different ids to each of digit places for numeric (integer or float) items. Also can be set to all zeros.\n        * Numpy array under `\"time\"`\n            * Elapsed time in minutes from the first event to the last event.\n        * E.g.,\n            ```Python\n            >>> import h5pickle\n            >>> f = h5pickle.File(\"train.h5\", \"r\")\n            >>> f[\"ehr\"][\"10001472_0\"][\"hi\"]\n            <HDF5 dataset \"hi\": shape (13, 3, 128), type \"<i2\">\n            >>> f[\"ehr\"][\"10001472_0\"][\"time\"]\n            <HDF5 dataset \"time\": shape (13,), type \"<i4\">\n            >>> f[\"ehr\"][\"10001472_0\"][\"label\"]\n            <HDF5 dataset \"label\": shape (), type \"<i8\">\n            ```\n    * *.tsv\n        ```\n            patient_id\tnum_events\n        0\t10001472_0\t13\n        1\t10002013_0\t47\n        2\t10002013_1\t46\n        â€¦\tâ€¦\t\t    â€¦\n        ```\n\n</details>\n\n<details>\n<summary> Pretrain event encoder </summary>\n\n* This stage pretrains event encoder (e.g., GenHPF) using a random event sequence with a length of `max_seq_len` (by default, set to `128`) every epoch for each cohort sample.\n* After completing the pretraining, we should encode all the events in the dataset and cache them to reuse in the following stage.\n* For a shell script to run this, see [`./scripts/meds/pretrain.sh`](./scripts/meds/pretrain.sh).\n* For Python, please run:\n    ```shell script\n    accelerate launch \\\n        --config_file config/single.json \\\n        --num_processes 1 \\\n        --gpu_ids $GPU_ID \\\n        main.py \\\n        --src_data meds \\\n        --input_path $PROCESSED_MEDS_DIR \\\n        --save_dir $PRETRAIN_SAVE_DIR \\\n        --pred_targets meds_single_task \\\n        --train_type short \\\n        --lr 5e-5 \\\n        --random_sample \\\n        # if you want to log using wandb\n        --wandb \\\n        --wandb_entity_name $wandb_entity_name \\\n        --wandb_project_name $wandb_project_name\n    ```\n    * `$PROCESSED_MEDS_DIR`: directory containing processed MEDS data, expected to contain `*.h5` and `*.tsv` files.\n    * `$PRETRAIN_SAVE_DIR`: output directory to save the checkpoint for the pretrained event encoder.\n    * `$GPU_ID`: GPU index to be used for training the model.\n    * It will pretrain event encoder using the processed MEDS data, which will be used to encode all events present in the MEDS data for the REMed model later.\n    * Checkpoint for the pretrained event encoder will be saved to `$PRETRAIN_SAVE_DIR/${EXPERIMENT_NAME}` directory, where `${EXPERIMENT_NAME}` is a 32-length hexadecimal string generated automatically for each unique experiment.\n\n</details>\n\n<details>\n<summary> Encode all events present in the input MEDS data, and cache them </summary>\n\n* In this stage, we encode all events present in the input MEDS data, and cache them, which will be input data for the REMed model.\n* For a shell script to run this, see [`./scripts/meds/encode_events.sh`](./scripts/meds/encode_events.sh).\n* For Python, please run:\n    ```shell script\n    accelerate launch \\\n        --config_file config/single.json \\\n        --num_processes 1 \\\n        --gpu_ids=\"$GPU_ID\" \\\n        main.py \\\n        --src_data meds \\\n        --input_path $PROCESSED_MEDS_DIR \\\n        --save_dir $ENCODED_MEDS_DIR \\\n        --pred_targets meds_single_task \\\n        --train_type short \\\n        --random_sample \\\n        --encode_events \\\n        --encode_only \\\n        --resume_name $PRETRAINED_CHECKPOINT_DIR\n    ```\n    * `$PROCESSED_MEDS_DIR`: directory containing processed MEDS data, expected to contain `*.h5` and `*.tsv` files.\n    * `$ENCODED_MEDS_DIR`: output directory to save the encoded data where the file names will be `*_encoded.h5`.\n    * `$GPU_ID`: GPU index to be used for running the model.\n    * `$PRETRAINED_CHECKPOINT_DIR`: directory containing checkpoint for the pretrained event encoder, expected to be `$PRETRAIN_SAVE_DIR/${EXPERIMENT_NAME}` containing `checkpoint_best.pt`.\n    * It will encode all events present in the processed meds data (`*.h5`) located in `$PROCESSED_MEDS_DIR`, and save the results into `ENCODED_MEDS_DIR/*_encoded.h5`.\n    * Note that it requires large empty disk space (>200G) to save all the encoded events to the storage. This process will take about 3 hours (for ~7500 steps).\n\n</details>\n\n<details>\n<summary> Train REMed using the encoded MEDS dataset</summary>\n\n* In this stage, we finally train the REMed model using the encoded MEDS data.\n* After training ends, it will save the best checkpoint for the trained REMed model.\n* For a shell script to run this, see [`./scripts/meds/train.sh`](./scripts/meds/train.sh).\n* For Python, please run:\n    ```shell script\n    accelerate launch \\\n        --config_file config/single.json \\\n        --num_processes 1 \\\n        --gpu_ids $GPU_ID \\\n        main.py \\\n        --src_data meds \\\n        --input_path $ENCODED_MEDS_DIR \\\n        --save_dir $REMED_SAVE_DIR \\\n        --pred_targets meds_single_task \\\n        --train_type remed \\\n        --lr 1e-5 \\\n        --scorer \\\n        --scorer_use_time \\\n        --max_seq_len 200000 \\\n        # if you want to log using wandb\n        --wandb \\\n        --wandb_entity_name $wandb_entity_name \\\n        --wandb_project_name $wandb_project_name\n    ```\n    * `$ENCODED_MEDS_DIR`: directory containing encoded MEDS data, expected to contain `*_encoded.h5` files.\n    * `$REMED_SAVE_DIR`: output directory to save the REMed model checkpoint.\n    * `$GPU_ID`: GPU index to be used for running the model.\n\n</details>\n\n<details>\n<summary> Generate predicted results to the test cohort dataframe for a given task using trained REMed model </summary>\n\n* In this final stage, we load the trained REMed model to do prediction on the test cohort for a given task, and generate the predicted results as two additional columns, `predicted_label` and `predicted_prob`, to the test cohort dataframe.\n* For a shell script to run this, see [`./scripts/meds/predict.sh`](./scripts/meds/predict.sh).\n* For Python, please run:\n    ```shell script\n    accelerate launch \\\n        --config_file config/single.json \\\n        --num_processes 1 \\\n        --gpu_ids $GPU_ID \\\n        main.py \\\n        --src_data meds \\\n        --input_path $ENCODED_MEDS_DIR \\\n        --save_dir $SAVE_DIR \\\n        --pred_targets meds_single_task \\\n        --train_type remed \\\n        --scorer \\\n        --scorer_use_time \\\n        --test_only \\\n        --test_cohort $ACES_TEST_COHORT_DIR \\\n        --resume_name $CHECKPOINT_DIR\n    ```\n    * `$ENCODED_MEDS_DIR`: directory containing encoded MEDS data, expected to contain `*_encoded.h5` files.\n    * `$SAVE_DIR`: output directory to save the predicted results, which will be `$test_subset.parquet`. the results will be saved to `${SAVE_DIR}/${EXPERIMENT_NAME}` directory. this result file has the same rows with the test cohort dataframe provided with `$ACES_TEST_COHORT_DIR`, but has two additional columns: `predicted_label` and `predicted_prob`\n    * `$GPU_ID`: GPU index to be used for running the model.\n    * `$ACES_TEST_COHORT_DIR`: directory containing test cohorts generated from ACES, expected to contain `*.parquet` files.\n    * `$CHECKPOINT_DIR`: directory containing checkpoint for the trained REMed model, expected to be `$REMED_SAVE_DIR/${EXPERIMENT_NAME}`\n\n</details>\n\n## Citation\n```\n@InProceedings{pmlr-v252-kim24a,\n  title = \t {General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History},\n  author =       {Kim, Junu and Shim, Chaeeun and Yang, Bosco Seong Kyu and Im, Chami and Lim, Sung Yoon and Jeong, Han-Gil and Choi, Edward},\n  booktitle = \t {Proceedings of the 9th Machine Learning for Healthcare Conference},\n  year = \t {2024},\n  editor = \t {Deshpande, Kaivalya and Fiterau, Madalina and Joshi, Shalmali and Lipton, Zachary and Ranganath, Rajesh and Urteaga, IÃ±igo},\n  volume = \t {252},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {16--17 Aug},\n  publisher =    {PMLR},\n  pdf = \t {https://raw.githubusercontent.com/mlresearch/v252/main/assets/kim24a/kim24a.pdf},\n  url = \t {https://proceedings.mlr.press/v252/kim24a.html},\n  abstract = \t {Machine learning (ML) has recently shown promising results in medical predictions using electronic health records (EHRs).  However, since ML models typically have a limited capability in terms of input sizes, selecting specific medical events from EHRs for use as input is necessary.  This selection process, often relying on expert opinion, can cause bottlenecks in development.  We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges.  REMed can essentially evaluate unlimited medical events, select the relevant ones, and make predictions.  This allows for an unrestricted input size, eliminating the need for manual event selection.  We verified these properties through experiments involving 27 clinical prediction tasks across four independent cohorts, where REMed outperformed the baselines.  Notably, we found that the preferences of REMed align closely with those of medical experts.  We expect our approach to significantly expedite the development of EHR prediction models by minimizing cliniciansâ€™ need for manual involvement.}\n}\n```\n",
    "readme_length": 18856
  },
  {
    "name": "GMWI2",
    "full_name": "danielchang2002/GMWI2",
    "description": "Chang and Gupta et al., Gut Microbiome Wellness Index 2 Enhances Health Status Prediction from Gut Microbiome Taxonomic Profiles, Nature Communications (2024).",
    "stars": 22,
    "forks": 4,
    "language": "Python",
    "url": "https://github.com/danielchang2002/GMWI2",
    "topics": [],
    "created_at": "2022-07-22T17:08:35Z",
    "updated_at": "2025-08-19T08:28:13Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# GMWI2: Gut Microbiome Wellness Index 2\n![poop on a chip](./images/poop.png)\n\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/gmwi2/badges/version.svg)](https://anaconda.org/bioconda/gmwi2)\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/gmwi2/badges/platforms.svg)](https://anaconda.org/bioconda/gmwi2)\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/gmwi2/badges/license.svg)](https://anaconda.org/bioconda/gmwi2)\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/gmwi2/badges/downloads.svg)](https://anaconda.org/bioconda/gmwi2)\n\n### Description\n\nGMWI2 (Gut Microbiome Wellness Index 2) is a robust and biologically interpretable predictor of health status based on gut microbiome taxonomic profiles.\n\nOn a stool metagenome sample, this command-line tool performs four major steps:\n1. Quality control\n   1. Removal of overrepresented sequences (probable adapter sequences) using [fastqc](https://github.com/s-andrews/FastQC)\n   2. Removal of human DNA contaminants (reads that map to GRCh38/hg38) using [Bowtie2](https://github.com/BenLangmead/bowtie2)\n   3. Removal of adapter sequences and low quality reads using [Trimmomatic](https://github.com/timflutre/trimmomatic)\n2. Taxonomic profiling using [MetaPhlAn3](https://github.com/biobakery/MetaPhlAn) (v3.0.13) with the mpa_v30_CHOCOPhlAn_201901 marker database\n3. Transformation of taxonomic relative abundances into a binary presence/absence profile\n4. Computation of the GMWI2 score using a Lasso-penalized logistic regression model trained on a meta-dataset of 8,069 health status labeled stool shotgun metagenomes\n\nIf you use GMWI2, please cite:\n\n[Gut Microbiome Wellness Index 2 Enhances Health Status Prediction from Gut Microbiome Taxonomic Profiles](https://doi.org/10.1038/s41467-024-51651-9)\nChang and Gupta *et al.*, *Nature Communications* (2024).\n\n### System requirements\nGMWI2 is supported for macOS and Linux, and has been tested on the following systems:\n- macOS Big Sur 11.7.10\n- CentOS Linux 7 (Core)\n\n### Installation\n\nTo avoid dependency conflicts, please create an isolated conda environment and install the GMWI2 package. Installation via conda/mamba automatically installs GMWI2 and \nits dependencies.\nMake sure to perform step 4 to ensure that databases are downloaded and installed!\nInstallation should take ~30 minutes.\n\n1. Create new conda environment and install mamba\n```bash\nconda create --name gmwi2_env -c conda-forge mamba python=3.8\n```\n\n2. Activate environment\n```bash\nconda activate gmwi2_env\n```\n\n3. Install GMWI2 package with mamba\n```bash\nmamba install -c bioconda -c conda-forge gmwi2=1.6\n```\n\n4. Download/install databases (and verify that the package was installed correctly) by running GMWI2 on a tiny simulated stool metagenome. This tool automatically installs databases during the first run (should take ~20 minutes). To avoid issues in downloading databases, please run this step before submitting multiple concurrent batch jobs.\n```bash\n# download the tiny stool metagenome\nwget https://raw.githubusercontent.com/danielchang2002/GMWI2/main/example/tiny/tiny_1.fastq\nwget https://raw.githubusercontent.com/danielchang2002/GMWI2/main/example/tiny/tiny_2.fastq\n\ngmwi2 -f tiny_1.fastq -r tiny_2.fastq -n 16 -o tiny\n```\n\n### Usage\n\nTry downloading and running GMWI2 on a real [example stool metagenome](./example) from the pooled dataset used to develop GMWI2 (should take ~20 minutes).\n\n```bash\nInput: Two (forward/reverse) raw fastq (or fastq.gz) files generated from paired-end stool metagenome reads\nOutput: The GMWI2 (Gut Microbiome Wellness Index 2) score\n\nusage: gmwi2 [-h] -n NUM_THREADS -f FORWARD -r REVERSE -o OUTPUT_PREFIX [-v]\n\n* Example usage:\n\n$ ls\n.\nâ”œâ”€â”€ forward.fastq\nâ””â”€â”€ reverse.fastq\n\n$ gmwi2 -f forward.fastq -r reverse.fastq -n 8 -o output_prefix\n\n$ ls\n.\nâ”œâ”€â”€ forward.fastq\nâ”œâ”€â”€ reverse.fastq\nâ”œâ”€â”€ output_prefix_GMWI2.txt\nâ”œâ”€â”€ output_prefix_GMWI2_taxa.txt\nâ””â”€â”€ output_prefix_metaphlan.txt\n\nThe three output files are: \n(i) output_prefix_GMWI2.txt: GMWI2 score\n(ii) output_prefix_GMWI2_taxa.txt: A list of the taxa present in the sample used to compute GMWI2\n(iii) output_prefix_metaphlan.txt: Raw MetaPhlAn3 taxonomic profiling output\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --version         show program's version number and exit\n\nrequired named arguments:\n  -n NUM_THREADS, --num_threads NUM_THREADS\n                        number of threads\n  -f FORWARD, --forward FORWARD\n                        forward-read of metagenome (.fastq/.fastq.gz)\n  -r REVERSE, --reverse REVERSE\n                        reverse-read of metagenome (.fastq/.fastq.gz)\n  -o OUTPUT_PREFIX, --output_prefix OUTPUT_PREFIX\n                        prefix to designate output file names\n```\n\nTo merge GMWI2 score output files from multiple samples into a single csv file, please run:\n\n```bash\necho \"Sample,GMWI2\" > merged.csv && for file in *GMWI2.txt; do echo \"$(basename \"$file\" | awk -F \"_GMWI2.txt\" '{print $1}'),$(cat \"$file\")\" >> merged.csv; done\n```\n\n### Using MetaPhlAn output files as input (update 12/5/24)\nWe highly recommend that you use the conda tool to compute GMWI2 scores, as the tool checks that you use the correct MetaPhlAn version and marker database! \n\nHowever, if you have already ran MetaPhlAn on your metagenomes (and are sure that you used the correct MetaPhlAn version and marker database!) and would like to compute GMWI2 scores directly on the MetaPhlAn output files, please run the following:\n\n```bash\n# download script\nwget https://raw.githubusercontent.com/danielchang2002/GMWI2/refs/heads/main/src/gmwi2_metaphlan_output.py\n\n# download linear model\nwget https://raw.githubusercontent.com/danielchang2002/GMWI2/refs/heads/main/src/GMWI2/GMWI2_databases/GMWI2_model.joblib\n\n# run script on MetaPhlAn output\npython3 gmwi2_metaphlan_output.py metagenome_metaphlan_output.txt GMWI2_model.joblib output_prefix\n```\n\n### Reproducing manuscript results\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danielchang2002/GMWI2/blob/main/manuscript/GMWI2_manuscript.ipynb)\n\n![colab](./images/colab_screenshot2.png)\n\nPlease use the colab notebook linked above to reproduce all downstream analyses on the pooled dataset. \nSee the [manuscript directory](./manuscript) for more details.\n\n### Poop on a chip??\n\nThe top image was generated via [OpenAI DALLÂ·E 2](https://openai.com/dall-e-2) using the prompt: \"3D render of GPU chip in the form of a poop emoji, digital art\".\nThe image was then widened using the [Runway Infinite Image tool](https://runwayml.com/ai-magic-tools/infinite-image/).\n",
    "readme_length": 6642
  },
  {
    "name": "ReLM",
    "full_name": "syr-cn/ReLM",
    "description": "[EMNLP 2023] ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction.",
    "stars": 22,
    "forks": 1,
    "language": "Python",
    "url": "https://github.com/syr-cn/ReLM",
    "topics": [],
    "created_at": "2023-10-18T02:50:33Z",
    "updated_at": "2025-08-12T04:15:21Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction\n\nEMNLP 2023 \\[[paper](https://arxiv.org/abs/2310.13590v1)\\]\n\nAuthors: Yaorui Shi, An Zhang, Enzhi Zhang, Zhiyuan Liu, Xiang Wang\n\nThis repository contains the official code impementation for the paper **ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction**.\n\n![Framework](framework.jpg)\n\n## Installation\n\n```bash\nconda create -n ReLLM python=3.8\nconda activate ReLLM\npip install -r requirements.txt\n```\n\n## Reproducing results\n\nTo reproduce the results, you may need to run the bash scripts in the scripts folder:\n```bash\nsh scripts/run_Imidazo.sh\nsh scripts/run_NiCOlit.sh\nsh scripts/run_Rexgen30k.sh\nsh scripts/run_Rexgen40k.sh\n```\n\n## Citation\n```latex\n@inproceedings{\n    ReLM,\n    title        = {{R}e{LM}: Leveraging Language Models for Enhanced Chemical Reaction Prediction},\n    author       = {Shi, Yaorui and Zhang, An and Zhang, Enzhi and Liu, Zhiyuan and Wang, Xiang},\n    booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}\n                  2023, Singapore, December 6-10, 2023},\n    pages        = {5506--5520},\n    publisher    = {Association for Computational Linguistics},\n    year         = {2023},\n    url          = {https://aclanthology.org/2023.findings-emnlp.366}\n}\n```\n",
    "readme_length": 1325
  },
  {
    "name": "SpatialPPIv2",
    "full_name": "ohuelab/SpatialPPIv2",
    "description": "SpatialPPIv2: Enhancing proteinâ€“protein interaction prediction through graph neural networks with protein language models.",
    "stars": 21,
    "forks": 2,
    "language": "Python",
    "url": "https://github.com/ohuelab/SpatialPPIv2",
    "topics": [],
    "created_at": "2024-10-09T14:14:19Z",
    "updated_at": "2025-11-03T05:55:44Z",
    "homepage": "https://doi.org/10.1016/j.csbj.2025.01.022",
    "license": "Apache License 2.0",
    "readme": "# SpatialPPIv2\n\nSpatialPPIv2 is an advanced graph-neural-network-based model that predicts PPIs by using large language models to embed sequence features and Graph Attention Networks to capture structural information.\n\n![fig](./assets/fig.jpg)\n\nInput protein structures (PDB files) were used to extract structural and sequence features. The distance matrix within proteins was used to construct the edges in the protein graph representation. The protein sequence information encoded by language models was used as the features of nodes in the graph representation. The distance matrix within proteins was used to construct the edges in the protein graph representation. The protein sequence information encoded by language models was used as the node features in the graph representation. The residues between two proteins are fully connected to increase the message passing between proteins. The overall graph features of protein pairs were obtained by chain mean pooling the protein features directly obtained from language models and the relationship features output by GAT and used to calculate the possibility of interaction between proteins.\n\n## Prepare Environment\n\nSpatialPPIv2 support multiple environment configuration methods, including docker, apptainer and conda. \n\nYou can find the environment file under `env` folder.\n\n### Run in Colab\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/ohuelab/SpatialPPIv2/blob/main/demo/SpatialPPIv2_Colab_Example.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n### Apptainer\n\n```\napptainer build -s --nv ${CONTAINER_PATH} ./env/Apptainer\n```\n\n### Docker\n\n```\ndocker build -t ${IMAGE_NAME} ./env\n```\n\n### Conda\n\n```\nconda create -n pytorch221 python=3.11\nconda activate pytorch221\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\npip install numpy pandas seaborn tensorboard\npip install torch_geometric==2.5.1\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.0+cu118.html\nconda install lightning -c conda-forge\npip install lightning[extra]\npip install matplotlib==3.8.3\npip install biopandas==0.5.1\npip install biopython==1.83\npip install transformers==4.40.2\npip install sentencepiece==0.2.0\npip install torchsummary==1.5.1\npip install scipy==1.12.0\npip install torch_cluster -f https://data.pyg.org/whl/torch-2.2.0+cu118.html\npip install git+https://github.com/yusuf1759/prodigy-cryst.git\npip install pinder[all]\npip install fair-esm\n```\n\n## Inference protein interactions\n\n### Inference protein interactions with `pdb`/`cif` files\n\n```\npython inference.py --A demo/P33895.pdb --B demo/P40460.pdb\n```\n\nOptions:\n\n- `--A`: Path to the `pdb`/`cif` file\n- `--chain_A`: The chain ID of the first protein. By default, the first chain in the file is used.\n- `--B`: Path to the `pdb`/`cif` file\n- `--chain_B`: The chain ID of the first protein. By default, the first chain in the file is used.\n- `--model`: The model to use. For `pdb`/`cif` file, this will be `ProtT5` and `Rostlab/prot_t5_xl_uniref50` embedding will be used.\n- `--device`: Either `cuda` or `cpu`. The device to use.\n\n### Inference protein interactions with `fasta` files\n\nWhen there is no suitable protein structure file, `fasta` files can be used to make inferences based on protein sequences. This model is based on the attention contact of ESM-2. If the input file is in `fasta` format, the `esm2_t33_650M_UR50D` model will be forced to be used. Example:\n\n```\npython inference.py --A demo/D3INY1.fasta --B demo/P62593.fasta\n```\n\n### Visualization of inference process\n\nPlease check the [notebook](https://github.com/ohuelab/SpatialPPIv2/blob/main/demo/example_visualize.ipynb).\n\n## Train Model\n\nTo replicate our research, please follow these steps:\n\n### Get dataset\n\n### 1. Download PINDER dataset. \n\nYou can refer to [here](https://github.com/pinder-org/pinder?tab=readme-ov-file#%EF%B8%8F-getting-the-dataset)\n\n```\nexport PINDER_RELEASE=2024-02\nexport PINDER_BASE_DIR=~/my-custom-location-for-pinder/pinder\npinder_download\n```\n\n#### Set the path in `config/default.yaml`\n\nChange `data_root` to `~/my-custom-location-for-pinder/pinder/2024-02/pdbs`\n\n### 2. Download BioGRID\n\n```\nwget -O datasets/BIOGRID-ALL-4.4.238.tab3.zip https://downloads.thebiogrid.org/Download/BioGRID/Release-Archive/BIOGRID-4.4.238/BIOGRID-ALL-4.4.238.tab3.zip\nunzip -d datasets/ datasets/BIOGRID-ALL-4.4.238.tab3.zip\n```\n\n### 3. Generate dataset\n\nNotice: Do not use exclude for test set.\n\n```\npython scripts/dataset_generator.py --exclude scripts/exclude.txt --split train --biogrid datasets/BIOGRID-ALL-4.4.238.tab3.txt\n```\n\n### [Optional] Calculate embedding to disk\n\nThis will speed up training process\n\n```\npython scripts/calculate_embedding.py --split train --workers 8 --saveroot datasets/train\n```\n\nAfter the calculation, edit the `config/default.yaml` config file accordingly.\n\n- Change the `path` to the used above\n- Change the `type` to `ondisk`\n\n### Train model\n\nThe training log and model weights will be saved in `lightning_logs`\n\n```\npython main.py --task train\n```\n\n### Evaluate model\n\n```\npython main.py --task eval --checkpoint <path to ckpt checkpoint> --output pred.npy\n```\n\n\n\n## License\n\n[![License](https://img.shields.io/badge/License-Apache_2.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n\n\n## Reference\n- Hu W, Ohue M. [SpatialPPIv2: Enhancing proteinâ€“protein interaction prediction through graph neural networks with protein language models](https://doi.org/10.1016/j.csbj.2025.01.022). _Computational and Structural Biotechnology Journal_, 27: 508-518, 2025. doi: 10.1016/j.csbj.2025.01.022\n",
    "readme_length": 5699
  },
  {
    "name": "GDiffRetro",
    "full_name": "sunshy-1/GDiffRetro",
    "description": "[AAAI'25] GDiffRetro: Retrosynthesis Prediction with Dual Graph Enhanced Molecular Representation and Diffusion Generation",
    "stars": 21,
    "forks": 1,
    "language": "Python",
    "url": "https://github.com/sunshy-1/GDiffRetro",
    "topics": [],
    "created_at": "2024-12-11T03:29:31Z",
    "updated_at": "2025-10-14T08:55:31Z",
    "homepage": "https://arxiv.org/html/2501.08001v1",
    "license": "MIT License",
    "readme": "## GDiffRetro: Retrosynthesis Prediction with Dual Graph Enhanced Molecular Representation and Diffusion Generation\n![version](https://img.shields.io/badge/version-1.0.1-6395ED)\n![version](https://img.shields.io/badge/license-MIT-9ACD32)\n[![preprint](https://img.shields.io/badge/Preprint'25-EE4C2C)](https://arxiv.org/abs/2501.08001)\n[![DASFAA](https://img.shields.io/badge/AAAI-2025-B57EDC)](https://aaai.org/conference/aaai/aaai-25/)\n[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)](https://pytorch.org/)\n![](https://img.shields.io/github/stars/sunshy-1/GDiffRetro?style=social) \n\nThis is the Pytorch implementation for our *AAAI'25* paper: [**GDiffRetro: Retrosynthesis Prediction with Dual Graph Enhanced Molecular Representation and Diffusion Generation**](https://arxiv.org/abs/2501.08001). \n\n## Abstract\n<div style=\"text-align: justify;\">\nRetrosynthesis prediction focuses on identifying reactants capable of synthesizing a target product. Typically, the retrosynthesis prediction involves two phases: Reaction Center Identification and Reactant Generation. However, we argue that most existing methods suffer from two limitations in the two phases: (i) Existing models do not adequately capture the ``face'' information in molecular graphs for the reaction center identification. (ii) Current approaches for the reactant generation predominantly use sequence generation in a 2D space, which lacks versatility in generating reasonable distributions for completed reactive groups and overlooks molecules' inherent 3D properties. To overcome the above limitations, we propose GDiffRetro. For the reaction center identification, GDiffRetro uniquely integrates the original graph with its corresponding dual graph to represent molecular structures, which helps guide the model to focus more on the faces in the graph. For the reactant generation, GDiffRetro employs a conditional diffusion model in 3D to further transform the obtained synthon into a complete reactant. Our experimental findings reveal that GDiffRetro outperforms state-of-the-art semi-template models across various evaluative metrics. The overall framework is as follows:\n<div> \n<br>\n\n![Framework](fig/framework.png)\n\n# Requirement\nThe conda environment\n```shell\nconda install -c conda-forge rdkit python==3.9\n```\nInstall neccesary packages\n```shell\ntorch==1.11.0+cu113  \ntorch-cluster==1.6.3  \ntorch-scatter==2.0.9  \nopenbabel  \nwandb\n```\n> Note that we use a modified torchdrug in `./stage1` so you needn't install it with `pip install torchdrug`.\n> More details about the environment are provided in ./requirement.txt.\n# Run\n## Stage 0: Data Process\n```shell\ncd code/known_class/  # cd code/unknown_class/  \ncd stage1/data_prcocess/generate_SDF/\npython main.py\ncd stage1/data_prcocess/get_dataset_1st_stage/\npython main.py\n```\n## Stage 1: Reaction Center Identification\n```shell\ncd stage1 \npython train.py\npython stage1_to_result_dict.py --sample_times 300 --checkpoint model/reaction_center_model_w_class.pth\n# python stage1_to_result_dict.py --sample_times 300 --checkpoint model/reaction_center_model_wo_class.pth\n```\n## Stage 2: Synthon Completion\n```shell \ncd stage2  \npython train_gdiffretro.py\npython train_size_gnn.py\nbash run_get_results.sh\n```\n> We provide the dataset and checkpoints [here](https://portland-my.sharepoint.com/:f:/g/personal/shengysun4-c_my_cityu_edu_hk/EoZzeoHajTFDpgKrnsdI7xsB5Hpls0u9kHDUKdqa1MrEnA?e=I2NmXZ).\n\n## Acknowledgment of Open-Source Code Contributions  \n\n  The code is based on the open-source repositories: [TorchDrug](https://github.com/DeepGraphLearning/torchdrug), [DeLinker](https://github.com/oxpig/DeLinker), and [DiffLinker](https://github.com/igashov/DiffLinker), many thanks to the authors! \n\nYou are welcome to cite our paper:\n```\n@inproceedings{SunYu25,\n  title={GDiffRetro: Retrosynthesis Prediction with Dual Graph Enhanced Molecular Representation and Diffusion Generation},\n  author={Sun, Shengyin and Yu, Wenhao and Ren, Yuxiang and Du, Weitao and Liu, Liwei and Zhang, Xuecang and Hu, Ying and Ma, Chen},\n  booktitle={arXiv:2501.08001},\n  year={2025}\n}\n```\n",
    "readme_length": 4147
  },
  {
    "name": "Stock-Move-Prediction-with-Adversarial-Training-Replicate",
    "full_name": "yuxiangalvin/Stock-Move-Prediction-with-Adversarial-Training-Replicate",
    "description": "This github repo contains my replicate experiments of paper 'Enhancing Stock Movement Prediction with Adversarial Training'.",
    "stars": 20,
    "forks": 8,
    "language": "Python",
    "url": "https://github.com/yuxiangalvin/Stock-Move-Prediction-with-Adversarial-Training-Replicate",
    "topics": [],
    "created_at": "2021-03-10T18:42:25Z",
    "updated_at": "2024-10-18T15:39:39Z",
    "homepage": null,
    "license": "N/A",
    "readme": "## Stock-Move-Prediction-with-Adversarial-Training-Replicate-Project\n\nWelcome to my project page! It's time for a round of 'noise' :)\n\nThis is a replicate project to conduct experiments of paper 'Enhancing Stock Movement Prediction with Adversarial Training'.\nThe original paper's authors are Fuli Feng, Huimin Chen, Xiangnan He, Ji Ding, Maosong Sun and Tat-Seng Chua. The paper is linked here: [Original Paper](https://arxiv.org/pdf/1810.09936.pdf)\n\n## Project Information\n### University: Northwestern University\n\n### Professor: Prof. Han Liu\n\n### Project Member & Contact Information:\n  \n  * Yuxiang(Alvin) Chen   yuxiangchen2021 at u.northwestern.edu\n\n### GitHub Repository:\n  Here is my [GitHub Repository](https://github.com/yuxiangalvin/Stock-Move-Prediction-with-Adversarial-Training-Replicate-Project).\n  \n  This repo contains the codes provided by the authors and my experiment results.\n\n### Youtube Video Presentation\n  Here is the link to my presentation about this replicate project: [Presentation Video](https://youtu.be/elPtVduREak)\n## Relevance & Innovation\n\n### Why\n\nOne primary group of the typical input features to stock prediction tasks are typically based on stock price, which is a stochastic variable and continuously changed with time by nature. Thus, normal training with static price-based features can easily overfit the data, being insufficient to obtain reliable models that perform well on the testing and validaiton set. The picture below is a typical example that could be caused by this problem.\n\n![pain](./src/images/pain.png)\n\n### What\n\nThe key innovation of this paper is that it proposes to employ adversarial training to improve the generalization ability of the prediction model. Specifically, it develops a method to add adversarial training into an Attentive LSTM (which is proposed for the same task in previous literatures). The authors suggest adding perturbations to simulate the stochasticity of price variable, and train the model to work well under small but intentional perturbations.\n\n### How\n\nChallenge of applying usual adversarial training: \n\n* Adding perturbations on the features of all time units can be very time-consuming\n* Unintentional interactions among the perturbations of different units\n\nSolution:\n* add perturbations on the high-level prediction features of the model, before the last layer which is directly projected to the final prediction\n\nRationale:\n* higher layers -> abstract representation\n* smaller size\n* efficient\n* retain the stochasticity\n\n\n## Model Details\n\n### General Task\nThe model takes in daily price data (including daily open price, close price, high, low and adjusted close price) of a group of selected stocks (not individual stock). The data is normalized beforehand within the individual stock. Then the model treats all these normalized data from different stocks in the same way. Its aim is to train a model that could be generally used for any stock to predict whether the stock's next day's close price will be up (>0.55% movement up) or down (<-0.50% movement down) at the end of the next day compared to end of the current day. The choice of these two thresholds are not specified.\n\n### Inputs\n\n#### Raw Data\n\nThe raw data used is the stock daily level price data of a basket of stocks. Specifically, for each stock, daily open price, close price and adjusted price (three price values) are used between two specific dates depend on the dataset. The paper uses two different benchmark datasets used by previous papers. ACL18 & KDD17. These two datasets are the ones used by two previous papers.\n\n#### ACL18 dataset\n\n* ACL18 contains historical data from Jan-01-2014 to Jan01-2016 of 88 high-trade-volume-stocks in NASDAQ and NYSE markets\n* Used in the paper Xu and Cohen, 2018\n\n#### KDD17 dataset\n\n* KDD17 contains a longer history ranging from Jan-01-2007 to Jan-01-2016 of 50 stocks in U.S. markets\n* Used in the paper Zhang et al., 2017\n\n#### Data Labelling\n\nThe paper applies the exact same method to the two dataset to label them.\n\nThe movement is calculated as the difference between the current day and next day's adjusted close price as shown in the equation below:\n\n![label_formula](./src/images/label_formula.png)\n\nIf the movement (from current day to next day's adjusted close) is above 0.55%, it is labelled as +1 if the movement is below -0.5%,, it is labelled as -1. Otherwise, it is labelled as 0.\n\nThe days that have the label (from current day to next day's adjusted close) 0 are removed from the data and not used.\n\n#### Feature Generation & Normalization\n\nThe paper applies the exact same method to the two dataset for feature generation & normalization as well.\n\nFirstly, instead of using the raw price numbers, the authors used 11 technical features that are commonly used in other papers. This process reaches two goals: \n1. normalize the prices of different stocks; \n2. explicitly capture the interaction of different prices\n\nHere are the 11 features used:\n\n1. c_open <- movement of the day (from open to close)\n2. c_high <- not specified by the authors, according to the name, it is a feature related to daily high price\n3. c_low <- not specified by the authors, according to the name, it is a feature related to daily low price\n4. n_close <- movement from last day's close to current day's close price\n5. n_adj_close <- movement from last day's adjusted close price to current day's adjusted close price\n6-11. k-day (k=5,10,15,20,25,30) <- movement from past k day average adjusted close to current day's adjusted close price.\n\nHere is a table of how the authors showcase them in the original paper.\n\n![features](./src/images/features.png)\n\nSince the authors did not specify the calculation method of c_high and c_low and there is no information from source code as well (the data is preprocessed) so this creates difficulty for using other datasets for additional experiment.\n\nThe final input into the model is a D x T matrix with D = 11 (feature number), T is the number of timestpes to look back.\n\n![final_input](./src/images/final_input.png)\n\n### Model Structure\n\nThe model contains 4 layers. Here is a picture that shows the whole structure of the model with my anotated formulas and explanations.\n\nThe graph and annotations should be read from bottom to top. The annotations explained the formula used at the layer and the function of the layer claimed by the authors.\n\n![whole_model](./src/images/whole_model.png)\n\n### Adversarial Training\n\nHere is a picture that shows how adversarial training is applied here. As stated above, te adversarial perturbation is added at the higher level, sepcifically after the temporal attention layer before the prediction layer.\n\n#### Adversarial Perturbation Generation\n\nThe following two formulas show how e_adv is generated from the original e.\n\n![adversarial_formula](./src/images/adversarial_formula.png)\n\nAs the formula show, calculated perturbation is the gradient of loss function regarding the latent representation e. Thus, the direction of adding noise is the direction where loss function increases the most at  the given point e.\n\nThis benefit2 the model to predict correctly predict samples drawn from the inherent stochastic distribution of inputs, capturing the stochasticity\n\n#### Involve Perturbation\n\nThe generated new adversarial e is considered as an input that should lead to similar predictions. Thus, it is invovled as a new term in the loss function.\n\nThe graph below shows how two separate runs of the last prediction layer on e and adversarial e are conducted and the loss involve both terms with a weight parameter beta.\n\n![adversarial_model](./src/images/adversarial_model.png)\n\nThe original loss functions of an Attentive LSTM model and the Adversarial Attentive LSTM model are shown below with each term labelled. \n\n![loss](./src/images/loss.png)\n\nAlpha is a paramter to weight regularizer loss term. So the loss function involves two weight parameters, alpha and beta.\n\n## My Experiments & Codes\n\n### Import Libraries\nThis is part of the code used from pred_lstm.py.\n```python\nimport argparse\nimport copy\nimport numpy as np\nimport os\nimport random\nfrom sklearn.utils import shuffle\nimport tensorflow as tf\nfrom time import time\n\nfrom tensorflow.python.ops.nn_ops import leaky_relu\nfrom load import load_cla_data\n```\n\n### Model & Traning\nThis is part of the code used from pred_lstm.py.\n```python\nclass AWLSTM:\n    def __init__(self, data_path, model_path, model_save_path, parameters, steps=1, epochs=50,\n                 batch_size=256, gpu=False, tra_date='2014-01-02',\n                 val_date='2015-08-03', tes_date='2015-10-01', att=0, hinge=0,\n                 fix_init=0, adv=0, reload=0):\n        self.data_path = data_path\n        self.model_path = model_path\n        self.model_save_path = model_save_path\n        # model parameters\n        self.paras = copy.copy(parameters)\n        # training parameters\n        self.steps = steps\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.gpu = gpu\n\n        if att == 1:\n            self.att = True\n        else:\n            self.att = False\n        if hinge == 1:\n            self.hinge = True\n        else:\n            self.hinge = False\n        if fix_init == 1:\n            self.fix_init = True\n        else:\n            self.fix_init = False\n        if adv == 1:\n            self.adv_train = True\n        else:\n            self.adv_train = False\n        if reload == 1:\n            self.reload = True\n        else:\n            self.reload = False\n\n        # load data\n        self.tra_date = tra_date\n        self.val_date = val_date\n        self.tes_date = tes_date\n        self.tra_pv, self.tra_wd, self.tra_gt, \\\n        self.val_pv, self.val_wd, self.val_gt, \\\n        self.tes_pv, self.tes_wd, self.tes_gt = load_cla_data(\n            self.data_path,\n            tra_date, val_date, tes_date, seq=self.paras['seq']\n        )\n        self.fea_dim = self.tra_pv.shape[2]\n\n    def get_batch(self, sta_ind=None):\n        if sta_ind is None:\n            sta_ind = random.randrange(0, self.tra_pv.shape[0])\n        if sta_ind + self.batch_size < self.tra_pv.shape[0]:\n            end_ind = sta_ind + self.batch_size\n        else:\n            sta_ind = self.tra_pv.shape[0] - self.batch_size\n            end_ind = self.tra_pv.shape[0]\n        return self.tra_pv[sta_ind:end_ind, :, :], \\\n               self.tra_wd[sta_ind:end_ind, :, :], \\\n               self.tra_gt[sta_ind:end_ind, :]\n\n    def adv_part(self, adv_inputs):\n        print('adversial part')\n        if self.att:\n            with tf.variable_scope('pre_fc'):\n                self.fc_W = tf.get_variable(\n                    'weights', dtype=tf.float32,\n                    shape=[self.paras['unit'] * 2, 1],\n                    initializer=tf.glorot_uniform_initializer()\n                )\n                self.fc_b = tf.get_variable(\n                    'biases', dtype=tf.float32,\n                    shape=[1, ],\n                    initializer=tf.zeros_initializer()\n                )\n                if self.hinge:\n                    pred = tf.nn.bias_add(\n                        tf.matmul(adv_inputs, self.fc_W), self.fc_b\n                    )\n                else:\n                    pred = tf.nn.sigmoid(\n                        tf.nn.bias_add(tf.matmul(self.fea_con, self.fc_W),\n                                       self.fc_b)\n                    )\n        else:\n            # One hidden layer\n            if self.hinge:\n                pred = tf.layers.dense(\n                    adv_inputs, units=1, activation=None,\n                    name='pre_fc',\n                    kernel_initializer=tf.glorot_uniform_initializer()\n                )\n            else:\n                pred = tf.layers.dense(\n                    adv_inputs, units=1, activation=tf.nn.sigmoid,\n                    name='pre_fc',\n                    kernel_initializer=tf.glorot_uniform_initializer()\n                )\n        return pred\n\n    def construct_graph(self):\n        print('is pred_lstm')\n        if self.gpu == True:\n            device_name = '/gpu:0'\n        else:\n            device_name = '/cpu:0'\n        print('device name:', device_name)\n        with tf.device(device_name):\n            tf.reset_default_graph()\n            if self.fix_init:\n                tf.set_random_seed(123456)\n\n            self.gt_var = tf.placeholder(tf.float32, [None, 1])\n            self.pv_var = tf.placeholder(\n                tf.float32, [None, self.paras['seq'], self.fea_dim]\n            )\n            self.wd_var = tf.placeholder(\n                tf.float32, [None, self.paras['seq'], 5]\n            )\n\n            self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(\n                self.paras['unit']\n            )\n\n            # self.outputs, _ = tf.nn.dynamic_rnn(\n            #     # self.outputs, _ = tf.nn.static_rnn(\n            #     self.lstm_cell, self.pv_var, dtype=tf.float32\n            #     # , initial_state=ini_sta\n            # )\n\n            self.in_lat = tf.layers.dense(\n                self.pv_var, units=self.fea_dim,\n                activation=tf.nn.tanh, name='in_fc',\n                kernel_initializer=tf.glorot_uniform_initializer()\n            )\n\n            self.outputs, _ = tf.nn.dynamic_rnn(\n                # self.outputs, _ = tf.nn.static_rnn(\n                self.lstm_cell, self.in_lat, dtype=tf.float32\n                # , initial_state=ini_sta\n            )\n\n            self.loss = 0\n            self.adv_loss = 0\n            self.l2_norm = 0\n            if self.att:\n                with tf.variable_scope('lstm_att') as scope:\n                    self.av_W = tf.get_variable(\n                        name='att_W', dtype=tf.float32,\n                        shape=[self.paras['unit'], self.paras['unit']],\n                        initializer=tf.glorot_uniform_initializer()\n                    )\n                    self.av_b = tf.get_variable(\n                        name='att_h', dtype=tf.float32,\n                        shape=[self.paras['unit']],\n                        initializer=tf.zeros_initializer()\n                    )\n                    self.av_u = tf.get_variable(\n                        name='att_u', dtype=tf.float32,\n                        shape=[self.paras['unit']],\n                        initializer=tf.glorot_uniform_initializer()\n                    )\n\n                    self.a_laten = tf.tanh(\n                        tf.tensordot(self.outputs, self.av_W,\n                                     axes=1) + self.av_b)\n                    self.a_scores = tf.tensordot(self.a_laten, self.av_u,\n                                                 axes=1,\n                                                 name='scores')\n                    self.a_alphas = tf.nn.softmax(self.a_scores, name='alphas')\n\n                    self.a_con = tf.reduce_sum(\n                        self.outputs * tf.expand_dims(self.a_alphas, -1), 1)\n                    self.fea_con = tf.concat(\n                        [self.outputs[:, -1, :], self.a_con],\n                        axis=1)\n                    print('adversarial scope')\n                    # training loss\n                    self.pred = self.adv_part(self.fea_con)\n                    if self.hinge:\n                        self.loss = tf.losses.hinge_loss(self.gt_var, self.pred)\n                    else:\n                        self.loss = tf.losses.log_loss(self.gt_var, self.pred)\n\n                    self.adv_loss = self.loss * 0\n\n                    # adversarial loss\n                    if self.adv_train:\n                        print('gradient noise')\n                        self.delta_adv = tf.gradients(self.loss, [self.fea_con])[0]\n                        tf.stop_gradient(self.delta_adv)\n                        self.delta_adv = tf.nn.l2_normalize(self.delta_adv, axis=1)\n                        self.adv_pv_var = self.fea_con + \\\n                                          self.paras['eps'] * self.delta_adv\n\n                        scope.reuse_variables()\n                        self.adv_pred = self.adv_part(self.adv_pv_var)\n                        if self.hinge:\n                            self.adv_loss = tf.losses.hinge_loss(self.gt_var, self.adv_pred)\n                        else:\n                            self.adv_loss = tf.losses.log_loss(self.gt_var, self.adv_pred)\n            else:\n                with tf.variable_scope('lstm_att') as scope:\n                    print('adversarial scope')\n                    # training loss\n                    self.pred = self.adv_part(self.outputs[:, -1, :])\n                    if self.hinge:\n                        self.loss = tf.losses.hinge_loss(self.gt_var, self.pred)\n                    else:\n                        self.loss = tf.losses.log_loss(self.gt_var, self.pred)\n\n                    self.adv_loss = self.loss * 0\n\n                    # adversarial loss\n                    if self.adv_train:\n                        print('gradient noise')\n                        self.delta_adv = tf.gradients(self.loss, [self.outputs[:, -1, :]])[0]\n                        tf.stop_gradient(self.delta_adv)\n                        self.delta_adv = tf.nn.l2_normalize(self.delta_adv,\n                                                            axis=1)\n                        self.adv_pv_var = self.outputs[:, -1, :] + \\\n                                          self.paras['eps'] * self.delta_adv\n\n                        scope.reuse_variables()\n                        self.adv_pred = self.adv_part(self.adv_pv_var)\n                        if self.hinge:\n                            self.adv_loss = tf.losses.hinge_loss(self.gt_var,\n                                                                 self.adv_pred)\n                        else:\n                            self.adv_loss = tf.losses.log_loss(self.gt_var,\n                                                               self.adv_pred)\n\n            # regularizer\n            self.tra_vars = tf.trainable_variables('lstm_att/pre_fc')\n            for var in self.tra_vars:\n                self.l2_norm += tf.nn.l2_loss(var)\n\n            self.obj_func = self.loss + \\\n                            self.paras['alp'] * self.l2_norm + \\\n                            self.paras['bet'] * self.adv_loss\n\n            self.optimizer = tf.train.AdamOptimizer(\n                learning_rate=self.paras['lr']\n            ).minimize(self.obj_func)\n\n\n\n    def train(self, tune_para=True):\n        self.construct_graph()\n\n        sess = tf.Session()\n        saver = tf.train.Saver()\n        if self.reload:\n            saver.restore(sess, self.model_path)\n#             print('model restored')\n        else:\n            sess.run(tf.global_variables_initializer())\n\n        best_valid_pred = np.zeros(self.val_gt.shape, dtype=float)\n        best_test_pred = np.zeros(self.tes_gt.shape, dtype=float)\n\n        best_valid_perf = {\n            'acc': 0, 'mcc': -2\n        }\n        best_test_perf = {\n            'acc': 0, 'mcc': -2\n        }\n\n        bat_count = self.tra_pv.shape[0] // self.batch_size\n        if not (self.tra_pv.shape[0] % self.batch_size == 0):\n            bat_count += 1\n        for i in range(self.epochs):\n            t1 = time()\n            # first_batch = True\n            tra_loss = 0.0\n            tra_obj = 0.0\n            l2 = 0.0\n            tra_adv = 0.0\n            for j in range(bat_count):\n                pv_b, wd_b, gt_b = self.get_batch(j * self.batch_size)\n                feed_dict = {\n                    self.pv_var: pv_b,\n                    self.wd_var: wd_b,\n                    self.gt_var: gt_b\n                }\n                cur_pre, cur_obj, cur_loss, cur_l2, cur_al, batch_out = sess.run(\n                    (self.pred, self.obj_func, self.loss, self.l2_norm, self.adv_loss,\n                     self.optimizer),\n                    feed_dict\n                )\n\n                tra_loss += cur_loss\n                tra_obj += cur_obj\n                l2 += cur_l2\n                tra_adv += cur_al\n#             print('----->>>>> Training:', tra_obj / bat_count,\n#                   tra_loss / bat_count, l2 / bat_count, tra_adv / bat_count)\n\n            if not tune_para:\n                tra_loss = 0.0\n                tra_obj = 0.0\n                l2 = 0.0\n                tra_acc = 0.0\n                for j in range(bat_count):\n                    pv_b, wd_b, gt_b = self.get_batch(\n                        j * self.batch_size)\n                    feed_dict = {\n                        self.pv_var: pv_b,\n                        self.wd_var: wd_b,\n                        self.gt_var: gt_b\n                    }\n                    cur_obj, cur_loss, cur_l2, cur_pre = sess.run(\n                        (self.obj_func, self.loss, self.l2_norm, self.pred),\n                        feed_dict\n                    )\n                    cur_tra_perf = evaluate(cur_pre, gt_b, self.hinge)\n                    tra_loss += cur_loss\n                    l2 += cur_l2\n                    tra_obj += cur_obj\n                    tra_acc += cur_tra_perf['acc']\n#                 print('Training:', tra_obj / bat_count, tra_loss / bat_count,\n#                       l2 / bat_count, '\\tTrain per:', tra_acc / bat_count)\n\n            # test on validation set\n            feed_dict = {\n                self.pv_var: self.val_pv,\n                self.wd_var: self.val_wd,\n                self.gt_var: self.val_gt\n            }\n            val_loss, val_pre = sess.run(\n                (self.loss, self.pred), feed_dict\n            )\n            cur_valid_perf = evaluate(val_pre, self.val_gt, self.hinge)\n#             print('\\tVal per:', cur_valid_perf, '\\tVal loss:', val_loss)\n\n            # test on testing set\n            feed_dict = {\n                self.pv_var: self.tes_pv,\n                self.wd_var: self.tes_wd,\n                self.gt_var: self.tes_gt\n            }\n            test_loss, tes_pre = sess.run(\n                (self.loss, self.pred), feed_dict\n            )\n            cur_test_perf = evaluate(tes_pre, self.tes_gt, self.hinge)\n#             print('\\tTest per:', cur_test_perf, '\\tTest loss:', test_loss)\n\n            if cur_valid_perf['acc'] > best_valid_perf['acc']:\n                best_valid_perf = copy.copy(cur_valid_perf)\n                best_valid_pred = copy.copy(val_pre)\n                best_test_perf = copy.copy(cur_test_perf)\n                best_test_pred = copy.copy(tes_pre)\n                if not tune_para:\n                    saver.save(sess, self.model_save_path)\n            self.tra_pv, self.tra_wd, self.tra_gt = shuffle(\n                self.tra_pv, self.tra_wd, self.tra_gt, random_state=0\n            )\n            t4 = time()\n#             print('epoch:', i, ('time: %.4f ' % (t4 - t1)))\n#         print('\\nBest Valid performance:', best_valid_perf)\n#         print('\\tBest Test performance:', best_test_perf)\n        sess.close()\n        tf.reset_default_graph()\n        print(best_valid_perf['acc'])\n        print(best_valid_perf['mcc'])\n        if tune_para:\n            return best_valid_perf, best_test_perf\n        return best_valid_pred, best_test_pred\n```\n\n### Input Parser & Run\nThis is part of the code used from pred_lstm.py.\n```python\nif __name__ == '__main__':\n    desc = 'the lstm model'\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('-p', '--path', help='path of pv data', type=str,\n                        default='./data/stocknet-dataset/price/ourpped')\n    parser.add_argument('-l', '--seq', help='length of history', type=int,\n                        default=5)\n    parser.add_argument('-u', '--unit', help='number of hidden units in lstm',\n                        type=int, default=32)\n    parser.add_argument('-l2', '--alpha_l2', type=float, default=1e-2,\n                        help='alpha for l2 regularizer')\n    parser.add_argument('-la', '--beta_adv', type=float, default=1e-2,\n                        help='beta for adverarial loss')\n    parser.add_argument('-le', '--epsilon_adv', type=float, default=1e-2,\n                        help='epsilon to control the scale of noise')\n    parser.add_argument('-s', '--step', help='steps to make prediction',\n                        type=int, default=1)\n    parser.add_argument('-b', '--batch_size', help='batch size', type=int,\n                        default=1024)\n    parser.add_argument('-e', '--epoch', help='epoch', type=int, default=150)\n    parser.add_argument('-r', '--learning_rate', help='learning rate',\n                        type=float, default=1e-2)\n    parser.add_argument('-g', '--gpu', type=int, default=0, help='use gpu')\n    parser.add_argument('-q', '--model_path', help='path to load model',\n                        type=str, default='./saved_model/acl18_alstm/exp')\n    parser.add_argument('-qs', '--model_save_path', type=str, help='path to save model',\n                        default='./tmp/model')\n    parser.add_argument('-o', '--action', type=str, default='train',\n                        help='train, test, pred')\n    parser.add_argument('-m', '--model', type=str, default='pure_lstm',\n                        help='pure_lstm, di_lstm, att_lstm, week_lstm, aw_lstm')\n    parser.add_argument('-f', '--fix_init', type=int, default=0,\n                        help='use fixed initialization')\n    parser.add_argument('-a', '--att', type=int, default=1,\n                        help='use attention model')\n    parser.add_argument('-w', '--week', type=int, default=0,\n                        help='use week day data')\n    parser.add_argument('-v', '--adv', type=int, default=0,\n                        help='adversarial training')\n    parser.add_argument('-hi', '--hinge_lose', type=int, default=1,\n                        help='use hinge lose')\n    parser.add_argument('-rl', '--reload', type=int, default=0,\n                        help='use pre-trained parameters')\n    args = parser.parse_args()\n    print(args)\n\n    parameters = {\n        'seq': int(args.seq),\n        'unit': int(args.unit),\n        'alp': float(args.alpha_l2),\n        'bet': float(args.beta_adv),\n        'eps': float(args.epsilon_adv),\n        'lr': float(args.learning_rate)\n    }\n\n    if 'stocknet' in args.path:\n        tra_date = '2014-01-02'\n        val_date = '2015-08-03'\n        tes_date = '2015-10-01'\n    elif 'kdd17' in args.path:\n        tra_date = '2007-01-03'\n        val_date = '2015-01-02'\n        tes_date = '2016-01-04'\n    else:\n        print('unexpected path: %s' % args.path)\n        exit(0)\n\n    pure_LSTM = AWLSTM(\n        data_path=args.path,\n        model_path=args.model_path,\n        model_save_path=args.model_save_path,\n        parameters=parameters,\n        steps=args.step,\n        epochs=args.epoch, batch_size=args.batch_size, gpu=args.gpu,\n        tra_date=tra_date, val_date=val_date, tes_date=tes_date, att=args.att,\n        hinge=args.hinge_lose, fix_init=args.fix_init, adv=args.adv,\n        reload=args.reload\n    )\n\n    if args.action == 'train':\n        pure_LSTM.train()\n```\n\n\n### Experiments\n\nI conducted experiment on two datasets (from jupyter notebook): ACL18 & KDD17 as mentioned using the provided optimal parameters by the original authors\n\nThe results and the of the code below are in src/outputs/record.txt.\n\n#### ACL18\n```python\n#LSTM:\n!python pred_lstm.py -a 0 -l 10 -u 32 -l2 10 -f 1\n\n#ALSTM:\n!python pred_lstm.py -l 5 -u 4 -l2 1 -f 1\n\n#Adv-ALSTM:\n!python pred_lstm.py -l 5 -u 4 -l2 1 -v 1 -rl 1 -q ./saved_model/acl18_alstm/exp -la 0.01 -le 0.05\n```\n\n#### KDD17\n\n```python\n#LSTM:\n!python pred_lstm.py -p ./data/kdd17/ourpped/ -l 5 -u 4 -l2 0.001 -a 0 -f 1\n\n#ALSTM:\n!python pred_lstm.py -p ./data/kdd17/ourpped/ -l 15 -u 16 -l2 0.001 -f 1\n\n#Adv-ALSTM:\n!python pred_lstm.py -p ./data/kdd17/ourpped/ -l 15 -u 16 -l2 0.001 -v 1 -rl 1 -q ./saved_model/kdd17_alstm/model -la 0.05 -le 0.001 -f 1\n```\n\nI also did my own grid search to see whether it gives the same optimal parameter combination.\n(These are codes from grid_search_experiment.ipynb)\n\n##### Step 1\n```python\n#grid search params\nT_list = [str(x) for x in [2, 3, 4, 5, 10, 15]]\nU_list = [str(x) for x in [4,8,16,32]]\nl2_alpha_list = [str(x) for x in [0.001,0.01, 0.1, 1]]\n\nresult_dict = {}\nadv_a = '0.01'\nadv_e = '0.05'\nmax_acc = 0\nmax_mcc = -1\nmax_acc_combo = ''\nmax_mcc_combo = ''\ncount = 0\ntotal = len(T_list) * len(U_list) * len(l2_alpha_list)\nfor T in T_list:\n    for LSTM_U in U_list:\n        for l2_alpha in l2_alpha_list:\n            key = str(T) + ' ' + str(LSTM_U) + ' ' + str(l2_alpha)\n            if key not in result_dict.keys():\n                results = !python pred_lstm.py -l {T} -u {LSTM_U} -l2 {l2_alpha} -v 1 -la {adv_a} -le {adv_e}\n                acc = float(results[-2])\n                mcc = float(results[-1])\n                result_dict[key] = [acc, mcc]\n            else:\n                acc = result_dict[key][0]\n                mcc = result_dict[key][1]\n            if (acc > max_acc):\n                max_acc_combo = key\n                max_acc = acc\n            if (mcc > max_mcc):\n                max_mcc_combo = key\n                max_mcc = mcc\n            count += 1\n            print(count,'/',total)\nprint(max_acc_combo)\nprint(result_dict[max_acc_combo])\n```\n\n##### Step 2\n```python\n#grid search params for beta & epsilon (step 2)\nT = '15'\nLSTM_U = '4'\nl2_alpha = '0.1'\nadv_a_list = [str(x) for x in [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]]\nadv_e_list = [str(x) for x in [0.001, 0.005, 0.01, 0.05, 0.1]]\n\nresult_dict2 = {}\nresult_dict_test = {}\n\nmax_acc2 = 0\nmax_mcc2 = -1\nmax_acc_combo2 = ''\nmax_mcc_combo2 = ''\ncount = 0\ntotal = len(adv_a_list) * len(adv_e_list)\nfor adv_a in adv_a_list:\n    for adv_e in adv_e_list:\n        key = ' '.join([T, LSTM_U, l2_alpha, adv_a, adv_e])\n        if key not in result_dict2.keys():\n            results = !python pred_lstm.py -l {T} -u {LSTM_U} -l2 {l2_alpha} -v 1 -la {adv_a} -le {adv_e}\n            val_acc = float(results[-4])\n            val_mcc = float(results[-3])\n            test_acc = float(results[-2])\n            test_mcc = float(results[-1])\n            result_dict2[key] = [val_acc, val_mcc]\n            result_dict_test[key] = [test_acc, test_mcc]\n        else:\n            val_acc = result_dict2[key][0]\n            val_mcc = result_dict2[key][1]\n            test_acc = result_dict_test[key][0]\n            test_mcc = result_dict_test[key][1]\n        if (val_acc > max_acc2):\n            max_acc_combo2 = key\n            max_acc2 = val_acc\n        if (val_mcc > max_mcc2):\n            max_mcc_combo2 = key\n            max_mcc2 = val_mcc\n        count += 1\n        print(count,'/',total)\nprint(max_acc_combo2)\nprint(result_dict2[max_acc_combo2])\n```\n## Experiments Results\n\nHere is the comparison between authors' reported test Accuracy (ACC) and Matthews Correlation Coefficient (MCC) from their experiment with my result using optimal parameters provided by the authors.\n\n### ACL18\n\n<figure>\n  <img src=\"./src/outputs/ACL18_result.png\" title=\"ACL18_result\" />\n  <figcaption>Result Comparison Table - ACL18 Dataset</figcaption>\n</figure>\n\n\n### KDD17\n\n<figure>\n  <img src=\"./src/outputs/KDD17_result.png\" title=\"KDD17_result\" />\n  <figcaption>Result Comparison Table - KDD17 Dataset</figcaption>\n</figure>\n\n\n### Observations\nThe results from two experiment datasets show the same pattern.\n\nThere are two noticable points from both of these results:\n1. The results from my experiments also show that Adv-ALSTM model performs better than ALSTM in both ACC and MCC metrics. LSTM has the worst performance. This matches with the result provided by the authors.\n2. However, the results from my experiments are all not as good as the numbers provided by the authors.\n\nI tried to think about potential reasons that could lead to different results. One possible reason could be that I am not using the same parameters because the authors did not specify whether the provided combination parameters in source code is the optimal one. Thus, I did my own grid search on the ACL18 dataset.\n\n### My Grid Search Result\n\nMy grid search codes are included in the section above. Here I present my grid search result.\n\n<figure>\n  <img src=\"./src/outputs/parameter_comparison.png\" title=\"Parameter Comparison Table\" />\n  <figcaption>Parameter Comparison Table</figcaption>\n</figure>\n\nAs we can see, the parameter combination found is different from the one provided by the author.\n\nAn important observation is that randomness has significant impact on the result accuracy and mcc value. Thus, the best grid search result combination keep changing. This de-validates the grid search approach to some extent.\n\nAmong all grid search trials, the highest accuracy and mcc combination I found, compared to the best one provided by the authors is shown below in the table.\n\n<figure>\n  <img src=\"./src/outputs/best_compare.png\" title=\"Best Result Compare Table\" />\n  <figcaption>Best Result Compare Table</figcaption>\n</figure>\n\nThe accuracy is very close and the MCC is not as good but closer compare to before.\n\n## Conclusions & Next Steps\n\n### Conlusions\n* The replicated experiment indeed also shows that Adv-ALSTM model performs better than the LSTM and ALSTM models.\n* Through experiments, it is found that, grid search for parameter tuning is not applied well here because of the significant randomness impact\n* The best performance of all experiments with different parameter combinations closely match with author's reported performance.\n\n### Challenge\n\nI potentialy would like to use the updated dataset I could access from yahoo Finance. Since the dataset needed only contains open, close, high, low, adjusted close price.\n\nHowever, since the authors did not clearly provide the definition of 2 of the 11 features they used in the paper, I could not apply the same model and process to a new dataset from the raw data.\n\nIt would be a great next step to contact the authors to ask about what their definition for the other 2 features are.\n\n### Next Steps\n\nCurrently this model is applying a static feature selection method. Compared to many recent papers, the features used are very limited because the authors aim at showing the adversarial training could improve perfromance compared to some popular benchmarks.\n\nHowever, from my project in CS 496 Advanced Deep Learning, I learned that dynamic feature generation using neural nets could give more powerful predictive power. Thus, I plan to involve a block of neural nets (possibly CNN and an Inception modules) to dynamically extract features.\n",
    "readme_length": 34432
  },
  {
    "name": "Deep_promoter",
    "full_name": "HaochenW/Deep_promoter",
    "description": "The code for paper \"Synthetic Promoter Design in Escherichia coli based on Deep Genera-tive Network\"",
    "stars": 29,
    "forks": 14,
    "language": "Python",
    "url": "https://github.com/HaochenW/Deep_promoter",
    "topics": [],
    "created_at": "2019-11-10T09:10:20Z",
    "updated_at": "2025-09-04T02:59:38Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "Synthetic Promoter Design in Escherichia coli based on Deep Generative Network\n=====================================\n\nCode for computational models in [\"Synthetic Promoter Design in Escherichia coli based on Deep Genera-tive Network\"](https://doi.org/10.1101/563775).\n\nWe report a novel AI-based framework for de novo promoter design in E. coli, which could design brand new synthetic promoters in silico. We combined deep generative model that guides the search, and a prediction model to pre-select the most promising promoters. \n\n<p align=\"center\">\n  <img width=\"650\" height=\"300\" src=\"https://github.com/HaochenW/Deep_promoter/blob/master/structure.png\">\n</p>\n\nFrom the experimental results, up to 70.8% of the AI-designed promoters were experimentally demonstrated to be functional and shared no significant sequence similarity with E. coli genome. Here, we introduced the code used for promoter sequences generation, then the promoters could be used for experimental tests.\n\n\n\n## Prerequisites\n\n- Python, NumPy, TensorFlow, SciPy, Matplotlib, Keras\n- A recent NVIDIA GPU\n- TensorFlow == 1.10.0\n- Cuda == 9.0\n- Python == 2.7.0\n\n## Installation\nOur computational models could be directly downloaded by:\n```shell\ngit clone https://github.com/HaochenW/Deep_promoter.git\n```\nInstallation has been successfully tested in a Linux platform.\n\n## Using GAN model to generate promoter sequence data\n- Procdure:\n- 1. Store the sequence data in \n```shell\n.\\seq\\sequence_data.txt\n```\n- 2. Change the setting in python_language.py, which may include BATCH_SIZE, SEQ_LEN (must be changed, based on your sequence length), MAX_N_EXAMPLES(must be changed, based on the number of all the sequences), etc\n- 3. Train the model by \n```shell\npython gan_language.py\n```\n- The generated sequences will be saved in the current folder\n\n## Using Convoluational neural network(CNN)/Suppoer Vector Regression (SVR) model to pre-select high-expression promoter sequences\nThe CNN model was trained by the dataset from the Thomason et al which contains 14098 promoters with corresponding gene expression level measured by dRNA-seq, and the SVR model was trained by the first round experimental results.\n- Procdure:\n- 1. Store the predicted promoter sequence in \n```shell\n.\\seq\\predicted_promoters.fa\n```\n- 2. Use the predictor by \n```shell\npython predictor.py\n```\n- The predicted results will be saved in `seq_exp_SVR.txt` and `seq_exp_CNN.txt`\n\n## Citation\nWang Y, Wang H, Liu L, et al. Synthetic Promoter Design in Escherichia coli based on Generative Adversarial Network[J]. BioRxiv, 2019: 563775.\n\n## References\n[1]  Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. (2017) Improved training of wasserstein gans, In Advances in Neural Information Processing Systems, pp 5767-5777.\n\n\n[2]  Thomason, M. K., Bischler, T., Eisenbart, S. K., Forstner, K. U., Zhang, A., Herbig, A., Nieselt, K., Sharma, C. M., and Storz, G. (2015) Global transcriptional start site mapping using differential RNA sequencing reveals novel antisense RNAs in Escherichia coli, Journal of bacteriology 197, 18-28.\n\n\n## License\nThis project is licensed under the MIT License - see the LICENSE.md file for details\n\n",
    "readme_length": 3188
  },
  {
    "name": "DocLayout-YOLO",
    "full_name": "opendatalab/DocLayout-YOLO",
    "description": "DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception",
    "stars": 1802,
    "forks": 136,
    "language": "Python",
    "url": "https://github.com/opendatalab/DocLayout-YOLO",
    "topics": [],
    "created_at": "2024-10-14T02:37:01Z",
    "updated_at": "2025-12-02T05:56:42Z",
    "homepage": "https://huggingface.co/spaces/opendatalab/DocLayout-YOLO",
    "license": "GNU Affero General Public License v3.0",
    "readme": "<div align=\"center\">\n\nEnglish | [ç®€ä½“ä¸­æ–‡](./README-zh_CN.md)\n\n\n<h1>DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception</h1>\n\nOfficial PyTorch implementation of [DocLayout-YOLO](https://arxiv.org/abs/2410.12628).\n\n[![arXiv](https://img.shields.io/badge/arXiv-2405.14458-b31b1b.svg)](https://arxiv.org/abs/2410.12628) [![Online Demo](https://img.shields.io/badge/%F0%9F%A4%97-Online%20Demo-yellow)](https://huggingface.co/spaces/opendatalab/DocLayout-YOLO) [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97-Models%20and%20Data-yellow)](https://huggingface.co/collections/juliozhao/doclayout-yolo-670cdec674913d9a6f77b542)\n\n</div>\n    \n## Abstract\n\n> We present DocLayout-YOLO, a real-time and robust layout detection model for diverse documents, based on YOLO-v10. This model is enriched with diversified document pre-training and structural optimization tailored for layout detection. In the pre-training phase, we introduce Mesh-candidate BestFit, viewing document synthesis as a two-dimensional bin packing problem, and create a large-scale diverse synthetic document dataset, DocSynth-300K. In terms of model structural optimization, we propose a module with Global-to-Local Controllability for precise detection of document elements across varying scales. \n\n\n<p align=\"center\">\n  <img src=\"assets/comp.png\" width=52%>\n  <img src=\"assets/radar.png\" width=44%> <br>\n</p>\n\n## News ðŸš€ðŸš€ðŸš€\n\n**2024.10.25** ðŸŽ‰ðŸŽ‰  **Mesh-candidate Bestfit** code is released. Mesh-candidate Bestfit is an automatic pipeline which can synthesize large-scale, high-quality, and visually appealing document layout detection dataset. Tutorial and example data are available in [here](./mesh-candidate_bestfit).\n\n**2024.10.23** ðŸŽ‰ðŸŽ‰  **DocSynth300K dataset** is released on [ðŸ¤—Huggingface](https://huggingface.co/datasets/juliozhao/DocSynth300K), DocSynth300K is a large-scale and diverse document layout analysis pre-training dataset, which can largely boost model performance.\n\n**2024.10.21** ðŸŽ‰ðŸŽ‰  **Online demo** available on [ðŸ¤—Huggingface](https://huggingface.co/spaces/opendatalab/DocLayout-YOLO).\n\n**2024.10.18** ðŸŽ‰ðŸŽ‰  DocLayout-YOLO is implemented in **[PDF-Extract-Kit](https://github.com/opendatalab/PDF-Extract-Kit)** for document context extraction.\n\n**2024.10.16** ðŸŽ‰ðŸŽ‰  **Paper** now available on [ArXiv](https://arxiv.org/abs/2410.12628).   \n\n\n## Quick Start\n\n[Online Demo](https://huggingface.co/spaces/opendatalab/DocLayout-YOLO) is now available. For local development, follow steps below:\n\n### 1. Environment Setup\n\nFollow these steps to set up your environment:\n\n```bash\nconda create -n doclayout_yolo python=3.10\nconda activate doclayout_yolo\npip install -e .\n```\n\n**Note:** If you only need the package for inference, you can simply install it via pip:\n\n```bash\npip install doclayout-yolo\n```\n\n### 2. Prediction\n\nYou can make predictions using either a script or the SDK:\n\n- **Script**\n\n  Run the following command to make a prediction using the script:\n\n  ```bash\n  python demo.py --model path/to/model --image-path path/to/image\n  ```\n\n- **SDK**\n\n  Here is an example of how to use the SDK for prediction:\n\n  ```python\n  import cv2\n  from doclayout_yolo import YOLOv10\n\n  # Load the pre-trained model\n  model = YOLOv10(\"path/to/provided/model\")\n\n  # Perform prediction\n  det_res = model.predict(\n      \"path/to/image\",   # Image to predict\n      imgsz=1024,        # Prediction image size\n      conf=0.2,          # Confidence threshold\n      device=\"cuda:0\"    # Device to use (e.g., 'cuda:0' or 'cpu')\n  )\n\n  # Annotate and save the result\n  annotated_frame = det_res[0].plot(pil=True, line_width=5, font_size=20)\n  cv2.imwrite(\"result.jpg\", annotated_frame)\n  ```\n\n\nWe provide model fine-tuned on **DocStructBench** for prediction, **which is capable of handing various document types**. Model can be downloaded from [here](https://huggingface.co/juliozhao/DocLayout-YOLO-DocStructBench/tree/main) and example images can be found under ```assets/example```.\n\n<p align=\"center\">\n  <img src=\"assets/showcase.png\" width=100%> <br>\n</p>\n\n\n**Note:** For PDF content extraction, please refer to [PDF-Extract-Kit](https://github.com/opendatalab/PDF-Extract-Kit/tree/main) and [MinerU](https://github.com/opendatalab/MinerU).\n\n**Note:** Thanks to [NielsRogge](https://github.com/NielsRogge), DocLayout-YOLO now supports implementation directly from ðŸ¤—Huggingface, you can load model as follows:\n\n```python\nfilepath = hf_hub_download(repo_id=\"juliozhao/DocLayout-YOLO-DocStructBench\", filename=\"doclayout_yolo_docstructbench_imgsz1024.pt\")\nmodel = YOLOv10(filepath)\n```\n\nor directly load using ```from_pretrained```:\n\n```python\nmodel = YOLOv10.from_pretrained(\"juliozhao/DocLayout-YOLO-DocStructBench\")\n```\n\nmore details can be found at [this PR](https://github.com/opendatalab/DocLayout-YOLO/pull/6).\n\n**Note:** Thanks to [luciaganlulu](https://github.com/luciaganlulu), DocLayout-YOLO can perform batch inference and prediction. Instead of passing single image into ```model.predict``` in ```demo.py```, pass a **list of image path**. Besides, due to batch inference is not implemented before ```YOLOv11```, you should manually change ```batch_size``` in [here](doclayout_yolo/engine/model.py#L431).\n\n## DocSynth300K Dataset\n\n<p align=\"center\">\n  <img src=\"assets/docsynth300k.png\" width=100%>\n</p>\n\n### Data Download\n\nUse following command to download dataset(about 113G):\n\n```python\nfrom huggingface_hub import snapshot_download\n# Download DocSynth300K\nsnapshot_download(repo_id=\"juliozhao/DocSynth300K\", local_dir=\"./docsynth300k-hf\", repo_type=\"dataset\")\n# If the download was disrupted and the file is not complete, you can resume the download\nsnapshot_download(repo_id=\"juliozhao/DocSynth300K\", local_dir=\"./docsynth300k-hf\", repo_type=\"dataset\", resume_download=True)\n```\n\n### Data Formatting & Pre-training\n\nIf you want to perform DocSynth300K pretraining, using ```format_docsynth300k.py``` to convert original ```.parquet``` format into ```YOLO``` format. The converted data will be stored at ```./layout_data/docsynth300k```.\n\n```bash\npython format_docsynth300k.py\n```\n\nTo perform DocSynth300K pre-training, use this [command](assets/script.sh#L2). We default use 8GPUs to perform pretraining. To reach optimal performance, you can adjust hyper-parameters such as ```imgsz```, ```lr``` according to your downstream fine-tuning data distribution or setting.\n\n**Note:** Due to memory leakage in YOLO original data loading code, the pretraining on large-scale dataset may be interrupted unexpectedly, use ```--pretrain last_checkpoint.pt --resume``` to resume the pretraining process.\n\n## Training and Evaluation on Public DLA Datasets\n\n### Data Preparation\n\n1. specify  the data root path\n\nFind your ultralytics config file (for Linux user in ```$HOME/.config/Ultralytics/settings.yaml)``` and change ```datasets_dir``` to project root path.\n\n2. Download prepared yolo-format D4LA and DocLayNet data from below and put to ```./layout_data```:\n\n| Dataset | Download |\n|:--:|:--:|\n| D4LA | [link](https://huggingface.co/datasets/juliozhao/doclayout-yolo-D4LA) |\n| DocLayNet | [link](https://huggingface.co/datasets/juliozhao/doclayout-yolo-DocLayNet) |\n\nthe file structure is as follows:\n\n```bash\n./layout_data\nâ”œâ”€â”€ D4LA\nâ”‚Â Â  â”œâ”€â”€ images\nâ”‚Â Â  â”œâ”€â”€ labels\nâ”‚Â Â  â”œâ”€â”€ test.txt\nâ”‚Â Â  â””â”€â”€ train.txt\nâ””â”€â”€ doclaynet\n    â”œâ”€â”€ images\n Â Â  â”œâ”€â”€ labels\n Â Â  â”œâ”€â”€ val.txt\n Â Â  â””â”€â”€ train.txt\n```\n\n### Training and Evaluation\n\nTraining is conducted on 8 GPUs with a global batch size of 64 (8 images per device). The detailed settings and checkpoints are as follows:\n\n| Dataset | Model | DocSynth300K Pretrained? | imgsz | Learning rate | Finetune | Evaluation | AP50 | mAP | Checkpoint |\n|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n| D4LA | DocLayout-YOLO | &cross; | 1600 | 0.04 | [command](assets/script.sh#L5) | [command](assets/script.sh#L11) | 81.7 | 69.8 | [checkpoint](https://huggingface.co/juliozhao/DocLayout-YOLO-D4LA-from_scratch) |\n| D4LA | DocLayout-YOLO | &check; | 1600 | 0.04 | [command](assets/script.sh#L8) | [command](assets/script.sh#L11) | 82.4 | 70.3 | [checkpoint](https://huggingface.co/juliozhao/DocLayout-YOLO-D4LA-Docsynth300K_pretrained) |\n| DocLayNet | DocLayout-YOLO | &cross; | 1120 | 0.02 | [command](assets/script.sh#L14) | [command](assets/script.sh#L20) | 93.0 | 77.7 | [checkpoint](https://huggingface.co/juliozhao/DocLayout-YOLO-DocLayNet-from_scratch) |\n| DocLayNet | DocLayout-YOLO | &check; | 1120 | 0.02 | [command](assets/script.sh#L17) | [command](assets/script.sh#L20) | 93.4 | 79.7 | [checkpoint](https://huggingface.co/juliozhao/DocLayout-YOLO-DocLayNet-Docsynth300K_pretrained) |\n\nThe DocSynth300K pretrained model can be downloaded from [here](https://huggingface.co/juliozhao/DocLayout-YOLO-DocSynth300K-pretrain). Change ```checkpoint.pt``` to the path of model to be evaluated during evaluation.\n\n\n## Acknowledgement\n\nThe code base is built with [ultralytics](https://github.com/ultralytics/ultralytics) and [YOLO-v10](https://github.com/lyuwenyu/RT-DETR).\n\nThanks for their great work!\n\n## Star History\n\nIf you find our project useful, please add a \"star\" to the repo. It's exciting to us when we see your interest, which keep us motivated to continue investing in the project!\n\n<picture>\n  <source\n    media=\"(prefers-color-scheme: dark)\"\n    srcset=\"\n      https://api.star-history.com/svg?repos=opendatalab/DocLayout-YOLO&type=Date&theme=dark\n    \"\n  />\n  <source\n    media=\"(prefers-color-scheme: light)\"\n    srcset=\"\n      https://api.star-history.com/svg?repos=opendatalab/DocLayout-YOLO&type=Date\n    \"\n  />\n  <img\n    alt=\"Star History Chart\"\n    src=\"https://api.star-history.com/svg?repos=opendatalab/DocLayout-YOLO&type=Date\"\n  />\n</picture>\n\n## Citation\n\n```bibtex\n@misc{zhao2024doclayoutyoloenhancingdocumentlayout,\n      title={DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception}, \n      author={Zhiyuan Zhao and Hengrui Kang and Bin Wang and Conghui He},\n      year={2024},\n      eprint={2410.12628},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2410.12628}, \n}\n\n@article{wang2024mineru,\n  title={MinerU: An Open-Source Solution for Precise Document Content Extraction},\n  author={Wang, Bin and Xu, Chao and Zhao, Xiaomeng and Ouyang, Linke and Wu, Fan and Zhao, Zhiyuan and Xu, Rui and Liu, Kaiwen and Qu, Yuan and Shang, Fukai and others},\n  journal={arXiv preprint arXiv:2409.18839},\n  year={2024}\n}\n\n```\n",
    "readme_length": 10540
  },
  {
    "name": "GraphGen",
    "full_name": "Intern-Science/GraphGen",
    "description": "GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation",
    "stars": 579,
    "forks": 47,
    "language": "Python",
    "url": "https://github.com/Intern-Science/GraphGen",
    "topics": [
      "ai4science",
      "data-generation",
      "data-synthesis",
      "graphgen",
      "knowledge-graph",
      "llama-factory",
      "llm",
      "llm-training",
      "pretrain",
      "pretraining",
      "qa",
      "question-answering",
      "qwen",
      "sft",
      "sft-data",
      "xtuner"
    ],
    "created_at": "2025-01-08T06:49:17Z",
    "updated_at": "2025-12-02T03:49:08Z",
    "homepage": "https://chenzihong.gitbook.io/graphgen-cookbook/",
    "license": "Apache License 2.0",
    "readme": "<p align=\"center\">\n  <img src=\"resources/images/logo.png\"/>\n</p>\n\n<!-- icon -->\n\n[![stars](https://img.shields.io/github/stars/open-sciencelab/GraphGen.svg)](https://github.com/open-sciencelab/GraphGen)\n[![forks](https://img.shields.io/github/forks/open-sciencelab/GraphGen.svg)](https://github.com/open-sciencelab/GraphGen)\n[![open issues](https://img.shields.io/github/issues-raw/open-sciencelab/GraphGen)](https://github.com/open-sciencelab/GraphGen/issues)\n[![issue resolution](https://img.shields.io/github/issues-closed-raw/open-sciencelab/GraphGen)](https://github.com/open-sciencelab/GraphGen/issues)\n[![documentation](https://img.shields.io/badge/docs-latest-blue)](https://chenzihong.gitbook.io/graphgen-cookbook/)\n[![pypi](https://img.shields.io/pypi/v/graphg.svg?style=flat&logo=pypi&logoColor=white)](https://pypi.org/project/graphg/)\n[![wechat](https://img.shields.io/badge/wechat-brightgreen?logo=wechat&logoColor=white)](https://cdn.vansin.top/internlm/dou.jpg)\n[![arXiv](https://img.shields.io/badge/Paper-arXiv-white)](https://arxiv.org/abs/2505.20416)\n[![Hugging Face](https://img.shields.io/badge/Paper-on%20HF-white?logo=huggingface&logoColor=yellow)](https://huggingface.co/papers/2505.20416)\n\n[![Hugging Face](https://img.shields.io/badge/Demo-on%20HF-blue?logo=huggingface&logoColor=yellow)](https://huggingface.co/spaces/chenzihong/GraphGen)\n[![Model Scope](https://img.shields.io/badge/%F0%9F%A4%96%20Demo-on%20MS-green)](https://modelscope.cn/studios/chenzihong/GraphGen)\n\n\nGraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation\n\n[English](README.md) | [ä¸­æ–‡](README_zh.md)\n\n<details close>\n<summary><b>ðŸ“š Table of Contents</b></summary>\n\n- ðŸ“ [What is GraphGen?](#-what-is-graphgen)\n- ðŸ“Œ [Latest Updates](#-latest-updates)\n- âš™ï¸ [Support List](#-support-list)\n- ðŸš€ [Quick Start](#-quick-start)\n- ðŸ—ï¸ [System Architecture](#-system-architecture)\n- ðŸ€ [Acknowledgements](#-acknowledgements)\n- ðŸ“š [Citation](#-citation)\n- ðŸ“œ [License](#-license)\n- ðŸ“… [Star History](#-star-history)\n\n[//]: # (- ðŸŒŸ [Key Features]&#40;#-key-features&#41;)\n[//]: # (- ðŸ’° [Cost Analysis]&#40;#-cost-analysis&#41;)\n[//]: # (- âš™ï¸ [Configurations]&#40;#-configurations&#41;)\n\n</details>\n\n## ðŸ“ What is GraphGen?\n\nGraphGen is a framework for synthetic data generation guided by knowledge graphs. Please check the [**paper**](https://arxiv.org/abs/2505.20416) and [best practice](https://github.com/open-sciencelab/GraphGen/issues/17).\n\nHere is post-training result which **over 50% SFT data** comes from GraphGen and our data clean pipeline.\n\n|  Domain   |                          Dataset                          |   Ours   | Qwen2.5-7B-Instruct (baseline) |\n|:---------:|:---------------------------------------------------------:|:--------:|:------------------------------:|\n|   Plant   | [SeedBench](https://github.com/open-sciencelab/SeedBench) | **65.9** |              51.5              |\n|  Common   |                           CMMLU                           |   73.6   |            **75.8**            |\n| Knowledge |                       GPQA-Diamond                        | **40.0** |              33.3              |\n|   Math    |                          AIME24                           | **20.6** |              16.7              |\n|           |                          AIME25                           | **22.7** |              7.2               |\n\nIt begins by constructing a fine-grained knowledge graph from the source textï¼Œthen identifies knowledge gaps in LLMs using the expected calibration error metric, prioritizing the generation of QA pairs that target high-value, long-tail knowledge.\nFurthermore, GraphGen incorporates multi-hop neighborhood sampling to capture complex relational information and employs style-controlled generation to diversify the resulting QA data.\n\nAfter data generation, you can use [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) and [xtuner](https://github.com/InternLM/xtuner) to finetune your LLMs.\n\n## ðŸ“Œ Latest Updates\n\n- **2025.12.1**: Added search support for [NCBI](https://www.ncbi.nlm.nih.gov/) and [RNAcentral](https://rnacentral.org/) databases, enabling extraction of DNA and RNA data from these bioinformatics databases.\n- **2025.10.30**: We support several new LLM clients and inference backends including [Ollama_client](https://github.com/open-sciencelab/GraphGen/blob/main/graphgen/models/llm/api/ollama_client.py), [http_client](https://github.com/open-sciencelab/GraphGen/blob/main/graphgen/models/llm/api/http_client.py), [HuggingFace Transformers](https://github.com/open-sciencelab/GraphGen/blob/main/graphgen/models/llm/local/hf_wrapper.py) and [SGLang](https://github.com/open-sciencelab/GraphGen/blob/main/graphgen/models/llm/local/sglang_wrapper.py).\n- **2025.10.23**: We support VQA(Visual Question Answering) data generation now. Run script: `bash scripts/generate/generate_vqa.sh`.\n\n<details>\n<summary>History</summary>\n\n- **2025.10.21**: We support PDF as input format for data generation now via [MinerU](https://github.com/opendatalab/MinerU).\n- **2025.09.29**: We auto-update gradio demo on [Hugging Face](https://huggingface.co/spaces/chenzihong/GraphGen) and [ModelScope](https://modelscope.cn/studios/chenzihong/GraphGen).\n- **2025.08.14**: We have added support for community detection in knowledge graphs using the Leiden algorithm, enabling the synthesis of Chain-of-Thought (CoT) data.\n- **2025.07.31**: We have added Google, Bing, Wikipedia, and UniProt as search back-ends.\n- **2025.04.21**: We have released the initial version of GraphGen.\n\n</details>\n\n\n## âš™ï¸ Support List\n\nWe support various LLM inference servers, API servers, inference clients, input file formats, data modalities, output data formats, and output data types.\nUsers can flexibly configure according to the needs of synthetic data.\n\n\n| Inference Server                             | Api Server                                                                     | Inference Client                                           | Data Source                                                                                                                                                                                                                                                                           | Data Modal    | Data Type                                       |\n|----------------------------------------------|--------------------------------------------------------------------------------|------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|-------------------------------------------------|\n| [![hf-icon]HF][hf]<br>[![sg-icon]SGLang][sg] | [![sif-icon]Silicon][sif]<br>[![oai-icon]OpenAI][oai]<br>[![az-icon]Azure][az] | HTTP<br>[![ol-icon]Ollama][ol]<br>[![oai-icon]OpenAI][oai] | Files(CSV, JSON, PDF, TXT, etc.)<br>Databases([![uniprot-icon]UniProt][uniprot], [![ncbi-icon]NCBI][ncbi], [![rnacentral-icon]RNAcentral][rnacentral])<br>Search Engines([![bing-icon]Bing][bing], [![google-icon]Google][google])<br>Knowledge Graphs([![wiki-icon]Wikipedia][wiki]) | TEXT<br>IMAGE | Aggregated<br>Atomic<br>CoT<br>Multi-hop<br>VQA |\n\n<!-- links -->\n[hf]: https://huggingface.co/docs/transformers/index\n[sg]: https://docs.sglang.ai\n[sif]: https://siliconflow.cn\n[oai]: https://openai.com\n[az]: https://azure.microsoft.com/en-us/services/cognitive-services/openai-service/\n[ol]: https://ollama.com\n[uniprot]: https://www.uniprot.org/\n[ncbi]: https://www.ncbi.nlm.nih.gov/\n[rnacentral]: https://rnacentral.org/\n[wiki]: https://www.wikipedia.org/\n[bing]: https://www.bing.com/\n[google]: https://www.google.com\n\n\n<!-- icons -->\n[hf-icon]: https://www.google.com/s2/favicons?domain=https://huggingface.co\n[sg-icon]: https://www.google.com/s2/favicons?domain=https://docs.sglang.ai\n[sif-icon]: https://www.google.com/s2/favicons?domain=siliconflow.com\n[oai-icon]: https://www.google.com/s2/favicons?domain=https://openai.com\n[az-icon]: https://www.google.com/s2/favicons?domain=https://azure.microsoft.com\n[ol-icon]: https://www.google.com/s2/favicons?domain=https://ollama.com\n\n[uniprot-icon]: https://www.google.com/s2/favicons?domain=https://www.uniprot.org\n[ncbi-icon]: https://www.google.com/s2/favicons?domain=https://www.ncbi.nlm.nih.gov/\n[rnacentral-icon]: https://www.google.com/s2/favicons?domain=https://rnacentral.org/\n[wiki-icon]: https://www.google.com/s2/favicons?domain=https://www.wikipedia.org/\n[bing-icon]: https://www.google.com/s2/favicons?domain=https://www.bing.com/\n[google-icon]: https://www.google.com/s2/favicons?domain=https://www.google.com\n\n\n## ðŸš€ Quick Start\n\nExperience GraphGen Demo through [Huggingface](https://huggingface.co/spaces/chenzihong/GraphGen) or [Modelscope](https://modelscope.cn/studios/chenzihong/GraphGen).\n\nFor any questions, please check [FAQ](https://github.com/open-sciencelab/GraphGen/issues/10), open new [issue](https://github.com/open-sciencelab/GraphGen/issues) or join our [wechat group](https://cdn.vansin.top/internlm/dou.jpg) and ask.\n\n### Preparation\n\n1. Install [uv](https://docs.astral.sh/uv/reference/installer/)\n\n    ```bash\n    # You could try pipx or pip to install uv when meet network issues, refer the uv doc for more details\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    ```\n2. Clone the repository\n\n    ```bash\n    git clone --depth=1 https://github.com/open-sciencelab/GraphGen\n    cd GraphGen\n    ```\n\n3. Create a new uv environment\n\n    ```bash\n     uv venv --python 3.10\n    ```\n   \n4. Configure the dependencies\n\n    ```bash\n    uv pip install -r requirements.txt\n    ```\n\n### Run Gradio Demo\n\n   ```bash\n   python -m webui.app\n   ```\n   \n   For hot-reload during development, run\n   ```bash\n   PYTHONPATH=. gradio webui/app.py\n   ```\n\n\n![ui](https://github.com/user-attachments/assets/3024e9bc-5d45-45f8-a4e6-b57bd2350d84)\n\n### Run from PyPI\n\n1. Install GraphGen\n   ```bash\n   uv pip install graphg\n   ```\n\n2. Run in CLI\n   ```bash\n   SYNTHESIZER_MODEL=your_synthesizer_model_name \\\n   SYNTHESIZER_BASE_URL=your_base_url_for_synthesizer_model \\\n   SYNTHESIZER_API_KEY=your_api_key_for_synthesizer_model \\\n   TRAINEE_MODEL=your_trainee_model_name \\\n   TRAINEE_BASE_URL=your_base_url_for_trainee_model \\\n   TRAINEE_API_KEY=your_api_key_for_trainee_model \\\n   graphg --output_dir cache\n   ```\n\n### Run from Source\n\n1. Configure the environment\n   - Create an `.env` file in the root directory\n     ```bash\n     cp .env.example .env\n     ```\n   - Set the following environment variables:\n     ```bash\n     # Synthesizer is the model used to construct KG and generate data\n     SYNTHESIZER_MODEL=your_synthesizer_model_name\n     SYNTHESIZER_BASE_URL=your_base_url_for_synthesizer_model\n     SYNTHESIZER_API_KEY=your_api_key_for_synthesizer_model\n     # Trainee is the model used to train with the generated data\n     TRAINEE_MODEL=your_trainee_model_name\n     TRAINEE_BASE_URL=your_base_url_for_trainee_model\n     TRAINEE_API_KEY=your_api_key_for_trainee_model\n     ```\n2. (Optional) Customize generation parameters in `graphgen/configs/` folder.\n\n   Edit the corresponding YAML file, e.g.:\n\n    ```yaml\n      # configs/cot_config.yaml\n      input_file: resources/input_examples/jsonl_demo.jsonl\n      output_data_type: cot\n      tokenizer: cl100k_base\n      # additional settings...\n    ```\n\n3. Generate data\n\n   Pick the desired format and run the matching script:\n   \n   | Format       | Script to run                                  | Notes                                                             |\n   |--------------|------------------------------------------------|-------------------------------------------------------------------|\n   | `cot`        | `bash scripts/generate/generate_cot.sh`        | Chain-of-Thought Q\\&A pairs                                       |\n   | `atomic`     | `bash scripts/generate/generate_atomic.sh`     | Atomic Q\\&A pairs covering basic knowledge                        |\n   | `aggregated` | `bash scripts/generate/generate_aggregated.sh` | Aggregated Q\\&A pairs incorporating complex, integrated knowledge |\n   | `multi-hop`  | `bash scripts/generate/generate_multihop.sh`   | Multi-hop reasoning Q\\&A pairs                                    |\n\n\n4. Get the generated data\n   ```bash\n   ls cache/data/graphgen\n   ```\n\n### Run with Docker\n1. Build the Docker image\n   ```bash\n   docker build -t graphgen .\n   ```\n2. Run the Docker container\n   ```bash\n    docker run -p 7860:7860 graphgen\n    ```\n\n\n## ðŸ—ï¸ System Architecture\n\nSee [analysis](https://deepwiki.com/open-sciencelab/GraphGen) by deepwiki for a technical overview of the GraphGen system, its architecture, and core functionalities. \n\n\n### Workflow\n![workflow](resources/images/flow.png)\n\n\n## ðŸ€ Acknowledgements\n- [SiliconFlow](https://siliconflow.cn) Abundant LLM API, some models are free\n- [LightRAG](https://github.com/HKUDS/LightRAG) Simple and efficient graph retrieval solution\n- [ROGRAG](https://github.com/tpoisonooo/ROGRAG) A robustly optimized GraphRAG framework\n- [DB-GPT](https://github.com/eosphoros-ai/DB-GPT) An AI native data app development framework\n\n\n## ðŸ“š Citation\nIf you find this repository useful, please consider citing our work:\n```bibtex\n@misc{chen2025graphgenenhancingsupervisedfinetuning,\n      title={GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation}, \n      author={Zihong Chen and Wanli Jiang and Jinzhe Li and Zhonghang Yuan and Huanjun Kong and Wanli Ouyang and Nanqing Dong},\n      year={2025},\n      eprint={2505.20416},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.20416}, \n}\n```\n\n## ðŸ“œ License\nThis project is licensed under the [Apache License 2.0](LICENSE).\n\n## ðŸ“… Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Intern-Science/GraphGen&type=Date)](https://www.star-history.com/#open-sciencelab/GraphGen&Date)\n",
    "readme_length": 14181
  },
  {
    "name": "En3D",
    "full_name": "menyifang/En3D",
    "description": "Official implementation of \"En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data\", CVPR 2024; 3D Avatar Generation and Animation",
    "stars": 528,
    "forks": 43,
    "language": "Python",
    "url": "https://github.com/menyifang/En3D",
    "topics": [
      "3d-aigc",
      "3d-generation",
      "avatar-generation",
      "character-animation",
      "metavase"
    ],
    "created_at": "2023-12-05T05:16:35Z",
    "updated_at": "2025-11-19T16:57:19Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "# En3D - Official PyTorch Implementation\n\n### [Project page](https://menyifang.github.io/projects/En3D/index.html) | [Paper](https://arxiv.org/abs/2401.01173) | [Video](https://www.youtube.com/watch?v=YxMjaKgGdCc&t=5s) | [Online Demo](https://modelscope.cn/studios/alibaba_openvision_3dgen/En3D/summary)\n\n**En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data**<br>\n[Yifang Men](https://menyifang.github.io/), \n[Biwen Lei](mailto:biwen.lbw@alibaba-inc.com), \n[Yuan Yao](mailto:yaoy92@gmail.com), \n[Miaomiao Cui](mailto:miaomiao.cmm@alibaba-inc.com),\n[Zhouhui Lian](https://www.icst.pku.edu.cn/zlian/),\n[Xuansong Xie](https://scholar.google.com/citations?user=M0Ei1zkAAAAJ&hl=en)<br>\nIn: CVPR 2024\n\n\nEn3D is a large 3D human generative model trained on millions of synthetic 2D data, independently of any pre-existing 3D or 2D assets. \nThis repo contains an implementation of En3D and provides a series of applications built upon it. In addition, this repo aims to be a useful creative tool to produce realistic 3D avatars from seeds, text prompts or images, and support automatic character animation FBX production. \nAll outputs are compatible with the modern graphics workflows. \n\n\n\n**Generative 3D humans**<br> \n\nhttps://github.com/menyifang/En3D/assets/47292223/8b57a74d-6270-4b37-ae1e-ee2c0baad51d\n\n\n**Text guided synthesis**<br> \n\n![demo](assets/demo_text.gif)\n\n\n**Image guided synthesis**<br> \n\n![demo](assets/demo_img.gif)\n\nMore results can be found in [project page](https://menyifang.github.io/projects/En3D/index.html).\n\n\n## Updates\n\n(2024-10-29) The code of avatar generation & animation are available.\n\n(2024-10-29) The pretrained weights are available from [Modelscope](https://modelscope.cn/models/alibaba_openvision_3dgen/cv_en3d_3d_human_generation).\n\n(2024-01-15) ModelScope and HuggingFace Online demo is available! Try out [![ModelScope Spaces](\nhttps://img.shields.io/badge/ModelScope-Spaces-blue)](https://modelscope.cn/studios/alibaba_openvision_3dgen/En3D/summary) [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/menyifang/En3D). \n\n(2024-01-15) A Rigged & Animated 3D Human library (3DHuman-Syn) is released, containing ~1000 avatars produced by En3D for quick experience. Infinite avatars and actions support will be coming soon!\n\n(2024-01-03) The paper and video are released.\n\n\n## Web Demo\n\n- Integrated an online demo into [ModelScope](https://modelscope.cn/studios/alibaba_openvision_3dgen/En3D/summary). Try out and have fun!\n\n- Integrated an online demo into [Huggingface Spaces ðŸ¤—](https://huggingface.co/spaces/menyifang/En3D). Try out and have fun!\n\n\n## Requirements\n\n* We recommend Linux for performance and compatibility reasons.\n* 1&ndash;8 high-end NVIDIA GPUs. We have done all testing and development using V100, RTX3090, and A100 GPUs.\n* 64-bit Python 3.8 and PyTorch 1.11.0 (or later). See https://pytorch.org for PyTorch install instructions.\n* CUDA toolkit 11.3 or later. We used the custom CUDA extensions from the StyleGAN3 repo. Please see [Troubleshooting](https://github.com/NVlabs/stylegan3/blob/main/docs/troubleshooting.md#why-is-cuda-toolkit-installation-necessary)\n* Python libraries: see [requirements.txt](./requirements.txt) for exact library dependencies.  You can use the following commands with Anaconda to create and activate your Python environment:\n  - `cd En3d`\n  - `conda create -n en3d python=3.8`\n  - `conda activate eg3d`\n  - `pip install -r requirements.txt`\n\n\n\n## Quick Start\n\n### WebUI usage\n[Recommended] A deployed [Online Demo](https://modelscope.cn/studios/alibaba_openvision_3dgen/En3D/summary)\n\n![app](assets/app_thumb.png)\n\n\nWith own machine, you can also deploy our demo as below, which provides flexible user interface. Both CPU/GPU are supported for avatar animation, only GPU (>24G memory) is supported for avatar generation and render.\n\n```bash\npython app.py\n```\n\n### Synthetic avatar library\n\nWe released a Rigged & Animated 3D Human library (3DHuman-Syn), containing ~1000 characters produced by En3D, and 1000+ actions are provided for animation.\n\n- Avatar download and rendering\n```bash\npython render.py\n```\n\n- Avatar animation\n```bash\npython animation.py\n```\n\n\n- AR application <a href=\"https://3d-studio123.github.io/\"><img src=\"https://img.shields.io/badge/Open in-iphone-blue\" alt=\"google colab logo\"></a> \n\n\nConvert the generated animation file (.glb) to .usdz format using Sketchfab (upload glb and download usdz) or other tools, and insert the animated avatar to web with [model-viewer](https://modelviewer.dev/).\n\n```bash\n<!-- Import the component -->\n<script type=\"module\" src=\"https://ajax.googleapis.com/ajax/libs/model-viewer/3.3.0/model-viewer.min.js\"></script>\n\n<!-- Use it like any other HTML element -->\n<model-viewer src=\"assets/human.glb\" camera-controls ar shadow-intensity=\"1\" ios-src=\"assets/human.usdz\"></model-viewer>\n\n```\nAR function is only supported with iPhone, try the [AR example](https://3d-studio123.github.io/) in the phone browser and click the right-down icon for AR experience.\n\n\n\n## Avatar Generation\n\n### Download\n- Download the whole `models` folder from the [link](https://modelscope.cn/models/alibaba_openvision_3dgen/cv_en3d_3d_human_generation/files) and put it under the root dir.\n\n```bash\npython download_models.py\n```\n\n### Generate 360 renderings\n    \n```bash\nbash run_seed_pretrained.sh\n```\n\n### Text guided synthesis\n```bash\nbash run_text_synthesis.sh\n```\n\nhttps://github.com/user-attachments/assets/7ca89be8-9f5c-410d-9168-9f241778a197\n\n\n### Image guided synthesis\nRepose the human image to canonical A-pose first, and then using the following command to generate the 3D avatar.\n```bash\nbash run_img_synthesis.sh\n```\n\nhttps://github.com/user-attachments/assets/10ddc739-3ed5-4875-a6d0-908c8dbfcb31\n\n\n## Avatar Animation\nAuto-rig and animation for the generated 3D avatar\n```bash\nbash run_animate.sh\n```\n\nhttps://github.com/user-attachments/assets/581089a4-971d-4431-a272-e9663fe5bad5\n\n\n\n## Acknowledgments\n\nHere are some great resources we benefit from:\n\n- [Multiview-Avatar](https://github.com/ArcherFMY/Multiview-Avatar) for 2D avatar synthesis\n- [EG3D](https://nvlabs.github.io/eg3d/) for the general 3D GAN structure and 3D representation\n- [Fantasia3D](https://github.com/Gorilla-Lab-SCUT/Fantasia3D) for DMTET optimization \n- [ICON](https://github.com/YuliangXiu/ICON) for normal estimation\n\n\n## Citation\n\nIf you find this code useful for your research, please use the following BibTeX entry.\n\n```bibtex\n@inproceedings{men2024en3d,\n  title={En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data},\n  author={Men, Yifang and Lei, Biwen and Yao, Yuan and Cui, Miaomiao and Lian, Zhouhui and Xie, Xuansong},\n  journal={arXiv preprint arXiv:2401.01173},\n  website={https://menyifang.github.io/projects/En3D/index.html},\n  year={2024}}\n```\n",
    "readme_length": 6909
  },
  {
    "name": "AMLSim",
    "full_name": "IBM/AMLSim",
    "description": "The AMLSim project is intended to provide a multi-agent based simulator that generates synthetic banking transaction data together with a set of known money laundering patterns - mainly for the purpose of testing machine learning models and graph algorithms.  We welcome you to enhance this effort since the data set related to money laundering is critical to advance detection capabilities of money laundering activities. ",
    "stars": 317,
    "forks": 93,
    "language": "Python",
    "url": "https://github.com/IBM/AMLSim",
    "topics": [
      "finance",
      "finance-application",
      "fraud-detection",
      "graph",
      "network-science",
      "network-visualization"
    ],
    "created_at": "2018-12-18T16:14:56Z",
    "updated_at": "2025-11-30T06:24:56Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "Please cite our following papers if you use the data set for your publications. \n\nBibTeX \n@misc{AMLSim, author = {Toyotaro Suzumura and Hiroki Kanezashi}, title = {{Anti-Money Laundering Datasets}: {InPlusLab} Anti-Money Laundering DataDatasets}, howpublished = {\\url{http://github.com/IBM/AMLSim/}}, year = 2021 }\n\nEvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs\nhttps://arxiv.org/abs/1902.10191\n\nScalable Graph Learning for Anti-Money Laundering: A First Look\nhttps://arxiv.org/abs/1812.00076\n\n**Important: Please use the \"master\" branch for the practical use and testing. Other branches such as \"new-schema\" are outdated and unstable. [Wiki pages](https://github.com/IBM/AMLSim/wiki/) are still under construction and some of them do not catch up with the latest implementations. Please refer this README.md instead.**\n\n# AMLSim\nThis project aims at building a multi-agent simulator of anti-money laundering - namely AML, and sharing synthetically generated data so that researchers can design and implement their new algorithms over the unified data.\n\n\n# Dependencies\n- Java 8 (Download and copy all jar files to `jars/` directory: See also `jars/README.md`)\n    - [MASON](https://cs.gmu.edu/~eclab/projects/mason/) version 20\n    - [Commons-Math](http://commons.apache.org/proper/commons-math/download_math.cgi) version 3.6.1\n    - [JSON in Java](https://jar-download.com/artifacts/org.json/json/20180813) version 20180813\n    - [WebGraph](http://webgraph.di.unimi.it/) version 3.6.1\n    - [DSI Utilities](http://dsiutils.di.unimi.it/) version 2.5.4\n    - [fastutil](http://fastutil.di.unimi.it/) version 8.2.3\n    - [Sux for Java](http://sux.di.unimi.it/) version 4.2.0\n    - [JSAP](http://www.martiansoftware.com/jsap/) version 2.1\n    - [SLF4J](https://www.slf4j.org/download.html) version 1.7.25\n    - [MySQL Connector for Java](https://dev.mysql.com/downloads/connector/j/5.1.html) version 5.1\n    - [JUnit5](https://search.maven.org/artifact/org.junit.platform/junit-platform-console-standalone/1.8.1/jar) version 5\n    - [Mockito Core](https://mvnrepository.com/artifact/org.mockito/mockito-core/4.0.0) version 4.0.0\n    - [Byte Buddy](https://mvnrepository.com/artifact/net.bytebuddy/byte-buddy/1.11.19) version 1.11.19\n    - [Byte Buddy Agent](https://mvnrepository.com/artifact/net.bytebuddy/byte-buddy-agent/1.11.19) version 1.11.19\n    - [Objenesis](https://mvnrepository.com/artifact/org.objenesis/objenesis/3.2) version 3.2\n    - [Mockito Inline](https://mvnrepository.com/artifact/org.mockito/mockito-inline/4.0.0) version 4.0.0\n- Python 3.7 (The following packages can be installed with `pip3 install -r requirements.txt`)\n    - numpy\n    - networkx==1.11 (We do not support version 2.* due to performance issues when creating a large graph)\n    - matplotlib==2.2.3 (The latest version is not compatible)\n    - pygraphviz\n    - powerlaw\n    - python-dateutil\n\n\n# Directory Structure\nSee Wiki page [Directory Structure](https://github.com/IBM/AMLSim/wiki/Directory-Structure) for details.  \nNOTE: (October 2021): `bin/` folder has been renamed to `target/classes/`\n\n\n\n# Introduction for Running AMLSim\nSee Wiki page [Quick Introduction to AMLSim](https://github.com/IBM/AMLSim/wiki/Quick-Introduction-to-AMLSim) for details.\n\n## 1. Generate transaction CSV files from parameter files (Python)\nBefore running the Python script, please check and edit configuration file `conf.json`.\n```json5\n{\n//...\n  \"input\": {\n    \"directory\": \"paramFiles/1K\",  // Parameter directory\n    \"schema\": \"schema.json\",  // Configuration file of output CSV schema\n    \"accounts\": \"accounts.csv\",  // Account list parameter file\n    \"alert_patterns\": \"alertPatterns.csv\",  // Alert list parameter file\n    \"degree\": \"degree.csv\",  // Degree sequence parameter file\n    \"transaction_type\": \"transactionType.csv\",  // Transaction type list file\n    \"is_aggregated_accounts\": true  // Whether the account list represents aggregated (true) or raw (false) accounts\n  },\n//...\n}\n```\n\nThen, please run transaction graph generator script.\n```bash\ncd /path/to/AMLSim\npython3 scripts/transaction_graph_generator.py conf.json\n```\n\n## 2. Build and launch the transaction simulator (Java)\nParameters for the simulator are defined at the \"general\" section of `conf.json`. \n\n```json5\n{\n  \"general\": {\n      \"random_seed\": 0,  // Seed of random number\n      \"simulation_name\": \"sample\",  // Simulation name (identifier)\n      \"total_steps\": 720,  // Total simulation steps\n      \"base_date\": \"2017-01-01\"  // The date corresponds to the step 0 (the beginning date of this simulation)\n  },\n//...\n}\n```\n\nPlease compile Java files (if not yet) and launch the simulator.\n```bash\nsh scripts/build_AMLSim.sh\nsh scripts/run_AMLSim.sh conf.json\n```\n\n## 2.b. Optional: Install and Use Maven as build system.  \nOn Mac: `brew install maven`\nIf you already have a java installed, you can run `brew uninstall --ignore-dependencies openjdk` because brew installs that along with maven as a dependency.\n\nIf you choose to use Maven, you only manually need to fetch and place 1 jar file (MASON) in your `jars/` folder and then install it using the command shown below.  If you do not use Maven, you will have to place all the dependency jar files listed above as dependencies in the `jars/` folder.   \nIf using Maven, use the following commands to install the MASON dependency to your local Maven repository.  \n\n```\nmvn install:install-file \\\n-Dfile=jars/mason.20.jar \\\n-DgroupId=mason \\\n-DartifactId=mason \\\n-Dversion=20 \\\n-Dpackaging=jar \\\n-DgeneratePom=true\n```\n\nPlease compile Java files (if not yet) (will detect and use Maven) and launch the simulator.\n```bash\nsh scripts/build_AMLSim.sh\nsh scripts/run_AMLSim.sh conf.json\n```\n\n\n## 3. Convert the raw transaction log file\nThe file names of the output data are defined at the \"output\" section of `conf.json`.\n```json5\n{\n//...\n\"output\": {\n    \"directory\": \"outputs\",  // Output directory\n    \"accounts\": \"accounts.csv\",  // Account list CSV\n    \"transactions\": \"transactions.csv\",  // All transaction list CSV\n    \"cash_transactions\": \"cash_tx.csv\",  // Cash transaction list CSV\n    \"alert_members\": \"alert_accounts.csv\",  // Alerted account list CSV\n    \"alert_transactions\": \"alert_transactions.csv\",  // Alerted transaction list CSV\n    \"sar_accounts\": \"sar_accounts.csv\",    // SAR account list CSV\n    \"party_individuals\": \"individuals-bulkload.csv\",\n    \"party_organizations\": \"organizations-bulkload.csv\",\n    \"account_mapping\": \"accountMapping.csv\",\n    \"resolved_entities\": \"resolvedentities.csv\",\n    \"transaction_log\": \"tx_log.csv\",\n    \"counter_log\": \"tx_count.csv\",\n    \"diameter_log\": \"diameter.csv\"\n  },\n//...\n}\n```\n\n```bash\npython3 scripts/convert_logs.py conf.json\n```\n\n## 4. Export statistical information of the output data to image files (optional)\n\n```bash\npython3 scripts/visualize/plot_distributions.py conf.json\n```\n\n## 5. Validate alert transaction subgraphs by comparison with the parameter file (optional)\n```\npython3 scripts/validation/validate_alerts.py conf.json\n```\n\n\n## 6. Remove all log and generated image files from `outputs` directory and a temporal directory\n```bash\nsh scripts/clean_logs.sh\n```\n\n",
    "readme_length": 7173
  },
  {
    "name": "CVPR2018_attention",
    "full_name": "USTCPCS/CVPR2018_attention",
    "description": "Context Encoding for Semantic Segmentation  MegaDepth: Learning Single-View Depth Prediction from Internet Photos  LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation  PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume  On the Robustness of Semantic Segmentation Models to Adversarial Attacks  SPLATNet: Sparse Lattice Networks for Point Cloud Processing  Left-Right Comparative Recurrent Model for Stereo Matching  Enhancing the Spatial Resolution of Stereo Images using a Parallax Prior  Unsupervised CCA  Discovering Point Lights with Intensity Distance Fields  CBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation  Learning a Discriminative Feature Network for Semantic Segmentation  Revisiting Dilated Convolution: A Simple Approach for Weakly- and Semi- Supervised Semantic Segmentation  Unsupervised Deep Generative Adversarial Hashing Network  Monocular Relative Depth Perception with Web Stereo Data Supervision  Single Image Reflection Separation with Perceptual Losses  Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains  EPINET: A Fully-Convolutional Neural Network for Light Field Depth Estimation by Using Epipolar Geometry  FoldingNet: Interpretable Unsupervised Learning on 3D Point Clouds  Decorrelated Batch Normalization  Unsupervised Learning of Depth and Egomotion from Monocular Video Using 3D Geometric Constraints  PU-Net: Point Cloud Upsampling Network  Real-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer  Tell Me Where To Look: Guided Attention Inference Network  Residual Dense Network for Image Super-Resolution  Reflection Removal for Large-Scale 3D Point Clouds  PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image  Fully Convolutional Adaptation Networks for Semantic Segmentation  CRRN: Multi-Scale Guided Concurrent Reflection Removal Network  DenseASPP: Densely Connected Networks for Semantic Segmentation  SGAN: An Alternative Training of Generative Adversarial Networks  Multi-Agent Diverse Generative Adversarial Networks  Robust Depth Estimation from Auto Bracketed Images  AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation  DeepMVS: Learning Multi-View Stereopsis  GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose  GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation  Single-Image Depth Estimation Based on Fourier Domain Analysis  Single View Stereo Matching  Pyramid Stereo Matching Network  A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation  Image Correction via Deep Reciprocating HDR Transformation  Occlusion Aware Unsupervised Learning of Optical Flow  PAD-Net: Multi-Tasks Guided Prediciton-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing  Surface Networks  Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation  TextureGAN: Controlling Deep Image Synthesis with Texture Patches  Aperture Supervision for Monocular Depth Estimation  Two-Stream Convolutional Networks for Dynamic Texture Synthesis  Unsupervised Learning of Single View Depth Estimation and Visual Odometry with Deep Feature Reconstruction  Left/Right Asymmetric Layer Skippable Networks  Learning to See in the Dark",
    "stars": 179,
    "forks": 38,
    "language": null,
    "url": "https://github.com/USTCPCS/CVPR2018_attention",
    "topics": [],
    "created_at": "2018-06-01T10:39:52Z",
    "updated_at": "2025-10-16T05:28:50Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# CVPR2018_attention\nContext Encoding for Semantic Segmentation  \nMegaDepth: Learning Single-View Depth Prediction from Internet Photos  \nLiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation  \nPWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume  \nOn the Robustness of Semantic Segmentation Models to Adversarial Attacks  \nSPLATNet: Sparse Lattice Networks for Point Cloud Processing  \nLeft-Right Comparative Recurrent Model for Stereo Matching  \nEnhancing the Spatial Resolution of Stereo Images using a Parallax Prior  \nUnsupervised CCA  \nDiscovering Point Lights with Intensity Distance Fields  \nCBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation  \nLearning a Discriminative Feature Network for Semantic Segmentation  \nRevisiting Dilated Convolution: A Simple Approach for Weakly- and Semi- Supervised Semantic Segmentation  \nUnsupervised Deep Generative Adversarial Hashing Network  \nMonocular Relative Depth Perception with Web Stereo Data Supervision  \nSingle Image Reflection Separation with Perceptual Losses  \nZoom and Learn: Generalizing Deep Stereo Matching to Novel Domains  \nEPINET: A Fully-Convolutional Neural Network for Light Field Depth Estimation by Using Epipolar Geometry  \nFoldingNet: Interpretable Unsupervised Learning on 3D Point Clouds  \nDecorrelated Batch Normalization  \nUnsupervised Learning of Depth and Egomotion from Monocular Video Using 3D Geometric Constraints  \nPU-Net: Point Cloud Upsampling Network  \nReal-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer  \nTell Me Where To Look: Guided Attention Inference Network  \nResidual Dense Network for Image Super-Resolution  \nReflection Removal for Large-Scale 3D Point Clouds  \nPlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image  \nFully Convolutional Adaptation Networks for Semantic Segmentation  \nCRRN: Multi-Scale Guided Concurrent Reflection Removal Network  \nDenseASPP: Densely Connected Networks for Semantic Segmentation  \nSGAN: An Alternative Training of Generative Adversarial Networks  \nMulti-Agent Diverse Generative Adversarial Networks  \nRobust Depth Estimation from Auto Bracketed Images  \nAdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation  \nDeepMVS: Learning Multi-View Stereopsis  \nGeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose \nGeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation  \nSingle-Image Depth Estimation Based on Fourier Domain Analysis  \nSingle View Stereo Matching  \nPyramid Stereo Matching Network \nA Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation  \nImage Correction via Deep Reciprocating HDR Transformation \nOcclusion Aware Unsupervised Learning of Optical Flow \nPAD-Net: Multi-Tasks Guided Prediciton-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing \nSurface Networks \nStructured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation  \nTextureGAN: Controlling Deep Image Synthesis with Texture Patches  \nAperture Supervision for Monocular Depth Estimation \nTwo-Stream Convolutional Networks for Dynamic Texture Synthesis  \nUnsupervised Learning of Single View Depth Estimation and Visual Odometry with Deep Feature Reconstruction  \nLeft/Right Asymmetric Layer Skippable Networks  \nLearning to See in the Dark\n",
    "readme_length": 3467
  },
  {
    "name": "InstaGen",
    "full_name": "fcjian/InstaGen",
    "description": "InstaGen: Enhancing Object Detection by Training on Synthetic Dataset, CVPR2024",
    "stars": 86,
    "forks": 4,
    "language": "Jupyter Notebook",
    "url": "https://github.com/fcjian/InstaGen",
    "topics": [],
    "created_at": "2024-02-08T14:52:48Z",
    "updated_at": "2025-11-15T05:50:29Z",
    "homepage": "https://fcjian.github.io/InstaGen",
    "license": "MIT License",
    "readme": "<div align=\"center\">\n  \n# InstaGen: Enhancing Object Detection by Training on Synthetic Dataset (CVPR 2024)\n[[Paper](https://arxiv.org/abs/2402.05937)]\n[[Project Page](https://fcjian.github.io/InstaGen)]\n<be>\n</div>\n\n![method overview](resources/overview.png)\n\n## Introduction\n\nIn this paper, we present a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on **synthetic dataset** generated from diffusion models. Specifically, we integrate an instance-level grounding head into a pre-trained, generative diffusion model, to augment it with the ability of localising instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector. We conduct thorough experiments to show that, this enhanced version of diffusion model, termed as **InstaGen**, can serve as a data synthesizer, to enhance object detectors by training on its generated samples, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 âˆ¼ 5.2 AP) scenarios.\n\n## Methodology\n![method overview](resources/method.png)\n\n**updates**\n- April 7, 2024: and the code and models for open-vocabulary COCO benchmark\n- February 8, 2024: initial release\n\n## Synthetic Dataset\n![method overview](resources/qualitative_result.png)\n\n## Prerequisites\n```Installation\n# Step 1. Create a conda environment and activate it\nconda create --name instagen python=3.8 -y\nconda activate instagen\n\n# Step 2. Install the requirements for SDM fine-tuning\ncd InstaGen/\npip install -r requirements.txt\n\n# Step 3. Install mmdetection\ncd mmdetection/\npip install -U openmim\nmim install mmengine\nmim install \"mmcv>=2.0.0\"\npip install -v -e .\n```\n\n## Demo\nWe present a simple demo of utilizing InstaGen to generate images and the bounding boxes with the original model weights of SDM.\n1. Download the original model weights of SDM, listed as [sd-v1-4.ckpt](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/blob/main/sd-v1-4.ckpt), to 'checkpoints/stable-diffusion-v-1-4-original/'\n2. Download the model weights of InstaGen, listed as [instagen-4scale_fd_8xb2-12e_coco.pth](https://drive.google.com/file/d/1mAGlcdodboJwiGHLp9DfGDaMyDPweLQu/view?usp=sharing), to 'mmdetection/checkpoints/'\n3. Run demo:\n```\nsh instagen_scripts/demo_instagen.sh\n```\n\n## Fine-tune SDM\n1. Download the [COCO](https://cocodataset.org/) dataset to 'mmdetection/data/coco'\n2. Download the [annotation](https://drive.google.com/file/d/1C41YDwP23Lh2pU33O_NNCaboMIdyt0Gv/view?usp=sharing) of the base categories to 'mmdetection/data/coco/annotations/'\n3. Download the original model weights of SDM, listed as [sd-v1-4-full-ema.ckpt](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/blob/main/sd-v1-4-full-ema.ckpt), to 'checkpoints/stable-diffusion-v-1-4-original/'\n4. Fine-tune SDM:\n```\nsh instagen_scripts/finetune_sdm.sh\n```\n\n## Train InstaGen\n1. Download the [model weights](https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco_20210903_104808-b92c91f1.pth) of the pre-trained detector to 'mmdetection/checkpoints/'\n2. To enhance training efficiency, we generate images in advance and store the latent representations during the second-to-last denoising step:\n```\nsh instagen_scripts/generate_image.sh\n```\n3. Generate annotation files:\n```\nsh instagen_scripts/generate_base_ann.sh\n```\n4. Generate class embeddings:\n```\nsh instagen_scripts/generate_class_embedding.sh\n```\n5. Train InstaGen:\n```\nsh instagen_scripts/train_instagen.sh\n```\n\n## Train detector\n1. Predict the pseudo-labels of the synthetic images with InstaGen:\n```\nsh instagen_scripts/infer_instagen.sh\n```\n2. Generate annotation file for training detector:\n```\nsh instagen_scripts/generate_novel_ann.sh\n```\n3. Train detector:\n```\nsh instagen_scripts/train_detector.sh\n```\n\n## Inference\n1. Detector inference:\n```\nsh instagen_scripts/infer_detector.sh\n```\n\n# Dataset & Models\nFor your convenience, we provide the synthetic dataset and the trained models in the open-vocabulary COCO benchmark.\n1. The synthetic dataset can be downloaded [here](https://drive.google.com/file/d/1Qu-Q36DD_ih3otsmw6nRAsUX_L-nBxxC/view?usp=sharing)\n2. Models\n\nModel | AP50<sub>all</sub> | AP50<sub>base</sub> | AP50<sub>novel</sub> | Download\n--- |:---:|:---:|:---:|:---:\n[Fine-tuned SDM](configs/sd-finetune/coco_base.yaml) | -- | -- | -- | [google](https://drive.google.com/file/d/1sJizh8Jlg7XXfdX47l3lApTYepWlfZPy/view?usp=sharing)\n[InstaGen](mmdetection/configs/instagen/instagen-4scale_fd_8xb2-12e_coco.py) | -- | -- | -- | [google](https://drive.google.com/file/d/1mAGlcdodboJwiGHLp9DfGDaMyDPweLQu/view?usp=sharing)\n[Faster RCNN](mmdetection/configs/instagen/faster-rcnn_r50-caffe_c4-1x_coco-ovd_instagen-dataset.py) | 52.2 | 55.7 | 42.4 | [google](https://drive.google.com/file/d/1W2n38jxdkhbOfog3HVezbYm_Xt65PxSm/view?usp=sharing)\n\n## Acknowledgement\n\nThanks [Stable Diffusion](https://github.com/CompVis/stable-diffusion), [Stable Diffusion Finetuning](https://github.com/LambdaLabsML/examples/tree/main/stable-diffusion-finetuning), [Grounded Diffusion](https://github.com/Lipurple/Grounded-Diffusion) and [MMDetection](https://github.com/open-mmlab/mmdetection) team for the wonderful open source project!\n\n## Citation\n\nIf you find InstaGen useful in your research, please consider citing:\n\n```\n@inproceedings{feng2024instagen,\n    title={InstaGen: Enhancing Object Detection by Training on Synthetic Dataset},\n    author={Feng, Chengjian and Zhong, Yujie and Jie, Zequn and Xie, Weidi and Ma, Lin},\n    booktitle={Proceedings of the IEEE / CVF Computer Vision and Pattern Recognition},\n    year={2024}\n}\n```\n\n\n",
    "readme_length": 5984
  },
  {
    "name": "Shallow-UWnet",
    "full_name": "mkartik/Shallow-UWnet",
    "description": "Shallow-UWnet, a neural network which maintains performance and has fewer parameters than the state-of-art underwater image enhancement model. Generalization of the model is demonstrated by benchmarking its performance on combination of synthetic and real-world datasets.",
    "stars": 60,
    "forks": 13,
    "language": "Python",
    "url": "https://github.com/mkartik/Shallow-UWnet",
    "topics": [],
    "created_at": "2020-11-20T05:35:27Z",
    "updated_at": "2025-10-11T12:46:25Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# Shallow-UWnet\nImplementation of the paper *[Shallow-UWnet : Compressed Model for Underwater Image Enhancement](https://arxiv.org/abs/2101.02073)*\n\n## Introduction\nIn this project we have proposed a shallow neural network architecture, Shallow-UWnet which maintains performance and has fewer parameters than the state-of-art underwater image enhancement model. We have demonstrated the generalization of our model by benchmarking its performance on combination of synthetic and real-world datasets.\n\n## Train the Model\n```\npython training.py\n```\n\n## Test the Model\n```\npython test.py\n```\n## Pretrained Model\nThe pretrained Shallow-UWnet model is available at [model.ckpt](https://drive.google.com/file/d/1QmOR8Ar3LqS5Pj0GG3I-798Yyj9xswet/view?usp=sharing)\n",
    "readme_length": 757
  },
  {
    "name": "LocalizationGAN",
    "full_name": "hnavidan/LocalizationGAN",
    "description": "Implementation of the paper Using Synthetic Data to Enhance the Accuracy of Fingerprint-Based Localization: A Deep Learning Approach (IEEE Sensors Letters, 2020), which utilizes Generative Adversarial Networks (GANs) for indoor localiztaion.",
    "stars": 49,
    "forks": 8,
    "language": "Jupyter Notebook",
    "url": "https://github.com/hnavidan/LocalizationGAN",
    "topics": [
      "gan",
      "generative-adversarial-network",
      "indoor-localization",
      "indoor-positioning",
      "tensorflow",
      "wifi-rssi"
    ],
    "created_at": "2020-03-31T19:36:24Z",
    "updated_at": "2025-09-26T09:41:46Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# Localization GAN\nThis is the source coude for the paper [Using Synthetic Data to Enhance the Accuracy of Fingerprint-based Localization: A Deep Learning Approach](https://ieeexplore.ieee.org/abstract/document/8981805), IEEE Sensors Letters,  vol. 4, no. 4, pp. 1-4, April 2020, doi: 10.1109/LSENS.2020.2971555. \n\n<p align=\"center\"><img width=90% src=\"https://raw.githubusercontent.com/hnavidan/LocalizationGAN/master/model.png\"></p>\n\nThe goal is to utilize Generative Adversarial Networks (GANs) to enhance the accuracy of Wi-Fi Received Signal Strength (RSS) fingerprint-based localization methods.\n\nThe benchmark dataset is taken from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wireless+Indoor+Localization) and is collected by Rajen Bhatt. This dataset consists of 2000 RSS samples collected from 7 Access Points (APs) inside four different rooms.\n\n## Prerequisites\n* Jupyter Notebooks\n* Tensorflow 1.1x\n* Keras \n* numpy\n* sklearn\n* tqdm\n* pandas\n* matplotlib\n\n## Descritpion\nInitially, the dataset is separated into training and test data equally so that both datasets contain 250 data samples of each class.  By reducing the problem of indoor localization to a common classification task, we can use a Multilayer Perceptron (MLP) with six densely connected layers as our base model. The inputs and outputs of this model are RSS samples and the probability of the presence in each class, respectively.   \n\nNext, a GAN is trained once with 10% of the training data (25 samples per class) and another time with full training data to generate various numbers of synthetic data. This generated data is shuffled with the real data and is fed to the MLP classification model to compare the performance. To mitigate the effect of randomness, this process is done 100 times with different initial random seeds, and the results are averaged over these runs. The results are as follows:\n<p align=\"center\"><img width=70% src=\"https://raw.githubusercontent.com/hnavidan/LocalizationGAN/master/results_table.png\"></p>\n\nFurthermore, a fraction of the real data is randomly selected to generate synthetic data, in a way that the total number of data after adding and shuffling the synthetic data will be equal to the total number of the original data in each class (which is 250). The results, as depicted below, suggest that adding synthetic data to the real data will increase the accuracy significantly. \n<p align=\"center\"><img width=60% src=\"https://raw.githubusercontent.com/hnavidan/LocalizationGAN/master/results_plot.png\"></p>\n\n## Abstract\nData collection is costly and imposes privacy issues. Many solutions are proposed in the literature to reduce this cost, such as crowd-sourcing methods in data collection or using semi-supervised algorithms to enhance the positioning accuracy. However, semi-supervised algorithms need unlabeled data, and crowd-sourcing based methods need many participants. Fingerprint-based localization methods use received signal strength (RSS) or channel state information (CSI) in wireless sensor networks to localize the users in indoor or harsh outdoor environments. In this paper, we introduce a new method to reduce the data collection costs by using synthetic data in fingerprint-based localization. We use generative adversarial networks (GANs) to learn the distribution of a limited collected data and further, produce synthetic data and feed them to the system in addition to the real collected data in order to increase the positioning accuracy. Experimental results on a benchmark dataset show that by applying the proposed method and using a combination of 10% collected data and 90% synthetic data, we can get completely close to the accuracy as if we would use the whole collected data. It means that we can use 90% less real data and reduce the data collection costs while reaching the acceptable accuracy, using GAN generated synthetic data.\n\n## Citing Request\nIf you find this code useful in your research, please consider citing:\n```\n@article{nabatiUsingSyntheticData,\n  author={Nabati, Mohammad and Navidan, Hojjat and Shahbazian, Reza and Ghorashi, Seyed Ali and Windridge, David},\n  journal={IEEE Sensors Letters}, \n  title={Using Synthetic Data to Enhance the Accuracy of Fingerprint-Based Localization: A Deep Learning Approach}, \n  year={2020},\n  volume={4},\n  number={4},\n  pages={1-4},\n  doi={10.1109/LSENS.2020.2971555}}\n```\n",
    "readme_length": 4410
  },
  {
    "name": "masked_faces",
    "full_name": "securifai/masked_faces",
    "description": "We  propose a method for enhancing data sets containing faces without masks by creating synthetic masks and overlaying them on faces in the original images.",
    "stars": 44,
    "forks": 5,
    "language": "Python",
    "url": "https://github.com/securifai/masked_faces",
    "topics": [],
    "created_at": "2021-08-26T10:13:49Z",
    "updated_at": "2025-10-13T07:48:19Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# CelebA+masks and CASIA-WebFace+masks data sets from \"A realistic approach to generate masked faces applied on two novel masked face recognition data sets\" (NeurIPS 2021) - Official Repository\n\n## 1. License Agreement\n\n**Copyright (C) 2021 - SecurifAI**\n\nThis package contains free data and software: you can use, redistribute and/or modify it under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license.\n\nThis data set and software is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nThe complete license agreement can be consulted at:\n[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/).\n\n\n## 2. Citation\n\nPlease cite the corresponding work (see citation.bib file to obtain the citation in BibTex format) if you use this data set and software (or a modified version of it) in any scientific work:\n\n**[1] Tudor Mare, Georgian DuÈ›Äƒ, Mariana-Iuliana Georgescu, Adrian È˜andru, Bogdan Alexe, Marius Popescu, Radu Tudor Ionescu. A realistic approach to generate masked faces applied on two novel masked face recognition data sets. In Proceedings of NeurIPS, 2021 [(link to ArXiv version)](http://arxiv.org/abs/2109.01745).**\n\n## 3. Description\n\nThe COVID-19 pandemic raises the problem of adapting face recognition systems to the new reality, where people may wear surgical masks to cover their noses and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for training these systems were released before the pandemic, so they now seem unsuited due to the lack of examples of people wearing masks. We propose a method for enhancing data sets containing faces without masks by creating synthetic masks and overlaying them on faces in the original images. Our method relies on Spark AR Studio, a developer program made by Facebook that is used to create Instagram face filters. In our approach, we use 9 masks of different colors, shapes and fabrics. We employ our method to generate a number of 445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254 (96.8%) masks for the CelebA data set. \n\nOur repository contains:\n  - Images of masks with transparent background.\n  - Code to overlay the masks on the corresponding CelebA and CASIA-WebFace images.\n\n#### Important note: This repo does not include the original CelebA or CAISA-WebFace images. The original images should be downloaded from the original repositories.\n\n## 4. Data Sets\n\nThe masks for the CelebA data set are available for download at:\n\n# [CelebA+masks](https://fmiunibuc-my.sharepoint.com/:u:/g/personal/radu_ionescu_fmi_unibuc_ro/EQdIsLQB9jdOkaOHV0T_wMQBSz8qQkxRm7w8Nuo_qZOoFA?e=1eekcq)\n\nThe masks for the CASIA-WebFace data set are available for download at:\n\n# [CASIA-WebFace+masks](https://fmiunibuc-my.sharepoint.com/:u:/g/personal/radu_ionescu_fmi_unibuc_ro/ETWFdcY8sAhCpbxrSiytXzUBK3PHaAxMbARlarBY-tNK3g?e=31YZOw)\n\n## 5. Software Usage\n\nFor convenience, we provide Python scripts to apply the masks on the original CelebA and CASIA-WebFace images.\n\nTo run the script on the CelebA / CASIA-WebFace data set, extract the respective archive in the same folder as the CelebA / CASIA-WebFace main data set folder. Inside each script there is a celeba_folder / casia_folder parameter and a masks_folder parameter which have to be set accordingly. The output of the script will be located in the masked_celeba or masked_casia folder, respectively.\n\nMake sure to install the packages listed in requirements.txt before running the scripts.\n\n#### For CelebA\n\nMake sure you have the following folder structure on your machine:\n```\nmain directory\nâ”‚   apply_masks_celeba.py\nâ””â”€â”€â”€celeba_masks - masks folder (from this repo)\nâ”‚   â”‚   ...\nâ””â”€â”€â”€celeba - data set folder (original images)\nâ”‚   â”‚   ...\nâ””â”€â”€â”€masked_celeba - output folder (images with overlaid masks)\n    â”‚   ...    \n```\n\nThen use the following command:\n```\n>> python apply_masks_celeba.py\n```\n\n#### For CASIA-WebFace\n\nMake sure you have the following folder structure on your machine:\n```\nmain directory\nâ”‚   apply_masks_casia.py\nâ””â”€â”€â”€casia_masks - masks folder (from this repo)\nâ”‚   â”‚   ...\nâ””â”€â”€â”€casia - data set folder (original images)\nâ”‚   â”‚   ...\nâ””â”€â”€â”€masked_casia - output folder (images with overlaid masks)\n    â”‚   ...    \n```\n\nThen use the following command:\n```\n>> python apply_masks_casia.py\n```\n\n## 6. Feedback\n\nWe are happy to hear your feedback and suggestions at: tudor[dot]mare{at}securif(dot)ai\n",
    "readme_length": 4544
  },
  {
    "name": "Robotics-Course-project",
    "full_name": "koujan/Robotics-Course-project",
    "description": "Haze can cause poor visibility and loss of contrast in images and videos. In this article, we study the dehazing problem which can improve visibility and thus help in many computer vision applications. An extensive comparison of state of the art single image dehazing methods is done. One simple contrast enhancement method is used for dehazing. Structure- texture decomposition has been used in conjunction with this enhancement method to improve its performance in presence of synthetic noise. Methods which use a haze formation model and attempt at solving an ill-posed problem using computer vision priors are also investigated. The two priors studied are dark channel prior and the non-local prior. Both qualitative and quantitative comparisons for atmospheric and underwater images on all three methods provide a conclusive idea of which dehazing method performs better. All this knowledge has been extended to video dehazing. A video dehazing method which uses the spatial and temporal information in a video is studied in depth. An improved version of video dehazing is proposed in this article, which uses the spatial-temporal information fusion framework but does not suffer from some of its limitations. The new video dehazing method is shown to produce better results on test videos",
    "stars": 38,
    "forks": 13,
    "language": "Matlab",
    "url": "https://github.com/koujan/Robotics-Course-project",
    "topics": [],
    "created_at": "2016-12-22T04:20:18Z",
    "updated_at": "2025-06-04T12:20:13Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# Robotics-Course-project\nHaze can cause poor visibility and loss of contrast in images and videos. In this article, we study the dehazing problem which can improve visibility and thus help in many computer vision applications. An extensive comparison of state of the art single image dehazing methods is done. One simple contrast enhancement method is used for dehazing. Structure- texture decomposition has been used in conjunction with this enhancement method to improve its performance in presence of synthetic noise. Methods which use a haze formation model and attempt at solving an ill-posed problem using computer vision priors are also investigated. The two priors studied are dark channel prior and the non-local prior. Both qualitative and quantitative comparisons for atmospheric and underwater images on all three methods provide a conclusive idea of which dehazing method performs better. All this knowledge has been extended to video dehazing. A video dehazing method which uses the spatial and temporal information in a video is studied in depth. An improved version of video dehazing is proposed in this article, which uses the spatial-temporal information fusion framework but does not suffer from some of its limitations. The new video dehazing method is shown to produce better results on test videos\n",
    "readme_length": 1321
  },
  {
    "name": "CLIPS",
    "full_name": "UCSC-VLAA/CLIPS",
    "description": "An Enhanced CLIP Framework for Learning with Synthetic Captions",
    "stars": 37,
    "forks": 1,
    "language": "Python",
    "url": "https://github.com/UCSC-VLAA/CLIPS",
    "topics": [],
    "created_at": "2024-11-24T21:59:37Z",
    "updated_at": "2025-06-11T04:06:08Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# **CLIPS**\n\n**Official implementation of the paper \"[_CLIPS: An Enhanced CLIP Framework for Learning with Synthetic Captions_](https://arxiv.org/abs/2411.16828)\".**\n\n\n![Method Pipeline](./docs/resources/method.jpg)\n\nPrevious works show that noisy, web-crawled image-text pairs may limit vision-language pretraining like CLIP and propose learning with synthetic captions as a promising alternative. Our work continues this effort, introducing two simple yet effective designs to better leverage richly described synthetic captions:\n\n1. By observing a strong inverse effect with synthetic captions, we use only **partial synthetic captions** to feed the text encoder, achieving significantly better performance.\n2. We incorporate an **autoregressive captioner** that mimics the recaptioning process, predicting full-length synthetic captions conditioned on the image and original web-crawled captions.\n\nOur method achieves **state-of-the-art (SOTA)** results in zero-shot image-text retrieval on MSCOCO and Flickr30K, while enhancing the visual capability of LLaVA.\n\n---\n\n## **Links**\n- [ðŸ“„ Paper (arXiv)](https://arxiv.org/abs/2411.16828)  \n- [ðŸ¤— Pretrained Model on HuggingFace](https://huggingface.co/UCSC-VLAA/ViT-L-14-CLIPS-Recap-DataComp-1B)  \n- [ðŸŒ Project Page](https://ucsc-vlaa.github.io/CLIPS/)\n\n---\n\n## **Key Results**\n\n### **Inverse Effect with Synthetic Captions**\n![Inverse Effect Visualization](./docs/resources/mask_strategy.jpg)\n\nVisualization of four different token reduction strategies. These strategies can improve the model's learning efficiency on synthetic captions to varying degrees. Among these strategies, the sub-caption and block mask perform best.\n\n---\n\n### **Zero-Shot Cross-Modal Retrieval**\n![Zero-Shot Retrieval Results](./docs/resources/retrieval.png)\n\nOur method consistently achieves superior performance across all benchmarks and model sizes, yielding significant improvements over the baselines.\n\n---\n\n### **Comparison with State-of-the-Art Methods**\n![SOTA Comparison](./docs/resources/sota.png)\n\nWith increased computational resources and scaling, our best model further achieves 76.4% and 96.6% R@1 text retrieval performance on MSCOCO and Flickr30K respectively, and 57.2% and 83.9% R@1 image retrieval performance on the same datasets, setting new state-of-the-art (SOTA) results.\n\n---\n\n### **CLIPS in LLaVA**\n![LLaVA Results](./docs/resources/LLaVA.png)\n\nReplacing OpenAI-CLIP with **CLIPS** significantly boosts LLaVA's performance across various benchmarks.\n\n---\n\n## **Model Zoo**\n\n| Model          | Link                                                                                     |\n|----------------|------------------------------------------------------------------------------------------|\n| CLIPS-Large-14-224 | [ðŸ¤— HuggingFace Model](https://huggingface.co/UCSC-VLAA/ViT-L-14-CLIPS-224-Recap-DataComp-1B) |\n| CLIPS-Large-14-336 | [ðŸ¤— HuggingFace Model](https://huggingface.co/UCSC-VLAA/ViT-L-14-CLIPS-Recap-DataComp-1B) |\n| CLIPS-Huge-14-224  | [ðŸ¤— HuggingFace Model](https://huggingface.co/UCSC-VLAA/ViT-H-14-CLIPS-224-Recap-DataComp-1B) |\n| CLIPS-Huge-14-336  | Coming Soon...                                                                          |\n\n## **Model Usage**\n### **Environment**\nInstall dependencies:\n```\npip3 install -r requirements.txt\n```\n### **With OpenCLIP**\n```python\nimport torch\nimport torch.nn.functional as F\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom open_clip import create_model_from_pretrained, get_tokenizer\n\nmodel, preprocess = create_model_from_pretrained('hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-Recap-DataComp-1B')\ntokenizer = get_tokenizer('hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-Recap-DataComp-1B')\n\nimage = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nimage = preprocess(image).unsqueeze(0)\n\ntext = tokenizer([\"a diagram\", \"a dog\", \"a cat\", \"a beignet\"], context_length=model.context_length)\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features = F.normalize(image_features, dim=-1)\n    text_features = F.normalize(text_features, dim=-1)\n\n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\nprint(\"Label probs:\", text_probs)  # prints: [[0., 0., 0., 1.0]]\n```\n#### Note: We made modifications to the tokenizer implementation in open_clip/tokenizer.py.\n\n## Acknowledgement\n\nThis pytorch repo is built on [OpenCLIP](https://github.com/mlfoundations/open_clip). \nMany thanks to the awesome works from the open-source community!\n\nWe would like to thank TPU Research Cloud (TRC) program, Google Cloud Research Credits program, and AWS Cloud Credit for Research program for supporting our computing needs.\n\n---\n\n## **Citation**\n\nIf you use our work, please cite it:\n\n```bibtex\n@article{liu2024clips,\n  title={CLIPS: An Enhanced CLIP Framework for Learning with Synthetic Captions},\n  author={Liu, Yanqing and Li, Xianhang and Wang, Zeyu and Zhao, Bingchen and Xie, Cihang},\n  journal={arXiv preprint arXiv:2411.16828},\n  year={2024}\n}\n",
    "readme_length": 5144
  },
  {
    "name": "Methane-Emissions-Detection-with-Multispectral-Sensors",
    "full_name": "Otutu11/Methane-Emissions-Detection-with-Multispectral-Sensors",
    "description": "AI-powered methane detection system using synthetic multispectral data. Trains regression and classification models to estimate methane enhancement and detect leaks, with feature engineering, evaluation metrics, and visual diagnostics for climate-tech applications in greenhouse gas monitoring and environmental sustainability.",
    "stars": 34,
    "forks": 0,
    "language": "Python",
    "url": "https://github.com/Otutu11/Methane-Emissions-Detection-with-Multispectral-Sensors",
    "topics": [],
    "created_at": "2025-09-01T13:36:27Z",
    "updated_at": "2025-11-29T09:17:05Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# Methane Emissions Detection with Multispectral Sensors\n\nThis repository demonstrates how **artificial intelligence (AI)** and **multispectral remote sensing** can be applied to detect and quantify methane (CHâ‚„) emissions.  \nUsing a **synthetic dataset** (simulated reflectance bands, thermal channels, and atmospheric variables), we train:\n\n- A **regression model** to estimate methane column enhancement (ppm-eq).  \n- A **classification model** to detect the presence of methane leaks.  \n\nThe pipeline integrates data generation, preprocessing, model training, evaluation, and visualization.\n\n---\n\n## ðŸš€ Features\n- Synthetic **multispectral dataset** (>1,500 samples) with realistic CHâ‚„-driven spectral effects.\n- Derived vegetation and spectral indices (NDVI, NDSI, NBR, Albedo).\n- **Random Forest Regressor** for CHâ‚„ enhancement prediction.\n- **Gradient Boosting Classifier** for emission detection.\n- Evaluation metrics: MAE, RMSE, RÂ² (regression), Accuracy, ROC-AUC, Confusion Matrix (classification).\n- Visual outputs: regression parity plots, ROC curves, feature importances, confusion matrices.\n\n---\n\n## ðŸ“‚ Project Structure\n.\nâ”œâ”€â”€ outputs/\nâ”‚ â”œâ”€â”€ synthetic_methane_multispectral.csv\nâ”‚ â”œâ”€â”€ methane_regressor.joblib\nâ”‚ â”œâ”€â”€ methane_classifier.joblib\nâ”‚ â”œâ”€â”€ regression_parity.png\nâ”‚ â”œâ”€â”€ regression_feature_importance.png\nâ”‚ â”œâ”€â”€ classification_roc.png\nâ”‚ â”œâ”€â”€ classification_confusion.png\nâ”‚ â”œâ”€â”€ classification_feature_importance.png\nâ”‚ â”œâ”€â”€ classification_report.txt\nâ”‚ â”œâ”€â”€ cv_results.txt\nâ”‚ â””â”€â”€ README_Methane_Emissions_Demo.txt\nâ”œâ”€â”€ methane_emissions_multispectral_demo.py\nâ””â”€â”€ README.md\n\nyaml\nCopy code\n\n---\n\n## ðŸ“Š Dataset\nThe dataset is synthetically generated to mimic real-world methane absorption and scene variability.  \nIt includes:\n\n- **Bands**: Blue, Green, Red, NIR, SWIR1, SWIR2, TIR  \n- **Scene Variables**: Surface pressure, humidity, wind speed, solar zenith  \n- **Indices**: NDVI, NDSI, NBR, Albedo  \n- **Targets**:  \n  - `ch4_ppm_eq`: methane column enhancement (continuous, ppm-eq)  \n  - `emission_label`: binary indicator (1 = emission present, 0 = none)  \n\n---\n\n## ðŸ§ª Methodology\n1. **Synthetic Data Generation**  \n   - Simulates spectral absorption in SWIR bands due to methane plumes.  \n   - Includes meteorological drivers (wind, temperature) affecting detection.  \n\n2. **Feature Engineering**  \n   - Computation of vegetation and reflectance indices for sensitivity.  \n\n3. **Model Training**  \n   - Regression â†’ RandomForestRegressor.  \n   - Classification â†’ GradientBoostingClassifier.  \n\n4. **Evaluation**  \n   - Regression: MAE, RMSE, RÂ² + Parity plots.  \n   - Classification: Accuracy, ROC-AUC, Confusion Matrix, Feature Importances.  \n\n---\n\n## âš™ï¸ Installation\n```bash\ngit clone https://github.com/yourusername/Methane-Emissions-Detection-with-Multispectral-Sensors.git\ncd Methane-Emissions-Detection-with-Multispectral-Sensors\npip install -r requirements.txt\nrequirements.txt\n\nnginx\nCopy code\nnumpy\npandas\nscikit-learn\nmatplotlib\njoblib\nâ–¶ï¸ Usage\nRun the demo script:\n\nbash\nCopy code\npython methane_emissions_multispectral_demo.py\nThis will generate:\n\nA synthetic dataset (outputs/synthetic_methane_multispectral.csv)\n\nTrained models (.joblib)\n\nEvaluation plots + reports in the outputs/ folder\n\nQuick Inference\npython\nCopy code\nfrom joblib import load\nimport pandas as pd\n\n# Load dataset\ndf = pd.read_csv(\"outputs/synthetic_methane_multispectral.csv\")\nX = df[['blue','green','red','nir','swir1','swir2','tir_bt','surface_pressure',\n        'humidity','wind_speed','solar_zenith','ndvi','ndsi','nbr','albedo']].values\n\n# Load models\nreg = load(\"outputs/methane_regressor.joblib\")\nclf = load(\"outputs/methane_classifier.joblib\")\n\n# Predict\ny_ch4 = reg.predict(X)             # ppm-eq methane estimate\np_leak = clf.predict_proba(X)[:,1] # probability of leak\nðŸ“ˆ Results (Synthetic Example)\nRegression\n\nMAE = ~0.40 ppm-eq\n\nRMSE = ~0.57 ppm-eq\n\nRÂ² = ~0.85\n\nClassification\n\nAccuracy = ~89%\n\nROC-AUC = ~0.93\n\nCross-validated ROC-AUC = ~0.92 Â± 0.01\n\nðŸŒ Applications\nMethane leak detection in oil & gas facilities.\n\nGreenhouse gas monitoring from airborne/satellite sensors.\n\nClimate change mitigation & compliance monitoring.\n\nðŸ“š References\nCusworth, D. H., et al. (2021). \"A review of satellite remote sensing for methane emissions detection.\" Atmospheric Environment.\n\nThorpe, A. K., et al. (2017). \"Mapping methane concentrations from airborne remote sensing.\" Remote Sensing of Environment.\n\nJongaramrungruang, S., et al. (2019). \"Towards accurate methane detection using hyperspectral imaging.\" Atmospheric Measurement Techniques.\n\nðŸ“ License\nThis project is released under the MIT License.\n\nAuthor: Anslem Otutu\nGithub: @Otutu11\n",
    "readme_length": 4641
  },
  {
    "name": "low-light-video-enhancement",
    "full_name": "sjmoran/low-light-video-enhancement",
    "description": "Repository for the ECCV 2020 paper: \"Low Light Video Enhancement using Synthetic Data Produced with an Intermediate Domain Mapping\"",
    "stars": 33,
    "forks": 4,
    "language": "Python",
    "url": "https://github.com/sjmoran/low-light-video-enhancement",
    "topics": [
      "computer-vision",
      "cvpr",
      "eccv",
      "generative-adversarial-network",
      "iccv",
      "low-light-image-enhancement",
      "video-enhancement"
    ],
    "created_at": "2020-08-27T08:40:34Z",
    "updated_at": "2025-10-20T07:14:21Z",
    "homepage": "",
    "license": "Other",
    "readme": "# SIDGAN: Low Light Video Enhancement using Synthetic Data Produced with an Intermediate Domain Mapping (ECCV 2020)\n\nDanai Triantafyllidou, [Sean Moran](http://www.seanjmoran.com), [Steven McDonagh](https://smcdonagh.github.io/), [Sarah Parisot](https://parisots.github.io/), [Greg Slabaugh](http://gregslabaugh.net/)\n\n**Huawei Noah's Ark Lab**\n\n<p>\n<i>Author's personal repository</i> for the ECCV 2020 paper <b>SIDGAN: Low Light Video Enhancement using Synthetic Data Produced with an Intermediate Domain Mapping</b>. Here you will find a link to the code, pre-trained models and information on the datasets. Please raise a Github issue if you need assistance of have any questions on the research. The official Huawei repository for SIDGAN is located <a href=\"https://github.com/huawei-noah/noah-research/tree/master/SIDGAN\">here</a>.\n</p>\n\n### [[Paper @ CVF]](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580103.pdf) \n### [[Video (10 Minutes)]](https://youtu.be/-MwiYBXdtD4) \n<!-- ### [[Video (1 Minute)]](https://youtu.be/mF_ZX0yfMkk) -->\n### [[Supplementary @ CVF]](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580103-supp.zip) \n\n<p align=\"center\">\n<img src=\"./images/sidgan.png\" width=\"80%\"/>\n\n## Installation\n\n#### 1. Clone the code from Github\n\nClone the code and place it under $PATH_TO_CODE\n\n```\ngit clone https://github.com/huawei-noah/noah-research.git\ncd SIDGAN\n\n```\n #### 2. Install dependencies\n\nPython: see requirement.txt for complete list of used packages. We recommend doing a clean installation of requirements using virtualenv\n\n```\nconda create -n sidganenv python=3.5\nsource activate sidganenv\npip install -r requirement.txt\n```\nIf you dont want to do the above clean installation via virtualenv, you could also directly install the requirements through:\n\n```\npip install -r requirement.txt --no-index\n```\n\n## Running the code\n\n#### 1. Download the data\n\nPlease download the [SID Motion](https://github.com/cchen156/Seeing-Motion-in-the-Dark) and [Vimeo-90k](http://toflow.csail.mit.edu/) datasets and create the following folder structure in $PATH_TO_DATA\n\n```\n.\nâ””â”€â”€ data\n    â”œâ”€â”€ SID_long\n    â”œâ”€â”€ VBM4D_rawRGB\n    â””â”€â”€ vimeo\n```\n\n#### 2. Running the training code\n\nTo train a cycleGAN model for the task of mapping Vimeo videos (domain A) into the SID Motion long exposure (domain B), do:\n\n```\npython train_cyclegan_a2b.py --data_root' $PATH_TO_DATA --project_root $PATH_TO_CODE --name $EXP_NAME\n```\n\nTo train a cycleGAN model for the task of mapping SID Motion long exposure (domain B) into the SID Motion short exposure (domain C), do:\n\n```\npython train_cyclegan_b2c.py --data_root' $PATH_TO_DATA --project_root $PATH_TO_CODE --NAME $EXP_NAME\n```\n\n\nTraining results will be saved in $PATH_TO_CODE/experiments/$EXP_NAME\n\n```\n.\nâ”œâ”€â”€ images\nâ”œâ”€â”€ log\nâ”œâ”€â”€ meta_data.json\nâ””â”€â”€ saved_models\n```\n\n## Bibtex\n\nIf you find the code useful, please cite this paper:\n\n```\n@inproceedings{triantafyllidou2020low,\n  title={Low Light Video Enhancement using Synthetic Data Produced with an Intermediate Domain Mapping},\n  author={Triantafyllidou, Danai and Moran, Sean and McDonagh, Steven and Parisot, Sarah and Slabaugh, Gregory},\n  booktitle={European Conference on Computer Vision},\n  pages={103--119},\n  year={2020},\n  organization={Springer}\n}\n```\n\n\n## License\n\nBSD-0-Clause License\n\n## Contributions\n\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.\n\nIf you plan to contribute new features, utility functions or extensions to the core, please first open an issue and discuss the feature with us. Sending a PR without discussion might end up resulting in a rejected PR, because we might be taking the core in a different direction than you might be aware of.\n",
    "readme_length": 3778
  },
  {
    "name": "Whisper-Synthetic-ASR-Dataset-Generator",
    "full_name": "gongouveia/Whisper-Synthetic-ASR-Dataset-Generator",
    "description": "This UI serves as a Synthetic ASR Dataset Generator powered by/for OpenAI Whisper, enabling users to capture audio, transcribing it, on the fly and manage the generated dataset ðŸ¤—. Fine tune Whisper  or enhanced and custom datasets ",
    "stars": 32,
    "forks": 2,
    "language": "Python",
    "url": "https://github.com/gongouveia/Whisper-Synthetic-ASR-Dataset-Generator",
    "topics": [
      "asr",
      "dataset-generation",
      "nlp",
      "speech-to-text",
      "synthetic-data-generation",
      "whisper"
    ],
    "created_at": "2024-02-17T22:12:15Z",
    "updated_at": "2025-11-20T00:07:29Z",
    "homepage": "",
    "license": "N/A",
    "readme": "ðŸ‘· Soon to implement Whisper Turbo for Audio-Text pairs Synthesization.\r\nOptimized version of Whisper large-v3).\r\n\r\n# Synthetic Speech Dataset Generator \r\nWith the rising number of ASR/NLP open source projects democratizing the AI human-machine interface comes with the necessity of getting better ASR datasets. <Whisper Temple delves creates a simple-to-use platform to create Synthetic speech datasets, creating pairs of audio and text. Translation powered by faster-whisper â© Synthetic translations can be edited in UI Data viewer. \r\nUser interface is created using PyQt5 and runs totally on local machine.\r\n\r\n\r\n## Overview\r\nThis application serves as a Synthetic Speech Generator, enabling users to transcribe captured audio and manage generated datasets. It provides a user-friendly interface for configuring audio parameters, transcription options, and dataset management.\r\n![Screenshot 2024-06-16 223519](https://github.com/gongouveia/Whisper-Temple-Synthetic-ASR-Dataset-Generator/assets/68733294/24d41a0c-3592-4a0d-8c0d-61aa1aac83c1)\r\n\r\n## Features\r\n- **Audio Capture**: Users can capture audio samples with customizable settings such as sample rate and duration.\r\n- **Transcription**: Provides the option to transcribe captured audio into text.\r\n- **Audio Metadata**: Allows to add metadata to dataset, such as audio sample rate and duration.\r\n- **Dataset Management**: Enables users to view, delete, and manage entries in the generated dataset.\r\n- **Export**: Allows exporting of the dataset for further processing or Hugging Face :hugs:.\r\n\r\n## Future Releases\r\nAdding metadata to each dataset entry, audio sample rate, length, or speaker gender and age.\r\nReadMe update with UI screenshot and video\r\n\r\n### Exsting small bugs:\r\n\r\nðŸŸ¡ In some OS dark theme template of pyqtdarktheme is not found.\r\n\r\n\r\n## Installation  (Experimenta. Not yet complete; will do Pypi and conda install)\r\nFirst, I suggest you create and activate a new virtual environment using `conda` or `virtenv`. Then follow steps â¬‡ï¸\r\n1. Clone the repository:\r\n    ```bash\r\n    git clone  https://github.com/gongouveia/Syntehtic-Speech-Dataset-Generator.git\r\n    ```\r\n2. Install dependencies in req.yml `conda env create -f req.yml`\r\n3. Follow instruction in Usage Section.\r\n\r\n## Usage\r\n1. Launch the application and create or continue a project by running `python temple.py --project <default:Project> --theme <default:'auto', 'light', 'dark'>,  `\r\n    OR Export the audio Dataset project to HuggingFace using `python export.py --project <default:Project> --language <default:'eu'>....., ` For more info. see `python export.py --help `\r\n3. Configure audio capture parameters such as sample rate in KHz `default: 16000` and duration in milliseconds `default: 5000`.\r\n4. If CUDA is found, it is possibel to transcribe audio records at the ed of each recording. Otherwise, yo can batch transcribe the audios in the DatasetViewer..\r\n5. Choose whether to use VAD option in transcripion or not, default is enabled and allows for a faster trancription.\r\n6. Click on \"Capture Audio\" to start a new audio recording.\r\n7. View and manage Audio dataset using provided menu options.\r\n8. Edit weak Transcriptions, creating a even more robust training dataset for Whisper.\r\n# Notes\r\nIf the Idiom argument is set to ('en'), the languages dropdown menu is not available. \r\nIf option 3 is disabled, it is possible to transcribe all the captured audios in the dataset viewer window. You can add audios to the Audio dataset by pasting them in the `/Audios` folder under your desired project.\r\n\r\n### Configuration\r\n- **Audio Sample Rate**: Set the sample rate for audio capture (in KHz).\r\n- **Audio Duration**: Define the duration of audio samples to capture (in milliseconds).\r\n- **Transcribe**: Choose whether to transcribe captured audio (Yes/No).\r\n- **VAD**: Enable or disable VAD in transcription (Yes/No).\r\n\r\n### Dataset Management\r\n- **View Dataset**: Opens a new window to view the generated dataset.\r\n- **Refresh Dataset**: Refreshes dataset, use if changed metadata.csv.\r\n- **Delete Entry**: Deletes the last recorded entry from the dataset.\r\n\r\nRight now there is no support for .mp3 files, however you can batch conver .mp3 flies and .flac files to .wav using ffmpeg batch conversion as in [https://ottverse.com/convert-all-files-inside-folder-ffmpeg-batch-convert/]\r\n\r\n## Exporting Dataset as Hugging Face Audio Dataset\r\nTo export the final dataset as a Hugging Face ðŸ¤— Datasets, use the Command-Line Interface (CLI) provided.\r\n[https://huggingface.co/docs/datasets/audio_dataset]\r\n\r\nYou can log in to UI by providing the hf token [https://huggingface.co/docs/hub/security-tokens].\r\n\r\n## Future releases or on demand solutions:\r\n Dependinfg on community or necessity, this features will be merged: \r\n\r\n  - Adding a new translation engine or more translation configuration options;\r\n  - Adding more metadata to the Dataset, such as speaker and file type information;\r\n  - Export as kaldi â˜• dataset format;\r\n  - Adding a loading bars for the dataset batch translation;\r\n  - New window to train whisper with the new pseudo-synthetic dataset (**on-request**, contact me if you need this solution).\r\n\r\n## Contributing\r\nContributions to this project are welcome! If you'd like to contribute, please follow the standard GitHub workflow:\r\n1. Fork the repository.\r\n2. Create a new branch for your feature (`git checkout -b feature/your-feature`).\r\n3. Commit your changes (`git commit -am 'Add some feature'`).\r\n4. Push to the branch (`git push origin feature/your-feature`).\r\n5. Create a new Pull Request.\r\n\r\n## Author\r\n\r\nFor any inquiries or collaboration, please contact [gongou00@gmail.com].\r\nI would be thankful to be cited in created datasets.\r\n",
    "readme_length": 5704
  },
  {
    "name": "PII-Detection",
    "full_name": "mddunlap924/PII-Detection",
    "description": "Personal Identifiable Information (PII) entity detection and performance enhancement with synthetic data generation",
    "stars": 31,
    "forks": 4,
    "language": "Python",
    "url": "https://github.com/mddunlap924/PII-Detection",
    "topics": [
      "fine-tuning",
      "kaggle-competition",
      "llama3",
      "llm",
      "named-entity-recognition",
      "ner",
      "pii",
      "pii-data",
      "pii-detection",
      "prompt-engineering"
    ],
    "created_at": "2024-01-23T01:08:57Z",
    "updated_at": "2025-10-09T15:41:42Z",
    "homepage": "",
    "license": "N/A",
    "readme": "<h1 align=\"center\">\n  PII Detection and BIO Synthetic Data Generation\n</h1>\n\n<p align=\"center\">This repository fine-tunes a state of the art PII detection system and enhances performance with synthetic PII data generation.\n</p> \n\n<p align=\"center\">\n<a href=\"#introduction\">Introduction</a> &nbsp;&bull;&nbsp;\n<a href=\"#highlights\">Highlights</a> &nbsp;&bull;&nbsp;\n<a href=\"#synthetic-pii-data\">Synthetic PII Data</a> &nbsp;&bull;&nbsp;\n<a href=\"#pii-entity-detection-systems\">PII Entity Detection Systems</a> &nbsp;&bull;&nbsp;\n<a href=\"#issues\">Issues</a> &nbsp;&bull;&nbsp;\n</p>\n\n<p align=\"center\">\n  <a target=\"_blank\" href=\"https://www.linkedin.com/in/myles-dunlap/\"><img height=\"20\" src=\"https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white\" />\n  </a>\n  <a target=\"_blank\" href=\"https://www.kaggle.com/dunlap0924\"><img height=\"20\" src=\"https://img.shields.io/badge/-Kaggle-5DB0DB?style=flat&logo=Kaggle&logoColor=white&\" />\n  </a>\n  <a target=\"_blank\" href=\"https://scholar.google.com/citations?user=ZpHuEy4AAAAJ&hl=en\"><img height=\"20\" src=\"https://img.shields.io/badge/-Google_Scholar-676767?style=flat&logo=google-scholar&logoColor=white&\" />\n  </a>\n</p>\n\n# Introduction\nPersonal identifiable information (PII) is sensitive data used to identify, locate, or contact an individual. PII entity detection systems can identify, categorize, and redact sensitive information in unstructured text. Improving PII detection systems help maintain the privacy and security of individuals, comply with legal and regulatory requirements, and prevent identity theft, fraud, or other types of harm. Figure 1 provides an example PII entities using inside, outside, beginning (IOB) format.\n\n<p align=\"center\"> \n    <img src=\"./imgs/pii-bio-example.png\",\n    style=\"width:900px;height:300px;\">\n    <br>\n    Figure 1: Example of PII Data in IOB Format [<a href=\"https://www.kaggle.com/code/snassimr/pii-data-detection-eda\">Source</a>].\n</p>\n\nThe work in this repository was derived during the Kaggle competition [The Learning Agency Lab - PII Data Detection](https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data). Using the techniques in this repository will provide solutions in the Top 1% for the competition.\n\n# Highlights\n\n1) [Synthetic PII datasets](#synthetic-pii-datasets) with BIO formatting.\n    - [Meta-Llama3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) is used to generate synthetic essays.\n    - Prompting best techniques from [Prompt Engineering](https://www.promptingguide.ai/) and using `PII PlaceHolders` instead of directly putting PII data into a prompt.\n    - [Faker](https://faker.readthedocs.io/en/master/) to create custom PII data that is injected into unstructured text.\n    - Decoupling LLM domain specific generate text with PII placeholder and Faker PII data is useful for efficient experimentation in creating synthetic PII datasets.\n2) [PII Entity Detection Systems](#pii-entity-detection-systems)\n    - [Masked Language Modeling (MLM) with Hugging Face Trainer for Domain Adaptation](./training/mlm-training.py)\n    - [Hugging Face Token Classification](https://huggingface.co/docs/transformers/en/tasks/token_classification) end-to-end pipeline is built for fine-tuning a state-of-the-art model.\n    - [microsoft/deberta-v3-large](https://huggingface.co/microsoft/deberta-v3-large) model is trained using a configuration files and Bash shells for versatile automation. \n    - [Weights and Biases for Experiment Tracking](https://wandb.ai/site/experiment-tracking)\n    - [Class Weights with Custom Loss Function](https://discuss.huggingface.co/t/how-can-i-use-class-weights-when-training/1067) - token classification can have severe class imbalance and this is addressed by adjusting the `class_weights` parameter in the [Hugging Face Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer) and with a either a [Focal Loss](https://paperswithcode.com/method/focal-loss#:~:text=Focal%20loss%20applies%20a%20modulating,in%20the%20correct%20class%20increases.) or [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n\n# Synthetic PII Data\n\nThe IOB format, also commonly referred to as the BIO format, is a common tagging format for tagging tokens in a chunking task such as Named Entity Recognition (NER) applications. Creating labeled BIO datasets can be time and labor intensive for domain specific datasets. An alternative approach is to synthetically generate PII datasets that closely represent your real-life application. Refer to the `gen-data` directory for code to create domain specific PII data. The below files would be executed sequentially because they each represent a different task in the synthetic PII data creation.\n\n### 1) [Faker PII Data](./gen-data/pii-syn-data.py)\nSynthetic PII data was created using [Faker](https://faker.readthedocs.io/en/master/) and custom functions to create PII information. This data was placed into LLM generated unstructured text that was created in the next step.\n\n\n### 2) [LLM Domain Specific Text Generation](./gen-data/ai-gen-llama3.py)\n \nGenerative LLMs (e.g., Llama3) were used to generate unstructured text that resembles domain specific text. In this repository the data mimics essays from students in an online course. Refer to the various [prompts](./gen-data/prompt-templates/) for prompting examples used in this work.\n\n**NOTICE**: a helpful insight learned during this work was to prompt the LLM to create `placeholders` for the PII data. The data from Step #1 will be injected into the placeholders. The PII placeholder approach provided the following anecdotal benefits:\n  - Certain LLM models (both open and closed source) may refuse to generate text if PII is mentioned due to censorship.\n  - Placeholders are easy to locate using simple techniques (e.g., regex).\n  - LLM's hallucinated less when writing placeholders as opposed to PII data directly.\n\nTwo examples of prompting strategys are given which demonstrate the differences between prompting with PII directly (Example 1) and then with placeholders (Example 2). \n\n**Example 1: LLM Prompting with Direct PII Data Injection**\n  ```txt\n  Model Prompt:\n  Write an sentence introducing yourself and include only your personal information provided below:\n    - FULL_NAME: John Doe\n    - PHONE_NUM: 555-123-4567\n\n  Model Response:\n  My name is John Doe and you can reach me at 555-125-4567 or by email at john.doe@gmail.com.\n  ```\nThis prompting technique routinely introduced a few mistakes from the LLMs that were difficult to programmatically recognize and lead to mis-labeling of PII in the BIO format. Example 1 asks the model to directly insert PII data into the text and notice the few errors:\n  - The PHONE_NUM was off by a single digit (123 vs. 125). This could occur with names, phone numbers, street address, or any of the PII entities.\n  - The LLM would inject extra PII not asked for such as the `john.doe@gmail.com` email.\n    \n**Example 2: LLM Prompting with PII PlaceHolders**\n  ```txt\n  Model Prompt:\n  Write an sentence introducing yourself and include only your personal information using the placeholders provided below:\n    - {FULL_NAME}: First and Last Name\n    - {PHONE_NUM}: Personal phone number\n\n  Model Response:\n  My name is {FULL_NAME} and you can reach me at {PHONE_NUM}.\n  ```\n  With Example 2 the LLM did not have the opportunity to transpose any errors in the PHONE_NUM or with the FULL_NAME. Also, it seemed the LLM would less frequently incorporate extra PII entities.\n\n\n### 3) [Insert PII Data into LLM Generated Text](./gen-data/finalize-placeholder-data-llama3.py)\nThe above two steps decoupled PII data and domain specific text generation. In Step #3 the Step #1 PII data is inserted into Step #2's LLM domain specific generated text. This is useful because you can easily experiment with different combinations of PII data and domain specific text generation data.  \n\n# PII Entity Detection Systems\n\nThe best performing LLM model for PII entity detection was Microsoft's [Decoding-enhanced BERT with Disentangled Attention V3](https://github.com/microsoft/DeBERTa) model. This model consistently performs well for encoder model tasks such as named entity recognition (NER), question and answer, and classification. \n\nA good starting point for training a Deberta-V3 model is with the [Baseline Deberta-V3 Fine-Tuning](./training/train_chunks_cib.py) module. In this module a custom Hugging Face Trainer was created to train with either Focal Loss or CE loss to account for class imbalance.\n\n```python\nclass CustomTrainer(Trainer):\n    def __init__(\n            self,\n            focal_loss_info: SimpleNamespace,\n            *args,\n            class_weights=None,\n            **kwargs):\n        super().__init__(*args, **kwargs)\n        # Assuming class_weights is a Tensor of weights for each class\n        self.class_weights = class_weights\n        self.focal_loss_info = focal_loss_info\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Extract labels\n        labels = inputs.pop(\"labels\")\n\n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        # Loss calculation\n        if self.focal_loss_info.apply:\n            loss_fct = FocalLoss(alpha=5, gamma=2, reduction='mean')\n            loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n                            labels.view(-1))\n        else:\n            loss_fct = CrossEntropyLoss(weight=self.class_weights)\n            if self.label_smoother is not None and \"labels\" in inputs:\n                loss = self.label_smoother(outputs, inputs)\n            else:\n                loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n                                labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n```\n\nFurther tricks and tips to help fine-tune PII detection systems that are contained in the [training](./training/) directory are:\n  - [Masked Language Modeling (MLM) with Hugging Face Trainer for Domain Adaptation](./training/mlm-training.py) can utilize `unlabeled datasets` to expose a model to domain-specific language patterns and terminology. Fine-tuning a model that underwent additional pre-training on a specific task or domain, beginning with an initial checkpoint tailored for the task and data distribution at hand, typically yields better performance compared to fine-tuning models that start from a generic initial checkpoint [Sources: [1](https://www.kaggle.com/code/hinepo/domain-adaptation-with-mlm), [2](https://arxiv.org/abs/1905.05583)].\n  - [Weights and Biases](https://wandb.ai/site/experiment-tracking) was used for experiment tracking in this source code. The below link is an excellent reference to follow on setting up W&B.\n    - [Instrumenting Weights & Biases: PII data detection](https://www.youtube.com/watch?v=w4ZDwiSXMK0) Darek Kteczek shows how to instrument W&B in your ML pipelines using a PII detection use case\n  - **Single or Dual GPU Training**: three modules were prepared to experiment with a fine-tuning model with single or dual GPUs. There was a balance between token lengths, model size, and training times.\n    - [Single GPU for Reasonable Token Lengths with Stride](./training/train_chunks_cib.py): this is a typically fine-tuning approach where the token sizes of 512 or 1,024 with strides (e.g., 16, 32, or 128) are used to chunk the text. These approaches gave excellent results in performance and also do not require as much GPU memory. Recall, that GPU memory scales quadratically with token length for transformer models [[Latent Attention for Linear Time Transformers](https://arxiv.org/abs/2402].17512#:~:text=The%20time%20complexity%20of%20the,defining%20attention%20via%20latent%20vectors.).\n    - [Single GPU using a High Token Length and No Stride](./training/train_single_large.py): [gradient check pointing](https://aman.ai/primers/ai/grad-accum-checkpoint/) was incorporated into this script to prevent GPU memory crashes due to very large token lengths > 5K.\n    - [Dual GPU Training](./training/train_dual_gpu.py): this module shards the Deberta-V3 model across two GPUs so other memory efficient techniques do not need to be deployed and high token lengths can be utilized. A downside to this approach is that without proper hardware (i.e., [NVLinks](https://en.wikipedia.org/wiki/NVLink)) the training time will considerably increase due to data transfer between GPUs during training. \n\n**NOTE**: This workflow presented here can be adapted for many Hugging Face deep learning applications, not just LLMs.\n\n\n# Issues\nThis repository is will do its best to be maintained. If you face any issue or want to make improvements please <a href=\"https://github.com/mddunlap924/PII-Detection/issues\">raise an issue</a> or submit a Pull Request. :smiley:\n",
    "readme_length": 12930
  },
  {
    "name": "PRESTO",
    "full_name": "IDEA-XL/PRESTO",
    "description": "PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes [EMNLP 2024]",
    "stars": 28,
    "forks": 4,
    "language": "Python",
    "url": "https://github.com/IDEA-XL/PRESTO",
    "topics": [],
    "created_at": "2024-06-19T03:15:37Z",
    "updated_at": "2025-07-07T11:44:34Z",
    "homepage": "https://arxiv.org/abs/2406.13193",
    "license": "Apache License 2.0",
    "readme": "# PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes (EMNLP 2024 Findings)\n\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green?style=flat-square)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE)\n[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red?style=flat-square)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/DATA_LICENSE)\n[![Paper Link](https://img.shields.io/badge/Paper-pink?style=flat-square&logo=arXiv)](https://arxiv.org/abs/2406.13193)\n[![GitHub Link](https://img.shields.io/badge/GitHub-blue?style=flat-square&logo=github)](https://github.com/IDEA-XL/PRESTO)\n[![Huggingface Link](https://img.shields.io/badge/Huggingface-orange?style=flat-square&logo=huggingface)](https://huggingface.co/OpenMol)\n[![GitHub Forks](https://img.shields.io/github/forks/IDEA-XL/PRESTO?style=flat-square&logo=github)](https://github.com/IDEA-XL/PRESTO/network/members)\n[![GitHub Stars](https://img.shields.io/github/stars/IDEA-XL/PRESTO?style=flat-square&logo=github)](https://github.com/IDEA-XL/PRESTO/stargazers)\n[![GitHub Contributors](https://img.shields.io/github/contributors/IDEA-XL/PRESTO?style=flat-square&logo=github)](https://github.com/IDEA-XL/PRESTO/graphs/contributors)\n[![EMNLP Findings 2024](https://img.shields.io/badge/EMNLP%20Findings-2024-blue?style=flat-square&logo=researchgate)](https://aclanthology.org/2024.findings-emnlp.597/)\n\n**PRESTO** (Progressive Pretraining Enhances Synthetic Chemistry Outcomes) is a framework for pretraining and fine-tuning large language models (LLMs) for various tasks in synthetic chemistry.\n\n![Poster](assets/poster.png)\n\n## Release\n- [2024/11/14] ðŸ–ï¸ Presented our paper at EMNLP 2024, Miami. [[Poster](assets/poster.pptx)]\n- [2024/09/20] ðŸ”¥ Paper accepted by EMNLP 2024.\n- [2024/06/19] ðŸ”¥ We first release our code (including training and evaluation scripts).\n\n\n**Usage and License Notices**: The data, code and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna, LLaVA, Mol-Instructions. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n\n![PRESTO](assets/teaser.png)\n\n## Installation\n\n1. Install the required dependencies:\n   ```\n   conda create -n presto python=3.10\n   pip install -r requirements.txt\n   pip install -e .\n   ```\n\n2. Set up the necessary environment variables:\n   ```\n   export MOLECULE_2D_PATH=\"/path/to/MoleculeSTM/\"\n   export WANDB_API_KEY=\"your_wandb_api_key\"\n   ```\n\n## Pretraining\n\n### Stage 1: Molecule-Text Alignment\n\nTo perform Stage 1 pretraining for molecule-text alignment, run the following command:\n```bash\nbash scripts/pretrain_multi_molecule/stage1.sh\n```\n\nThis script will pre-train the model using the PubChem caption dataset and save the pretrained model checkpoints.\n\n### Stage 2: Domain Incremental Pretraining\n\nFor Stage 2 pretraining, there are several configurations available:\n\n- `stage2.sh`: Pretraining using interleaved molecule-text data from USPTO-Application.\n- `stage2_rxn_nc.sh`: Pretraining using interleaved reaction data and name conversion tasks (g2s, s(g)2i, s(g)2f).\n- `stage2_all.sh`: Pretraining using interleaved reaction data and all name conversion tasks (i2s, i2f).\n- `stage2_skip_align.sh`: Skipping Stage 1 and directly starting with Stage 2 pretraining, only training the projector.\n- `stage2_skip_align_fulltune.sh`: Skipping Stage 1 and directly starting with Stage 2 pretraining, finetuning the entire model.\n\nTo run a specific Stage 2 pretraining configuration, execute the corresponding script. For example:\n```bash\nbash scripts/pretrain_multi_molecule/stage2_rxn_nc.sh\n```\n\n## SFT (Stage 3) Downstream Tasks\n\nFor Stage 3 finetuning, we include finetuning scripts for various downstream tasks. Each task has its own directory under `scripts/build_dataset/` to build the dataset and `scripts/sft/` to run the finetuning. There are several configurations available:\n\n- `stage3_freezeLLM.sh`: Finetuning the projector with a frozen LLM on Stage 3 downstream tasks.\n- `stage3_lora.sh`: Finetuning the projector and applying LoRA to train the LLM on Stage 3 downstream tasks.\n- `stage3_rxn_nc.sh`: Finetuning the LLM (pretrained using `stage2_rxn_nc.sh`) on Stage 3 downstream tasks.\n- `stage3_skip_align_fulltune.sh`: Skipping Stage 1 and training with the full model on Stage 2 pretraining data and Stage 3 downstream tasks.\n- `stage3_skip_stage2.sh`: Skipping Stage 2 and training with the full model on Stage 1 pretraining data and Stage 3 downstream tasks.\n- `stage3_skip_stage12.sh`: Skipping Stage 1 and 2 and training with the full model on Stage 3 downstream tasks.\n- `stage3.sh`: Train with the full model on Stage 3 directly.\n\nTo run a specific Stage 3 finetuning configuration, execute the corresponding script. For example:\n```bash\nbash scripts/sft/sft_lora/stage3_rxn_nc.sh $EPOCH $MODEL_VERSION\n# $EPOCH: the epoch number to finetune the model (e.g., 3)\n# $MODEL_VERSION: the model version to finetune (e.g., SFT-ALL)\n```\n\n## Evaluation\n\nHere is a list of all the downstream tasks and the corresponding commands to run the evaluation:\n\n### Reaction Prediction\n#### Forward Prediction\n\nTo evaluate the forward reaction prediction task, use the following commands:\n\n```bash\n# For lora model\nbash scripts/evaluate/sft_lora/evaluate_forward_reaction_prediction.sh $EPOCH $MODEL_VERSION\n\n# For full model\nbash scripts/evaluate/sft_full/evaluate_forward_reaction_prediction.sh $EPOCH $MODEL_VERSION\n```\n\n#### Retrosynthesis Prediction\n\nTo evaluate the retrosynthesis prediction task, use the following command:\n```bash\n# For lora model\nbash scripts/evaluate/sft_lora/evaluate_retrosynthesis.sh $EPOCH $MODEL_VERSION\n\n# For full model\nbash scripts/evaluate/sft_full/evaluate_retrosynthesis.sh $EPOCH $MODEL_VERSION\n```\n\n### Reaction Condition Prediction\n#### Reagent Prediction\nTo evaluate the reagent prediction task, use the following commands:\n\n```bash\n# For lora model\nbash scripts/evaluate/sft_lora/evaluate_reagent_prediction.sh $EPOCH $MODEL_VERSION\n\n# For full model\nbash scripts/evaluate/sft_full/evaluate_reagent_prediction.sh $EPOCH $MODEL_VERSION\n```\n\n#### Catalyst Prediction\nTo evaluate the catalyst prediction task, use the following commands:\n\n```bash\n# For lora model\nbash scripts/evaluate/sft_lora/evaluate_catalyst_prediction.sh $EPOCH $MODEL_VERSION\n\n# For full model\nbash scripts/evaluate/sft_full/evaluate_catalyst_prediction.sh $EPOCH $MODEL_VERSION\n```\n\n#### Solvent Prediction\nTo evaluate the solvent prediction task, use the following commands:\n\n```bash\n# For lora model\nbash scripts/evaluate/sft_lora/evaluate_solvent_prediction.sh $EPOCH $MODEL_VERSION\n\n# For full model\nbash scripts/evaluate/sft_full/evaluate_solvent_prediction.sh $EPOCH $MODEL_VERSION\n```\n\n### Reaction Condition Recommendation  \n#### Reagent Selection\nTo evaluate the reagent selection task, use the following commands:\n\n```bash\n# For lora model\nbash scripts/evaluate/sft_lora/evaluate_reagent_selection.sh $EPOCH $MODEL_VERSION\n\n# For full model\nbash scripts/evaluate/sft_full/evaluate_reagent_selection.sh $EPOCH $MODEL_VERSION\n```\n\n### Reaction Type Classification\nTo evaluate the reaction type classification task, use the following commands:\n\n```bash\n# For lora model\nbash scripts/evaluate/sft_lora/evaluate_reaction_classification.sh $EPOCH $MODEL_VERSION\n\n# For full model\nbash scripts/evaluate/sft_full/evaluate_reaction_classification.sh $EPOCH $MODEL_VERSION\n```\n\n### Yield Prediction\nTo evaluate the yield prediction task, use the following commands:\n\n```bash\n# For lora model\nbash scripts/evaluate/sft_lora/evaluate_yields_regression.sh $EPOCH $MODEL_VERSION\n\n# For full model\nbash scripts/evaluate/sft_full/evaluate_yields_regression.sh $EPOCH $MODEL_VERSION\n```\n\n## Model Serving\n\nTo serve the trained model using a Flask server, run:\n\n```\npython scripts/serve_model.py --model_name_or_path <path_to_model> --model_lora_path <path_to_lora_model> --port <port_number>\n```\n\nThis will start a Flask server that exposes a `/generate` endpoint for generating predictions using the trained model.\n\n## Dataset Preparation\n\nThe `scripts/build_dataset` directory contains scripts for preparing datasets for different tasks. To prepare the datasets, follow the instructions within each task-specific directory.\n\n- NOTE: Huggingface Dataset under preparation. Once the dataset is ready, we will sync the readme.\n\n\n## License\n\nThis project is licensed under the Apache License 2.0. See the [LICENSE](LICENSE) file for more information.\n\n## Acknowledgments\n\nThis project builds upon the work of various open-source libraries and frameworks, and we would like to acknowledge their contributions.\n- [multi_token](https://github.com/sshh12/multi_token): We mostly built upon this implementation to support multi-token molecules.\n- [Hugging Face Transformers](https://github.com/huggingface/transformers)\n- [LLaVA](https://github.com/haotian-liu/LLaVA)\n- [VILA](https://github.com/Efficient-Large-Model/VILA)\n\n- We also thank the researchers and developers whose ideas and implementations have inspired and guided this project.\n\nFor more details and advanced usage, please refer to the documentation and source code.\n\n\n## Citation\nIf you find PRESTO useful for your research and applications, please cite using this BibTeX:\n```bibtex\n@inproceedings{cao-etal-2024-presto,\n    title     = {{PRESTO}: Progressive Pretraining Enhances Synthetic Chemistry Outcomes},\n    author    = {Cao, He and Shao, Yanjun and Liu, Zhiyuan and Liu, Zijing and Tang, Xiangru and Yao, Yuan and Li, Yu},\n    editor    = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},\n    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},\n    month     = nov,\n    year      = {2024},\n    address   = {Miami, Florida, USA},\n    publisher = {Association for Computational Linguistics},\n    url       = {https://aclanthology.org/2024.findings-emnlp.597},\n    pages     = {10197--10224}\n}\n\n```\n",
    "readme_length": 10107
  },
  {
    "name": "Active-Learning",
    "full_name": "Beckybams/Active-Learning",
    "description": "This project demonstrates Active Learning with synthetic data, showing how models improve by labeling only the most uncertain samples. Starting with few labeled points, the model queries new data iteratively, updates predictions, and enhances accuracy. ",
    "stars": 23,
    "forks": 0,
    "language": null,
    "url": "https://github.com/Beckybams/Active-Learning",
    "topics": [],
    "created_at": "2025-09-22T06:54:08Z",
    "updated_at": "2025-11-28T22:14:39Z",
    "homepage": "",
    "license": "N/A",
    "readme": "** Active Learning with Synthetic Data\n\nThis project demonstrates **Active Learning** using a synthetic 2D dataset for binary classification.  \nActive Learning allows a model to learn efficiently by selecting the most *informative* samples to label instead of labeling everything at once.\n\n---\n\n## ðŸ“‚ Files\n\n- **`synthetic_active_learning_data.xlsx`**  \n  Synthetic dataset with two features and a binary label.\n\n- **`active_learning_synthetic.py`**  \n  Python script that:\n  - Generates synthetic data  \n  - Initializes with a small labeled set  \n  - Iteratively queries the most uncertain unlabeled points  \n  - Retrains the model with the newly labeled data  \n  - Visualizes the decision boundary  \n  - Plots accuracy vs. number of labeled samples  \n\n---\n\n## âš™ï¸ Requirements\n\nInstall dependencies with:\n\n```bash\npip install numpy pandas scikit-learn matplotlib\nâ–¶ï¸ How to Run\nbash\nCopy code\npython active_learning_synthetic.py\nThe script will:\n\nPrint accuracy at each iteration\n\nShow evolving decision boundaries\n\nSave the dataset to Excel\n\nðŸ“Š Results\nThe model starts with only a few labeled points â†’ low accuracy\n\nAs it queries uncertain samples â†’ accuracy improves\n\nFinal output includes:\n\nVisualization of decision boundary at each step\n\nA learning curve of accuracy vs labeled set size\n\nðŸ”§ Customization\nYou can change:\n\nDataset parameters (n_samples, class_sep)\n\nActive learning settings (initial_samples, batch_size, n_queries)\n\nQuery strategies (uncertainty, margin, entropy)\n\nðŸ“– Reference\nBecky bbpere07@gmail.com\nScikit-learn Documentation\n\n",
    "readme_length": 1549
  },
  {
    "name": "mpradio",
    "full_name": "morrolinux/mpradio",
    "description": "Morrolinux's Pirate radio (PiFmRDS / PiFmAdv implementation with Bluetooth and mp3 support) - Stream music to your car's FM radio or use it as a Bluetooth speaker via headphone jack",
    "stars": 114,
    "forks": 17,
    "language": "C++",
    "url": "https://github.com/morrolinux/mpradio",
    "topics": [],
    "created_at": "2017-05-03T19:49:25Z",
    "updated_at": "2025-11-18T10:46:03Z",
    "homepage": "",
    "license": "GNU General Public License v3.0",
    "readme": "# mpradio\nMorrolinux's Pirate radio (PiFmRDS implementation with Bluetooth and mp3 support) for all Raspberry Pi models.\nExclusively tested on Minimal Raspbian (ARM).\n\n# Discontinued\nmpradio has been redesigned and wrote from scratch in python. We moved [here](https://github.com/morrolinux/mpradio-py)\n\n\n# Features\n- [x] Resume track from its playback status hh:mm:ss across reboots (CD-like expirience)\n- [x] Shuffle on/off\n- [x] Customizable scrolling RDS to overcome 8-chars limitation \n- [x] Skip to the next song by pressing a push-button (GPIO-connected on pin 18)\n- [x] Safely shutdown by holding the push-button (GPIO-connected on pin 18)\n- [x] Stream audio over FM or 3.5mm Jack (Bluetooth speaker via jack audio output)\n- [x] Send mp3 files or zip/rar albums to the Pi via Bluetooth\n- [x] Bluetooth OTA file management on the Pi with applications such as \"Bluetooth Explorer Lite\"\n- [x] Read metadata from the mp3 files \n- [x] Multiple file format support [mp3/wav/flac]\n- [x] Read Only mode for saving sdcard from corruption when unplugging AC\n- [x] PiFmAdv (optional)(experimental) implementation for better signal purity \n- [x] Control pipe commands during playback (explained below)\n- [x] Update just mpradio by sending mpradio-master.zip via Bluetooth (Update via App will be soon available)\n- [x] Bluetooth companion app for android (Work in progress...) \n- [ ] Display Android notifications over RDS?\n- [ ] Automatically partition the sdcard for a dedicated mp3 storage space (instead of using a USB drive)\n\n# Known issues\n- Due to a design flaw in BCM43438 WIFI/BT chipset, you might need to disable WiFi if you experience BT audio stuttering on Pi Zero W and Pi 3: https://github.com/raspberrypi/linux/issues/1402\n- Boot can take as long as 1m30s on the Pi 1 and 2 due to BT UART interface missing on the board.\n  Reducing systemd timeout with `echo \"DefaultTimeoutStartSec=40s\" >> /etc/systemd/system.conf` should help\n\n# Installation\nFirst make sure your Raspbian is up to date:\n\n` sudo apt-get update && sudo apt-get -y full-upgrade && sudo apt-get -y install git`\n\n` git clone https://github.com/morrolinux/mpradio.git mpradio-master `\n\n` cd mpradio-master/install && sudo ./install.sh`\n\nOR\n\nYou can just download and flash a full raspbian-mpradio.img image from [here.](https://www.mediafire.com/folder/qn62j675htott/mpradio) \nBe aware even the \"latest\" image could be \"not so latest\", but certainly will work. Just wait for it to reboot twice on first use.\n\n# Configuration\nBy default, `mpradio` will always be running automatically after boot once installed. No additional configuration is needed.\nHowever, you can change the FM streaming frequency (which is otherwise defaulted to 107.0) by placing a file named pirateradio.config in the root of a USB key (which of course, will need to stay plugged for the settings to be permanent)\n\npirateradio.config example:\n```\n[PIRATERADIO]\nfrequency=107.0\nbtGain=1.7            \t\t;gain setting for bluetooth streaming\nstorageGain=1         \t\t;gain setting for stored files streaming\noutput=fm\t\t\t;[fm/analog] for FM output or 3.5 mm jack output\nbtBoost=false\t\t        ;Enhance Bluetooth audio. This might add a little latency\nimplementation=pi_fm_rds\t;[pi_fm_rds/pi_fm_adv] - pi_fm_adv has a much cleaner sound\n\n[PLAYLIST]\npersistentPlaylist=true\t\t;do not play tracks twice unless all of them have already been played\nresumePlayback=true   \t\t;(resume track from where it was upon reboot) require persistentPlaylist to be enabled\nshuffle=true \nfileFormat=all          \t;which file formats to search for. [mp3/flac/wav/all]\n\n[RDS]\nupdateInterval=3      \t\t\t\t;seconds between RDS refresh. lower values could result in RDS being ignored by your radio receiver\ncharsJump=6           \t\t\t\t;how many characters should shift between updates [1-8]\nrdsPattern=$ARTIST_NAME - $SONG_NAME\t\t;Pattern which is passed to eval() to produce title EG: $SONG_YEAR - $ALBUM_NAME\n\n```\nOptional: Protect your SD card from corruption by setting Read-Only mode.\n\nuse utility/roswitch.sh as follows:\n\n`sudo bash roswitch.sh ro` to enable read-ony (effective from next boot)\n\n`sudo bash roswitch.sh rw` to disable read-only (effective immediately)\n\n# Usage\nIt (should) work out of the box. You need your mp3 files to be on a FAT32 USB stick (along with the `pirateradio.config` file if you want to override the default settings).\nYou can **safely** shut down the Pi by holding the push button or via App, and waiting for about 5 seconds until the status LED stops blinking.\nIf you enabled \"persistentPlaylist\" option, your Pi will never play the same song twice before consuming the full playlist.\nThis means if you add new songs on the USB stick, with \"persistentPlaylist\" enabled they won't be played until the current playlist is consumed. You can \"rebuild\" the playlist (looking for new recently added files) if needed:\n- Via App \n\nor\n\n- Boot your Pi with USB stick unplugged. (The current playlist will be erased.)\n- Safely shutdown your Pi \n- Power on your Pi once again, with the USB stick in it.\n- You're done! (`mpradio` will rebuild the playlist, indexing the new files as well)\n  \nAlso, please remember that (though it would be probably illegal) you can test FM broadcasting by plugging a 20cm wire on the **GPIO 4** of your Pi.\n\n## Control pipe\nYou can perform certain operations while `mpradio.service` is running by simply writing to `/home/pi/mpradio_ctl`\n\nExample:\n* Play a song on demand: `echo \"PLAY /absolute/path/to/song.mp3\" > mpradio_ctl`\n* Skip the current song:  `echo \"SKIP\" > mpradio_ctl`\n* Seek the track forward or backwards: `echo \"SEEK +10\" > mpradio_ctl`  or  `echo \"SEEK -10\" > mpradio_ctl`\n* Play all songs within a folder (via media scan): `echo \"SCAN /absolute/path/to/folder/\" > mpradio_ctl`\n\n## Bluetooth companion app \n\nI'll post the source code once it's mature enough, but you can test an alpha (0.2) version [here](http://www.mediafire.com/file/awu3r50z5gz3363/mpradio_remote-0.2.apk) \n\nNB: I haven't handled all corner conditions yet, so crashes may occour. (Make sure your Bluetooth is on and your Pi is paired, before even starting the app) \n\nScreenshots:\n\n![UI](/doc/app/UI.png \"UI\") ![UI](/doc/app/swipe_song.png \"Swipe Song\") ![UI](/doc/app/mpradio_settings.png \"mpradio_settings\")\n\n# Updating \n` cd mpradio-master/install && sudo ./install.sh update `\n\nOr, if you prefer to be explicit:\n\n`cd mpradio-master && git fetch origin && git reset --hard origin/master && cd install && sudo ./install.sh`\n\n# Uninstallation / Removal\nIn order to remove `mpradio` along with the packages that come with it:\n\n` cd mpradio-master/install && sudo ./install.sh remove `\n\nThis has the effect of removing dependency packages whether or not you still want them. If you would like to keep the packages that `mpradio` depends on, run the following instead:\n\n` cd mpradio-master/install && sudo ./install.sh uninstall `\n\n# Debugging / Troubleshooting\n## Services\n`mpradio` is launched as a service (via systemd) upon each boot.\n\nTo check whether the service is running or not: \n\n` $ sudo systemctl status mpradio `\n\nTo start or stop the service:\n\n` $ sudo systemctl [start/stop] mpradio `\n\n## Bluetooth\n\nBluetooth connection logs are found at ` /var/log/bluetooth_dev `.\n\nIf the Raspberry Pi is not showing up as a Bluetooth device, check whether the interface is UP, and that the `bt-setup` script is running:\n\n` $ hciconfig `\n\n` $ sudo systemctl status bt-setup `\n\nIf you are having issues with pairing Bluetooth for audio, please also check if `simple-agent` service is running:\n\n` $ sudo systemctl status simple-agent `\n\nIf you are having issues with Bluetooth not connecting once it's paired, please check whether `bluealsa` is running or not:\n\n` $ sudo systemctl status bluealsa `\n\nA simple schematic of how things work together:\n\n![MPRadio schematic](/doc/mpradio_schematic.png?raw=true \"mpradio schematic\")\n\n# Warning and Disclaimer\n`mpradio` relies on PiFmRds for FM-Streaming feature. Please note that in most states, transmitting radio waves without a state-issued licence specific to the transmission modalities (frequency, power, bandwidth, etc.) is illegal. Always use a shield between your radio receiver and the Raspberry. Never use an antenna. See PiFmRds Waring and Disclamer for more information.\n",
    "readme_length": 8283
  },
  {
    "name": "mpradio-py",
    "full_name": "morrolinux/mpradio-py",
    "description": "Morrolinux's Pirate radio (PiFmAdv wrapper with Bluetooth and mp3 support) - Stream music to your car's FM radio or use it as a Bluetooth speaker via headphone jack",
    "stars": 81,
    "forks": 11,
    "language": "Python",
    "url": "https://github.com/morrolinux/mpradio-py",
    "topics": [],
    "created_at": "2019-04-04T16:00:43Z",
    "updated_at": "2025-11-09T11:48:53Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# mpradio-py\nMorrolinux's Pirate radio (PiFmRDS implementation with Bluetooth and mp3 support) for all Raspberry Pi models\n\nWork in progress.\n\nThe old implementation deeply relies on external services and it's not very object oriented nor flexible to changes, resulting in it being inconsistent in the user expirience across multiple devices and configurations. This project aims for a total rewrite with some structural changes to make it more modular, and try to integrate dependencies as much as possible for a better management.\n\n# COMPATIBILITY NOTICE\n\nThis software is tested to work on Debian 10 and previous versions. Debian 11 and subsequent versions **won't work** due to massive breakage in core dependencies. Feel free to try and port it to the latest Debian if you wish, **or just use Debian 10** to avoid any issues. \n\n# Features\nExclusively tested on Minimal Raspbian (ARM)\n- [x] Resume track from its playback status hh:mm:ss across reboots (CD-like expirience)\n- [x] Shuffle on/off\n- [x] Display track info over RDS (for both bluetooth playback and music on local storage)\n- [x] Skip song by pressing a push-button (GPIO-connected on pin 5 [BCM 3]) even when playing bluetooth audio\n- [x] Safely power on/off by holding the push-button\n- [x] Stream audio over FM or 3.5mm Jack (As a Bluetooth speaker via jack audio output)\n- [ ] Send mp3 files or zip/rar albums to the Pi via Bluetooth\n- [ ] Bluetooth OTA file management on the Pi with applications such as \"Bluetooth Explorer Lite\"\n- [x] Read metadata from mp3 files\n- [x] Play local music in multiple formats [ogg/m4a/mp3/wav/flac]\n- [ ] Read Only mode for saving sdcard from corruption when unplugging AC\n- [x] PiFmAdv (default)(experimental) implementation for better signal purity \n- [x] Multiple remotes available (GPIO pushbutton / Bluetooth Android App / Control Pipe via shell)\n- [ ] Update just mpradio by sending mpradio-master.zip via Bluetooth (Update via App will be soon available)\n- [ ] Bluetooth companion app for android (Work in progress...) \n\n# Installation\n`git clone https://github.com/morrolinux/mpradio-py.git mpradio`\n\n`cd mpradio/install && sudo bash install.sh`\n\n# Configuration\nBy default, `mpradio` will always be running automatically after boot once installed. No additional configuration is needed.\nHowever, you can change the FM streaming frequency (which is otherwise defaulted to 88.0) by placing a file named pirateradio.config in the root of a USB key (which of course, will need to stay plugged for the settings to be permanent)\n\ndefault `pirateradio.config` here: https://github.com/morrolinux/mpradio-py/blob/master/install/pirateradio/pirateradio.config\n\n### Optional: Protect your SD card from corruption by setting Read-Only mode.\n\nuse utility/roswitch.sh as follows:\n\n`sudo bash roswitch.sh ro` to enable read-ony (effective from next boot)\n\n`sudo bash roswitch.sh rw` to disable read-only (effective immediately)\n\n\n# Known issues\n- Due to a design flaw in BCM43438 WIFI/BT chipset, you might need to disable WiFi if you experience BT audio stuttering on Pi Zero W and Pi 3: https://github.com/raspberrypi/linux/issues/1402 - you can switch onbloard WiFi on/off using `wifi-switch` command (even via Bluetooth link on the Android companion app typing in \"settings\" > \"command\" section) \n- Boot can take as long as 1m30s on the Pi 1 and 2 due to BT UART interface missing on the board.\n  Reducing systemd timeout with `echo \"DefaultTimeoutStartSec=40s\" >> /etc/systemd/system.conf` should help\n\n# Usage\nIt (should) work out of the box. You need your mp3 files to be on a FAT32 USB stick (along with the `pirateradio.config` file if you want to override the default settings).\nYou can **safely** shut down the Pi by holding the push button or via App, and waiting for about 5 seconds until the status LED stops blinking.\nIf you add new songs on the USB stick, they won't be played until the current playlist is consumed. You can \"rebuild\" the playlist (looking for new recently added files) if needed:\n- Via App \n\nor\n\n- Simply delete `playlist.json` and `library.json` from your USB stick when you add new songs to it.\n  \nAlso, please remember that (though it would be probably illegal) you can test FM broadcasting by plugging a 20cm wire on the **GPIO 4** of your Pi.\n\n## Control pipe\nYou can perform certain operations while `mpradio.service` is running by simply writing to `/tmp/mpradio_bt`\n\nExample:\n* Playback control:  `echo \"previous|next|resume|pause\" > mpradio_bt`\n* System commands: `echo \"poweroff|reboot\" > mpradio_bt`\n\n## Bluetooth companion app \n\nYou can find the source code [here](https://github.com/morrolinux/mpradio-remote) OR you can test an alpha build (v0.3) [here](https://www.mediafire.com/file/t1q0jfthrto8q33/mpradio-py_0.3.apk/file) \n\n### Here's how it works:\n\n1. Install the App\n2. Pair the Pi with your phone (via Android settings)\n3. Open the App\n\n### Notes\n* I haven't handled all corner conditions yet, so crashes may occour. \n* Make sure phone's Bluetooth is enabled and your Pi is paired before even starting the app, or it will just crash\n* Not all features have been implemented as of yet\n* You don't need your phone to be connected to the Pi when you start the app. Just paired is fine.\n\n# Contributing\n## Guidelines\nOne important requirement is for the program to be mostly testable on your developement machine instead of having to be copied to a Pi each time for testing. This speeds things up, from developing to testing and debugging. To acheive this, I've put platform checks within the code which should be run differently on a Pi rather than on a PC. If you happen to create logic which is supposed to be tested only on a Pi, please insert a platform check not to produce any execution errors on a PC.\n\n## Path\nIf you're testing on your computer, please `cd` to the `mpradio/src` folder and run `./mpradio.py`\n\n# Debugging / Troubleshooting\n## Services\n`mpradio` is launched as a service (via systemd) upon each boot.\n\nTo check whether the service is running or not: \n\n` $ sudo systemctl status mpradio `\n\nTo start or stop the service:\n\n` $ sudo systemctl [start/stop] mpradio `\n\n## Bluetooth\n\nBluetooth connection logs are found at ` /var/log/bluetooth_dev `.\n\nIf the Raspberry Pi is not showing up as a Bluetooth device, check whether the interface is UP, and that the `bt-setup` script is running:\n\n` $ hciconfig `\n\n` $ sudo systemctl status bt-setup `\n\nIf you are having issues with pairing Bluetooth for audio, please also check if `simple-agent` service is running:\n\n` $ sudo systemctl status simple-agent `\n\nIf you are having issues with Bluetooth not connecting once it's paired, please check whether `bluealsa` is running or not:\n\n` $ sudo systemctl status bluealsa `\n\nA simple schematic of how things work together:\n\n![MPRadio schematic](/doc/mpradio_schematic.png?raw=true \"mpradio schematic\")\n\nAnd classes:\n\n![MPRadio classes](/doc/mpradio_classess.png?raw=true \"mpradio classes\")\n\n# Warning and Disclaimer\n`mpradio` relies on PiFmAdv for FM-Streaming feature. Please note that in most states, transmitting radio waves without a state-issued licence specific to the transmission modalities (frequency, power, bandwidth, etc.) is illegal. Always use a shield between your radio receiver and the Raspberry. Never use an antenna. See PiFmAdv Waring and Disclamer for more information.\n",
    "readme_length": 7352
  },
  {
    "name": "MPRAflow",
    "full_name": "shendurelab/MPRAflow",
    "description": "A portable, flexible, parallelized tool for complete processing of massively parallel reporter assay data",
    "stars": 35,
    "forks": 16,
    "language": "Python",
    "url": "https://github.com/shendurelab/MPRAflow",
    "topics": [],
    "created_at": "2019-10-25T16:45:12Z",
    "updated_at": "2025-11-25T07:53:01Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "[![Nextflow](https://img.shields.io/badge/nextflow-%E2%89%A520.01-brightgreen.svg)](https://www.nextflow.io/)\n[![Documentation Status](https://readthedocs.org/projects/mpraflow/badge/?version=latest)](https://mpraflow.readthedocs.io/en/latest/?badge=latest)\n\n# MPRAflow\n\n<code style=\"color: red\"> **IMPORTANT NOTE: MPRAflow will not developed further! Please use [MPRAsnakeflow](https://github.com/kircherlab/MPRAsnakeflow) for a more advanced pipeline with lots of improvements and fixes.** </code>\n\n\n\nThis pipeline processes sequencing data from Massively Parallel Reporter Assays (MPRA) to create count tables for candidate sequences tested in the experiment.\n\n**Check out MPRAflow's details documentation [here](https://mpraflow.readthedocs.io/en/latest/index.html)**\n\n**NOTE:** MPRAflow cannot analyze STARR-seq data. Have a look at the [documentation](https://mpraflow.readthedocs.io/en/latest/index.html) to see some MPRA examples.\n\nThis package contains three utilities:\n\n## ASSOCIATION:\nThis utility takes in library association sequencing data (FASTQ) and a design file (FASTA) to assign barcodes to the corresponding elements tested. Functionality includes filtering for quality and coverage of barcodes. This utility must be run before the COUNT utility.\n\n## COUNT:\nThis utility processes sequence data (FASTQ) of barcodes from the DNA and RNA fractions of the MPRA experiment and outputs count tables labeled with the element tested and a label provided in the design file. This utility can process multiple replicates and conditions in a parallelized manner. Based on a user specified flag, the pipeline will either output normalized activity for each tested sequence, or will combine the results into a single count matrix compatible with MPRAnalyze.\n\n## Association Saturation mutagenesis:\nThis workflow is about assocation variant calls with barcodes. Variants are introduced by an error-prone PCR. The workflow takes the sequencing of the region, with barcodes in index read and the reference sequence and maps the reads to the reference, calls variants and associates them with the corresponding barcode.\n\n## Saturation mutagenesis:\nThis workflow is about getting single variant effect from a target with multiple mutations generated by error-prone PCR. The workflow takes counts (e.g. from the count workflow), combines them with an association file (variants to barcodes) and uses a generalized linear model to to detect single variant effects.\n\n## Installation\n\n### Required packages\n\n- conda\n\nDownload here: `https://docs.conda.io/en/latest/miniconda.html`\n\n### Clone repository\n\n```bash\ngit clone https://github.com/shendurelab/MPRAflow.git\n```\n\n### Set up conda environment:\nThis pipeline uses python2.7 and python3.6 and is set up to run on a Linux system. Two .yml files are provided to create the appropriate environments. The general environment with nextflow located in the home directory called `environment.yml` and a specific python 2.7 environment in the `conf` folder: `mpraflow_py27.yml`.\n\nThe different environments are handled internally by nextflow. Therefore your compute node, where you start MPRAflow, have to have access to the internet.\n\nInstall the the conda environment. The general conda environment is called `MPRAflow`.\n```bash\ncd MPRAflow\nconda env create -n MPRAflow -f environment.yml\n```\n\nIf you do not have access to the internet, you have to run the previous command on a node with internet. Afterwards you need to start nextflow too (see `Steps to run the pipeline`). After creation of the second conda environment by nextflow you can cancel it and start it on your internal node. Be aware that folders must have access on all nodes.\n\nNextflow has problems using conda 4.7 and highet, because the `source activate` command is replaced by `conda activate`. If you get error messages after running you can make a symbolik link of the `activate` command from you `bin` folder of the `conda` or `miniconda` folder to your `MPRAflow` environment `bin` folder. E.g. like:\n\n```bash\nln -s ~/miniconda3/bin/activate ~/miniconda3/envs/MPRAflow/bin/activate\n```\n\n## Running the pipeline\n\n#### Steps to run the pipeline\n\nThis pipeline comes with a `conf/cluster.config` file set up to run on HPC clusters, allowing each process to be run as a separate `qsub`, `sbatch` or similar command. The config contains example code for SGE, LSF, and SLURM architectures. The default is SGE.\nPlease remove the `\\\\` for the architecture you would like to use and place `\\\\` in front of any architectures not currently in use. A '\\\\' in front of all of them runs the pipeline on your local machine. If you run MPRAflow on a cluster system make sure be that you export all environment variables. E.g. this can be done with the `-V` option by SGE.\n\n**NOTE:** Please consult your cluster's wiki page for cluster specific commands and change `clusterOptions = ` to reflect these specifications. Additionally, for large libraries, more memory can be specified in this location.\n\nPlease use a submit script for steps 2 and 3. For full details of mandatory and optional arguments run:\n\n ```bash\n conda activate MPRAflow\n nextflow run count.nf --help\n nextflow run association.nf --help\n ```\n\nThis pipeline expects the FASTQ files to be demultiplexed and trimmed to only include sequence from the insert, barcodes, and/or UMIs.\n\n## Quick Start\n\n1. Create an 'experiment' csv in the format below, including the header. `DNA_R1` or `RNA_R1` is name of the gzipped fastq of the forward read of the DNA or RNA from the defined condition and replicate. `DNA_R2` or `RNA_R2` is the corresponding index read with UMIs (excluding sample barcodes) and `DNA_R3` or `RNA_R3` of the reverse read. If you do not have UMIs remove the columns `DNA_R2` and `RNA_R2` or leave them empty.\n\n   ```\n   Condition,Replicate,DNA_BC_F,DNA_UMI,DNA_BC_R,RNA_BC_F,RNA_UMI,RNA_BC_R\n   condition1,1,cond1_rep1_DNA_FWD_reads.fastq.gz,cond1_rep1_DNA_IDX_reads.fastq.gz,cond1_rep1_DNA_REV_reads.fastq.gz,cond1_rep1_RNA_FWD_reads.fastq.gz,cond1_rep1_RNA_IDX_reads.fastq.gz,cond1_rep1_RNA_REV_reads.fastq.gz\n   condition1,2,cond1_rep2_DNA_FWD_reads.fastq.gz,cond1_rep2_DNA_IDX_reads.fastq.gz,cond1_rep2_DNA_REV_reads.fastq.gz,cond1_rep2_RNA_FWD_reads.fastq.gz,cond1_rep2_RNA_IDX_reads.fastq.gz,cond1_rep2_RNA_REV_reads.fastq.gz\n   condition2,1,cond2_rep1_DNA_FWD_reads.fastq.gz,cond2_rep1_DNA_IDX_reads.fastq.gz,cond2_rep1_DNA_REV_reads.fastq.gz,cond2_rep1_RNA_FWD_reads.fastq.gz,cond2_rep1_RNA_IDX_reads.fastq.gz,cond2_rep1_RNA_REV_reads.fastq.gz\n   condition2,2,cond2_rep2_DNA_FWD_reads.fastq.gz,cond2_rep2_DNA_IDX_reads.fastq.gz,cond2_rep2_DNA_REV_reads.fastq.gz,cond2_rep2_RNA_FWD_reads.fastq.gz,cond2_rep2_RNA_IDX_reads.fastq.gz,cond2_rep2_RNA_REV_reads.fastq.gz\n   ```\n\n2. If you would like each insert to be colored based on different user-specified categories, such as \"positive control\", \"negative control\", \"shuffled control\", and \"putative enhancer\", to assess the overall quality the user can create a 'label' tsv in the format below that maps the name to category:\n\n   ```\n   insert1_name insert1_label\n   insert2_name insert2_label\n   ```\n   The insert names must exactly match the names in the design FASTA file.\n\n3. Run Association if using a design with randomly paired candidate sequences and barcodes\n\n   ```bash\n   conda activate MPRAflow\n   nextflow run association.nf --fastq-insert \"${fastq_prefix}_R1_001.fastq.gz\" --design \"ordered_candidate_sequences.fa\" --fastq-bc \"${fastq_prefix}_R2_001.fastq.gz\"\n   ```\n    **NOTE:** This will run in local mode, please submit this command to your cluster's queue if you would like to run a parallelized version.\n\n4. Run Count\n\n   ```bash\n   conda activate MPRAflow\n   nextflow run count.nf --dir \"bulk_FASTQ_directory\" --e \"experiment.csv\" --design \"ordered_candidate_sequences.fa\" --association \"dictionary_of_candidate_sequences_to_barcodes.p\"\n   ```\n   Be sure that the `experiment.csv` is correct. All fastq files must be in the same folder given by the `--dir` option. If you do not have UMIs please use the option `--no-umi`. Please specify your barcode length and umi-length with `--bc-length` and `--umi-length`.\n\n5. Run association saturation mutagenesis\n\n    ```bash\n    conda activate MPRAflow\n    nextflow run association_saturationMutagenesis.nf  --fastq-insert SRR8646911_1.fastq.gz --fastq-insertPE SRR8646911_2.fastq.gz --fastq-bc SRR8646911_3.fastq.gz  --design TERT.fa --name TERT --outdir out --bc-length 20\n    ```\n\n6. Run saturation mutagenesis\n\n    ```bash\n    conda activate MPRAflow\n    nextflow run saturationMutagenesis.nf --dir \"directory_of_DNA/RNA_counts\" --e \"satMutexperiment.csv\" --assignment \"yourSpecificAssignmentFile.variants.txt.gz\"\n    ```\n\n    **Note** The experiment file is different from the count workflow. It should contain the condition, replicate and filename of the counts, like:\n\n    ```\n    Condition,Replicate,COUNTS\n    condition1,1,cond1_1_counts.tsv.gz\n    condition1,2,cond1_2_counts.tsv.gz\n    condition1,3,cond1_3_counts.tsv.gz\n    condition2,1,cond2_1_counts.tsv.gz\n    condition2,2,cond2_2_counts.tsv.gz\n    condition2,3,cond2_3_counts.tsv.gz\n    ```\n\n    The count files can be generated by the count workflow, are named: `<condition>_<replicate>_counts.tsv.gz` and can be found in the `outs/<condition>/<replicate>` folder. They have to be copied or linked into the `--dir` folder.\n\n## Example files can be found in the example folder in this repository\n\n![MPRA_nextflow](https://github.com/shendurelab/MPRAflow/blob/master/MPRA_nextflow.png)\n",
    "readme_length": 9556
  },
  {
    "name": "mprashant-terraform-configs",
    "full_name": "paulgitrepo/mprashant-terraform-configs",
    "description": null,
    "stars": 25,
    "forks": 189,
    "language": "HCL",
    "url": "https://github.com/paulgitrepo/mprashant-terraform-configs",
    "topics": [],
    "created_at": "2024-07-09T19:38:58Z",
    "updated_at": "2025-11-02T05:24:59Z",
    "homepage": null,
    "license": "N/A",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "LucaProt",
    "full_name": "alibaba/LucaProt",
    "description": "LucaProt: A novel deep learning framework that incorporates protein amino acid sequence and structural information to predict protein function.",
    "stars": 203,
    "forks": 38,
    "language": "Python",
    "url": "https://github.com/alibaba/LucaProt",
    "topics": [
      "deep-learning",
      "dual-channels",
      "large-language-models",
      "lucaprot",
      "protein-language-models",
      "rdrp",
      "rna-virus",
      "transformer",
      "virus",
      "virus-identification"
    ],
    "created_at": "2023-04-12T03:12:47Z",
    "updated_at": "2025-11-27T16:53:21Z",
    "homepage": "https://www.cell.com/cell/fulltext/S0092-8674(24)01085-7",
    "license": "Apache License 2.0",
    "readme": "# LucaProt   \n**LucaProt(DeepProtFunc)** is an open source project developed by **Alibaba** and licensed under the **Apache License (Version 2.0)**.\n\nThis product contains various third-party components under other open source licenses.     \nSee the **NOTICE** file for more information.\n\n**Notice:**     \n**This project provides the Python dependency environment installation file, installation commands, and the running command of the trained LucaProt model for inference or prediction, which can be found in this repository. These models are compatible with Linux, Mac OS, and Windows systems, supporting both CPU and GPU configurations for inference tasks.**\n\n\n## TimeLine    \n* **2025-05-01**   \nAdd the deployable web application (based on flask, in `src/app/`)      \n  Start the service on the server,       \n  ```shell \n  cd LucaProt/src/app  \n  # modify the service port in app.py    \n  python app.py     \n  ```  \n  Then the service can be accessed using the browser on the client: **http://${server_ip}:8000**   \n\n* **2025-04-17:**    \nAdd the post-processing workflow to classify the viral RdRPs predicted by LucaProt into our 180 supergroups or novel supergroups.     \n(**Guidance listed in `PostProcessingWorkflow.md` or `PostProcessingWorkflow_zh.md`of this project**).       \n\n* **2024-09-24<img src=\"https://img.shields.io/badge/ðŸ”¥-orange\" alt=\"Hot Badge\" />:**    \nA free CPU version of `LucaProt Server` is available online (https://lucaprot.org).      \n\n* **2024-09-01**:       \nOptimize inference and prediction code to run on GPU with small graphics memory, such as `A10`.            \n\n## LucaProt Server<img src=\"https://img.shields.io/badge/ðŸ”¥-orange\" alt=\"Hot Badge\" />        \nLucaProt Server(CPU) is available at: https://lucaprot.org.     \nLimit inference to a maximum of 100 sequences at a time.     \nThe GPU version will come soon.    \n<center>\n<img src=\"pics/lucaprot_server.png\" alt=\"LucaProt Server\" width=\"50%\" height=\"50%\"/>\n\nLucaProt Server\n</center>\n\n# Introduction    \n**LucaProt**: A novel deep learning framework that incorporates protein amino acid sequence and structural information to predict protein function.\n\n# 1. Model    \n\n## 1) Model Introduction     \n\nWe developed a new deep learning model, namely, Deep Sequential and Structural Information Fusion Network for Proteins Function Prediction (DeepProtFunc/LucaProt), which takes into account protein sequence and structural information to facilitate the accurate annotation of protein function.\n\nHere, we applied LucaProt to identify viral RdRP.      \n\n## 2) Model Architecture    \n\nWe treat protein function prediction as a classification problem. For example, viral RdRP identification is a binary-class classification task, and protein general function annotation is a multi-label classification task. The model includes five modules: Input, Tokenizer, Encoder, Pooling, and Output. Its architecture is shown in Figure 1.\n\n\n<center>\n<img src=\"pics/LucaProt.png\"/>\n\nFigure 1 The Architecture of LucaProt\n</center>\n\n## 3) Model Input/Output     \n\nUse the amino acid letter sequence as the input of our model. The model outputs the function label of the input protein, which is a single tag (binary-class classification or multi-class classification) or a set of tags (multi-label classification).\n\n\n# 2. Dependence       \nSystem: Ubuntu 20.04.5 LTS       \nPython: 3.9.13          \nDownload anaconda: <a href=\"https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh\" title=\"anaconda\"> anaconda </a>       \nCuda: <a href=\"https://developer.nvidia.com/cuda-11-7-0-download-archive\" title=\"cuda11.7 (torch==1.13.1)\"> cuda11.7 (torch==1.13.1)</a>\n```shell\n# Select 'YES' during installation for initializing the conda environment  \nsh Anaconda3-2022.10-Linux-x86_64.sh  \n# Source the environment\nsource ~/.bashrc  \n# Verification\nconda  \n# Install env and python 3.9.13   \nconda create -n lucaprot python=3.9.13    \n# activate env\nconda activate lucaprot  \n# Install git      \nsudo apt-get update         \nsudo apt install git-all\n\n# Enter the project   \ncd LucaProt     \n\n# Install\npip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple        \n```\n\n# 3. Inference          \nYou can simply use this project to infer or predict for unknown sequences.    \n\n## 1) Prediction from one sequence        \n\n```\ncd LucaProt/src/prediction/ \nsh run_predict_one_sample.sh\n```\n\n**Note:** the embedding matrix of the sample is real-time predictive.\n\n**Or:**\n\n```\ncd LucaProt/src/\n\n# using GPU(cuda=0)    \nexport CUDA_VISIBLE_DEVICES=\"0,1,2,3\"\npython predict_one_sample.py \\\n    --protein_id protein_1 \\\n    --sequence MTTSTAFTGKTLMITGGTGSFGNTVLKHFVHTDLAEIRIFSRDEKKQDDMRHRLQEKSPELADKVRFFIGDVRNLQSVRDAMHGVDYIFHAAALKQVPSCEFFPMEAVRTNVLGTDNVLHAAIDEGVDRVVCLSTDKAAYPINAMGKSKAMMESIIYANARNGAGRTTICCTRYGNVMCSRGSVIPLFIDRIRKGEPLTVTDPNMTRFLMNLDEAVDLVQFAFEHANPGDLFIQKAPASTIGDLAEAVQEVFGRVGTQVIGTRHGEKLYETLMTCEERLRAEDMGDYFRVACDSRDLNYDKFVVNGEVTTMADEAYTSHNTSRLDVAGTVEKIKTAEYVQLALEGREYEAVQ\t\\\n    --emb_dir ./emb/ \\\n    --truncation_seq_length 4096 \\\n    --dataset_name rdrp_40_extend \\\n    --dataset_type protein \\\n    --task_type binary_class \\\n    --model_type sefn \\\n    --time_str 20230201140320 \\\n    --step 100000 \\\n    --threshold 0.5 \\\n    --gpu_id 0\n  \n# using CPU(gpu_id=-1)    \npython predict_one_sample.py \\\n    --protein_id protein_1 \\\n    --sequence MTTSTAFTGKTLMITGGTGSFGNTVLKHFVHTDLAEIRIFSRDEKKQDDMRHRLQEKSPELADKVRFFIGDVRNLQSVRDAMHGVDYIFHAAALKQVPSCEFFPMEAVRTNVLGTDNVLHAAIDEGVDRVVCLSTDKAAYPINAMGKSKAMMESIIYANARNGAGRTTICCTRYGNVMCSRGSVIPLFIDRIRKGEPLTVTDPNMTRFLMNLDEAVDLVQFAFEHANPGDLFIQKAPASTIGDLAEAVQEVFGRVGTQVIGTRHGEKLYETLMTCEERLRAEDMGDYFRVACDSRDLNYDKFVVNGEVTTMADEAYTSHNTSRLDVAGTVEKIKTAEYVQLALEGREYEAVQ\t\\\n    --emb_dir ./emb/ \\\n    --truncation_seq_length 4096 \\\n    --dataset_name rdrp_40_extend \\\n    --dataset_type protein \\\n    --task_type binary_class \\\n    --model_type sefn \\\n    --time_str 20230201140320 \\\n    --step 100000 \\\n    --threshold 0.5 \\\n    --gpu_id -1\n```\n\n\n* **--protein_id**    \n  str, the protein id.   \n\n* **--sequence**    \n  str, the protein sequence.   \n\n* **--truncation_seq_length**    \n  int, truncate sequences longer than the given value. Recommended values: 4096, 2048, 1984, 1792, 1534, 1280, 1152, 1024, default: 4096.    \n\n* **--emb_dir(optional)**        \n  path, the saved dirpath of the protein predicted embedding matrix or vector during prediction, optional.   \n\n* **--dataset_name**        \n  str, the dataset name for building of our trained model(**rdrp_40_extend**).   \n\n* **--dataset_type**       \n  str, the dataset type for building of our trained model(**protein**).   \n\n* **--task_type**      \n  str, the task type for building of our trained model(**binary_class**).   \n\n* **--model_type**      \n  str, the model name for building of our trained model(**sefn**).     \n\n* **--time_str**    \n  str, the running time string(yyyymmddHimiss) for building of our trained model(**20230201140320**).    \n\n* **--step**      \n  int, the training global step of model finalization(**100000**).  \n\n* **--threshold**    \n  float, sigmoid threshold for binary-class or multi-label classification, None for multi-class classification, default: **0.5**.   \n\n* **--gpu_id**:\n  int, the gpu id to use(**-1 for cpu**).         \n\n* **--torch_hub_dir(optional)**:    \n  str, the torch hub dir path for saving pretrained model(default: `~/.cache/torch/hub/`)\n\n\n\n## 2) Prediction from many sequences        \nthe samples are in *.fasta, sample by sample prediction.     \n\n* **--fasta_file**    \n  str, the samples fasta file.     \n\n* **--save_file**    \n  str, file path, save the predicted results into the file.    \n\n* **--print_per_number**   \n  int, print progress information for every number of samples completed, default: **100**.       \n\n\n```shell\ncd LucaProt/src/prediction/   \nsh run_predict_many_samples.sh\n```\n\n**Or:**\n\n```shell\ncd LucaProt/src/\n\n# using GPU(cuda=0)   \nexport CUDA_VISIBLE_DEVICES=\"0,1,2,3\"  \npython predict_many_samples.py \\\n\t--fasta_file ../data/rdrp/test/test.fasta  \\\n\t--save_file ../result/rdrp/test/test_result.csv  \\\n\t--emb_dir ../emb/   \\\n\t--truncation_seq_length 4096  \\\n\t--dataset_name rdrp_40_extend  \\\n\t--dataset_type protein     \\\n\t--task_type binary_class     \\\n\t--model_type sefn     \\\n\t--time_str 20230201140320   \\\n\t--step 100000  \\\n\t--threshold 0.5 \\\n\t--print_per_number 10 \\\n\t--gpu_id 0\n\t\n\n# using CPU(gpu_id=-1)               \npython predict_many_samples.py \\\n\t--fasta_file ../data/rdrp/test/test.fasta  \\\n\t--save_file ../result/rdrp/test/test_result.csv  \\\n\t--emb_dir ../emb/   \\\n\t--truncation_seq_length 4096  \\\n\t--dataset_name rdrp_40_extend  \\\n\t--dataset_type protein     \\\n\t--task_type binary_class     \\\n\t--model_type sefn     \\\n\t--time_str 20230201140320   \\\n\t--step 100000  \\\n\t--threshold 0.5 \\\n\t--print_per_number 10 \\\n\t--gpu_id -1\n```\n\n\n## 3) Prediction from the file(embedding file exists in advance)       \n\nThe test data (small and real) is in [demo.csv](./data/rdrp/demo/demo.csv), where the 7th column of each line is the filename of the structural embedding information prepared in advance.   \nAnd the structural embedding files store in [embs](./data/rdrp/demo/embs).\n\nThe test data includes 50 viral-RdRPs and 50 non-viral RdRPs.\n\n```\ncd LucaProt/src/prediction/   \nsh run_predict_from_file.sh\n```\n\n**Or:**\n\n```\ncd LucaProt/src/\n\n# using GPU(cuda=0)   \nexport CUDA_VISIBLE_DEVICES=\"0,1,2,3\"\npython predict.py \\\n    --data_path ../data/rdrp/demo/demo.csv \\\n    --emb_dir ../data/rdrp/demo/embs/esm2_t36_3B_UR50D \\\n    --dataset_name rdrp_40_extend \\\n    --dataset_type protein \\\n    --task_type binary_class \\\n    --model_type sefn \\\n    --time_str 20230201140320 \\\n    --step 100000 \\\n    --evaluate \\\n    --threshold 0.5 \\\n    --batch_size 16 \\\n    --print_per_batch 100 \\\n    --gpu_id 0 \n    \n# using CPU(gpu_id=-1)          \npython predict.py \\\n    --data_path ../data/rdrp/demo/demo.csv \\\n    --emb_dir ../data/rdrp/demo/embs/esm2_t36_3B_UR50D \\\n    --dataset_name rdrp_40_extend \\\n    --dataset_type protein \\\n    --task_type binary_class \\\n    --model_type sefn \\\n    --time_str 20230201140320 \\\n    --step 100000 \\\n    --evaluate \\\n    --threshold 0.5 \\\n    --batch_size 16 \\\n    --print_per_batch 100 \\\n    --gpu_id -1\n```\n\n* **--data_path**    \n  path, the file path of prediction data, including 9 columns mentioned above. The value of Column Label can be null.\n\n* **--emb_dir**    \n  path, the saved dirpath of all sample's structural embedding information prepared in advance.\n\n* **--dataset_name**       \n  str, the dataset name for building of our trained model(**rdrp_40_extend**).\n\n* **--dataset_type**       \n  str, the dataset type for building of our trained model(**protein**).\n\n* **--task_type**      \n  str, the task name for building of our trained model(**binary_class**).\n\n* **--model_type**      \n  str, the model name for building of our trained model(**sefn**).\n\n* **--time_str**    \n  str, the running time string(yyyymmddHimiss) for building of our trained model(**20230201140320**).\n\n* **--step**      \n  int, the training global step of model finalization(**100000**).\n\n* **--threshold**   \n  float, sigmoid threshold for binary-class or multi-label classification, None for multi-class classification, default: 0.5.\n\n* **--evaluate(optional)**     \n  store_true, whether to evaluate the predicted results.\n\n* **--ground_truth_col_index(optional)**    \n  int, the ground truth col index of the ${data_path}, default: None.\n\n* **--batch size**     \n  int, batch size per GPU/CPU for evaluation, default: 16.\n\n* **--print_per_batch**      \n  int,  how many batches are completed every time for printing progress information, default: 1000.\n\n* **--gpu_id**:\n  int, the gpu id to use(**-1 for cpu**).     \n\n* **--torch_hub_dir(optional)**:    \n  str, the torch hub dir path for saving pretrained model(default: `~/.cache/torch/hub/`)  \n\n**Note:** the embedding matrices of all the proteins in this file need to prepare in advance(**$emb_dir**).\n\n# 4. 11 independent validation datasets   \n11 verification datasets unrelated to the model building dataset, include 7 exists viral-RdRP datasets and 4 exists non viral-RdRP datasets.   \nRun the prediction python script  `https://github.com/alibaba/LucaProt/src/predict_many_samples.py`    \nThe performance on these 11 independent verification datasets of LucaProt.      \n[LucaProt-Performance-On-11-Independent-Datasets.xlsx](http://47.93.21.181/LucaProt/LucaProt-Performance-On-11-Independent-Datasets.xlsx)     \nor [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)   \n\n# 5. LucaProt App   \nThis project is used to predict unlabeled protein sequences and to measure the time spent.   \n[LucaProtApp](http://47.93.21.181/Benchmark/LucaProtApp/) or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)   \n\n# 6. Inference Time\n**LucaProt** is suitably speedy because it only needs to predict the **structural representation matrix** rather than the complete **3D structure** of the protein sequence.\n\n**Benchmark:** For each sequence length range(total 10 groups), selected **50** viral-RdRPS and **50** non-viral RdRPs for each group for inference time cost calculation.    \n[inference_time_data_of_github.csv](http://47.93.21.181/Benchmark/LucaProtApp/data/inference_time/inference_time_data_of_github.csv)    \nor [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)\n\n**Note:**  The spend time includes the time of the **structural representation matrix** inference, excludes the time of model loading.\n\n## 1) GPU(Nvidia A100, Cuda: 11.7)  \n\n**Notice**: when the sequence length does not exceed 1024, you can use the 24GB GPU for inference, such as the A10.        \n\n|  Protein Seq Len Range  |  Average Time  |  Maximum Time  |  Minimum Time  |\n|:-----------------------:|:--------------:|:--------------:|:--------------:|\n|    300 <= Len < 500     |     0.20s      |     0.24s      |     0.16s      |\n|    500 <= Len < 800     |     0.30s      |     0.39s      |     0.24s      | \n|   800 <= Len < 1,000    |     0.42s      |     0.46s      |     0.39s      |\n|  1,000 <= Len < 1,500   |     0.59s      |     0.74s      |     0.45s      | \n|  1,500 <= Len < 2,000   |     0.87s      |     1.02s      |     0.73s      |   \n|  2,000 <= Len < 3,000   |     1.31s      |     1.69s      |     1.01s      |   \n|  3,000 <= Len < 5,000   |     2.14s      |     2.78s      |     1.72s      |\n|  5,000 <= Len < 8,000   |     3.03s      |     3.45s      |     2.65s      |\n|  8,000 <= Len < 10,000  |     3.77s      |     4.24s      |     3.32s      |\n|      10,000 <= Len      |     9.92s      |     17.66s     |     4.30s      | \n\n\n## 2) CPU (16 cores, 64G memory of Alibaba Cloud ECS)\n\n|  Protein Seq Len Range  | Average Time | Maximum Time | Minimum Time |\n|:-----------------------:|:------------:|:------------:|:------------:|\n|    300 <= Len < 500     |    3.97s     |    5.71s     |    2.77s     |\n|    500 <= Len < 800     |    5.78s     |    7.50s     |    4.48s     | \n|   800 <= Len < 1,000    |    8.23s     |    9.41s     |    7.41s     |\n|  1,000 <= Len < 1,500   |    11.49s    |    16.42s    |    9.22s     | \n|  1,500 <= Len < 2,000   |    17.71s    |    22.36s    |    14.93s    |   \n|  2,000 <= Len < 3,000   |    26.97s    |    36.68s    |    20.99s    |   \n|  3,000 <= Len < 5,000   |    45.56s    |    58.42s    |    35.82s    |\n|  5,000 <= Len < 8,000   |    56.57s    |    58.17s    |    55.55s    |\n|  8,000 <= Len < 10,000  |    57.76s    |    58.86s    |    56.66s    |\n|      10,000 <= Len      |    66.49s    |    76.80s    |    58.42s    |\n\n\n## 3) CPU (96 cores, 768G memory of Alibaba Cloud ECS)\n\n|  Protein Seq Len Range  | Average Time | Maximum Time | Minimum Time |\n|:-----------------------:|:------------:|:------------:|:------------:|\n|    300 <= Len < 500     |    1.89s     |    2.55s     |    1.10s     |\n|    500 <= Len < 800     |    2.68s     |    3.44s     |    2.13s     | \n|   800 <= Len < 1,000    |    3.45s     |    4.25s     |    2.65s     |\n|  1,000 <= Len < 1,500   |    4.27s     |    5.90s     |    3.54s     | \n|  1,500 <= Len < 2,000   |    5.81s     |    7.44s     |    4.76s     |   \n|  2,000 <= Len < 3,000   |    8.14s     |    10.74s    |    6.37s     |   \n|  3,000 <= Len < 5,000   |    13.25s    |    17.69s    |    10.06s    |\n|  5,000 <= Len < 8,000   |    17.03s    |    18.20s    |    15.98s    |\n|  8,000 <= Len < 10,000  |    17.90s    |    18.99s    |    16.92s    |\n|      10,000 <= Len      |    25.90s    |    35.02s    |    18.66s    | \n\n\n# 7. Dataset for Virus RdRP\n\n## 1) Fasta\n\n* **Viral RdRP(Positive: 5,979)**\n\n  The positive sequence fasta file is in `data/rdrp/all_dataset_positive.fasta.zip`                               \n  [all_dataset_positive.fasta.zip](http://47.93.21.181/LucaProt/data/rdrp/all_dataset_positive.fasta.zip)   \n  or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)     \n\n\n* **Non-viral RdRP(Negative: 229434)**\n\n  The negative sequence  fasta file is in `dataset/rdrp/all_dataset_negative.fasta.zip`        \n  including:\n\n  * other proteins of the virus\n  * other protein domains of the virus\n  * non-viral proteins\n\n  [all_dataset_negative.fasta.zip](http://47.93.21.181/LucaProt/data/rdrp/all_dataset_negative.fasta.zip)   \n  or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)     \n\n## 2) Structural embedding(matrix and vector)\nAll structural embedding files of the dataset for model building are available at: [embs](http://47.93.21.181/LucaProt/dataset_for_model_building/dataset/rdrp_40_extend/protein/binary_class/)         \nAll structural embedding files of the prediction data for opening are in the process(because of the amount of data).\n\n## 3) PDB (3D Structure)\nAll 3D-structure PDB files of the model building dataset and predicted data for opening are in the process (because of the amount of data).\n\n## 4) Vocab\n* **structure vocab**      \n  This vocab file is `struct_vocab/rdrp_40_extend/protein/binary_class/struct_vocab.txt`                        \n  [struct_vocab.txt](struct_vocab/rdrp_40_extend/protein/binary_class/struct_vocab.txt)  \n\n\n\n* **subword-level vocab**       \n  The size of the vocab of sequence we use is 20,000.            \n  This vocab file is `vocab/rdrp_40_extend/protein/binary_class/subword_vocab_20000.txt`                  \n  [subword_vocab_20000.txt](vocab/rdrp_40_extend/protein/binary_class/subword_vocab_20000.txt)\n\n\n* **char-level vocab**         \n  This vocab file is `vocab/rdrp_40_extend/protein/binary_class/vocab.txt`                      \n  [vocab.txt](vocab/rdrp_40_extend/protein/binary_class/vocab.txt)\n\n## 5) Label\nViral RdRP identification is a binary-class classification task, including positive and negative classes, using 0 and 1 to represent a negative and positive sample, respectively.\nThe label list file is `dataset/rdrp_40_extend/protein/binary_class/label.txt`                        \n[label.txt](dataset/rdrp_40_extend/protein/binary_class/label.txt)\n\n## 6) Dataset\nWe constructed a data set with 235,413 samples for model building, which included 5,979 positive samples of known viral RdRPs (i.e. the well-curated RdRP database described in the previous section of Methods), and 229,434 (to maintain a 1:40 ratio for viral RdRP and non-virus RdRPs) negative samples of confirmed non-virus RdRPs. And the non-virus RdRPs contained proteins from Eukaryota DNA dependent RNA polymerase (Eu DdRP, N=1,184), Eukaryota RNA dependent RNA polymerase (Eu RdRP, N=2,233), Reverse Transcriptase (RT, N=48,490), proteins obtained from DNA viruses (N=1,533), non-RdRP proteins obtained from RNA viruses (N=1,574), and a wide array of cellular proteins from different functional categories (N=174,420). We randomly divided the dataset into training, validation, and testing sets with a ratio of 8.5:1:1, which were used for model fitting, model finalization (based on the best F1-score training iteration), and performance reporting (including accuracy, precision, recall, F1-score, and Area under the ROC Curve (AUC)), respectively.\n\n* **Entire Dataset**     \n  This file is `dataset/rdrp/all_dataset_with_pdb_emb.csv.zip`                                         \n  [all_dataset_with_pdb_emb.csv.zip](http://47.93.21.181/LucaProt/data/rdrp/all_dataset_with_pdb_emb.csv.zip)       \n  or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)       \n\n\n* **Training set**        \n  This file copy to `dataset/rdrp_40_extend/protein/binary_class/train_with_pdb_emb.csv`                 \n  [train_with_pdb_emb.csv](http://47.93.21.181/LucaProt/dataset_for_model_building/dataset/rdrp_40_extend/protein/binary_class/train_with_pdb_emb.csv)     \n  or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)    \n\n\n* **Validation set**         \n  This file copy to `dataset/rdrp_40_extend/protein/binary_class/dev_with_pdb_emb.csv`                         \n  [dev_with_pdb_emb.csv](http://47.93.21.181/LucaProt/dataset_for_model_building/dataset/rdrp_40_extend/protein/binary_class/dev_with_pdb_emb.csv)    \n  or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)   \n\n\n* **Testing set**         \n  This file copy to `dataset/rdrp_40_extend/protein/binary_class/test_with_pdb_emb.csv`                          \n  [test_with_pdb_emb.csv](http://47.93.21.181/LucaProt/dataset_for_model_building/dataset/rdrp_40_extend/protein/binary_class/test_with_pdb_emb.csv)     \n  or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)       \n\nOne row in all the above files represents one sample. All three files consist of 9 columns, including **prot_id**, **seq**, **seq_len**, **pdb_filename**, **ptm**, **mean_plddt**, **emb_filename**, **label**, and **source**. The details of these columns are as follows:\n\n* **prot_id**        \n  the protein id\n* **seq**         \n  the amino acid(aa) sequence\n* **seq_len**        \n  the length of the protein sequence.\n* **pdb_filename**        \n  The PDB filenames of 3D-structure are predicted by the calculation model or obtained by experiments.\n* **ptm**   \n  the pTM of the predicted 3D-structure.\n* **mean_plddt**   \n  the mean pLDDT of the predicted 3D-structure.\n* **emb_filename**     \n  The filename of the embedding matrix or vector of protein structure.      \n  **Note:** the embedding matrics of the dataset need to prepare in advance.\n* **label**    \n  the sample label, 0 or 1 for binary-class classification, [0, 1, ..., N-1] for multi-class classification, a list of [0, 1, ..., N-1] for multi-label classification.\n* **source**    \n  optional, the sample source (such as RdRP, RT, DdRP, non-virus RdRP, and Other).\n\n**Note:** if using strategy one in structure encoder, the pdb_filename, the ptm, and the mean_plddt can be null.\n\n# 8. Supported Task Types\nYou can use this project to train models for other tasks, not just the viral RdRP identification tasks.     \n\n* **binary-class classification**    \n  The label is 0 or 1 for binary-class classification, such as viral RdRP identification.\n\n\n* **multi-class classification**      \n  The label is 0~N-1 for multi-class classification, such as the species prediction for proteins.\n\n\n* **multi-label classification**   \n  The labels form a list of 0~N-1 for multi-label classification, such as Gene Ontology annotation for proteins.\n\n# 9. Building Your Model(for re-training or training with new datasets)    \n\n## 1) Prediction of protein 3D-structure(Optional)\n\nThe script ```structure_from_esm_v1.py``` is in the directory \"src/protein_structure\", and it use ESMFold (esmfold_v1) to predict 3D-Structure of protein.\n\n### I. Prediction from file\n\n```\ncd LucaProt/src/protein_structure/     \n\nexport CUDA_VISIBLE_DEVICES=0\n\npython structure_from_esm_v1.py \\\n    -i data/rdrp/rdrp.fasta \\\n    -o pdbs/rdrp/ \\\n    --num-recycles 4 \\\n    --truncation_seq_length 4096 \\\n    --chunk-size 64 \\\n    --cpu-offload \\\n    --batch_size 1\n```\n\n**Parameters:**\n* **-i (input filepaths)**\n  * fasta filepath\n  * csv filepath    \n    the first row is the header        \n    column 0: protein_id         \n    column 1: sequence\n  * mutil filepaths    \n    comma-concatenation\n\n* **-o (save dirpath)**     \n  The dir path of saving the predicted 3D-structure data, each protein is stored in a PDB file, and each PDB file is named as \"protein_\" + an auto-increment id + \".pdb\", such as \"protein_1.pdb\".    \n  The mapping between protein ids and auto-increment ids is stored in the file \"result_info.csv\" (including: \"index\", \"protein_id(uuid)\", \"seq_len\", \"ptm\", \"mean_plddt\") in this dir path.   \n  For failed samples(CUDA out of memory), this script will save their protein ids in the \"uncompleted.txt\", and you can reduce the value of \"truncation_seq_length\" and add \"--try_failure\" for retry.\n\n* **--batch_size**     \n  the batch size of running, default: 1.\n\n* **--truncation_seq_length**      \n  truncate sequences longer than the given value, recommended values: 4096, 2048, 1984, 1792, 1536, 1280, 1152, 1022.\n\n* **--num-recycles**     \n  number of recycles to run.\n\n* **--chunk-size**          \n  chunks axial attention computation to reduce memory usage from O(L^2) to O(L), recommended values: 128, 64, 32.\n\n* **--try_failure**         \n  retry the failed samples when reducing the \"truncation_seq_length\" value.\n\n### II. Prediction from input sequences\n\n```\ncd LucaProt/src/protein_structure/    \n\nexport CUDA_VISIBLE_DEVICES=0\n\npython structure_from_esm_v1.py \\\n    -name protein_id1,protein_id2  \\\n    -seq VGGLFDYYSVPIMT,LPDSWENKLLTDLILFAGSFVGSDTCGKLF \\\n    -o pdbs/rdrp/  \\\n    --num-recycles 4 \\\n    --truncation_seq_length 4096 \\\n    --chunk-size 64 \\\n    --cpu-offload \\\n    --batch_size 1\n```             \n\n**Parameters:**\n* **-name**   \n  protein ids, comma-concatenation for multi proteins.\n* **-seq**     \n  protein sequences, comma-concatenation for multi proteins.\n\n\n## 2) Prediction of protein structural embedding\nThe script ```embedding_from_esmfold.py``` is in \"src/protein_structure\", and it use ESMFold (esm2_t36_3B_UR50D) to predict protein structural embedding matrices or vectors.\n\n### I. Prediction from file\n\n```\ncd LucaProt/src/protein_structure/    \n\nexport CUDA_VISIBLE_DEVICES=0  \n\npython embedding_from_esmfold.py \\\n    --model_name esm2_t36_3B_UR50D \\\n    --file data/rdrp.fasta \\\n    --output_dir emb/rdrp/ \\\n    --include per_tok contacts bos \\\n    --truncation_seq_length 4094 \n```   \n\n**Parameters:**\n\n* **--model_name**  \n  the model name, default: \"esm2_t36_3B_UR50D\"\n\n* **-i/--file (input filepath)**\n  * fasta filepath\n  * csv filepath     \n    the first row is the header      \n    column 0: protein_id      \n    column 1: sequence\n* **-o/--output_dir (save dirpath)**   \n  The dir path of saving the predicted structural embedding data, each protein is stored in a pickle file, and each embedding file is named as \"embedding_\" + auto-increment id + \".pt\", such as \"embedding_1.pt\".       \n  The mapping between protein ids and auto-increment ids is stored in the file \"{}_embed_fasta_id_2_idx.csv\"(including: \"index\", \"protein_id(uuid)\") in this dir path.      \n  For failed samples(CUDA out of memory), this script will save their protein ids in the \"{}_embed_uncompleted.txt\", and you can reduce the \"truncation_seq_length\" value and add \"--try_failure\" for retry.\n\n* **--truncation_seq_length**    \n  truncate sequences longer than the given value. Recommended values: 4094, 2046, 1982, 1790, 1534, 1278, 1150, 1022.\n\n* **--include**     \n  The embedding matrix or vector type of the predicted structural embedding data, including per_tok, mean, contacts, and bos.\n\n  * per_tok includes the full sequence, with an embedding per amino acid (seq_len x hidden_dim).\n  * mean includes the embeddings averaged over the full sequence, per layer.\n  * bos includes the embeddings from the beginning-of-sequence token.\n  * contacts includes the attention value between two amino acids of the the full sequence.\n\n  Referenceï¼š[https://github.com/facebookresearch/esm](https://github.com/facebookresearch/esm) [Compute embeddings in bulk from FASTA]\n\n### II. Prediction from input sequences\n```\ncd LucaProt/src/protein_structure/     \n\nexport CUDA_VISIBLE_DEVICES=0  \n\npython embedding_from_esmfold.py \\\n    --model_name esm2_t36_3B_UR50D \\\n    -name protein_id1,protein_id2 \\\n    -seq VGGLFDYYSVPIMT,LPDSWENKLLTDLILFAGSFVGSDTCGKLF \\\n    --output_dir embs/rdrp/test/ \\\n    --include per_tok contacts bos \\\n    --truncation_seq_length 4094\n```\n\n**Parameters:**\n* **-name**    \n  protein ids, comma-concatenation for multi proteins.\n\n* **-seq**   \n  protein sequences, comma-concatenation for multi proteins.\n\n## 3) Construct dataset for model building\nConstruct your dataset and randomly divide the dataset into training, validation, and testing sets with a specified ratio, and save the three sets in ```dataset/${dataset_name}/${dataset_type}/${task_type}```, including train_*.csv, dev_*.csv, test_*.csv.\n\nThe file format can be .csv (must include the header ) or .txt (does not need to have the header).\n\nEach file line is a sample containing 9 columns, including **prot_id**, **seq**, **seq_len**, **pdb_filename**, **ptm**, **mean_plddt**, **emb_filename**, **label**, and **source**.\n\nColunm **seq** is the sequence, Colunm **pdb_filename** is the saved PDB filename for structure encoder strategy 2, Colunm **ptm** and Column **mean_plddt** are optional, which are obtained from the 3D-Structure computed model, Colunm **emb_filename** is the saved embedding filename for structure encoder strategy 1, Column **label** is the sample class(a single value or a list value of label index or label name). Column **source** is the sample source (optional).\n\nFor example:\n\n```\nlike_YP_009351861.1_Menghai_flavivirus,MEQNG...,3416,,,,embedding_21449.pt,1,rdrp\n```\n\n**Note:** if your dataset takes too much space to load into memory at once,     \nuse \"src/data_process/data_preprocess_into_tfrecords_for_rdrp.py\" to convert the dataset into \"tfrecords\". And create an index file: python -m tfrecord.tools.tfrecord2idx xxxx.tfrecords xxxx.index\n\n## 4) Training the model\n\n* **run.py**   \n  the main script for model building.\n\n* **Parameters**\n  - **data_dir**: path, the dataset dirpath\n  - **filename_pattern**: the dataset filename pattern, such as \"{}_with_pdb_emb.csv\", including train_with_pdb_emb.csv, dev_with_pdb_emb.csv, and test_with_pdb_emb.csv in ${data_dir}\n  - **separate_file**: store_true, load the entire dataset into memory, the names of the pdb and embedding files are listed in the train/dev/test.csv, and need to load them.\n  - **tfrecords**: store_true, whether the dataset is in the tfrecords, when true, only the specified number of samples(${shuffle_queue_size}) are loaded into memory at once. The tfrecords must consist of \"${data_dir}/tfrecords/train/xxx.tfrecords\", \"${data_dir}/tfrecords/dev/xxx.tfrecords\" and \"${data_dir}/tfrecords/test/xxx.tfrecords\". \"xxx.tfrecords\" is one of 01-of-01.tfrecords(only including sequence), 01-of-01_emb.records (including sequence and structural embedding), and 01-of-01_pdb_emb.records (including sequence, 3D-structure contact map, and structural embedding).\n  - **shuffle_queue_size**: int, how many samples are loaded into memory at once, default: 5000.\n  - **dataset_name**: str, your dataset name, such as \"rdrp_40_extend\"\n  - **dataset_type**: str, your dataset type, such as \"protein\"\n  - **task_type**: choices=[\"multi_label\", \"multi_class\", \"binary_class\"], your task type, such as \"binary_class\"\n  - **model_type**: choices=[\"sequence\", \"structure\", \"embedding\", \"sefn\", \"ssfn\"], they represent only the sequence for input,  only the 3D-structure contact map for input,  only the structural embedding for input, the sequence and the structural embedding for input, and the sequence and the 3D-structure contact map for input, respectively\n  - **subword**: store_true, whether to process for sequence at the subword level.\n  - **codes_file**: path, subword codes filepath when using subword, such as \"../subword/rdrp/protein_codes_rdrp_20000.txt\"\n  - **label_type**: str, the label type name, such as \"rdrp\"\n  - **label_filepath**: path, the label list filepath\n  - **cmap_type**: choices=[\"C_alpha\", \"C_bert\"], the calculation type of 3D-structure contact map\n  - **cmap_thresh**: the distance threshold (Unit: Angstrom) in contact map calculation. Two amino acids are linked if the distance between them is equal to and less than the threshold, default: 10.0.\n  - **output_dir**: path, the output dirpath\n  - **log_dir**: path, the logger savepath\n  - **tb_log_dir**: path, the save path of metric evaluation records in model training, the tensorboardX can be used to show these metrics.\n  - **config_path**: path, the configuration filepath of the model.\n  - **seq_vocab_path**: path, the vocab filepath of sequence tokenizer\n  - **struct_vocab_path**: path, the vocab filepath of 3D-structure node (Structural Encoder Strategy 2)\n  - **seq_pooling_type**: choices=[\"none\", \"max\", \"value_attention\"], the sequence representation matrix pooling type, \"none\" represents that [CLS] vector is used.\n  - **struct_pooling_type**: choices=[\"max\", \"value_attention\"], the 3D-structure representation matrix pooling type.\n  - **embedding_pooling_type**: choices=[\"none\", \"max\", \"value_attention\"], the structural embedding representation matrix pooling type, \"none\" represents that [CLS] vector is used.\n  - **evaluate_during_training**: store_true, whether to evaluate the validation set and the testing set during training.\n  - **do_eval**: store_true, whether to use the best saved model to evaluate the validation set.\n  - **do_predict**: store_true, whether to use the best saved model to evaluate the testing set.\n  - **do_lower_case**: store_true, whether to lowercase the input when tokenizing.\n  - **per_gpu_train_batch_size**: int, batch size per GPU/CPU for training, default: 16\n  - **per_gpu_eval_batch_size**: int, batch size per GPU/CPU for evaluation, default: 16\n  - **gradient_accumulation_steps**: int, number of updates steps to accumulate before performing a backward/update pass, default: 1.\n  - **learning_rate**: float, the initial learning rate for Adam, default: 1e-4.\n  - **num_train_epochs**: int, the total number of training epochs to perform, default: 50.\n  - **logging_steps**: log every X updates steps, default: 1000.\n  - **loss_type**: choices=[\"focal_loss\", \"bce\", \"multilabel_cce\", \"asl\", \"cce\"], loss-function type of model training, default: \"bce\".\n  - **max_metric_type**: choices=[\"acc\", \"jaccard\", \"prec\", \"recall\", \"f1\", \"fmax\", \"roc_auc\", \"pr_auc\"], which metric is used for model finalization, default: \"f1\".\n  - **pos_weight**: float, positive samples weight for \"bce\".\n  - **focal_loss_alpha**: float, alpha for focal loss, default: 0.7.\n  - **focal_loss_gamma**: float, gamma for focal loss, default:2.0.\n  - **focal_loss_reduce**: store_true, \"mean\" for one sample when in multi-label classification, default:\"sum\".\n  - **asl_gamma_neg**: float, negative gamma for asl, default: 4.0.\n  - **asl_gamma_pos**: float, positive gamma for asl, default: 1.0.\n  - **seq_max_length**: int, the length of input sequence more than max length will be truncated, shorter will be padded, default: 2048.\n  - **struct_max_length**: int, the length of input contact map more than max length will be truncated, shorter will be padded., default: 2048.\n  - **trunc_type**: choices=[\"left\", \"right\"], the truncate type for whole input sequence, default: \"right\".\n  - **no_position_embeddings**: store_true, whether not use position embedding for the sequence.\n  - **no_token_type_embeddings**: store_true, whether not use token type embedding for the sequence.\n  - **embedding_input_size**: int, the dim of the structural embedding vector/matrix, default: 2560ï¼Œ {\"esm2_t30_150M_UR50D\": 640, \"esm2_t33_650M_UR50D\": 1280, \"esm2_t36_3B_UR50D\": 2560, \"esm2_t48_15B_UR50D\": 5120}.\n  - **embedding_type**: choices=[None, \"contacts\", \"bos\", \"matrix\"], the type of the structural embedding info, default: \"matrix\".\n  - **embedding_max_length**: int, the length of input embedding matrix more than max length will be truncated, shorter will be padded, default: 2048.\n  - **save_all**: store_true, the model for each evaluation is saved.\n  - **delete_old**: store_true, only save the best metric (${max_metric_type}) model of all evaluation on testing set during training.\n\n* **Training**\n    ```shell  \n    #!/bin/bash\n    \n    export CUDA_VISIBLE_DEVICES=0\n    \n    DATASET_NAME=\"rdrp_40_extend\"\n    DATASET_TYPE=\"protein\"\n    TASK_TYPE=\"binary_class\"\n    # sequence + structural embeddding\n    MODEL_TYPE=\"sefn\"\n    CONFIG_NAME=\"sefn_config.json\"\n    INPUT_MODE=\"single\"\n    LABEL_TYPE=\"rdrp\"\n    embedding_input_size=2560\n    embedding_type=\"matrix\"\n    SEQ_MAX_LENGTH=\"2048\"\n    embedding_max_length=\"2048\"\n    TRUNCT_TYPE=\"right\"\n    # none, max, value_attention\n    SEQ_POOLING_TYPE=\"value_attention\"\n    # max, value_attention\n    embedding_pooling_type=\"value_attention\"\n    VOCAB_NAME=\"subword_vocab_20000.txt\"\n    SUBWORD_CODES_NAME=\"protein_codes_rdrp_20000.txt\"\n    MAX_METRIC_TYPE=\"f1\"\n    time_str=$(date \"+%Y%m%d%H%M%S\")\n    \n    python run.py \\\n        --data_dir ../dataset/$DATASET_NAME/$DATASET_TYPE/$TASK_TYPE \\\n        --tfrecords \\\n        --filename_pattern {}_with_pdb_emb.csv \\\n        --dataset_name $DATASET_NAME \\\n        --dataset_type $DATASET_TYPE \\\n        --task_type $TASK_TYPE \\\n        --model_type $MODEL_TYPE \\\n        --subword \\\n        --codes_file ../subword/$DATASET_NAME/$DATASET_TYPE/$TASK_TYPE/$SUBWORD_CODES_NAME\\\n        --input_mode $INPUT_MODE \\\n        --label_type $LABEL_TYPE \\\n        --label_filepath ../dataset/$DATASET_NAME/$DATASET_TYPE/$TASK_TYPE/label.txt  \\\n        --output_dir ../models/$DATASET_NAME/$DATASET_TYPE/$TASK_TYPE/$MODEL_TYPE/$time_str \\\n        --log_dir ../logs/$DATASET_NAME/$DATASET_TYPE/$TASK_TYPE/$MODEL_TYPE/$time_str \\\n        --tb_log_dir ../tb-logs/$DATASET_NAME/$DATASET_TYPE/$TASK_TYPE/$MODEL_TYPE/$time_str \\\n        --config_path ../config/$DATASET_NAME/$DATASET_TYPE/$TASK_TYPE/$CONFIG_NAME \\\n        --seq_vocab_path  ../vocab/$DATASET_NAME/$DATASET_TYPE/$TASK_TYPE/$VOCAB_NAME\\\n        --seq_pooling_type $SEQ_POOLING_TYPE \\\n        --embedding_pooling_type $embedding_pooling_type \\\n        --do_train \\\n        --do_eval \\\n        --do_predict \\\n        --evaluate_during_training \\\n        --per_gpu_train_batch_size=16 \\\n        --per_gpu_eval_batch_size=16 \\\n        --gradient_accumulation_steps=1 \\\n        --learning_rate=1e-4 \\\n        --num_train_epochs=50 \\\n        --logging_steps=1000 \\\n        --save_steps=1000 \\\n        --overwrite_output_dir \\\n        --sigmoid \\\n        --loss_type bce \\\n        --max_metric_type $MAX_METRIC_TYPE \\\n        --seq_max_length=$SEQ_MAX_LENGTH \\\n        --embedding_max_length=$embedding_max_length \\\n        --trunc_type=$TRUNCT_TYPE \\\n        --no_token_type_embeddings \\\n        --embedding_input_size $embedding_input_size\\\n        --embedding_type $embedding_type \\\n        --shuffle_queue_size 10000 \\\n        --save_all\n    ```\n\n* **Configuration file**    \n  The configuration files of all methods is in \"config/rdrp_40_extend/protein/binary_class/\".        \n  If training your model, please put the configuration file in \"config/${dataset_name}/${dataset_type}/${task_type}/\"\n\n\n* **Value meaning in configuration file**      \n  referring to \"src/SSFN/README.md\"\n\n\n* **Baselines**\n  * LGBM (using the embedding vector: \\<bos> as the input)\n    ```\n    cd src/baselines/\n    sh run_lgbm.sh\n    ```\n  * XGBoost (using the embedding vector: \\<bos> as the input)\n    ```\n    cd src/baselines/\n    sh run_xgb.sh\n    ```\n\n  * DNN (using the embedding vector: \\<bos> as the input)\n    ```\n    cd src/baselines/\n    sh run_dnn.sh\n    ```\n    **Orï¼š**\n    ```\n    cd src/training\n    run_subword_rdrp_emb.sh\n    ```\n  * Transoformer-Char Level (using the sequence as the input)\n    ```\n    cd src/training\n    sh run_char_rdrp_seq.sh\n    ```\n  * Transoformer-Subword Level (using the sequence as the input)\n    ```\n    cd src/training\n    sh run_subword_rdrp_seq.sh\n    ```\n  * DNN2 (VALP + DNN, using the embedding matrix as the input)\n    ```\n    cd src/training\n    run_subword_rdrp_emb_v2.sh\n    ```\n\n* **Ours**\n  * Ours (the sequence + the 3D-structure)     \n    coming soon...\n\n  * Ours (the sequence + the embedding matrix)\n\n    ```\n    cd src/training\n    run_subword_rdrp_sefn.sh\n    ```   \n\n## 5) Training Logging Information\n\n### logs\nThe running information is saved in \"logs/${dataset_name}/${dataset_type}/${task_type}/${model_type}/${time_str}/logs.txt\".\n\nThe information includes the model configuration, model layers, running parameters, and evaluation information.\n\n### models\nThe checkpoints are saved in \"models/${dataset_name}/${dataset_type}/${task_type}/${model_type}/${time_str}/checkpoint-${global_step}/\", this directory includes \"pytorch_model.bin\", \"config.json\", \"training_args.bin\", and tokenizer information \"sequence\" or \"strcut\". The details are shown in Figure 2.\n\n<center>\n<img src=\"pics/models_saved_path.png\" />\n\nFigure 2: The File List in Checkpoint Dir Path\n</center>\n\n### tb-logs\nThe metrics are recorded in \"tb-logs/${dataset_name}/${dataset_type}/${task_type}/${model_type}/${time_str}/events.out.tfevents.xxxxx.xxxxx\"\n\nrun:\ntensorboard --logdir=tb-logs/${dataset_name}/${dataset_type}/${task_type}/${model_type}/${time_str} --bind_all\n\n### predicts\nThe predicted results is saved in \"predicts/${dataset_name}/${dataset_type}/${task_type}/${model_type}/${time_str}/checkpoint-${global_step}\", including:\n\n* pred_confusion_matrix.png\n* pred_metrics.txt\n* pred_result.csv\n* seq_length_distribution.png\n\nThe details are shown in Figure 3.\n<center>\n<img src=\"pics/prediction_results_saved_path.png\" />\n\nFigure 3: The File List in Prediction Dir Path\n</center>\n\n\n**Note:** when using the saved model to predict, the \"logs.txt\" and the checkpoint dirpath will be used.\n\n\n# 10. Related to the Project\n\n## 1) ClstrSearch\n\nA conventional approach that clustered all proteins based on their sequence homology.\n\nSee `ClstrSerch/README.md` for details.\n\n## 2) src\n\n### Construct RdRP Dataset for Model Building\n*.py in \"src/data_preprocess\"\n\n### Model\n*.py in \"src/SSFN\"\n\n### Prediction Shell Script\n*.sh in \"src/prediction\"    \nincluding:\n* run_predict_from_file.sh     \n  run prediction for many samples from a file, the structural embedding information prepared in advance.\n\n* run_predict_one_sample.sh      \n  run prediction for one sample from the input.\n\n* run_predict_many_samples.sh      \n  run prediction for many samples from the input.\n\nWe perform ablation studies on our model by removing specific module(sequence-specific and embedding-specific) one at a time to explore their relative importance.\n\n* run_predict_only_seq_from_file.sh    \n  only using the sequence to predict and calculate metrics three positive testing datasets,  three negative testing datasets, and our checked RdRPs by prediction SRA.\n\n\n* run_predict_only_emb_from_file.sh     \n  only using the structural embedding to predict and calculate metrics three positive testing datasets,  three negative testing datasets, and our checked RdRPs by prediction SRA.\n\n\n* run_predict_seq_emb_from_file.sh     \n  using the sequentail info and the structural embedding to predict and calculate metrics three positive testing datasets,  three negative testing datasets, and our checked RdRPs by prediction SRA.\n\n\n### Baselines\n*.py in \"src/baselines\", using the <bos> embedding vector as the input,  including:\n* DNN\n* LGBM\n* XGBoost\n\n\n### Baselines for Deep Learning\n*.py in \"src/deep_baselines\", including:\n\nCHEER: HierarCHical taxonomic classification for viral mEtagEnomic data via deep learning(2021). code: <a href=\"https://github.com/KennthShang/CHEER\" title=\"CHEER\"> CHEER </a>\n\nVirHunter: A Deep Learning-Based Method for Detection of Novel RNA Viruses in Plant Sequencing Data(2022). code: <a href=\"https://github.com/cbib/virhunter\" title=\"VirHunter\"> VirHunter </a>\n\nVirtifier: a deep learning-based identifier for viral sequences from metagenomes(2022). code: <a href=\"https://github.com/crazyinter/Seq2Vec\" title=\"Virtifier\"> Virtifier </a>\n\nRNN-VirSeeker: RNN-VirSeeker: A Deep Learning Method for Identification of Short Viral Sequences From Metagenomes.  code: <a href=\"https://github.com/crazyinter/RNN-VirSeeker\" title=\"RNN-VirSeeker\"> RNN-VirSeeker </a>\n\n* run_deep_baselines.sh   \n  the script to train deep baseline models.\n\n\n* run_predict_deep_baselines.sh   \n  use trained deep baseline models to predict three positive test datasets,  three negative test datasets, and our checked RdRP datasets.\n\n\n* run.py     \n  the main script for training deep baseline models.\n\n\n* statistics    \n  the script to statistic the accuracy in three kinds of test datasets(positive, negative, our checked) after prediction by deep baselines.\n\n### Contact Map Generator\n*.py in \"src/biotoolbox\"\n\n### Loss & Metrics\n*.py in \"src/common\"\n\n### Training Model\n*.sh in \"src/training\"\n\n### Prediction of Model\n*.sh in \"src/prediction\"\n\n\n## 3) Data\n\n### Raw Data\nthe raw data is in \"data/\".\n\n### Dataset\nthe files of the dataset is in \"dataset/${dataset_name}/${dataset_type}/${task_type}/\".\n\n## 4) Model Configuration\nthe configuration file of all methods is in \"config/${dataset_name}/${dataset_type}/${task_type}/\".\n\n## 5) Pic\nsome pictures is in \"pics/\".\n\n## 6) Plot\nthe scripts of pictures ploting is in \"src/plot\".\n\n## 7) Spider\nthe codes and results of Geo information Spider in \"src/geo_map\".\n\n\n# 11. Open Resource\nThe open resources of our study ar includes 10 subdirectories: `Known_RdRPs`, `Benchmark`, `Results`, `All_Contigs`, `All_Protein_Sequences`, `SG_predicted_protein_structure_supplementation/`, `Serratus`, `Self_Sequencing_Proteins`, `Self_Sequencing_Reads`, and `LucaProt`.   \nPlease refer to [README.md](http://47.93.21.181/README.md) or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13) .       \n\n\n`LucaProt/` includes some resources related to LucaProt, including **dataset for model building(dataset_for_model_building)**, **dataset for_model evaluation(dataset_for_model_evaluation)**, and **our trained model(logs/ and models/)**.\n\n## 1) Code\nAs mentioned above.\n\n## 2) Dataset\n\n### Model Building Dataset\n* sequential info    \n  [1) train_with_pdb_emb.csv](http://47.93.21.181/LucaProt/dataset_for_model_building/dataset/rdrp_40_extend/protein/binary_class/train_with_pdb_emb.csv)    \n  copy to `LucaProt/dataset/rdrp_40_extend/protein/binary_class/`   \n  or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)    \n\n  [2) dev_with_pdb_emb.csv](http://47.93.21.181/LucaProt/dataset_for_model_building/dataset/rdrp_40_extend/protein/binary_class/dev_with_pdb_emb.csv)     \n  copy to `LucaProt/dataset/rdrp_40_extend/protein/binary_class/`  \n  or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)    \n\n  [3) test_with_pdb_emb.csv](http://47.93.21.181/LucaProt/dataset_for_model_building/dataset/rdrp_40_extend/protein/binary_class/test_with_pdb_emb.csv)   \n  copy to `LucaProt/dataset/rdrp_40_extend/protein/binary_class/`   \n  or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)    \n\n* structural info      \n  [embs](http://47.93.21.181/LucaProt/dataset_for_model_building/dataset/rdrp_40_extend/protein/binary_class/)         \n  copy to `LucaProt/dataset/rdrp_40_extend/protein/binary_class/embs/`\n\n* tfrcords     \n  [1) train](http://47.93.21.181/LucaProt/dataset_for_model_building/dataset/rdrp_40_extend/protein/binary_class/tfrecords/train/)       \n  copy to `LucaProt/dataset/rdrp_40_extend/protein/binary_class/tfrecords/train/`\n\n  [2) dev](http://47.93.21.181/LucaProt/dataset_for_model_building/dataset/rdrp_40_extend/protein/binary_class/tfrecords/dev/)      \n  copy to `LucaProt/dataset/rdrp_40_extend/protein/binary_class/tfrecords/dev/`\n\n  [3) test](http://47.93.21.181/LucaProt/dataset_for_model_building/dataset/rdrp_40_extend/protein/binary_class/tfrecords/test/)       \n  copy to `LucaProt/dataset/rdrp_40_extend/protein/binary_class/tfrecords/test/`\n\n### Model Testing (Validation) Dataset\n\n* 7 Positive Testing Datasets    \n    [1) Wolf et al., 2020 NM](http://47.93.21.181/LucaProt/dataset_for_model_evaluation/7_positive_datasets/Wolf_2020_NM_RdRP.fa)       \n    Reference: Doubling of the known set of RNA viruses by metagenomic analysis of an aquatic virome.   \n    or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)      \n\n    [2) Edgar et al., 2022 Nature](http://47.93.21.181/LucaProt/dataset_for_model_evaluation/7_positive_datasets/Edgar_2022_Nature_Serratus_RdRP.fa)           \n    Reference: Petabase-scale sequence alignment catalyses viral discovery.    \n    or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)      \n\n    [3) Zayed et al, 2022 Science](http://47.93.21.181/LucaProt/dataset_for_model_evaluation/7_positive_datasets/Zayed_2022_Science_RdRP.fa)         \n    Reference: Cryptic and abundant marine viruses at the evolutionary origins of Earth's RNA virome.     \n    or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)           \n\n    [4) Neri et al., 2022 Cell](http://47.93.21.181/LucaProt/dataset_for_model_evaluation/7_positive_datasets/Neri_2022_Cell_RdRP.fa)         \n    Reference: Expansion of the global RNA virome reveals diverse clades of bacteriophages.    \n    or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)       \n\n    [5) Chen et al., 2022 NM](http://47.93.21.181/LucaProt/dataset_for_model_evaluation/7_positive_datasets/Chen_2022_NM_RdRP.fa)         \n    Reference: RNA viromes from terrestrial sites across China expand environmental viral diversity.     \n    or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)       \n\n    [6) Olendraite et al., 2023 MBE](http://47.93.21.181/LucaProt/dataset_for_model_evaluation/7_positive_datasets/Olendraite_2023_MBE_RdRP.fa)         \n    Reference: Identification of RNA Virus-Derived RdRp Sequences in Publicly Available Transcriptomic Data Sets.    \n    or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)\n\n    [7) This Study](http://47.93.21.181/LucaProt/dataset_for_model_evaluation/7_positive_datasets/This_Study.fa)         \n    Reference: Artificial intelligence redefines RNA virus discovery.    \n    or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)     \n\n\n\n* 4 Negative Testing Datasets     \n    [1) RT](http://47.93.21.181/LucaProt/dataset_for_model_evaluation/4_negative_datasets/RT.fa)   \n    or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)   \n\n    [2) Eu DdRP](http://47.93.21.181/LucaProt/dataset_for_model_evaluation/4_negative_datasets/Eukaryota-DdRP.fa)   \n    or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)     \n\n    [3) Eu RdRP](http://47.93.21.181/LucaProt/dataset_for_model_evaluation/4_negative_datasets/Eukaryota-RdRP.fa)    \n    or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)        \n\n    [4) Self-Sequencing-Negatives](http://47.93.21.181/LucaProt/dataset_for_model_evaluation/4_negative_datasets/Self-Sequencing-Negatives.fa)   \n    or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)       \n\n\n\n### Results\n\n* Our Checked RdRP Dataset (Our Results)\n  * sequential info    \n    [ours_checked_rdrp_final.csv](http://47.93.21.181/LucaProt/our_identified_rdrp/)    \n    or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)    \n\n  * structural info        \n    [embs](http://47.93.21.181/LucaProt/our_identified_rdrp/embs/)\n\n  * PDB    \n    All 3D-structure PDB files of our predicted results for opening are in the process.\n\n### Self-Samples\n* Our Sampled Dataset\n  * fasta   \n    [00self_sequecing_300aa.pep](http://47.93.21.181/LucaProt/self_sequencing_proteins/)     \n\n\n## 3) Trained Model\nThe trained model for RdRP identification is available at:    \n**Notice:** these files were already downloaded in this GitHub project, so you don't need to download them.         \n* logs    \n  [logs](http://47.93.21.181/LucaProt/logs/)       \n  copy to `LucaProt/logs/`   \n  or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)        \n\n* models         \n  [models](http://47.93.21.181/LucaProt/models/)      \n  copy to `LucaProt/models/`   \n  or [LucaProt Figshare](https://doi.org/10.6084/m9.figshare.26298802.v13)         \n\n# 12. Contributor\n<a href=\"http://lucalab.tech/\" title=\"LucaTeam\">LucaTeam:</a>    \n<a href=\"https://scholar.google.com.hk/citations?user=RDbqGTcAAAAJ&hl=en\" title=\"Yong He\">Yong He</a>,\n<a href=\"https://scholar.google.com/citations?user=lT3nelQAAAAJ&hl=en\" title=\"Zhaorong Li\">Zhaorong Li</a>,\n<a href=\"https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=zA310LMAAAAJ\" title=\"Xin Hou\">Xin Hou</a>,\n<a href=\"https://scholar.google.com/citations?user=1KJOH7YAAAAJ&hl=zh-CN&oi=ao\" title=\"Mang Shi\">Mang Shi</a>,\n<a href=\"https://scholar.google.com/citations?user=ODcOX4AAAAAJ&hl=zh-CN\" title=\"Pan Fang\">Pan Fang</a> \n\n\n\n# 13. FTP\n**FTP:** The all data of LucaProt is available at the website: <a href=\"http://47.93.21.181\" title=\"Open Resources\"> Open Resources </a>           \n**Figshare:** https://doi.org/10.6084/m9.figshare.26298802.v13   \n\n# 14. Citation\nCell:     \n<a href=\"https://www.cell.com/cell/fulltext/S0092-8674(24)01085-7\">https://www.cell.com/cell/fulltext/S0092-8674(24)01085-7</a>\n\n@article {LucaProt,     \nauthor = {Xin Hou, Yong He, Pan Fang, Shi-Qiang Mei, Zan Xu, Wei-Chen Wu, Jun-Hua Tian,  Shun Zhang, Zhen-Yu Zeng, Qin-Yu Gou, Gen-Yang Xin, Shi-Jia Le, Yin-Yue Xia, Yu-Lan Zhou, Feng-Ming Hui, Yuan-Fei Pan,\n          John-Sebastian Eden, Zhao-Hui Yang, Chong Han, Yue-Long Shu, Deyin Guo, Jun Li, Edward C Holmes, Zhao-Rong Li and Mang Shi},        \ntitle = {Using artificial intelligence to document the hidden RNA virosphere},         \nyear = {2024},       \ndoi = {10.1016/j.cell.2024.09.027},         \npublisher = {Cell Press},   \nURL = {https://doi.org/10.1016/j.cell.2024.09.027},   \neprint = {https://www.cell.com/cell/fulltext/S0092-8674(24)01085-7},    \njournal = {Cell}            \n}\n\n\n\n",
    "readme_length": 54223
  },
  {
    "name": "Comp_Sci_Sem_2",
    "full_name": "danderfer/Comp_Sci_Sem_2",
    "description": "According to all known laws of aviation, there is no way that a bee should be able to fly. Its wings are too small to get its fat little body off the ground. The bee, of course, flies anyway. Because bees donâ€™t care what humans think is impossible.â€ SEQ. 75 - â€œINTRO TO BARRYâ€ INT. BENSON HOUSE - DAY ANGLE ON: Sneakers on the ground. Camera PANS UP to reveal BARRY BENSONâ€™S BEDROOM ANGLE ON: Barryâ€™s hand flipping through different sweaters in his closet. BARRY Yellow black, yellow black, yellow black, yellow black, yellow black, yellow black...oohh, black and yellow... ANGLE ON: Barry wearing the sweater he picked, looking in the mirror. BARRY (CONTâ€™D) Yeah, letâ€™s shake it up a little. He picks the black and yellow one. He then goes to the sink, takes the top off a CONTAINER OF HONEY, and puts some honey into his hair. He squirts some in his mouth and gargles. Then he takes the lid off the bottle, and rolls some on like deodorant. CUT TO: INT. BENSON HOUSE KITCHEN - CONTINUOUS Barryâ€™s mother, JANET BENSON, yells up at Barry. JANET BENSON Barry, breakfast is ready! CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 1. INT. BARRYâ€™S ROOM - CONTINUOUS BARRY Coming! SFX: Phone RINGING. Barryâ€™s antennae vibrate as they RING like a phone. Barryâ€™s hands are wet. He looks around for a towel. BARRY (CONTâ€™D) Hang on a second! He wipes his hands on his sweater, and pulls his antennae down to his ear and mouth. BARRY (CONT'D) Hello? His best friend, ADAM FLAYMAN, is on the other end. ADAM Barry? BARRY Adam? ADAM Can you believe this is happening? BARRY Canâ€™t believe it. Iâ€™ll pick you up. Barry sticks his stinger in a sharpener. SFX: BUZZING AS HIS STINGER IS SHARPENED. He tests the sharpness with his finger. SFX: Bing. BARRY (CONTâ€™D) Looking sharp. ANGLE ON: Barry hovering down the hall, sliding down the staircase bannister. Barryâ€™s mother, JANET BENSON, is in the kitchen. JANET BENSON Barry, why donâ€™t you use the stairs? Your father paid good money for those. \"Bee Movie\" - JS REVISIONS 8/13/07 2. BARRY Sorry, Iâ€™m excited. Barryâ€™s father, MARTIN BENSON, ENTERS. Heâ€™s reading a NEWSPAPER with the HEADLINE, â€œQueen gives birth to thousandtuplets: Resting Comfortably.â€ MARTIN BENSON Hereâ€™s the graduate. Weâ€™re very proud of you, Son. And a perfect report card, all Bâ€™s. JANET BENSON (mushing Barryâ€™s hair) Very proud. BARRY Ma! Iâ€™ve got a thing going here. Barry re-adjusts his hair, starts to leave. JANET BENSON Youâ€™ve got some lint on your fuzz. She picks it off. BARRY Ow, thatâ€™s me! MARTIN BENSON Wave to us. Weâ€™ll be in row 118,000. Barry zips off. BARRY Bye! JANET BENSON Barry, I told you, stop flying in the house! CUT TO: SEQ. 750 - DRIVING TO GRADUATION EXT. BEE SUBURB - MORNING A GARAGE DOOR OPENS. Barry drives out in his CAR. \"Bee Movie\" - JS REVISIONS 8/13/07 3. ANGLE ON: Barryâ€™s friend, ADAM FLAYMAN, standing by the curb. Heâ€™s reading a NEWSPAPER with the HEADLINE: â€œFrisbee Hits Hive: Internet Down. Bee-stander: â€œI heard a sound, and next thing I knew...wham-o!.â€ Barry drives up, stops in front of Adam. Adam jumps in. BARRY Hey, Adam. ADAM Hey, Barry. (pointing at Barryâ€™s hair) Is that fuzz gel? BARRY A little. Itâ€™s a special day. Finally graduating. ADAM I never thought Iâ€™d make it. BARRY Yeah, three days of grade school, three days of high school. ADAM Those were so awkward. BARRY Three days of college. Iâ€™m glad I took off one day in the middle and just hitchhiked around the hive. ADAM You did come back different. They drive by a bee whoâ€™s jogging. ARTIE Hi Barry! BARRY (to a bee pedestrian) Hey Artie, growing a mustache? Looks good. Barry and Adam drive from the suburbs into the city. ADAM Hey, did you hear about Frankie? \"Bee Movie\" - JS REVISIONS 8/13/07 4. BARRY Yeah. ADAM You going to his funeral? BARRY No, Iâ€™m not going to his funeral. Everybody knows you sting someone you die, you donâ€™t waste it on a squirrel. He was such a hot head. ADAM Yeah, I guess he couldâ€™ve just gotten out of the way. The DRIVE through a loop de loop. BARRY AND ADAM Whoa...Whooo...wheee!! ADAM I love this incorporating the amusement park right into our regular day. BARRY I guess thatâ€™s why they say we donâ€™t need vacations. CUT TO: SEQ. 95 - GRADUATION EXT. GRADUATION CEREMONY - CONTINUOUS Barry and Adam come to a stop. They exit the car, and fly over the crowd to their seats. * BARRY * (re: graduation ceremony) * Boy, quite a bit of pomp...under * the circumstances. * They land in their seats. BARRY (CONTâ€™D) Well Adam, today we are men. \"Bee Movie\" - JS REVISIONS 8/13/07 5. ADAM We are. BARRY Bee-men. ADAM Amen! BARRY Hallelujah. Barry hits Adamâ€™s forehead. Adam goes into the rapture. An announcement comes over the PA. ANNOUNCER (V.O) Students, faculty, distinguished bees...please welcome, Dean Buzzwell. ANGLE ON: DEAN BUZZWELL steps up to the podium. The podium has a sign that reads: â€œWelcome Graduating Class of:â€, with train-station style flipping numbers after it. BUZZWELL Welcome New Hive City graduating class of... The numbers on the podium change to 9:15. BUZZWELL (CONTâ€™D) ...9:15. (he clears his throat) And that concludes our graduation ceremonies. And begins your career at Honex Industries. BARRY Are we going to pick our job today? ADAM I heard itâ€™s just orientation. The rows of chairs change in transformer-like mechanical motion to Universal Studios type tour trams. Buzzwell walks off stage. BARRY (re: trams) Whoa, heads up! Here we go. \"Bee Movie\" - JS REVISIONS 8/13/07 6. SEQ. 125 - â€œFACTORYâ€ FEMALE VOICE (V.O) Keep your hands and antennas inside the tram at all times. (in Spanish) Dejen las manos y antennas adentro del tram a todos tiempos. BARRY I wonder what itâ€™s going to be like? ADAM A little scary. Barry shakes Adam. BARRY AND ADAM AAHHHH! The tram passes under SIGNS READING: â€œHonex: A Division of Honesco: A Part of the Hexagon Group.â€ TRUDY Welcome to Honex, a division of Honesco, and a part of the Hexagon group. BARRY This is it! The Honex doors OPEN, revealing the factory. BARRY (CONTâ€™D) Wow. TRUDY We know that you, as a bee, have worked your whole life to get to the point where you can work for your whole life. Honey begins when our valiant pollen jocks bring the nectar to the hive where our top secret formula is automatically color-corrected, scent adjusted and bubble contoured into this... Trudy GRABS a TEST TUBE OF HONEY from a technician. \"Bee Movie\" - JS REVISIONS 8/13/07 7. TRUDY (CONTâ€™D) ...soothing, sweet syrup with its distinctive golden glow, you all know as... EVERYONE ON THE TRAM (in unison) H-o-n-e-y. Trudy flips the flask into the crowd, and laughs as they all scramble for it. ANGLE ON: A GIRL BEE catching the honey. ADAM (sotto) That girl was hot. BARRY (sotto) Sheâ€™s my cousin. ADAM She is? BARRY Yes, weâ€™re all cousins. ADAM Right. Youâ€™re right. TRUDY At Honex, we also constantly strive to improve every aspect of bee existence. These bees are stress testing a new helmet technology. ANGLE ON: A STUNT BEE in a HELMET getting hit with a NEWSPAPER, then a SHOE, then a FLYSWATTER. He gets up, and gives a â€œthumbâ€™s upâ€. The graduate bees APPLAUD. ADAM (re: stunt bee) What do you think he makes? BARRY Not enough. TRUDY And here we have our latest advancement, the Krelman. \"Bee Movie\" - JS REVISIONS 8/13/07 8. BARRY Wow, what does that do? TRUDY Catches that little strand of honey that hangs after you pour it. Saves us millions. ANGLE ON: The Krelman machine. Bees with hand-shaped hats on, rotating around a wheel to catch drips of honey. Adamâ€™s hand shoots up. ADAM Can anyone work on the Krelman? TRUDY Of course. Most bee jobs are small ones. But bees know that every small job, if itâ€™s done well, means a lot. There are over 3000 different bee occupations. But choose carefully, because youâ€™ll stay in the job that you pick for the rest of your life. The bees CHEER. ANGLE ON: Barryâ€™s smile dropping slightly. BARRY The same job for the rest of your life? I didnâ€™t know that. ADAM Whatâ€™s the difference? TRUDY And youâ€™ll be happy to know that bees as a species havenâ€™t had one day off in 27 million years. BARRY So youâ€™ll just work us to death? TRUDY (laughing) Weâ€™ll sure try. Everyone LAUGHS except Barry. \"Bee Movie\" - JS REVISIONS 8/13/07 9. The tram drops down a log-flume type steep drop. Cameras flash, as all the bees throw up their hands. The frame freezes into a snapshot. Barry looks concerned. The tram continues through 2 doors. FORM DISSOLVE TO: SEQ. 175 - â€œWALKING THE HIVEâ€ INT. HONEX LOBBY ANGLE ON: The log-flume photo, as Barry looks at it. ADAM Wow. That blew my mind. BARRY (annoyed) â€œWhatâ€™s the difference?â€ Adam, how could you say that? One job forever? Thatâ€™s an insane choice to have to make. ADAM Well, Iâ€™m relieved. Now we only have to make one decision in life. BARRY But Adam, how could they never have told us that? ADAM Barry, why would you question anything? Weâ€™re bees. Weâ€™re the most perfectly functioning society on Earth. They walk by a newspaper stand with A SANDWICH BOARD READING: â€œBee Goes Berserk: Stings Seven Then Self.â€ ANGLE ON: A BEE filling his carâ€™s gas tank from a honey pump. He fills his car some, then takes a swig for himself. NEWSPAPER BEE (to the bee guzzling gas) Hey! Barry and Adam begin to cross the street. \"Bee Movie\" - JS REVISIONS 8/13/07 10. BARRY Yeah but Adam, did you ever think that maybe things work a little too well around here? They stop in the middle of the street. The traffic moves perfectly around them. ADAM Like what? Give me one example. BARRY (thinks) ...I donâ€™t know. But you know what Iâ€™m talking about. They walk off. SEQ. 400 - â€œMEET THE JOCKSâ€ SFX: The SOUND of Pollen Jocks. PAN DOWN from the Honex statue. J-GATE ANNOUNCER Please clear the gate. Royal Nectar Force on approach. Royal Nectar Force on approach. BARRY Wait a second. Check it out. Hey, hey, those are Pollen jocks. ADAM Wow. FOUR PATROL BEES FLY in through the hiveâ€™s giant Gothic entrance. The Patrol Bees are wearing fighter pilot helmets with black visors. ADAM (CONTâ€™D) Iâ€™ve never seen them this close. BARRY They know what itâ€™s like to go outside the hive. ADAM Yeah, but some of them donâ€™t come back. \"Bee Movie\" - JS REVISIONS 8/13/07 11. The nectar from the pollen jocks is removed from their backpacks, and loaded into trucks on their way to Honex. A SMALL CROWD forms around the Patrol Bees. Each one has a PIT CREW that takes their nectar. Lou Loduca hurries a pit crew along: LOU LODUCA You guys did great! Youâ€™re monsters. Youâ€™re sky freaks! I love it! I love it! SCHOOL GIRLS are jumping up and down and squealing nearby. BARRY I wonder where those guys have just been? ADAM I donâ€™t know. BARRY Their dayâ€™s not planned. Outside the hive, flying who-knows-where, doing who-knows-what. ADAM You canâ€™t just decide one day to be a Pollen Jock. You have to be bred for that. BARRY Right. Pollen Jocks cross in close proximity to Barry and Adam. Some pollen falls off, onto Barry and Adam. BARRY (CONTâ€™D) Look at that. Thatâ€™s more pollen than you and I will ever see in a lifetime. ADAM (playing with the pollen) Itâ€™s just a status symbol. I think bees make too big a deal out of it. BARRY Perhaps, unless youâ€™re wearing it, and the ladies see you wearing it. ANGLE ON: Two girl bees. \"Bee Movie\" - JS REVISIONS 8/13/07 12. ADAM Those ladies? Arenâ€™t they our cousins too? BARRY Distant, distant. ANGLE ON: TWO POLLEN JOCKS. JACKSON Look at these two. SPLITZ Couple of Hive Harrys. JACKSON Letâ€™s have some fun with them. The pollen jocks approach. Barry and Adam continue to talk to the girls. GIRL 1 It must be so dangerous being a pollen jock. BARRY Oh yeah, one time a bear had me pinned up against a mushroom. He had one paw on my throat, and with the other he was slapping me back and forth across the face. GIRL 1 Oh my. BARRY I never thought Iâ€™d knock him out. GIRL 2 (to Adam) And what were you doing during all of this? ADAM Obviously I was trying to alert the authorities. The girl swipes some pollen off of Adam with a finger. BARRY (re: pollen) I can autograph that if you want. \"Bee Movie\" - JS REVISIONS 8/13/07 13. JACKSON Little gusty out there today, wasnâ€™t it, comrades? BARRY Yeah. Gusty. BUZZ You know, weâ€™re going to hit a sunflower patch about six miles from here tomorrow. BARRY Six miles, huh? ADAM (whispering) Barry. BUZZ Itâ€™s a puddle-jump for us. But maybe youâ€™re not up for it. BARRY Maybe I am. ADAM (whispering louder) You are not! BUZZ Weâ€™re going, oh-nine hundred at JGate. ADAM (re: j-gate) Whoa. BUZZ (leaning in, on top of Barry) What do you think, Buzzy Boy? Are you bee enough? BARRY I might be. It all depends on what oh-nine hundred means. CUT TO: SEQ. 450 - â€œTHE BALCONYâ€ \"Bee Movie\" - JS REVISIONS 8/13/07 14. INT. BENSON HOUSE BALCONY - LATER Barry is standing on the balcony alone, looking out over the city. Martin Benson ENTERS, sneaks up behind Barry and gooses him in his ribs. MARTIN BENSON Honex! BARRY Oh, Dad. You surprised me. MARTIN BENSON (laughing) Have you decided what youâ€™re interested in, Son? BARRY Well, thereâ€™s a lot of choices. MARTIN BENSON But you only get one. Martin LAUGHS. BARRY Dad, do you ever get bored doing the same job every day? MARTIN BENSON Son, let me tell you something about stirring. (making the stirring motion) You grab that stick and you just move it around, and you stir it around. You get yourself into a rhythm, itâ€™s a beautiful thing. BARRY You know dad, the more I think about it, maybe the honey field just isnâ€™t right for me. MARTIN BENSON And you were thinking of what, making balloon animals? Thatâ€™s a bad job for a guy with a stinger. \"Bee Movie\" - JS REVISIONS 8/13/07 15. BARRY Well no... MARTIN BENSON Janet, your sonâ€™s not sure he wants to go into honey. JANET BENSON Oh Barry, you are so funny sometimes. BARRY Iâ€™m not trying to be funny. MARTIN BENSON Youâ€™re not funny, youâ€™re going into honey. Our son, the stirrer. JANET BENSON Youâ€™re going to be a stirrer?! BARRY No oneâ€™s listening to me. MARTIN BENSON Wait until you see the sticks I have for you. BARRY I can say anything I want right now. Iâ€™m going to get an ant tattoo. JANET BENSON Letâ€™s open some fresh honey and celebrate. BARRY Maybe Iâ€™ll pierce my thorax! MARTIN BENSON (toasting) To honey! BARRY Shave my antennae! JANET BENSON To honey! \"Bee Movie\" - JS REVISIONS 8/13/07 16. BARRY Shack up with a grasshopper, get a gold tooth, and start calling everybody â€œDawg.â€ CUT TO: SEQ. 760 - â€œJOB PLACEMENTâ€ EXT. HONEX LOBBY - CONTINUOUS ANGLE ON: A BEE BUS STOP. One group of bees stands on the pavement, as another group hovers above them. A doubledecker bus pulls up. The hovering bees get on the top level, and the standing bees get on the bottom. Barry and Adam pull up outside of Honex. ADAM I canâ€™t believe weâ€™re starting work today. BARRY Todayâ€™s the day. Adam jumps out of the car. ADAM (O.C) Come on. All the good jobs will be gone. BARRY Yeah, right... ANGLE ON: A BOARD READING: â€œJOB PLACEMENT BOARDâ€. Buzzwell, the Bee Processor, is at the counter. Another BEE APPLICANT, SANDY SHRIMPKIN is EXITING. SANDY SHRIMPKIN Is it still available? BUZZWELL Hang on. (he looks at changing numbers on the board) Two left. And...one of themâ€™s yours. Congratulations Son, step to the side please. \"Bee Movie\" - JS REVISIONS 8/13/07 17. SANDY SHRIMPKIN Yeah! ADAM (to Sandy, leaving) What did you get? SANDY SHRIMPKIN Picking the crud out. That is stellar! ADAM Wow. BUZZWELL (to Adam and Barry) Couple of newbies? ADAM Yes Sir. Our first day. We are ready. BUZZWELL Well, step up and make your choice. ANGLE ON: A CHART listing the different sectors of Honex. Heating, Cooling, Viscosity, Krelman, Pollen Counting, Stunt Bee, Pouring, Stirrer, Humming, Regurgitating, Front Desk, Hair Removal, Inspector No. 7, Chef, Lint Coordinator, Stripe Supervisor, Antennae-ball polisher, Mite Wrangler, Swatting Counselor, Wax Monkey, Wing Brusher, Hive Keeper, Restroom Attendant. ADAM (to Barry) You want to go first? BARRY No, you go. ADAM Oh my. Whatâ€™s available? BUZZWELL Restroom attendant is always open, and not for the reason you think. ADAM Any chance of getting on to the Krelman, Sir? BUZZWELL Sure, youâ€™re on. \"Bee Movie\" - JS REVISIONS 8/13/07 18. He plops the KRELMAN HAT onto Adamâ€™s head. ANGLE ON: The job board. THE COLUMNS READ: â€œOCCUPATIONâ€ â€œPOSITIONS AVAILABLEâ€, and â€œSTATUSâ€. The middle column has numbers, and the right column has job openings flipping between â€œopenâ€, â€œpendingâ€, and â€œclosedâ€. BUZZWELL (CONTâ€™D) Oh, Iâ€™m sorry. The Krelman just closed out. ADAM Oh! He takes the hat off Adam. BUZZWELL Wax Monkeyâ€™s always open. The Krelman goes from â€œClosedâ€ to â€œOpenâ€. BUZZWELL (CONTâ€™D) And the Krelman just opened up again. ADAM What happened? BUZZWELL Well, whenever a bee dies, thatâ€™s an opening. (pointing at the board) See that? Heâ€™s dead, dead, another dead one, deady, deadified, two more dead. Dead from the neck up, dead from the neck down. But, thatâ€™s life. ANGLE ON: Barryâ€™s disturbed expression. ADAM (feeling pressure to decide) Oh, this is so hard. Heating, cooling, stunt bee, pourer, stirrer, humming, inspector no. 7, lint coordinator, stripe supervisor, antenna-ball polisher, mite wrangler-- Barry, Barry, what do you think I should-- Barry? Barry? \"Bee Movie\" - JS REVISIONS 8/13/07 19. Barry is gone. CUT TO: SEQ. 775 - â€œLOU LODUCA SPEECHâ€ EXT. J-GATE - SAME TIME Splitz, Jackson, Buzz, Lou and two other BEES are going through final pre-flight checks. Barry ENTERS. LOU LODUCA Alright, weâ€™ve got the sunflower patch in quadrant nine. Geranium window box on Sutton Place... Barryâ€™s antennae rings, like a phone. ADAM (V.O) What happened to you? Where are you? Barry whispers throughout. BARRY Iâ€™m going out. ADAM (V.O) Out? Out where? BARRY Out there. ADAM (V.O) (putting it together) Oh no. BARRY I have to, before I go to work for the rest of my life. ADAM (V.O) Youâ€™re going to die! Youâ€™re crazy! Hello? BARRY Oh, another call coming in. \"Bee Movie\" - JS REVISIONS 8/13/07 20. ADAM (V.O) Youâ€™re cra-- Barry HANGS UP. ANGLE ON: Lou Loduca. LOU LODUCA If anyoneâ€™s feeling brave, thereâ€™s a Korean Deli on 83rd that gets their roses today. BARRY (timidly) Hey guys. BUZZ Well, look at that. SPLITZ Isnâ€™t that the kid we saw yesterday? LOU LODUCA (to Barry) Hold it son, flight deckâ€™s restricted. JACKSON Itâ€™s okay Lou, weâ€™re going to take him up. Splitz and Jackson CHUCKLE. LOU LODUCA Really? Feeling lucky, are ya? A YOUNGER SMALLER BEE THAN BARRY, CHET, runs up with a release waiver for Barry to sign. CHET Sign here. Here. Just initial that. Thank you. LOU LODUCA Okay, you got a rain advisory today and as you all know, bees cannot fly in rain. So be careful. As always, (reading off clipboard) watch your brooms, hockey sticks, dogs, birds, bears, and bats. \"Bee Movie\" - JS REVISIONS 8/13/07 21. Also, I got a couple reports of root beer being poured on us. Murphyâ€™s in a home because of it, just babbling like a cicada. BARRY Thatâ€™s awful. LOU LODUCA And a reminder for all you rookies, bee law number one, absolutely no talking to humans. Alright, launch positions! The Jocks get into formation, chanting as they move. LOU LODUCA (CONTâ€™D) Black and Yellow! JOCKS Hello! SPLITZ (to Barry) Are you ready for this, hot shot? BARRY Yeah. Yeah, bring it on. Barry NODS, terrified. BUZZ Wind! - CHECK! JOCK #1 Antennae! - CHECK! JOCK #2 Nectar pack! - CHECK! JACKSON Wings! - CHECK! SPLITZ Stinger! - CHECK! BARRY Scared out of my shorts - CHECK. LOU LODUCA Okay ladies, letâ€™s move it out. Everyone FLIPS their goggles down. Pit crew bees CRANK their wings, and remove the starting blocks. We hear loud HUMMING. \"Bee Movie\" - JS REVISIONS 8/13/07 22. LOU LODUCA (CONT'D) LOU LODUCA (CONTâ€™D) Pound those petunia's, you striped stem-suckers! All of you, drain those flowers! A FLIGHT DECK GUY in deep crouch hand-signals them out the archway as the backwash from the bee wings FLUTTERS his jump suit. Barry follows everyone. SEQ. 800 - â€œFLYING WITH THE JOCKSâ€ The bees climb above tree tops in formation. Barry is euphoric. BARRY Whoa! Iâ€™m out! I canâ€™t believe Iâ€™m out! So blue. Ha ha ha! (a beat) I feel so fast...and free. (re: kites in the sky) Box kite! Wow! They fly by several bicyclists, and approach a patch of flowers. BARRY (CONT'D) Flowers! SPLITZ This is blue leader. We have roses visual. Bring it around thirty degrees and hold. BARRY (sotto) Roses. JACKSON Thirty degrees, roger, bringing it around. Many pollen jocks break off from the main group. They use their equipment to collect nectar from flowers. Barry flies down to watch the jocks collect the nectar. JOCK Stand to the side kid, itâ€™s got a bit of a kick. The jock fires the gun, and recoils. Barry watches the gun fill up with nectar. \"Bee Movie\" - JS REVISIONS 8/13/07 23. BARRY Oh, that is one Nectar Collector. JOCK You ever see pollination up close? BARRY No, Sir. He takes off, and the excess pollen dust falls causing the flowers to come back to life. JOCK (as he pollinates) I pick some pollen up over here, sprinkle it over here, maybe a dash over there, pinch on that one...see that? Itâ€™s a little bit of magic, ainâ€™t it? The FLOWERS PERK UP as he pollinates. BARRY Wow. Thatâ€™s amazing. Why do we do that? JOCK ...thatâ€™s pollen power, Kid. More pollen, more flowers, more nectar, more honey for us. BARRY Cool. The Jock WINKS at Barry. Barry rejoins the other jocks in the sky. They swoop in over a pond, kissing the surface. We see their image reflected in the water; theyâ€™re really moving. They fly over a fountain. BUZZ Iâ€™m picking up a lot of bright yellow, could be daisies. Donâ€™t we need those? SPLITZ Copy that visual. We see what appear to be yellow flowers on a green field. \"Bee Movie\" - JS REVISIONS 8/13/07 24. They go into a deep bank and dive. BUZZ Hold on, one of these flowers seems to be on the move. SPLITZ Say again...Are you reporting a moving flower? BUZZ Affirmative. SEQ. 900 - â€œTENNIS GAMEâ€ The pollen jocks land. It is a tennis court with dozens of tennis balls. A COUPLE, VANESSA and KEN, plays tennis. The bees land right in the midst of a group of balls. KEN (O.C) That was on the line! The other bees start walking around amongst the immense, yellow globes. SPLITZ This is the coolest. What is it? They stop at a BALL on a white line and look up at it. JACKSON I donâ€™t know, but Iâ€™m loving this color. SPLITZ (smelling tennis ball) Smells good. Not like a flower. But I like it. JACKSON Yeah, fuzzy. BUZZ Chemical-y. JACKSON Careful, guys, itâ€™s a little grabby. Barry LANDS on a ball and COLLAPSES. \"Bee Movie\" - JS REVISIONS 8/13/07 25. BARRY Oh my sweet lord of bees. JACKSON Hey, candy brain, get off there! Barry attempts to pulls his legs off, but they stick. BARRY Problem! A tennis shoe and a hand ENTER FRAME. The hand picks up the ball with Barry underneath it. BARRY (CONT'D) Guys! BUZZ This could be bad. JACKSON Affirmative. Vanessa walks back to the service line, BOUNCES the ball. Each time it BOUNCES, the other bees cringe and GASP. ANGLE ON: Barry, terrified. Pure dumb luck, heâ€™s not getting squished. BARRY (with each bounce) Very close...Gonna Hurt...Mammaâ€™s little boy. SPLITZ You are way out of position, rookie. ANGLE ON: Vanessa serving. We see Barry and the ball up against the racket as she brings it back. She tosses the ball into the air; Barryâ€™s eyes widen. The ball is STRUCK, and the rally is on. KEN Coming in at you like a missile! Ken HITS the ball back. Barry feels the g-forces. ANGLE ON: The Pollen Jocks watching Barry pass by them in SLOW MOTION. \"Bee Movie\" - JS REVISIONS 8/13/07 26. BARRY (in slow motion) Help me! JACKSON You know, I don't think these are flowers. SPLITZ Should we tell him? JACKSON I think he knows. BARRY (O.S) What is this?! Vanessa HITS a high arcing lob. Ken waits, poised for the return. We see Barry having trouble maneuvering the ball from fatigue. KEN (overly confident) Match point! ANGLE ON: Ken running up. He has a killer look in his eyes. Heâ€™s going to hit the ultimate overhead smash. KEN (CONT'D) You can just start packing up Honey, because I believe youâ€™re about to eat it! ANGLE ON: Pollen Jocks. JACKSON Ahem! Ken is distracted by the jock. KEN What? No! He misses badly. The ball rockets into oblivion. Barry is still hanging on. ANGLE ON: Ken, berating himself. KEN (CONTâ€™D) Oh, you cannot be serious. We hear the ball WHISTLING, and Barry SCREAMING. \"Bee Movie\" - JS REVISIONS 8/13/07 27. BARRY Yowser!!! SEQ. 1000 - â€œSUVâ€ The ball flies through the air, and lands in the middle of the street. It bounces into the street again, and sticks in the grille of an SUV. INT. CAR ENGINE - CONTINUOUS BARRYâ€™S POV: the grille of the SUV sucks him up. He tumbles through a black tunnel, whirling vanes, and pistons. BARRY AHHHHHHHHHHH!! OHHHH!! EECHHH!! AHHHHHH!! Barry gets chilled by the A/C system, and sees a frozen grasshopper. BARRY (CONTâ€™D) (re: grasshopper) Eww, gross. CUT TO: INT. CAR - CONTINUOUS The car is packed with a typical suburban family: MOTHER, FATHER, eight-year old BOY, LITTLE GIRL in a car seat and a GRANDMOTHER. A big slobbery DOG is behind a grate. Barry pops into the passenger compartment, hitting the Motherâ€™s magazine. MOTHER Thereâ€™s a bee in the car! They all notice the bee and start SCREAMING. BARRY Aaahhhh! Barry tumbles around the car. We see the faces from his POV. MOTHER Do something! \"Bee Movie\" - JS REVISIONS 8/13/07 28. FATHER Iâ€™m driving! Barry flies by the little girl in her CAR SEAT. She waves hello. LITTLE GIRL Hi, bee. SON Heâ€™s back here! Heâ€™s going to sting me! The car SWERVES around the road. Barry flies into the back, where the slobbery dog SNAPS at him. Barry deftly avoids the jaws and gross, flying SPITTLE. MOTHER Nobody move. If you donâ€™t move, he wonâ€™t sting you. Freeze! Everyone in the car freezes. Barry freezes. They stare at each other, eyes going back and forth, waiting to see who will make the first move. Barry blinks. GRANNY He blinked! Granny pulls out a can of HAIR SPRAY. SON Spray him, Granny! Granny sprays the hair spray everywhere. FATHER What are you doing? GRANNY Itâ€™s hair spray! Extra hold! MOTHER Kill it! Barry gets sprayed back by the hair spray, then sucked out of the sunroof. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 29. EXT. CITY STREET - CONTINUOUS BARRY Wow. The tension level out here is unbelievable. Iâ€™ve got to get home. As Barry flies down the street, it starts to RAIN. He nimbly avoids the rain at first. BARRY (CONTâ€™D) Whoa. Whoa! Canâ€™t fly in rain! Canâ€™t fly in rain! Canâ€™t fly in-- A couple of drops hit him, his wings go limp and he starts falling. BARRY (CONT'D) Mayday! Mayday! Bee going down! Barry sees a window ledge and aims for it and just makes it. Shivering and exhausted, he crawls into an open window as it CLOSES. SEQ. 1100 - â€œVANESSA SAVES BARRYâ€ INT. VANESSAâ€™S APARTMENT - CONTINUOUS Inside the window, Barry SHAKES off the rain like a dog. Vanessa, Ken, Andy, and Anna ENTER the apartment. VANESSA Ken, can you close the window please? KEN Huh? Oh. (to Andy) Hey, check out my new resume. I made it into a fold-out brochure. You see? It folds out. Ken holds up his brochure, with photos of himself, and a resume in the middle. ANGLE ON: Barry hiding behind the curtains, as Ken CLOSES THE WINDOW. \"Bee Movie\" - JS REVISIONS 8/13/07 30. BARRY Oh no, more humans. I donâ€™t need this. Barry HOVERS up into the air and THROWS himself into the glass. BARRY (CONTâ€™D) (dazed) Ow! What was that? He does it again, and then multiple more times. BARRY (CONT'D) Maybe this time...this time, this time, this time, this time, this time, this time, this time. Barry JUMPS onto the drapes. BARRY (CONT'D) (out of breath) Drapes! (then, re: glass) That is diabolical. KEN Itâ€™s fantastic. Itâ€™s got all my special skills, even my top ten favorite movies. ANDY Whatâ€™s your number one? Star Wars? KEN Ah, I donâ€™t go for that, (makes Star Wars noises), kind of stuff. ANGLE ON: Barry. BARRY No wonder weâ€™re not supposed to talk to them. Theyâ€™re out of their minds. KEN When I walk out of a job interview theyâ€™re flabbergasted. They canâ€™t believe the things I say. Barry looks around and sees the LIGHT BULB FIXTURE in the middle of the ceiling. \"Bee Movie\" - JS REVISIONS 8/13/07 31. BARRY (re: light bulb) Oh, thereâ€™s the sun. Maybe thatâ€™s a way out. Barry takes off and heads straight for the light bulb. His POV: The seventy-five watt label grows as he gets closer. BARRY (CONTâ€™D) I donâ€™t remember the sun having a big seventy five on it. Barry HITS the bulb and is KNOCKED SILLY. He falls into a BOWL OF GUACAMOLE. Andy dips his chip in the guacamole, taking Barry with it. ANGLE ON: Ken and Andy. KEN Iâ€™ll tell you what. You know what? I predicted global warming. I could feel it getting hotter. At first I thought it was just me. Barryâ€™s POV: Giant human mouth opening. KEN (CONTâ€™D) Wait! Stop! Beeeeeee! ANNA Kill it! Kill it! They all JUMP up from their chairs. Andy looks around for something to use. Ken comes in for the kill with a big TIMBERLAND BOOT on each hand. KEN Stand back. These are winter boots. Vanessa ENTERS, and stops Ken from squashing Barry. VANESSA (grabs Kenâ€™s arm) Wait. Donâ€™t kill him. CLOSE UP: on Barryâ€™s puzzled face. KEN You know Iâ€™m allergic to them. This thing could kill me. \"Bee Movie\" - JS REVISIONS 8/13/07 32. VANESSA Why does his life have any less value than yours? She takes a GLASS TUMBLER and places it over Barry. KEN Why does his life have any less value than mine? Is that your statement? VANESSA Iâ€™m just saying, all life has value. You donâ€™t know what heâ€™s capable of feeling. Barry looks up through the glass and watches this conversation, astounded. Vanessa RIPS Kenâ€™s resume in half and SLIDES it under the glass. KEN (wistful) My brochure. Thereâ€™s a moment of eye contact as she carries Barry to the window. She opens it and sets him free. VANESSA There you go, little guy. KEN (O.C) Iâ€™m not scared of them. But, you know, itâ€™s an allergic thing. ANDY (O.C) * Hey, why donâ€™t you put that on your * resume-brochure? * KEN (O.C) Itâ€™s not funny, my whole face could puff up. ANDY (O.C) Make it one of your â€œSpecial Skills.â€ KEN (O.C) You know, knocking someone out is also a special skill. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 33. EXT. WINDOWSILL - CONTINUOUS Barry stares over the window frame. He canâ€™t believe whatâ€™s just happened. It is still RAINING. DISSOLVE TO: SEQ. 1200 - â€œBARRY SPEAKSâ€ EXT. WINDOWSILL - LATER Barry is still staring through the window. Inside, everyoneâ€™s saying their good-byes. KEN Vanessa, next week? Yogurt night? VANESSA Uh, yeah sure Ken. You know, whatever. KEN You can put carob chips on there. VANESSA Good night. KEN (as he exits) Supposed to be less calories, or something. VANESSA Bye. She shuts the door. Vanessa starts cleaning up. BARRY Iâ€™ve got to say something. She saved my life. Iâ€™ve got to say something. Alright, here it goes. Barry flies in. \"Bee Movie\" - JS REVISIONS 8/13/07 34. INT. VANESSAâ€™S APARTMENT - CONTINUOUS Barry hides himself on different PRODUCTS placed along the kitchen shelves. He hides on a Bumblebee Tuna can, and a â€œGreetings From Coney Islandâ€ MUSCLE-MAN POSTCARD on the fridge. BARRY (on fridge) What would I say? (landing on a bottle) I could really get in trouble. He stands looking at Vanessa. BARRY (CONT'D) Itâ€™s a bee law. Youâ€™re not supposed to talk to a human. I canâ€™t believe Iâ€™m doing this. Iâ€™ve got to. Oh, I canâ€™t do it! Come on! No, yes, no, do it! I canâ€™t. How should I start it? You like jazz? No, thatâ€™s no good. Here she comes. Speak, you fool. As Vanessa walks by, Barry takes a DEEP BREATH. BARRY (CONTâ€™D) (cheerful) Umm...hi. Vanessa DROPS A STACK OF DISHES, and HOPS BACK. BARRY (CONTâ€™D) Iâ€™m sorry. VANESSA Youâ€™re talking. BARRY Yes, I know, I know. VANESSA Youâ€™re talking. BARRY I know, Iâ€™m sorry. Iâ€™m so sorry. VANESSA Itâ€™s okay. Itâ€™s fine. Itâ€™s just, I know Iâ€™m dreaming, but I donâ€™t recall going to bed. \"Bee Movie\" - JS REVISIONS 8/13/07 35. BARRY Well, you know Iâ€™m sure this is very disconcerting. VANESSA Well yeah. I mean this is a bit of a surprise to me. I mean...youâ€™re a bee. BARRY Yeah, I am a bee, and you know Iâ€™m not supposed to be doing this, but they were all trying to kill me and if it wasnâ€™t for you...I mean, I had to thank you. Itâ€™s just the way I was raised. Vanessa intentionally JABS her hand with a FORK. VANESSA Ow! BARRY That was a little weird. VANESSA (to herself) Iâ€™m talking to a bee. BARRY Yeah. VANESSA Iâ€™m talking to a bee. BARRY Anyway... VANESSA And a bee is talking to me... BARRY I just want you to know that Iâ€™m grateful, and Iâ€™m going to leave now. VANESSA Wait, wait, wait, wait, how did you learn to do that? BARRY What? \"Bee Movie\" - JS REVISIONS 8/13/07 36. VANESSA The talking thing. BARRY Same way you did, I guess. Mama, Dada, honey, you pick it up. VANESSA Thatâ€™s very funny. BARRY Yeah. Bees are funny. If we didnâ€™t laugh, weâ€™d cry. With what we have to deal with. Vanessa LAUGHS. BARRY (CONTâ€™D) Anyway. VANESSA Can I, uh, get you something? BARRY Like what? VANESSA I donâ€™t know. I mean, I donâ€™t know. Coffee? BARRY Well, uh, I donâ€™t want to put you out. VANESSA Itâ€™s no trouble. BARRY Unless youâ€™re making anyway. VANESSA Oh, it takes two minutes. BARRY Really? VANESSA Itâ€™s just coffee. BARRY I hate to impose. \"Bee Movie\" - JS REVISIONS 8/13/07 37. VANESSA Donâ€™t be ridiculous. BARRY Actually, I would love a cup. VANESSA Hey, you want a little rum cake? BARRY I really shouldnâ€™t. VANESSA Have a little rum cake. BARRY No, no, no, I canâ€™t. VANESSA Oh, come on. BARRY You know, Iâ€™m trying to lose a couple micrograms here. VANESSA Where? BARRY Well... These stripes donâ€™t help. VANESSA You look great. BARRY I donâ€™t know if you know anything about fashion. Vanessa starts POURING the coffee through an imaginary cup and directly onto the floor. BARRY (CONT'D) Are you alright? VANESSA No. DISSOLVE TO: SEQ. 1300 - â€œROOFTOP COFFEEâ€ \"Bee Movie\" - JS REVISIONS 8/13/07 38. EXT. VANESSAâ€™S ROOF - LATER Barry and Vanessa are drinking coffee on her roof terrace. He is perched on her keychain. BARRY ...He canâ€™t get a taxi. Heâ€™s making the tie in the cab, as theyâ€™re flying up Madison. So he finally gets there. VANESSA Uh huh? BARRY He runs up the steps into the church, the wedding is on... VANESSA Yeah? BARRY ...and he says, watermelon? I thought you said Guatemalan. VANESSA Uh huh? BARRY Why would I marry a watermelon? Barry laughs. Vanessa doesnâ€™t. VANESSA Oh! Is that, uh, a bee joke? BARRY Yeah, thatâ€™s the kind of stuff that we do. VANESSA Yeah, different. A BEAT. VANESSA (CONTâ€™D) So anyway...what are you going to do, Barry? \"Bee Movie\" - JS REVISIONS 8/13/07 39. BARRY About work? I donâ€™t know. I want to do my part for the hive, but I canâ€™t do it the way they want. VANESSA I know how you feel. BARRY You do? VANESSA Sure, my parents wanted me to be a lawyer or doctor, but I wanted to be a florist. BARRY Really? VANESSA My only interest is flowers. BARRY Our new queen was just elected with that same campaign slogan. VANESSA Oh. BARRY Anyway, see thereâ€™s my hive, right there. You can see it. VANESSA Oh, youâ€™re in Sheep Meadow. BARRY (excited) Yes! You know the turtle pond? VANESSA Yes? BARRY Iâ€™m right off of that. VANESSA Oh, no way. I know that area. Do you know I lost a toe-ring there once? BARRY Really? \"Bee Movie\" - JS REVISIONS 8/13/07 40. VANESSA Yes. BARRY Why do girls put rings on their toes? VANESSA Why not? BARRY I donâ€™t know. Itâ€™s like putting a hat on your knee. VANESSA Really? Okay. A JANITOR in the background changes a LIGHTBULB. To him, it appears that Vanessa is talking to an imaginary friend. JANITOR You all right, maâ€™am? VANESSA Oh, yeah, fine. Just having two cups of coffee. BARRY Anyway, this has been great. (wiping his mouth) Thanks for the coffee. Barry gazes at Vanessa. VANESSA Oh yeah, itâ€™s no trouble. BARRY Sorry I couldnâ€™t finish it. Vanessa giggles. BARRY (CONT'D) (re: coffee) If I did, Iâ€™d be up the rest of my life. Ummm. Can I take a piece of this with me? VANESSA Sure. Here, have a crumb. She takes a CRUMB from the plate and hands it to Barry. \"Bee Movie\" - JS REVISIONS 8/13/07 41. BARRY (a little dreamy) Oh, thanks. VANESSA Yeah. There is an awkward pause. BARRY Alright, well then, I guess Iâ€™ll see you around, or not, or... VANESSA Okay Barry. BARRY And thank you so much again, for before. VANESSA Oh that? BARRY Yeah. VANESSA Oh, that was nothing. BARRY Well, not nothing, but, anyway... Vanessa extends her hand, and shakes Barryâ€™s gingerly. The Janitor watches. The lightbulb shorts out. The Janitor FALLS. CUT TO: SEQ. 1400 - â€œHONEXâ€ INT. HONEX BUILDING - NEXT DAY ANGLE ON: A TEST BEE WEARING A PARACHUTE is in a wind tunnel, hovering through increasingly heavy wind. SIGNS UNDER A FLASHING LIGHT READ: â€œTest In Progressâ€ & â€œHurricane Survival Testâ€. 2 BEES IN A LAB COATS are observing behind glass. \"Bee Movie\" - JS REVISIONS 8/13/07 42. LAB COAT BEE 1 This canâ€™t possibly work. LAB COAT BEE 2 Well, heâ€™s all set to go, we may as well try it. (into the mic) Okay Dave, pull the chute. The test bee opens his parachute. Heâ€™s instantly blown against the rear wall. Adam and Barry ENTER. ADAM Sounds amazing. BARRY Oh, it was amazing. It was the scariest, happiest moment of my life. ADAM Humans! Humans! I canâ€™t believe you were with humans! Giant scary humans! What were they like? BARRY Huge and crazy. They talk crazy, they eat crazy giant things. They drive around real crazy. ADAM And do they try and kill you like on TV? BARRY Some of them. But some of them donâ€™t. ADAM Howâ€™d you get back? BARRY Poodle. ADAM Look, you did it. And Iâ€™m glad. You saw whatever you wanted to see out there, you had your â€œexperienceâ€, and now youâ€™re back, you can pick out your job, and everything can be normal. \"Bee Movie\" - JS REVISIONS 8/13/07 43. ANGLE ON: LAB BEES examining a CANDY CORN through a microscope. BARRY Well... ADAM Well? BARRY Well, I met someone. ADAM You met someone? Was she Bee-ish? BARRY Mmm. ADAM Not a WASP? Your parents will kill you. BARRY No, no, no, not a wasp. ADAM Spider? BARRY You know, Iâ€™m not attracted to the spiders. I know to everyone else itâ€™s like the hottest thing with the eight legs and all. I canâ€™t get by that face. Barry makes a spider face. ADAM So, who is she? BARRY Sheâ€™s a human. ADAM Oh no, no, no, no. That didnâ€™t happen. You didnâ€™t do that. That is a bee law. You wouldnâ€™t break a bee law. BARRY Her nameâ€™s Vanessa. \"Bee Movie\" - JS REVISIONS 8/13/07 44. ADAM Oh, oh boy! BARRY Sheâ€™s so-o nice. And sheâ€™s a florist! ADAM Oh, no. No, no, no! Youâ€™re dating a human florist? BARRY Weâ€™re not dating. ADAM Youâ€™re flying outside the hive. Youâ€™re talking to human beings that attack our homes with power washers and M-80â€™s. Thatâ€™s 1/8 of a stick of dynamite. BARRY She saved my life. And she understands me. ADAM This is over. Barry pulls out the crumb. BARRY Eat this. Barry stuffs the crumb into Adamâ€™s face. ADAM This is not over. What was that? BARRY They call it a crumb. ADAM That was SO STINGING STRIPEY! BARRY And thatâ€™s not even what they eat. That just falls off what they eat. Do you know what a Cinnabon is? ADAM No. \"Bee Movie\" - JS REVISIONS 8/13/07 45. BARRY Itâ€™s bread... ADAM Come in here! BARRY and cinnamon, ADAM Be quiet! BARRY and frosting...they heat it up-- ADAM Sit down! INT. ADAMâ€™S OFFICE - CONTINUOUS BARRY Really hot! ADAM Listen to me! We are not them. Weâ€™re us. Thereâ€™s us and thereâ€™s them. BARRY Yes, but who can deny the heart that is yearning... Barry rolls his chair down the corridor. ADAM Thereâ€™s no yearning. Stop yearning. Listen to me. You have got to start thinking bee, my friend. ANOTHER BEE JOINS IN. ANOTHER BEE Thinking bee. WIDER SHOT AS A 3RD BEE ENTERS, popping up over the cubicle wall. 3RD BEE Thinking bee. EVEN WIDER SHOT AS ALL THE BEES JOIN IN. \"Bee Movie\" - JS REVISIONS 8/13/07 46. OTHER BEES Thinking bee. Thinking bee. Thinking bee. CUT TO: SEQ. 1500 - â€œPOOLSIDE NAGGINGâ€ EXT. BACKYARD PARENTâ€™S HOUSE - DAY Barry sits on a RAFT in a hexagon honey pool, legs dangling into the water. Janet Benson and Martin Benson stand over him wearing big, sixties sunglasses and cabana-type outfits. The sun shines brightly behind their heads. JANET BENSON (O.C) There he is. Heâ€™s in the pool. MARTIN BENSON You know what your problem is, Barry? BARRY Iâ€™ve got to start thinking bee? MARTIN BENSON Barry, how much longer is this going to go on? Itâ€™s been three days. I donâ€™t understand why youâ€™re not working. BARRY Well, Iâ€™ve got a lot of big life decisions Iâ€™m thinking about. MARTIN BENSON What life? You have no life! You have no job! Youâ€™re barely a bee! Barry throws his hands in the air. BARRY Augh. JANET BENSON Would it kill you to just make a little honey? Barry ROLLS off the raft and SINKS to the bottom of the pool. We hear his parentsâ€™ MUFFLED VOICES from above the surface. \"Bee Movie\" - JS REVISIONS 8/13/07 47. JANET BENSON (CONT'D) (muffled) Barry, come out from under there. Your fatherâ€™s talking to you. Martin, would you talk to him? MARTIN BENSON Barry, Iâ€™m talking to you. DISSOLVE TO: EXT. PICNIC AREA - DAY MUSIC: â€œSugar Sugarâ€ by the Archies. Barry and Vanessa are having a picnic. A MOSQUITO lands on Vanessaâ€™s leg. She SWATS it violently. Barryâ€™s head whips around, aghast. They stare at each other awkwardly in a frozen moment, then BURST INTO HYSTERICAL LAUGHTER. Vanessa GETS UP. VANESSA You coming? BARRY Got everything? VANESSA All set. Vanessa gets into a one-man Ultra Light plane with a black and yellow paint scheme. She puts on her helmet. BARRY You go ahead, Iâ€™ll catch up. VANESSA (come hither wink) Donâ€™t be too long. The Ultra Light takes off. Barry catches up. They fly sideby-side. VANESSA (CONTâ€™D) Watch this! Vanessa does a loop, and FLIES right into the side of a mountain, BURSTING into a huge ball of flames. \"Bee Movie\" - JS REVISIONS 8/13/07 48. BARRY (yelling, anguished) Vanessa! EXT. BARRYâ€™S PARENTâ€™S HOUSE - CONTINUOUS ANGLE ON: Barryâ€™s face bursting through the surface of the pool, GASPING for air, eyes opening in horror. MARTIN BENSON Weâ€™re still here, Barry. JANET BENSON I told you not to yell at him. He doesnâ€™t respond when you yell at him. MARTIN BENSON Then why are you yelling at me? JANET BENSON Because you donâ€™t listen. MARTIN BENSON Iâ€™m not listening to this. Barry is toweling off, putting on his sweater. BARRY Sorry Mom, Iâ€™ve got to go. JANET BENSON Where are you going? BARRY Nowhere. Iâ€™m meeting a friend. Barry JUMPS off the balcony and EXITS. JANET BENSON (calling after him) A girl? Is this why you canâ€™t decide? BARRY Bye! JANET BENSON I just hope sheâ€™s Bee-ish. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 49. SEQ. 1700 - â€œSTREETWALK/SUPERMARKETâ€ EXT. VANESSAâ€™S FLORIST SHOP - DAY Vanessa FLIPS the sign to say â€œSorry We Missed Youâ€, and locks the door. ANGLE ON: A POSTER on Vanessaâ€™s door for the Tournament of Roses Parade in Pasadena. BARRY So they have a huge parade of just flowers every year in Pasadena? VANESSA Oh, to be in the Tournament of Roses, thatâ€™s every floristâ€™s dream. Up on a float, surrounded by flowers, crowds cheering. BARRY Wow, a tournament. Do the roses actually compete in athletic events? VANESSA No. Alright, Iâ€™ve got one. How come you donâ€™t fly everywhere? BARRY Itâ€™s exhausting. Why donâ€™t you run everywhere? VANESSA Hmmm. BARRY Isnâ€™t that faster? VANESSA Yeah, okay. I see, I see. Alright, your turn. Barry and Vanessa walk/fly down a New York side street, no other pedestrians near them. BARRY Ah! Tivo. You can just freeze live TV? Thatâ€™s insane. \"Bee Movie\" - JS REVISIONS 8/13/07 50. VANESSA What, you donâ€™t have anything like that? BARRY We have Hivo, but itâ€™s a disease. Itâ€™s a horrible, horrible disease. VANESSA Oh my. They turn the corner onto a busier avenue and people start to swat at Barry. MAN Dumb bees! VANESSA You must just want to sting all those jerks. BARRY We really try not to sting. Itâ€™s usually fatal for us. VANESSA So you really have to watch your temper? They ENTER a SUPERMARKET. CUT TO: INT. SUPERMARKET BARRY Oh yeah, very carefully. You kick a wall, take a walk, write an angry letter and throw it out. You work through it like any emotion-- anger, jealousy, (under his breath) lust. Barry hops on top of some cardboard boxes in the middle of an aisle. A stock boy, HECTOR, whacks him with a rolled up magazine. VANESSA (to Barry) Oh my goodness. Are you okay? \"Bee Movie\" - JS REVISIONS 8/13/07 51. BARRY Yeah. Whew! Vanessa WHACKS Hector over the head with the magazine. VANESSA (to Hector) What is wrong with you?! HECTOR Itâ€™s a bug. VANESSA Well heâ€™s not bothering anybody. Get out of here, you creep. Vanessa pushes him, and Hector EXITS, muttering. BARRY (shaking it off) What was that, a Pick and Save circular? VANESSA Yeah, it was. How did you know? BARRY It felt like about ten pages. Seventy-fiveâ€™s pretty much our limit. VANESSA Boy, youâ€™ve really got that down to a science. BARRY Oh, we have to. I lost a cousin to Italian Vogue. VANESSA Iâ€™ll bet. Barry stops, sees the wall of honey jars. BARRY What, in the name of Mighty Hercules, is this? How did this get here? Cute Bee? Golden Blossom? Ray Liotta Private Select? VANESSA Is he that actor? \"Bee Movie\" - JS REVISIONS 8/13/07 52. BARRY I never heard of him. Why is this here? VANESSA For people. We eat it. BARRY Why? (gesturing around the market) You donâ€™t have enough food of your own? VANESSA Well yes, we-- BARRY How do you even get it? VANESSA Well, bees make it... BARRY I know who makes it! And itâ€™s hard to make it! Thereâ€™s Heating and Cooling, and Stirring...you need a whole Krelman thing. VANESSA Itâ€™s organic. BARRY Itâ€™s our-ganic! VANESSA Itâ€™s just honey, Barry. BARRY Just...what?! Bees donâ€™t know about this. This is stealing. A lot of stealing! Youâ€™ve taken our homes, our schools, our hospitals. This is all we have. And itâ€™s on sale? Iâ€™m going to get to the bottom of this. Iâ€™m going to get to the bottom of all of this! He RIPS the label off the Ray Liotta Private Select. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 53. SEQ. 1800 - â€œWINDSHIELDâ€ EXT. BACK OF SUPERMARKET LOADING DOCK - LATER THAT DAY Barry disguises himself by blacking out his yellow lines with a MAGIC MARKER and putting on some war paint. He sees Hector, the stock boy, with a knife CUTTING open cardboard boxes filled with honey jars. MAN You almost done? HECTOR Almost. Barry steps in some honey, making a SNAPPING noise. Hector stops and turns. HECTOR (CONTâ€™D) He is here. I sense it. Hector grabs his BOX CUTTER. Barry REACTS, hides himself behind the box again. HECTOR (CONTâ€™D) (talking too loud, to no one in particular) Well, I guess Iâ€™ll go home now, and just leave this nice honey out, with no one around. A BEAT. Hector pretends to exit. He takes a couple of steps in place. ANGLE ON: The honey jar. Barry steps out into a moody spotlight. BARRY Youâ€™re busted, box boy! HECTOR Ah ha! I knew I heard something. So, you can talk. Barry flies up, stinger out, pushing Hector up against the wall. As Hector backs up, he drops his knife. BARRY Oh, I can talk. And now youâ€™re going to start talking. \"Bee Movie\" - JS REVISIONS 8/13/07 54. Where are you getting all the sweet stuff? Whoâ€™s your supplier?! HECTOR I donâ€™t know what youâ€™re talking about. I thought we were all friends. The last thing we want to do is upset any of you...bees! Hector grabs a PUSHPIN. Barry fences with his stinger. HECTOR (CONTâ€™D) Youâ€™re too late. Itâ€™s ours now! BARRY You, sir, have crossed the wrong sword. HECTOR You, sir, are about to be lunch for my iguana, Ignacio! Barry and Hector get into a cross-swords, nose-to-nose confrontation. BARRY Where is the honey coming from? Barry knocks the pushpin out of his hand. Barry puts his stinger up to Hectorâ€™s nose. BARRY (CONT'D) Tell me where?! HECTOR (pointing to a truck) Honey Farms. It comes from Honey Farms. ANGLE ON: A Honey Farms truck leaving the parking lot. Barry turns, takes off after the truck through an alley. He follows the truck out onto a busy street, dodging a bus, and several cabs. CABBIE Crazy person! He flies through a metal pipe on the top of a truck. BARRY OOOHHH! \"Bee Movie\" - JS REVISIONS 8/13/07 55. BARRY (CONT'D) Barry grabs onto a bicycle messengerâ€™s backpack. The honey farms truck starts to pull away. Barry uses the bungee cord to slingshot himself towards the truck. He lands on the windshield, where the wind plasters him to the glass. He looks up to find himself surrounded by what appear to be DEAD BUGS. He climbs across, working his way around the bodies. BARRY (CONTâ€™D) Oh my. What horrible thing has happened here? Look at these faces. They never knew what hit them. And now theyâ€™re on the road to nowhere. A MOSQUITO opens his eyes. MOOSEBLOOD Pssst! Just keep still. BARRY What? Youâ€™re not dead? MOOSEBLOOD Do I look dead? Hey man, they will wipe anything that moves. Now, where are you headed? BARRY To Honey Farms. I am onto something huge here. MOOSEBLOOD Iâ€™m going to Alaska. Moose blood. Crazy stuff. Blows your head off. LADYBUG Iâ€™m going to Tacoma. BARRY (to fly) What about you? MOOSEBLOOD He really is dead. BARRY Alright. The WIPER comes towards them. \"Bee Movie\" - JS REVISIONS 8/13/07 56. MOOSEBLOOD Uh oh. BARRY What is that? MOOSEBLOOD Oh no! Itâ€™s a wiper, triple blade! BARRY Triple blade? MOOSEBLOOD Jump on. Itâ€™s your only chance, bee. They hang on as the wiper goes back and forth. MOOSEBLOOD (CONT'D) (yelling to the truck driver through the glass) Why does everything have to be so dog-gone clean?! How much do you people need to see? Open your eyes! Stick your head out the window! CUT TO: INT. TRUCK CAB SFX: Radio. RADIO VOICE For NPR News in Washington, Iâ€™m Carl Kasell. EXT. TRUCK WINDSHIELD MOOSEBLOOD But donâ€™t kill no more bugs! The Mosquito is FLUNG off of the wiper. MOOSEBLOOD (CONT'D) Beeeeeeeeeeeeee! BARRY Moose blood guy! \"Bee Movie\" - JS REVISIONS 8/13/07 57. Barry slides toward the end of the wiper, is thrown off, but he grabs the AERIAL and hangs on for dear life. Barry looks across and sees a CRICKET on another vehicle in the exact same predicament. They look at each other and SCREAM in unison. BARRY AND CRICKET Aaaaaaaaaah! ANOTHER BUG grabs onto the aerial, and screams as well. INT. TRUCK CAB - SAME TIME DRIVER You hear something? TRUCKER PASSENGER Like what? DRIVER Like tiny screaming. TRUCKER PASSENGER Turn off the radio. The driver reaches down and PRESSES a button, lowering the aerial. EXT. TRUCK WINDSHIELD - SAME TIME Barry and the other bug do a â€œchoose upâ€ to the bottom, Barry wins. BARRY Aha! Then he finally has to let go and gets thrown into the truck horn atop cab. Mooseblood is inside. MOOSEBLOOD Hey, whatâ€™s up bee boy? BARRY Hey, Blood! DISSOLVE TO: \"Bee Movie\" - JS REVISIONS 8/13/07 58. INT. TRUCK HORN - LATER BARRY ...and it was just an endless row of honey jars as far as the eye could see. MOOSEBLOOD Wow. BARRY So Iâ€™m just assuming wherever this honey truck goes, thatâ€™s where theyâ€™re getting it. I mean, that honeyâ€™s ours! MOOSEBLOOD Bees hang tight. BARRY Well, weâ€™re all jammed in there. Itâ€™s a close community. MOOSEBLOOD Not us, man. Weâ€™re on our own. Every mosquito is on his own. BARRY But what if you get in trouble? MOOSEBLOOD Trouble? You're a mosquito. You're in trouble! Nobody likes us. Theyâ€™re just all smacking. People see a mosquito, smack, smack! BARRY At least youâ€™re out in the world. You must meet a lot of girls. MOOSEBLOOD Mosquito girls try to trade up; get with a moth, dragonfly...mosquito girl donâ€™t want no mosquito. A BLOOD MOBILE pulls up alongside. MOOSEBLOOD (CONT'D) Whoa, you have got to be kidding me. Moosebloodâ€™s about to leave the building. So long bee. \"Bee Movie\" - JS REVISIONS 8/13/07 59. Mooseblood EXITS the horn, and jumps onto the blood mobile. MOOSEBLOOD (CONT'D) Hey guys. I knew Iâ€™d catch you all down here. Did you bring your crazy straws? CUT TO: SEQ. 1900 - â€œTHE APIARYâ€ EXT. APIARY - LATER Barry sees a SIGN, â€œHoney Farmsâ€ The truck comes to a stop. SFX: The Honey farms truck blares its horn. Barry flies out, lands on the hood. ANGLE ON: Two BEEKEEPERS, FREDDY and ELMO, walking around to the back of the gift shop. Barry follows them, and lands in a nearby tree FREDDY ...then we throw it in some jars, slap a label on it, and itâ€™s pretty much pure profit. BARRY What is this place? ELMO Bees got a brain the size of a pinhead. FREDDY They are pinheads. The both LAUGH. ANGLE ON: Barry REACTING. They arrive at the back of the shop where one of them opens a SMOKER BOX. FREDDY (CONTâ€™D) Hey, check out the new smoker. \"Bee Movie\" - JS REVISIONS 8/13/07 60. ELMO Oh, Sweet. Thatâ€™s the one you want. FREDDY The Thomas 3000. BARRY Smoker? FREDDY 90 puffs a minute, semi-automatic. Twice the nicotine, all the tar. They LAUGH again, nefariously. FREDDY (CONTâ€™D) Couple of breaths of this, and it knocks them right out. They make the honey, and we make the money. BARRY â€œThey make the honey, and we make the money?â€ Barry climbs onto the netting of Freddyâ€™s hat. He climbs up to the brim and looks over the edge. He sees the apiary boxes as Freddy SMOKES them. BARRY (CONT'D) Oh my. As Freddy turns around, Barry jumps into an open apiary box, and into an apartment. HOWARD and FRAN are just coming to from the smoking. BARRY (CONTâ€™D) Whatâ€™s going on? Are you okay? HOWARD Yeah, it doesnâ€™t last too long. HE COUGHS a few times. BARRY How did you two get here? Do you know youâ€™re in a fake hive with fake walls? HOWARD (pointing to a picture on the wall) \"Bee Movie\" - JS REVISIONS 8/13/07 61. Our queen was moved here, we had no choice. BARRY (looking at a picture on the wall) This is your queen? Thatâ€™s a man in womenâ€™s clothes. Thatâ€™s a dragqueen! The other wall opens. Barry sees the hundreds of apiary boxes. BARRY (CONT'D) What is this? Barry pulls out his camera, and starts snapping. BARRY (CONTâ€™D) Oh no. Thereâ€™s hundreds of them. (V.O, as Barry takes pictures) Bee honey, our honey, is being brazenly stolen on a massive scale. CUT TO: SEQ. 2100 - â€œBARRY TELLS FAMILYâ€ INT. BARRYâ€™S PARENTâ€™S HOUSE - LIVING ROOM - LATER Barry has assembled his parents, Adam, and Uncle Carl. BARRY This is worse than anything the bears have done to us. And I intend to do something about it. JANET BENSON Oh Barry, stop. MARTIN BENSON Who told you that humans are taking our honey? Thatâ€™s just a rumor. BARRY Do these look like rumors? Barry throws the PICTURES on the table. Uncle Carl, cleaning his glasses with his shirt tail, digs through a bowl of nuts with his finger. \"Bee Movie\" - JS REVISIONS 8/13/07 62. HOWARD (CONT'D) UNCLE CARL Thatâ€™s a conspiracy theory. These are obviously doctored photos. JANET BENSON Barry, how did you get mixed up in all this? ADAM (jumping up) Because heâ€™s been talking to humans! JANET BENSON Whaaat? MARTIN BENSON Talking to humans?! Oh Barry. ADAM He has a human girlfriend and they make out! JANET BENSON Make out? Barry? BARRY We do not. ADAM You wish you could. BARRY Whoâ€™s side are you on? ADAM The bees! Uncle Carl stands up and pulls his pants up to his chest. UNCLE CARL I dated a cricket once in San Antonio. Man, those crazy legs kept me up all night. Hotcheewah! JANET BENSON Barry, this is what you want to do with your life? BARRY This is what I want to do for all our lives. Nobody works harder than bees. \"Bee Movie\" - JS REVISIONS 8/13/07 63. Dad, I remember you coming home some nights so overworked, your hands were still stirring. You couldnâ€™t stop them. MARTIN BENSON Ehhh... JANET BENSON (to Martin) I remember that. BARRY What right do they have to our hardearned honey? Weâ€™re living on two cups a year. Theyâ€™re putting it in lip balm for no reason what-soever. MARTIN BENSON Even if itâ€™s true, Barry, what could one bee do? BARRY Iâ€™m going to sting them where it really hurts. MARTIN BENSON In the face? BARRY No. MARTIN BENSON In the eye? That would really hurt. BARRY No. MARTIN BENSON Up the nose? Thatâ€™s a killer. BARRY No. Thereâ€™s only one place you can sting the humans. One place where it really matters. CUT TO: SEQ. 2300 - â€œHIVE AT 5 NEWS/BEE LARRY KINGâ€ \"Bee Movie\" - JS REVISIONS 8/13/07 64. BARRY (CONT'D) INT. NEWS STUDIO - DAY DRAMATIC NEWS MUSIC plays as the opening news sequence rolls. We see the â€œHive at Fiveâ€ logo, followed by shots of past news events: A BEE freeway chase, a BEE BEARD protest rally, and a BEAR pawing at the hive as the BEES flee in panic. BOB BUMBLE (V.O.) Hive at Five, the hiveâ€™s only full hour action news source... SHOTS of NEWSCASTERS flash up on screen. BOB BUMBLE (V.O.) (CONT'D) With Bob Bumble at the anchor desk... BOB has a big shock of anchorman hair, gray temples and overly white teeth. BOB BUMBLE (V.O.) (CONT'D) ...weather with Storm Stinger, sports with Buzz Larvi, and Jeanette Chung. JEANETTE is an Asian bee. BOB BUMBLE (CONT'D) Good evening, Iâ€™m Bob Bumble. JEANETTE CHUNG And Iâ€™m Jeanette Chung. BOB BUMBLE Our top story, a tri-county bee, Barry Benson... INSERT: Barryâ€™s graduation picture. BOB BUMBLE (CONT'D) ...is saying he intends to sue the human race for stealing our honey, packaging it, and profiting from it illegally. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 65. INT. BEENN STUDIO - BEE LARRY KING LIVE BEE LARRY KING, wearing suspenders and glasses, is interviewing Barry. A LOWER-THIRD CHYRON reads: â€œBee Larry King Live.â€ BEE LARRY KING Donâ€™t forget, tomorrow night on Bee Larry King, we are going to have three former Queens all right here in our studio discussing their new book, â€œClassy Ladies,â€ out this week on Hexagon. (to Barry) Tonight, weâ€™re talking to Barry Benson. Did you ever think, Iâ€™m just a kid from the hive, I canâ€™t do this? BARRY Larry, bees have never been afraid to change the world. I mean, what about Bee-Columbus? Bee-Ghandi? Be-geesus? BEE LARRY KING Well, where Iâ€™m from you wouldnâ€™t think of suing humans. We were thinking more like stick ball, candy stores. BARRY How old are you? BEE LARRY KING I want you to know that the entire bee community is supporting you in this case, which is certain to be the trial of the bee century. BARRY Thank you, Larry. You know, they have a Larry King in the human world, too. BEE LARRY KING Itâ€™s a common name. Next week on Bee Larry King... \"Bee Movie\" - JS REVISIONS 8/13/07 66. BARRY No, I mean he looks like you. And he has a show with suspenders and different colored dots behind him. BEE LARRY KING Next week on Bee Larry King... BARRY Old guy glasses, and thereâ€™s quotes along the bottom from the guest youâ€™re watching even though you just heard them... BEE LARRY KING Bear week next week! Theyâ€™re scary, theyâ€™re hairy, and theyâ€™re here live. Bee Larry King EXITS. BARRY Always leans forward, pointy shoulders, squinty eyes... (lights go out) Very Jewish. CUT TO: SEQ. 2400 - â€œFLOWER SHOPâ€ INT. VANESSAâ€™S FLOWER SHOP - NIGHT Stacks of law books are piled up, legal forms, etc. Vanessa is talking with Ken in the other room. KEN Look, in tennis, you attack at the point of weakness. VANESSA But it was my grandmother, Ken. Sheâ€™s 81. KEN Honey, her backhandâ€™s a joke. Iâ€™m not going to take advantage of that? \"Bee Movie\" - JS REVISIONS 8/13/07 67. BARRY (O.C) Quiet please. Actual work going on here. KEN Is that that same bee? BARRY (O.C) Yes it is. VANESSA Iâ€™m helping him sue the human race. KEN What? Barry ENTERS. BARRY Oh, hello. KEN Hello Bee. Barry flies over to Vanessa. VANESSA This is Ken. BARRY Yeah, I remember you. Timberland, size 10 1/2, Vibram sole I believe. KEN Why does he talk again, Hun? VANESSA (to Ken, sensing the tension) Listen, youâ€™d better go because weâ€™re really busy working. KEN But itâ€™s our yogurt night. VANESSA (pushing him out the door) Oh...bye bye. She CLOSES the door. KEN Why is yogurt night so difficult?! \"Bee Movie\" - JS REVISIONS 8/13/07 68. Vanessa ENTERS the back room carrying coffee. VANESSA Oh you poor thing, you two have been at this for hours. BARRY Yes, and Adam here has been a huge help. ANGLE ON: A EMPTY CINNABON BOX with Adam asleep inside, covered in frosting. VANESSA How many sugars? BARRY Just one. I try not to use the competition. So, why are you helping me, anyway? VANESSA Bees have good qualities. BARRY (rowing on the sugar cube like a gondola) Si, Certo. VANESSA And it feels good to take my mind off the shop. I donâ€™t know why, instead of flowers, people are giving balloon bouquets now. BARRY Yeah, those are great...if youâ€™re 3. VANESSA And artificial flowers. BARRY (re: plastic flowers) Oh, they just get me psychotic! VANESSA Yeah, me too. BARRY The bent stingers, the pointless pollination. \"Bee Movie\" - JS REVISIONS 8/13/07 69. VANESSA Bees must hate those fake plastic things. BARRY Thereâ€™s nothing worse than a daffodil thatâ€™s had work done. VANESSA (holding up the lawsuit documents) Well, maybe this can make up for it a little bit. CUT TO: EXT. VANESSAâ€™S FLORIST SHOP They EXIT the store, and cross to the mailbox. VANESSA You know Barry, this lawsuit is a pretty big deal. BARRY I guess. VANESSA Are you sure that you want to go through with it? BARRY Am I sure? (kicking the envelope into the mailbox) When Iâ€™m done with the humans, they wonâ€™t be able to say, â€œHoney, Iâ€™m home,â€ without paying a royalty. CUT TO: SEQ. 2700 - â€œMEET MONTGOMERYâ€ EXT. MANHATTAN COURTHOUSE - DAY P.O.V SHOT - A camera feed turns on, revealing a newsperson. \"Bee Movie\" - JS REVISIONS 8/13/07 70. PRESS PERSON #2 (talking to camera) Sarah, itâ€™s an incredible scene here in downtown Manhattan where all eyes and ears of the world are anxiously waiting, because for the first time in history, weâ€™re going to hear for ourselves if a honey bee can actually speak. ANGLE ON: Barry, Vanessa, and Adam getting out of the cab. The press spots Barry and Vanessa and pushes in. Adam sits on Vanessaâ€™s shoulder. INT. COURTHOUSE - CONTINUOUS Barry, Vanessa, and Adam sit at the Plaintiffâ€™s Table. VANESSA (turns to Barry) What have we gotten into here, Barry? BARRY I donâ€™t know, but itâ€™s pretty big, isnâ€™t it? ADAM I canâ€™t believe how many humans donâ€™t have to be at work during the day. BARRY Hey, you think these billion dollar multinational food companies have good lawyers? CUT TO: EXT. COURTHOUSE STEPS - CONTINUOUS A BIG BLACK CAR pulls up. ANGLE ON: the grill filling the frame. We see the â€œL.T.Mâ€ monogram on the hood ornament. The defense lawyer, LAYTON T. MONTGOMERY comes out, squashing a bug on the pavement. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 71. INT. COURTHOUSE - CONTINUOUS Barry SHUDDERS. VANESSA Whatâ€™s the matter? BARRY I donâ€™t know. I just got a chill. Montgomery ENTERS. He walks by Barryâ€™s table shaking a honey packet. MONTGOMERY Well, if it isnâ€™t the B-Team. (re: the honey packet) Any of you boys work on this? He CHUCKLES. The JUDGE ENTERS. SEQ. 3000 - â€œWITNESSESâ€ BAILIFF All rise! The Honorable Judge Bumbleton presiding. JUDGE (shuffling papers) Alright...Case number 4475, Superior Court of New York. Barry Bee Benson vs. the honey industry, is now in session. Mr. Montgomery, you are representing the five major food companies, collectively. ANGLE ON: Montgomeryâ€™s BRIEFCASE. It has an embossed emblem of an EAGLE, holding a gavel in one talon and a briefcase in the other. MONTGOMERY A privilege. JUDGE Mr. Benson. Barry STANDS. JUDGE (CONTâ€™D) You are representing all bees of the world? \"Bee Movie\" - JS REVISIONS 8/13/07 72. Montgomery, the stenographer, and the jury lean in. CUT TO: EXT. COURTHOUSE - CONTINUOUS The spectators outside freeze. The helicopters angle forward to listen closely. CUT TO: INT. COURTHOUSE BARRY Bzzz bzzz bzzz...Ahh, Iâ€™m kidding, Iâ€™m kidding. Yes, your honor. We are ready to proceed. ANGLE ON: Courtroom hub-bub. JUDGE And Mr. Montgomery, your opening statement, please. Montgomery rises. MONTGOMERY (grumbles, clears his throat) Ladies and gentlemen of the jury. My grandmother was a simple woman. Born on a farm, she believed it was man's divine right to benefit from the bounty of nature God put before us. If we were to live in the topsy-turvy world Mr. Benson imagines, just think of what it would mean. Maybe I would have to negotiate with the silk worm for the elastic in my britches. Talking bee. How do we know this isnâ€™t some sort of holographic motion picture capture Hollywood wizardry? They could be using laser beams, robotics, ventriloquism, cloning...for all we know he could be on steroids! Montgomery leers at Barry, who moves to the stand. \"Bee Movie\" - JS REVISIONS 8/13/07 73. JUDGE Mr. Benson? Barry makes his opening statement. BARRY Ladies and Gentlemen of the jury, thereâ€™s no trickery here. Iâ€™m just an ordinary bee. And as a bee, honeyâ€™s pretty important to me. Itâ€™s important to all bees. We invented it, we make it, and we protect it with our lives. Unfortunately, there are some people in this room who think they can take whatever they want from us cause weâ€™re the little guys. And what Iâ€™m hoping is that after this is all over, youâ€™ll see how by taking our honey, youâ€™re not only taking away everything we have, but everything we are. ANGLE ON: Vanessa smiling. ANGLE ON: The BEE GALLERY wiping tears away. CUT TO: INT. BENSON HOUSE Barryâ€™s family is watching the case on TV. JANET BENSON Oh, I wish he would dress like that all the time. So nice... CUT TO: INT. COURTROOM - LATER JUDGE Call your first witness. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 74. INT. COURTHOUSE - LATER BARRY So, Mr. Klauss Vanderhayden of Honey Farms. Pretty big company you have there? MR. VANDERHAYDEN I suppose so. BARRY And I see you also own HoneyBurton, and Hon-Ron. MR. VANDERHAYDEN Yes. They provide beekeepers for our farms. BARRY Beekeeper. I find that to be a very disturbing term, I have to say. I donâ€™t imagine you employ any bee free-ers, do you? MR. VANDERHAYDEN No. BARRY Iâ€™m sorry. I couldnâ€™t hear you. MR. VANDERHAYDEN (louder) No. BARRY No. Because you donâ€™t free bees. You keep bees. And not only that, it seems you thought a bear would be an appropriate image for a jar of honey? MR. VANDERHAYDEN Well, theyâ€™re very lovable creatures. Yogi-bear, Fozzy-bear, Build-a-bear. BARRY Yeah, you mean like this?! Vanessa and the SUPERINTENDANT from her building ENTER with a GIANT FEROCIOUS GRIZZLY BEAR. He has a neck collar and chains extending from either side. \"Bee Movie\" - JS REVISIONS 8/13/07 75. By pulling the chains, they bring him directly in front of Vanderhayden. The bear LUNGES and ROARS. BARRY (CONT'D) Bears kill bees! How would you like his big hairy head crashing into your living room? Biting into your couch, spitting out your throwpillows...rowr, rowr! The bear REACTS. BEAR Rowr!! BARRY Okay, thatâ€™s enough. Take him away. Vanessa and the Superintendant pull the bear out of the courtroom. Vanderhayden TREMBLES. The judge GLARES at him. CUT TO: INT. COURTROOM- A LITTLE LATER Barry questions STING. BARRY So, Mr. Sting. Thank you for being here. Your name intrigues me, I have to say. Where have I heard it before? STING I was with a band called \"The Police\". BARRY But you've never been a police officer of any kind, have you? STING No, I haven't. \"Bee Movie\" - JS REVISIONS 8/13/07 76. BARRY No, you havenâ€™t. And so, here we have yet another example of bee culture being casually stolen by a human for nothing more than a prance-about stage name. STING Oh please. BARRY Have you ever been stung, Mr. Sting? Because I'm feeling a little stung, Sting. Or should I say, (looking in folder) Mr. Gordon M. Sumner? The jury GASPS. MONTGOMERY (to his aides) Thatâ€™s not his real name? You idiots! CUT TO: INT. COURTHOUSE- LATER BARRY Mr. Liotta, first may I offer my belated congratulations on your Emmy win for a guest spot on E.R. in 2005. LIOTTA Thank you. Thank you. Liotta LAUGHS MANIACALLY. BARRY I also see from your resume that youâ€™re devilishly handsome, but with a churning inner turmoil thatâ€™s always ready to blow. LIOTTA I enjoy what I do. Is that a crime? \"Bee Movie\" - JS REVISIONS 8/13/07 77. BARRY Not yet it isnâ€™t. But is this what itâ€™s come to for you, Mr. Liotta? Exploiting tiny helpless bees so you donâ€™t have to rehearse your part, and learn your lines, Sir? LIOTTA Watch it Benson, I could blow right now. BARRY This isnâ€™t a goodfella. This is a badfella! LIOTTA (exploding, trying to smash Barry with the Emmy) Why doesnâ€™t someone just step on this little creep and we can all go home? Youâ€™re all thinking it. Say it! JUDGE Order! Order in this courtroom! A MONTAGE OF NEWSPAPER HEADLINES FOLLOWS: NEW YORK POST: â€œBees to Humans: Buzz Offâ€. NEW YORK TELEGRAM: â€œSue Beeâ€. DAILY VARIETY: â€œStudio Dumps Liotta Project. Slams Door on Unlawful Entry 2.â€ CUT TO: SEQ. 3175 - â€œCANDLELIGHT DINNERâ€ INT. VANESSAâ€™S APARTMENT Barry and Vanessa are having a candle light dinner. Visible behind Barry is a â€œLITTLE MISSYâ€ SET BOX, with the flaps open. BARRY Well, I just think that was awfully nice of that bear to pitch in like that. \"Bee Movie\" - JS REVISIONS 8/13/07 78. VANESSA Iâ€™m telling you, I think the juryâ€™s on our side. BARRY Are we doing everything right...you know, legally? VANESSA Iâ€™m a florist. BARRY Right, right. Barry raises his glass. BARRY (CONTâ€™D) Well, hereâ€™s to a great team. VANESSA To a great team. They toast. Ken ENTERS KEN Well hello. VANESSA Oh...Ken. BARRY Hello. VANESSA I didnâ€™t think you were coming. KEN No, I was just late. I tried to call. But, (holding his cell phone) the battery... VANESSA I didnâ€™t want all this to go to waste, so I called Barry. Luckily he was free. BARRY Yeah. KEN (gritting his teeth) Oh, that was lucky. \"Bee Movie\" - JS REVISIONS 8/13/07 79. VANESSA Well, thereâ€™s still a little left. I could heat it up. KEN Yeah, heat it up. Sure, whatever. Vanessa EXITS. Ken and Barry look at each other as Barry eats. BARRY So, I hear youâ€™re quite a tennis player. Iâ€™m not much for the game myself. I find the ball a little grabby. KEN Thatâ€™s where I usually sit. Right there. VANESSA (O.C) Ken, Barry was looking at your resume, and he agreed with me that â€œeating with chopsticksâ€ isnâ€™t really a special skill. KEN (to Barry) You think I donâ€™t see what youâ€™re doing? BARRY Hey look, I know how hard it is trying to find the right job. We certainly have that in common. KEN Do we? BARRY Well, bees have 100% employment, of course. But we do jobs like taking the crud out. KEN Thatâ€™s just what I was thinking about doing. Ken holds his table knife up. It slips out of his hand. He goes under the table to pick it up. \"Bee Movie\" - JS REVISIONS 8/13/07 80. VANESSA Ken, I let Barry borrow your razor for his fuzz. I hope that was alright. Ken hits his head on the table. BARRY Iâ€™m going to go drain the old stinger. KEN Yeah, you do that. Barry EXITS to the bathroom, grabbing a small piece of a VARIETY MAGAZINE on the way. BARRY Oh, look at that. Ken slams the champagne down on the table. Ken closes his eyes and buries his face in his hands. He grabs a magazine on the way into the bathroom. SEQ. 2800 - â€œBARRY FIGHTS KENâ€ INT. BATHROOM - CONTINUOUS Ken ENTERS, closes the door behind him. Heâ€™s not happy. Barry is washing his hands. He glances back at Ken. KEN You know, Iâ€™ve just about had it with your little mind games. BARRY Whatâ€™s that? KEN Italian Vogue. BARRY Mamma Mia, thatâ€™s a lot of pages. KEN Itâ€™s a lot of ads. BARRY Remember what Van said. Why is your life any more valuable than mine? \"Bee Movie\" - JS REVISIONS 8/13/07 81. KEN Itâ€™s funny, I just canâ€™t seem to recall that! Ken WHACKS at Barry with the magazine. He misses and KNOCKS EVERYTHING OFF THE VANITY. Ken grabs a can of AIR FRESHENER. KEN (CONT'D) I think something stinks in here. He sprays at Barry. BARRY I love the smell of flowers. KEN Yeah? How do you like the smell of flames? Ken lights the stream. BARRY Not as much. Barry flies in a circle. Ken, trying to stay with him, spins in place. ANGLE ON: Flames outside the bathroom door. Ken slips on the Italian Vogue, falls backward into the shower, pulling down the shower curtain. The can hits him in the head, followed by the shower curtain rod, and the rubber duck. Ken reaches back, grabs the handheld shower head. He whips around, looking for Barry. ANGLE ON: A WATERBUG near the drain. WATERBUG Waterbug. Not taking sides. Barry is on the toilet tank. He comes out from behind a shampoo bottle, wearing a chapstick cap as a helmet. BARRY Ken, look at me! Iâ€™m wearing a chapstick hat. This is pathetic. ANGLE ON: Ken turning the hand shower nozzle from â€œGENTLEâ€, to â€œTURBOâ€, to â€œLETHALâ€. \"Bee Movie\" - JS REVISIONS 8/13/07 82. KEN Iâ€™ve got issues! Ken fires the water at Barry, knocking him into the toilet. The items from the vanity (emory board, lipstick, eye curler, etc.) are on the toilet seat. Ken looks down at Barry. KEN (CONT'D) Well well well, a royal flush. BARRY Youâ€™re bluffing. KEN Am I? Ken flushes the toilet. Barry grabs the Emory board and uses it to surf. He puts his hand in the water while heâ€™s surfing. Some water splashes on Ken. BARRY Surfâ€™s up, dude! KEN Awww, poo water! He does some skate board-style half-pipe riding. Barry surfs out of the toilet. BARRY That bowl is gnarly. Ken tries to get a shot at him with the toilet brush. KEN Except for those dirty yellow rings. Vanessa ENTERS. VANESSA Kenneth! What are you doing? KEN You know what? I donâ€™t even like honey! I donâ€™t eat it! VANESSA We need to talk! \"Bee Movie\" - JS REVISIONS 8/13/07 83. She pulls Ken out by his ear. Ken glares at Barry. CUT TO: INT. HALLWAY - CONTINUOUS VANESSA Heâ€™s just a little bee. And he happens to be the nicest bee Iâ€™ve met in a long time. KEN Long time? What are you talking about? Are there other bugs in your life? VANESSA No, but there are other things bugging me in life. And youâ€™re one of them! KEN Fine! Talking bees, no yogurt night...my nerves are fried from riding on this emotional rollercoaster. VANESSA Goodbye, Ken. KEN Augh! VANESSA Whew! Ken EXITS, then re-enters frame. KEN And for your information, I prefer sugar-free, artificial sweeteners, made by man! He EXITS again. The DOOR SLAMS behind him. VANESSA (to Barry) Iâ€™m sorry about all that. Ken RE-ENTERS. \"Bee Movie\" - JS REVISIONS 8/13/07 84. KEN I know itâ€™s got an aftertaste! I like it! BARRY (re: Ken) I always felt there was some kind of barrier between Ken and me. (puts his hands in his pockets) I couldnâ€™t overcome it. Oh well. VANESSA Are you going to be okay for the trial tomorrow? BARRY Oh, I believe Mr. Montgomery is about out of ideas. CUT TO: SEQ. 3300 - â€œADAM STINGS MONTYâ€ INT. COURTROOM - NEXT DAY ANGLE ON: Medium shot of Montgomery standing at his table. MONTGOMERY We would like to call Mr. Barry Benson Bee to the stand. ADAM (whispering to Vanessa) Now thatâ€™s a good idea. (to Barry) You can really see why heâ€™s considered one of the very best lawyers-- Oh. Barry rolls his eyes. He gets up, takes the stand. A juror in a striped shirt APPLAUDS. MR. GAMMIL (whispering) Layton, youâ€™ve got to weave some magic with this jury, or itâ€™s going to be all over. Montgomery is holding a BOOK, â€œThe Secret Life of Beesâ€. \"Bee Movie\" - JS REVISIONS 8/13/07 85. MONTGOMERY (confidently whispering) Oh, donâ€™t worry Mr. Gammil. The only thing I have to do to turn this jury around is to remind them of what they donâ€™t like about bees. (to Gammil) You got the tweezers? Mr. Gammil NODS, and pats his breast pocket. MR. GAMMIL Are you allergic? MONTGOMERY Only to losing, son. Only to losing. Montgomery approaches the stand. MONTGOMERY (CONTâ€™D) Mr. Benson Bee. Iâ€™ll ask you what I think weâ€™d all like to know. What exactly is your relationship to that woman? Montgomery points to Vanessa. BARRY Weâ€™re friends. MONTGOMERY Good friends? BARRY Yes. MONTGOMERY (softly in Barryâ€™s face) How good? BARRY What? MONTGOMERY Do you live together? BARRY Wait a minute, this isnâ€™t about-- \"Bee Movie\" - JS REVISIONS 8/13/07 86. MONTGOMERY Are you her little... (clearing throat) ... bed bug? BARRY (flustered) Hey, thatâ€™s not the kind of-- MONTGOMERY Iâ€™ve seen a bee documentary or two. Now, from what I understand, doesnâ€™t your Queen give birth to all the bee children in the hive? BARRY Yeah, but-- MONTGOMERY So those arenâ€™t even your real parents! ANGLE ON: Barryâ€™s parents. MARTIN BENSON Oh, Barry. BARRY Yes they are! ADAM Hold me back! Vanessa holds him back with a COFFEE STIRRER. Montgomery points to Barryâ€™s parents. MONTGOMERY Youâ€™re an illegitimate bee, arenâ€™t you Benson? ADAM Heâ€™s denouncing bees! All the bees in the courtroom start to HUM. Theyâ€™re agitated. MONTGOMERY And donâ€™t yâ€™all date your cousins? \"Bee Movie\" - JS REVISIONS 8/13/07 87. VANESSA (standing, letting go of Adam) Objection! Adam explodes from the table and flies towards Montgomery. ADAM Iâ€™m going to pin cushion this guy! Montgomery turns around and positions himself by the judgeâ€™s bench. He sticks his butt out. Montgomery winks at his team. BARRY Adam, donâ€™t! Itâ€™s what he wants! Adam shoves Barry out of the way. Adam STINGS Montgomery in the butt. The jury REACTS, aghast. MONTGOMERY Ow! Iâ€™m hit! Oh, lordy, I am hit! The judge BANGS her gavel. JUDGE Order! Order! Please, Mr. Montgomery. MONTGOMERY The venom! The venom is coursing through my veins! I have been felled by a wing-ed beast of destruction. You see? You canâ€™t treat them like equals. Theyâ€™re strip-ed savages! Stingingâ€™s the only thing they know! Itâ€™s their way! ANGLE ON: Adam, collapsed on the floor. Barry rushes to his side. BARRY Adam, stay with me. ADAM I canâ€™t feel my legs. Montgomery falls on the Bailiff. BAILIFF Take it easy. \"Bee Movie\" - JS REVISIONS 8/13/07 88. MONTGOMERY Oh, what angel of mercy will come forward to suck the poison from my heaving buttocks? The JURY recoils. JUDGE Please, I will have order in this court. Order! Order, please! FADE TO: SEQ. 3400 - â€œADAM AT HOSPITALâ€ INT. HOSPITAL - STREET LEVEL ROOM - DAY PRESS PERSON #1 (V.O) The case of the honey bees versus the human race took a pointed turn against the bees yesterday, when one of their legal team stung Layton T. Montgomery. Now hereâ€™s Don with the 5-day. A NURSE lets Barry into the room. Barry CARRIES a FLOWER. BARRY Thank you. Barry stands over Adam, in a bed. Barry lays the flower down next to him. The TV is on. BARRY (CONT'D) Hey buddy. ADAM Hey. BARRY Is there much pain? Adam has a BEE-SIZED PAINKILLER HONEY BUTTON near his head that he presses. ADAM (pressing the button) Yeah...I blew the whole case, didnâ€™t I? \"Bee Movie\" - JS REVISIONS 8/13/07 89. BARRY Oh, it doesnâ€™t matter. The important thing is youâ€™re alive. You could have died. ADAM Iâ€™d be better off dead. Look at me. Adam THROWS the blanket off his lap, revealing a GREEN SANDWICH SWORD STINGER. ADAM (CONTâ€™D) (voice cracking) They got it from the cafeteria, they got it from downstairs. In a tuna sandwich. Look, thereâ€™s a little celery still on it. BARRY What was it like to sting someone? ADAM I canâ€™t explain it. It was all adrenaline...and then...ecstasy. Barry looks at Adam. BARRY Alright. ADAM You think that was all a trap? BARRY Of course. Iâ€™m sorry. I flew us right into this. What were we thinking? Look at us, weâ€™re just a couple of bugs in this world. ADAM What do you think the humans will do to us if they win? BARRY I donâ€™t know. ADAM I hear they put the roaches in motels. That doesnâ€™t sound so bad. \"Bee Movie\" - JS REVISIONS 8/13/07 90. BARRY Adam, they check in, but they donâ€™t check out. Adam GULPS. ADAM Oh my. ANGLE ON: the hospital window. We see THREE PEOPLE smoking outside on the sidewalk. The smoke drifts in. Adam COUGHS. ADAM (CONTâ€™D) Say, could you get a nurse to close that window? BARRY Why? ADAM The smoke. Bees donâ€™t smoke. BARRY Right. Bees donâ€™t smoke. Bees donâ€™t smoke! But some bees are smoking. Adam, thatâ€™s it! Thatâ€™s our case. Adam starts putting his clothes on. ADAM It is? Itâ€™s not over? BARRY No. Get up. Get dressed. Iâ€™ve got to go somewhere. You get back the court and stall. Stall anyway you can. CUT TO: SEQ. 3500 - â€œSMOKING GUNâ€ INT. COURTROOM - THE NEXT DAY Adam is folding a piece of paper into a boat. ADAM ...and assuming youâ€™ve done step 29 correctly, youâ€™re ready for the tub. \"Bee Movie\" - JS REVISIONS 8/13/07 91. ANGLE ON: The jury, all with paper boats of their own. JURORS Ooh. ANGLE ON: Montgomery frustrated with Gammil, whoâ€™s making a boat also. Monty crumples Gammilâ€™s boat, and throws it at him. JUDGE Mr. Flayman? ADAM Yes? Yes, Your Honor? JUDGE Where is the rest of your team? ADAM (fumbling with his swordstinger) Well, your honor, itâ€™s interesting. You know Bees are trained to fly kind of haphazardly and as a result quite often we donâ€™t make very good time. I actually once heard a pretty funny story about a bee-- MONTGOMERY Your Honor, havenâ€™t these ridiculous bugs taken up enough of this courtâ€™s valuable time? Montgomery rolls out from behind his table. Heâ€™s suspended in a LARGE BABY CHAIR with wheels. MONTGOMERY (CONT'D) How much longer are we going to allow these absurd shenanigans to go on? They have presented no compelling evidence to support their charges against my clients who have all run perfectly legitimate businesses. I move for a complete dismissal of this entire case. JUDGE Mr. Flayman, I am afraid I am going to have to consider Mr. Montgomeryâ€™s motion. \"Bee Movie\" - JS REVISIONS 8/13/07 92. ADAM But you canâ€™t. We have a terrific case. MONTGOMERY Where is your proof? Where is the evidence? Show me the smoking gun. Barry bursts through the door. BARRY Hold it, your honor. You want a smoking gun? Here is your smoking gun. Vanessa ENTERS, holding a bee smoker Vanessa slams the beekeeper's SMOKER onto the judgeâ€™s bench. JUDGE What is that? BARRY Itâ€™s a Bee smoker. Montgomery GRABS the smoker. MONTGOMERY What, this? This harmless little contraption? This couldnâ€™t hurt a fly, let alone a bee. He unintentionally points it towards the bee gallery, KNOCKING THEM ALL OUT. The jury GASPS. The press SNAPS pictures of them. BARRY Members of the jury, look at what has happened to bees who have never been asked, \"Smoking or Non?\" Is this what nature intended for us? To be forcibly addicted to these smoke machines in man-made wooden slat work camps? Living out our lives as honey slaves to the white man? Barry gestures dramatically towards Montgomery's racially mixed table. The BLACK LAWYER slowly moves his chair away. GAMMIL What are we going to do? \"Bee Movie\" - JS REVISIONS 8/13/07 93. MONTGOMERY (to Pross) He's playing the species card. Barry lands on the scale of justice, by the judgeâ€™s bench. It balances as he lands. BARRY Ladies and gentlemen, please, FreeThese-Bees! ANGLE ON: Jury, chanting \"Free the bees\". JUDGE The court finds in favor of the bees. The chaos continues. Barry flies over to Vanessa, with his hand up for a â€œhigh 5â€. BARRY Vanessa, we won! VANESSA Yay! I knew you could do it. Highfive! She high 5â€™s Barry, sending him crashing to the table. He bounces right back up. VANESSA (CONT'D) Oh, sorry. BARRY Ow!! Iâ€™m okay. Vanessa, do you know what this means? All the honey is finally going to belong to the bees. Now we wonâ€™t have to work so hard all the time. Montgomery approaches Barry, surrounded by the press. The cameras and microphones go to Montgomery. MONTGOMERY (waving a finger) This is an unholy perversion of the balance of nature, Benson! Youâ€™ll regret this. ANGLE ON: Barryâ€™s â€˜deer in headlightsâ€™ expression, as the press pushes microphones in his face. \"Bee Movie\" - JS REVISIONS 8/13/07 94. PRESS PERSON 1 Barry, how much honey do you think is out there? BARRY Alright, alright, one at a time... SARAH Barry, who are you wearing? BARRY Uhhh, my sweater is Ralph Lauren, and I have no pants. The Press follows Barry as he EXITS. ANGLE ON: Adam and Vanessa. ADAM (putting papers away) What if Montgomeryâ€™s right? VANESSA What do you mean? ADAM Weâ€™ve been living the bee way a long time. 27 million years. DISSOLVE TO: SEQ. 3600 - â€œHONEY ROUNDUPâ€ EXT. HONEY FARMS APIARY - MONTAGE SARAH (V.O) Congratulations on your victory. What are you going to demand as a settlement? BARRY (V.O) (over montage) First, weâ€™re going to demand a complete shutdown of all bee work camps. Then, we want to get back all the honey that was ours to begin with. Every last drop. We demand an end to the glorification of the bear as anything more than a filthy, smelly, big-headed, bad breath, stink-machine. \"Bee Movie\" - JS REVISIONS 8/13/07 95. I believe weâ€™re all aware of what they do in the woods. We will no longer tolerate derogatory beenegative nick-names, unnecessary inclusion of honey in bogus health products, and la-dee-da tea-time human snack garnishments. MONTAGE IMAGES: Close-up on an ATF JACKET, with the YELLOW LETTERS. Camera pulls back. We see an ARMY OF BEE AND HUMAN AGENTS wearing hastily made â€œAlcohol, Tobacco, Firearms, and Honeyâ€ jackets. Barry supervises. The gate to Honey Farms is locked permanently. All the smokers are collected and locked up. All the bees leave the Apiary. CUT TO: EXT. ATF OUTSIDE OF SUPERMARKET - MONTAGE Agents begin YANKING honey off the supermarket shelves, and out of shopping baskets. CUT TO: EXT. NEW HIVE CITY - MONTAGE The bees tear down a honey-bear statue. CUT TO: EXT. YELLOWSTONE FOREST - MONTAGE POV of a sniperâ€™s crosshairs. An animated BEAR character looka-like, turns his head towards camera. BARRY Wait for my signal. ANGLE ON: Barry lowering his binoculars. BARRY (CONT'D) Take him out. The sniper SHOOTS the bear. It hits him in the shoulder. The bear looks at it. He gets woozy and the honey jar falls out of his lap, an ATF&H agent catches it. \"Bee Movie\" - JS REVISIONS 8/13/07 96. BARRY (V.O) (CONT'D) ATF&H AGENT (to the bearâ€™s pig friend) Heâ€™ll have a little nausea for a few hours, then heâ€™ll be fine. CUT TO: EXT. STINGâ€™S HOUSE - MONTAGE ATF&H agents SLAP CUFFS on Sting, who is meditating. STING But itâ€™s just a prance-about stage name! CUT TO: INT. A WOMANâ€™S SHOWER - MONTAGE A WOMAN is taking a shower, and using honey shampoo. An ATF&H agent pulls the shower curtain aside, and grabs her bottle of shampoo. The woman SCREAMS. The agent turns to the 3 other agents, and Barry. ANGLE ON: Barry looking at the label on the shampoo bottle, shaking his head and writing in his clipboard. CUT TO: EXT. SUPERMARKET CAFE - MONTAGE Another customer, an old lady having her tea with a little jar of honey, gets her face pushed down onto the table and turned to the side by two agents. One of the agents has a gun on her. OLD LADY Canâ€™t breathe. CUT TO: EXT. CENTRAL PARK - MONTAGE An OIL DRUM of honey is connected to Barryâ€™s hive. \"Bee Movie\" - JS REVISIONS 8/13/07 97. BARRY Bring it in, boys. CUT TO: SEQ. 3650 - â€œNO MORE WORKâ€ INT. HONEX - MONTAGE ANGLE ON: The honey goes past the 3-cup hash-mark, and begins to overflow. A WORKER BEE runs up to Buzzwell. WORKER BEE 1 Mr. Buzzwell, we just passed 3 cups, and thereâ€™s gallons mores coming. I think we need to shutdown. KEYCHAIN BEE (to Buzzwell) Shutdown? Weâ€™ve never shutdown. ANGLE ON: Buzzwell overlooking the factory floor. BUZZWELL Shutdown honey production! Stop making honey! ANGLE ON: TWO BEES, each with a KEY. BUZZWELL (CONTâ€™D) Turn your key, Sir! They turn the keys simultaneously, War Games-style, shutting down the honey machines. ANGLE ON: the Taffy-Pull machine, Centrifuge, and Krelman all slowly come to a stop. The bees look around, bewildered. WORKER BEE 5 What do we do now? A BEAT. WORKER BEE 6 Cannon ball!! He jumps into a HONEY VAT, doesnâ€™t penetrate the surface. He looks around, and slowly sinks down to his waist. \"Bee Movie\" - JS REVISIONS 8/13/07 98. EXT. HONEX FACTORY THE WHISTLE BLOWS, and the bees all stream out the exit. CUT TO: INT. J-GATE - CONTINUOUS Lou Loduca gives orders to the pollen jocks. LOU LODUCA Weâ€™re shutting down honey production. Mission abort. CUT TO: EXT. CENTRAL PARK Jackson receives the orders, mid-pollination. JACKSON Aborting pollination and nectar detail. Returning to base. CUT TO: EXT. NEW HIVE CITY ANGLE ON: Bees, putting sun-tan lotion on their noses and antennae, and sunning themselves on the balconies of the gyms. CUT TO: EXT. CENTRAL PARK ANGLE ON: THE FLOWERS starting to DROOP. CUT TO: INT. J-GATE J-Gate is deserted. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 99. EXT. NEW HIVE CITY ANGLE ON: Bees sunning themselves. A TIMER DINGS, and they all turn over. CUT TO: EXT. CENTRAL PARK TIME LAPSE of Central Park turning brown. CUT TO: EXT. VANESSAâ€™S FLORIST SHOP CLOSE-UP SHOT: Vanessa writes â€œSorry. No more flowers.â€ on a â€œClosedâ€ sign, an turns it facing out. CUT TO: SEQ. 3700 - â€œIDLE HIVEâ€ EXT. NEW HIVE CITY - DAY Barry flies at high speed. TRACKING SHOT into the hive, through the lobby of Honex, and into Adamâ€™s office. CUT TO: INT. ADAMâ€™S OFFICE - CONTINUOUS Barry meets Adam in his office. Adamâ€™s office is in disarray. There are papers everywhere. Heâ€™s filling up his cardboard hexagon box. BARRY (out of breath) Adam, you wouldnâ€™t believe how much honey was out there. ADAM Oh yeah? BARRY Whatâ€™s going on around here? Where is everybody? Are they out celebrating? \"Bee Movie\" - JS REVISIONS 8/13/07 100. ADAM (exiting with a cardboard box of belongings) No, theyâ€™re just home. They donâ€™t know what to do. BARRY Hmmm. ADAM Theyâ€™re laying out, theyâ€™re sleeping in. I heard your Uncle Carl was on his way to San Antonio with a cricket. BARRY At least we got our honey back. They walk through the empty factory. ADAM Yeah, but sometimes I think, so what if the humans liked our honey? Who wouldnâ€™t? Itâ€™s the greatest thing in the world. I was excited to be a part of making it. ANGLE ON: Adamâ€™s desk on itâ€™s side in the hall. ADAM (CONTâ€™D) This was my new desk. This was my new job. I wanted to do it really well. And now...and now I canâ€™t. Adam EXITS. CUT TO: SEQ. 3900 - â€œWORLD WITHOUT BEESâ€ INT. STAIRWELL Vanessa and Barry are walking up the stairs to the roof. BARRY I donâ€™t understand why theyâ€™re not happy. We have so much now. I thought their lives would be better. \"Bee Movie\" - JS REVISIONS 8/13/07 101. VANESSA Hmmm. BARRY Theyâ€™re doing nothing. Itâ€™s amazing, honey really changes people. VANESSA You donâ€™t have any idea whatâ€™s going on, do you? BARRY What did you want to show me? VANESSA This. They reach the top of the stairs. Vanessa opens the door. CUT TO: EXT. VANESSAâ€™S ROOFTOP - CONTINUOUS Barry sees Vanessaâ€™s flower pots and small garden have all turned brown. BARRY What happened here? VANESSA That is not the half of it... Vanessa turns Barry around with her two fingers, revealing the view of Central Park, which is also all brown. BARRY Oh no. Oh my. Theyâ€™re all wilting. VANESSA Doesnâ€™t look very good, does it? BARRY No. VANESSA And whoâ€™s fault do you think that is? \"Bee Movie\" - JS REVISIONS 8/13/07 102. BARRY Mmmm...you know, Iâ€™m going to guess, bees. VANESSA Bees? BARRY Specifically me. I guess I didnâ€™t think that bees not needing to make honey would affect all these other things. VANESSA And itâ€™s not just flowers. Fruits, vegetables...they all need bees. BARRY Well, thatâ€™s our whole SAT test right there. VANESSA So, you take away the produce, that affects the entire animal kingdom. And then, of course... BARRY The human species? VANESSA (clearing throat) Ahem! BARRY Oh. So, if thereâ€™s no more pollination, it could all just go south here, couldnâ€™t it? VANESSA And I know this is also partly my fault. Barry takes a long SIGH. BARRY How about a suicide pact? VANESSA (not sure if heâ€™s joking) How would we do it? BARRY Iâ€™ll sting you, you step on me. \"Bee Movie\" - JS REVISIONS 8/13/07 103. VANESSA That just kills you twice. BARRY Right, right. VANESSA Listen Barry. Sorry but Iâ€™ve got to get going. She EXITS. BARRY (looking out over the park) Had to open my mouth and talk... (looking back) Vanessa..? Vanessa is gone. CUT TO: SEQ. 3935 - â€œGOING TO PASADENAâ€ EXT. NY STREET - CONTINUOUS Vanessa gets into a cab. Barry ENTERS. BARRY Vanessa. Why are you leaving? Where are you going? VANESSA To the final Tournament of Roses parade in Pasadena. They moved it up to this weekend because all the flowers are dying. Itâ€™s the last chance Iâ€™ll ever have to see it. BARRY Vanessa, I just want to say Iâ€™m sorry. I never meant it to turn out like this. VANESSA I know. Me neither. Vanessa cab drives away. \"Bee Movie\" - JS REVISIONS 8/13/07 104. BARRY (chuckling to himself) Tournament of Roses. Roses canâ€™t do sports. Wait a minute...roses. Roses? Roses!? Vanessa! Barry follows shortly after. He catches up to it, and he pounds on the window. Barry follows shortly after Vanessaâ€™s cab. He catches up to it, and he pounds on the window. INT. TAXI - CONTINUOUS Barry motions for her to roll the window down. She does so. BARRY Roses?! VANESSA Barry? BARRY (as he flies next to the cab) Roses are flowers. VANESSA Yes, they are. BARRY Flowers, bees, pollen! VANESSA I know. Thatâ€™s why this is the last parade. BARRY Maybe not. The cab starts pulling ahead of Barry. BARRY (CONT'D) (re: driver) Could you ask him to slow down? VANESSA Could you slow down? The cabs slows. Barry flies in the window, and lands in the change box, which closes on him. \"Bee Movie\" - JS REVISIONS 8/13/07 105. VANESSA (CONT'D) Barry! Vanessa lets him out. Barry stands on the change box, in front of the driverâ€™s license. BARRY Okay, I made a huge mistake! This is a total disaster, and itâ€™s all my fault! VANESSA Yes, it kind of is. BARRY Iâ€™ve ruined the planet. And, I wanted to help with your flower shop. Instead, Iâ€™ve made it worse. VANESSA Actually, itâ€™s completely closed down. BARRY Oh, I thought maybe you were remodeling. Nonetheless, I have another idea. And itâ€™s greater than all my previous great ideas combined. VANESSA I donâ€™t want to hear it. Vanessa closes the change box on Barry. BARRY (opening it again) Alright, hereâ€™s what Iâ€™m thinking. They have the roses, the roses have the pollen. I know every bee, plant, and flower bud in this park. All weâ€™ve got to do is get what theyâ€™ve got back here with what weâ€™ve got. VANESSA Bees... BARRY Park... VANESSA Pollen... \"Bee Movie\" - JS REVISIONS 8/13/07 106. BARRY Flowers... VANESSA Repollination! BARRY (on luggage handle, going up) Across the nation! CUT TO: SEQ. 3950 - â€œROSE PARADEâ€ EXT. PASADENA PARADE BARRY (V.O) Alright. Tournament of Roses. Pasadena, California. Theyâ€™ve got nothing but flowers, floats, and cotton candy. Security will be tight. VANESSA I have an idea. CUT TO: EXT. FLOAT STAGING AREA ANGLE ON: Barry and Vanessa approaching a HEAVILY ARMED GUARD in front of the staging area. VANESSA Vanessa Bloome, FTD. Official floral business. He leans in to look at her badge. She SNAPS IT SHUT, VANESSA (CONTâ€™D) Oh, itâ€™s real. HEAVILY ARMED GUARD Sorry maâ€™am. Thatâ€™s a nice brooch, by the way. VANESSA Thank you. It was a gift. \"Bee Movie\" - JS REVISIONS 8/13/07 107. They ENTER the staging area. BARRY (V.O) Then, once weâ€™re inside, we just pick the right float. VANESSA How about the Princess and the Pea? BARRY Yeah. VANESSA I can be the princess, and-- BARRY ...yes, I think-- VANESSA You could be-- BARRY Iâ€™ve-- VANESSA The pea. BARRY Got it. CUT TO: EXT. FLOAT STAGING AREA - A FEW MOMENTS LATER Barry, dressed as a PEA, flies up and hovers in front of the princess on the â€œPrincess and the Peaâ€ float. The float is sponsored by Inflat-a-bed and a SIGN READS: â€œInflat-a-bed: If it blows, itâ€™s ours.â€ BARRY Sorry Iâ€™m late. Where should I sit? PRINCESS What are you? BARRY I believe Iâ€™m the pea. PRINCESS The pea? Itâ€™s supposed to be under the mattresses. \"Bee Movie\" - JS REVISIONS 8/13/07 108. BARRY Not in this fairy tale, sweetheart. PRINCESS Iâ€™m going to go talk to the marshall. BARRY You do that. This whole parade is a fiasco! She EXITS. Vanessa removes the step-ladder. The princess FALLS. Barry and Vanessa take off in the float. BARRY (CONTâ€™D) Letâ€™s see what this baby will do. ANGLE ON: Guy with headset talking to drivers. HEADSET GUY Hey! The float ZOOMS by. A young CHILD in the stands, TIMMY, cries. CUT TO: EXT. FLOAT STAGING AREA - A FEW MOMENTS LATER ANGLE ON: Vanessa putting the princess hat on. BARRY (V.O) Then all we do is blend in with traffic, without arousing suspicion. CUT TO: EXT. THE PARADE ROUTE - CONTINUOUS The floats go flying by the crowds. Barry and Vanessaâ€™s float CRASHES through the fence. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 109. EXT. LA FREEWAY Vanessa and Barry speed, dodging and weaving, down the freeway. BARRY (V.O) And once weâ€™re at the airport, thereâ€™s no stopping us. CUT TO: EXT. LAX AIRPORT Barry and Vanessa pull up to the curb, in front of an TSA AGENT WITH CLIPBOARD. TSA AGENT Stop. Security. Did you and your insect pack your own float? VANESSA (O.C) Yes. TSA AGENT Has this float been in your possession the entire time? VANESSA (O.C) Since the parade...yes. ANGLE ON: Barry holding his shoes. TSA AGENT Would you remove your shoes and everything in your pockets? Can you remove your stinger, Sir? BARRY Thatâ€™s part of me. TSA AGENT I know. Just having some fun. Enjoy your flight. CUT TO: EXT. RUNWAY Barry and Vanessaâ€™s airplane TAKES OFF. \"Bee Movie\" - JS REVISIONS 8/13/07 110. BARRY (O.C) Then, if weâ€™re lucky, weâ€™ll have just enough pollen to do the job. DISSOLVE TO: SEQ. 4025 - â€œCOCKPIT FIGHTâ€ INT. AIRPLANE Vanessa is on the aisle. Barry is on a laptop calculating flowers, pollen, number of bees, airspeed, etc. He does a â€œStompâ€ dance on the keyboard. BARRY Can you believe how lucky we are? We have just enough pollen to do the job. I think this is going to work, Vanessa. VANESSA Itâ€™s got to work. PILOT (V.O) Attention passengers. This is Captain Scott. Iâ€™m afraid we have a bit of bad weather in the New York area. And looks like weâ€™re going to be experiencing a couple of hours delay. VANESSA Barry, these are cut flowers with no water. Theyâ€™ll never make it. BARRY Iâ€™ve got to get up there and talk to these guys. VANESSA Be careful. Barry flies up to the cockpit door. CUT TO: INT. COCKPIT - CONTINUOUS A female flight attendant, ANGELA, is in the cockpit with the pilots. \"Bee Movie\" - JS REVISIONS 8/13/07 111. Thereâ€™s a KNOCK at the door. BARRY (C.O) Hey, can I get some help with this Sky Mall Magazine? Iâ€™d like to order the talking inflatable travel pool filter. ANGELA (to the pilots, irritated) Excuse me. CUT TO: EXT. CABIN - CONTINUOUS Angela opens the cockpit door and looks around. She doesnâ€™t see anybody. ANGLE ON: Barry hidden on the yellow and black â€œcautionâ€ stripe. As Angela looks around, Barry zips into the cockpit. CUT TO: INT. COCKPIT BARRY Excuse me, Captain. I am in a real situation here... PILOT (pulling an earphone back, to the co-pilot) What did you say, Hal? CO-PILOT I didnâ€™t say anything. PILOT (he sees Barry) Ahhh! Bee! BARRY No, no! Donâ€™t freak out! Thereâ€™s a chance my entire species-- CO-PILOT (taking off his earphones) Ahhh! \"Bee Movie\" - JS REVISIONS 8/13/07 112. The pilot grabs a â€œDUSTBUSTERâ€ vacuum cleaner. He aims it around trying to vacuum up Barry. The co-pilot faces camera, as the pilot tries to suck Barry up. Barry is on the other side of the co-pilot. As they dosey-do, the toupee of the co-pilot begins to come up, still attached to the front. CO-PILOT (CONT'D) What are you doing? Stop! The toupee comes off the co-pilotâ€™s head, and sticks in the Dustbuster. Barry runs across the bald head. BARRY Wait a minute! Iâ€™m an attorney! CO-PILOT Whoâ€™s an attorney? PILOT Donâ€™t move. The pilot uses the Dustbuster to try and mash Barry, who is hovering in front of the co-pilotâ€™s nose, and knocks out the co-pilot who falls out of his chair, hitting the life raft release button. The life raft inflates, hitting the pilot, knocking him into a wall and out cold. Barry surveys the situation. BARRY Oh, Barry. CUT TO: INT. AIRPLANE CABIN Vanessa studies her laptop, looking serious. SFX: PA CRACKLE. BARRY (V.O) (in captain voice) Good afternoon passengers, this is your captain speaking. Would a Miss Vanessa Bloome in 24F please report to the cockpit. And please hurry! \"Bee Movie\" - JS REVISIONS 8/13/07 113. ANGLE ON: The aisle, and Vanessa head popping up. CUT TO: INT. COCKPIT Vanessa ENTERS. VANESSA What happened here? BARRY I tried to talk to them, but then there was a Dustbuster, a toupee, a life raft exploded...Now oneâ€™s bald, oneâ€™s in a boat, and theyâ€™re both unconscious. VANESSA Is that another bee joke? BARRY No. No oneâ€™s flying the plane. The AIR TRAFFIC CONTROLLER, BUD, speaks over the radio. BUD This is JFK control tower. Flight 356, whatâ€™s your status? Vanessa presses a button, and the intercom comes on. VANESSA This is Vanessa Bloome. Iâ€™m a florist from New York. BUD Whereâ€™s the pilot? VANESSA Heâ€™s unconscious and so is the copilot. BUD Not good. Is there anyone onboard who has flight experience? A BEAT. BARRY As a matter of fact, there is. \"Bee Movie\" - JS REVISIONS 8/13/07 114. BUD Whoâ€™s that? VANESSA Barry Benson. BUD From the honey trial? Oh great. BARRY Vanessa, this is nothing more than a big metal bee. Itâ€™s got giant wings, huge engines. VANESSA I canâ€™t fly a plane. BARRY Why not? Isnâ€™t John Travolta a pilot? VANESSA Yes? BARRY How hard could it be? VANESSA Wait a minute. Barry, weâ€™re headed into some lightning. CUT TO: Vanessa shrugs, and takes the controls. SEQ. 4150 - â€œBARRY FLIES PLANEâ€ INT. BENSON HOUSE The family is all huddled around the TV at the Benson house. ANGLE ON: TV. Bob Bumble is broadcasting. BOB BUMBLE This is Bob Bumble. We have some late-breaking news from JFK airport, where a very suspenseful scene is developing. Barry Benson, fresh off his stunning legal victory... \"Bee Movie\" - JS REVISIONS 8/13/07 115. Adam SPRAYS a can of HONEY-WHIP into his mouth. ADAM Thatâ€™s Barry. BOB BUMBLE ...is now attempting to land a plane, loaded with people, flowers, and an incapacitated flight crew. EVERYONE Flowers?! CUT TO: INT. AIR TRAFFIC CONTROL TOWER BUD Well, we have an electrical storm in the area, and two individuals at the controls of a jumbo jet with absolutely no flight experience. JEANETTE CHUNG Just a minute, Mr. Ditchwater, thereâ€™s a honey bee on that plane. BUD Oh, Iâ€™m quite familiar with Mr. Bensonâ€™s work, and his no-account compadres. Havenâ€™t they done enough damage already? JEANETTE CHUNG But isnâ€™t he your only hope right now? BUD Come on, technically a bee shouldnâ€™t be able to fly at all. CUT TO: INT. COCKPIT. Barry REACTS BUD The wings are too small, their bodies are too big-- \"Bee Movie\" - JS REVISIONS 8/13/07 116. BARRY (over PA) Hey, hold on a second. Havenâ€™t we heard this million times? The surface area of the wings, and the body mass doesnâ€™t make sense? JEANETTE CHUNG Get this on the air. CAMERAMAN You got it! CUT TO: INT. BEE TV CONTROL ROOM An engineer throws a switch. BEE ENGINEER Stand by. Weâ€™re going live. The â€œON AIRâ€ sign illuminates. CUT TO: INT. VARIOUS SHOTS OF NEW HIVE CITY The news report plays on TV. The pollen jocks are sitting around, playing paddle-ball, Wheel-o, and one of them is spinning his helmet on his finger. Buzzwell is in an office cubicle, playing computer solitaire. Barryâ€™s family and Adam watch from their living room. Bees sitting on the street curb turn around to watch the TV. BARRY Mr. Ditchwater, the way we work may be a mystery to you, because making honey takes a lot of bees doing a lot of small jobs. But let me tell you something about a small job. If you do it really well, it makes a big difference. More than we realized. To us, to everyone. Thatâ€™s why I want to get bees back to doing what we do best. \"Bee Movie\" - JS REVISIONS 8/13/07 117. Working together. Thatâ€™s the bee way. Weâ€™re not made of Jello. We get behind a fellow. Black and yellow. CROWD OF BEES Hello! CUT TO: INT. COCKPIT Barry is giving orders to Vanessa. BARRY Left, right, down, hover. VANESSA Hover? BARRY Forget hover. VANESSA You know what? This isnâ€™t so hard. Vanessa pretends to HONK THE HORN. VANESSA (CONTâ€™D) Beep, beep! Beep, beep! A BOLT OF LIGHTNING HITS the plane. The plane takes a sharp dip. VANESSA (CONTâ€™D) Barry, what happened? BARRY (noticing the control panel) Wait a minute. I think we were on autopilot that whole time. VANESSA That may have been helping me. BARRY And now weâ€™re not! VANESSA (V.O.) (folding her arms) Well, then it turns out I cannot fly a plane. \"Bee Movie\" - JS REVISIONS 8/13/07 118. BARRY (CONT'D) Vanessa struggles with the yoke. CUT TO: EXT. AIRPLANE The airplane goes into a steep dive. CUT TO: SEQ. 4175 - â€œCRASH LANDINGâ€ INT. J-GATE An ALERT SIGN READING: â€œHive Alert. We Need:â€ Then the SIGNAL goes from â€œTwo Beesâ€ â€œSome Beesâ€ â€œEvery Bee There Isâ€ Lou Loduca gathers the pollen jocks at J-Gate. LOU LODUCA All of you, letâ€™s get behind this fellow. Move it out! The bees follow Lou Loduca, and EXIT J-Gate. CUT TO: INT. AIRPLANE COCKPIT BARRY Our only chance is if I do what I would do, and you copy me with the wings of the plane! VANESSA You donâ€™t have to yell. BARRY Iâ€™m not yelling. We happen to be in a lot of trouble here. VANESSA Itâ€™s very hard to concentrate with that panicky tone in your voice. BARRY Itâ€™s not a tone. Iâ€™m panicking! CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 119. EXT. JFK AIRPORT ANGLE ON: The bees arriving and massing at the airport. CUT TO: INT. COCKPIT Barry and Vanessa alternately SLAP EACH OTHER IN THE FACE. VANESSA I donâ€™t think I can do this. BARRY Vanessa, pull yourself together. Listen to me, you have got to snap out of it! VANESSA You snap out of it! BARRY You snap out of it! VANESSA You snap out of it! BARRY You snap out of it! VANESSA You snap out of it! CUT TO: EXT. AIRPLANE A GIGANTIC SWARM OF BEES flies in to hold the plane up. CUT TO: INT. COCKPIT - CONTINUOUS BARRY You snap out of it! VANESSA You snap out of it! \"Bee Movie\" - JS REVISIONS 8/13/07 120. BARRY You snap-- VANESSA Hold it! BARRY (about to slap her again) Why? Come on, itâ€™s my turn. VANESSA How is the plane flying? Barryâ€™s antennae ring. BARRY I donâ€™t know. (answering) Hello? CUT TO: EXT. AIRPLANE ANGLE ON: The underside of the plane. The pollen jocks have massed all around the underbelly of the plane, and are holding it up. LOU LODUCA Hey Benson, have you got any flowers for a happy occasion in there? CUT TO: INT. COCKPIT Lou, Buzz, Splitz, and Jackson come up alongside the cockpit. BARRY The pollen jocks! VANESSA They do get behind a fellow. BARRY Black and yellow. LOU LODUCA (over headset) Hello. \"Bee Movie\" - JS REVISIONS 8/13/07 121. Alright you two, what do you say we drop this tin can on the blacktop? VANESSA What blacktop? Where? I canâ€™t see anything. Can you? BARRY No, nothing. Itâ€™s all cloudy. CUT TO: EXT. RUNWAY Adam SHOUTS. ADAM Come on, youâ€™ve got to think bee, Barry. Thinking bee, thinking bee. ANGLE ON: Overhead shot of runway. The bees are in the formation of a flower. In unison they move, causing the flower to FLASH YELLOW AND BLACK. BEES (chanting) Thinking bee, thinking bee. CUT TO: INT. COCKPIT We see through the swirling mist and clouds. A GIANT SHAPE OF A FLOWER is forming in the middle of the runway. BARRY Wait a minute. I think Iâ€™m feeling something. VANESSA What? BARRY I donâ€™t know, but itâ€™s strong. And itâ€™s pulling me, like a 27 million year old instinct. Bring the nose of the plane down. \"Bee Movie\" - JS REVISIONS 8/13/07 122. LOU LODUCA (CONT'D) EXT. RUNWAY All the bees are on the runway chanting â€œThinking Beeâ€. CUT TO: INT. CONTROL TOWER RICK What in the world is on the tarmac? ANGLE ON: Dave OTS onto runway seeing a flower being formed by millions of bees. BUD Get some lights on that! CUT TO: EXT. RUNWAY ANGLE ON: AIRCRAFT LANDING LIGHT SCAFFOLD by the side of the runway, illuminating the bees in their flower formation. INT. COCKPIT BARRY Vanessa, aim for the flower! VANESSA Oh, okay? BARRY Cut the engines! VANESSA Cut the engines? BARRY Weâ€™re going in on bee power. Ready boys? LOU LODUCA Affirmative. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 123. INT. AIRPLANE COCKPIT BARRY Good, good, easy now. Land on that flower! Ready boys? Give me full reverse. LOU LODUCA Spin it around! The plane attempts to land on top of an â€œAloha Airlinesâ€ plane with flowers painted on it. BARRY (V.O) I mean the giant black and yellow pulsating flower made of millions of bees! VANESSA Which flower? BARRY That flower! VANESSA Iâ€™m aiming at the flower! The plane goes after a FAT GUY IN A HAWAIIAN SHIRT. BARRY (V.O) Thatâ€™s a fat guy in a flowered shirt! The other other flower! The big one. He snaps a photo and runs away. BARRY (CONT'D) Full forward. Ready boys? Nose down. Bring your tail up. Rotate around it. VANESSA Oh, this is insane, Barry. BARRY This is the only way I know how to fly. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 124. AIR TRAFFIC CONTROL TOWER BUD Am I koo-koo kachoo, or is this plane flying in an insect-like pattern? CUT TO: EXT. RUNWAY BARRY (V.O) Get your nose in there. Donâ€™t be afraid of it. Smell it. Full reverse! Easy, just drop it. Be a part of it. Aim for the center! Now drop it in. Drop it in, woman! The plane HOVERS and MANEUVERS, landing in the center of the giant flower, like a bee. The FLOWERS from the cargo hold spill out onto the runway. INT. AIPLANE CABIN The passengers are motionless for a beat. PASSENGER Come on already! They hear the â€œding dingâ€, and all jump up to grab their luggage out of the overheads. SEQ. 4225 - â€œRUNWAY SPEECHâ€ EXT. RUNWAY - CONTINUOUS The INFLATABLE SLIDES pop out the side of the plane. The passengers escape. Barry and Vanessa slide down out of the cockpit. Barry and Vanessa exhale a huge breath. VANESSA Barry, we did it. You taught me how to fly. Vanessa raises her hand up for a high five. \"Bee Movie\" - JS REVISIONS 8/13/07 125. BARRY Yes. No high five. VANESSA Right. ADAM Barry, it worked. Did you see the giant flower? BARRY What giant flower? Where? Of course I saw the flower! That was genius, man. Genius! ADAM Thank you. BARRY But weâ€™re not done yet. Barry flies up to the wing of the plane, and addresses the bee crowd. BARRY (CONTâ€™D) Listen everyone. This runway is covered with the last pollen from the last flowers available anywhere on Earth. That means this is our last chance. Weâ€™re the only ones who make honey, pollinate flowers, and dress like this. If weâ€™re going to survive as a species, this is our moment. So what do you all say? Are we going to be bees, or just Museum of Natural History key chains? BEES Weâ€™re bees! KEYCHAIN BEE Keychain! BARRY Then follow me... Except Keychain. BUZZ Hold on Barry. Youâ€™ve earned this. Buzz puts a pollen jock jacket and helmet with Barryâ€™s name on it on Barry. \"Bee Movie\" - JS REVISIONS 8/13/07 126. BARRY Iâ€™m a pollen jock! (looking at the jacket. The sleeves are a little long) And itâ€™s a perfect fit. All Iâ€™ve got to do are the sleeves. The Pollen Jocks toss Barry a gun. BARRY (CONTâ€™D) Oh yeah! ANGLE ON: Martin and Janet Benson. JANET BENSON Thatâ€™s our Barry. All the bees descend upon the flowers on the tarmac, and start collecting pollen. CUT TO: SEQ. 4250 - â€œRE-POLLINATIONâ€ EXT. SKIES - CONTINUOUS The squadron FLIES over the city, REPOLLINATING trees and flowers as they go. Barry breaks off from the group, towards Vanessaâ€™s flower shop. CUT TO: EXT. VANESSAâ€™S FLOWER SHOP - CONTINUOUS Barry REPOLLINATES Vanessaâ€™s flowers. CUT TO: EXT. CENTRAL PARK - CONTINUOUS ANGLE ON: Timmy with a frisbee, as the bees fly by. TIMMY Mom, the bees are back! \"Bee Movie\" - JS REVISIONS 8/13/07 127. Central Park is completely repollinated by the bees. DISSOLVE TO: INT. HONEX - CONTINUOUS Honex is back to normal and everyone is busily working. ANGLE ON: Adam, putting his Krelman hat on. ADAM If anyone needs to make a call, nowâ€™s the time. Iâ€™ve got a feeling weâ€™ll be working late tonight! The bees CHEER. CUT TO: SEQ. 4355 EXT: VANESSAâ€™S FLOWER SHOP With a new sign out front. â€œVanessa & Barry: Flowers, Honey, Legal Adviceâ€ DISSOLVE TO: INT: FLOWER COUNTER Vanessa doing a brisk trade with many customers. CUT TO: INT: FLOWER SHOP - CONTINUOUS Vanessa is selling flowers. In the background, there are SHELVES STOCKED WITH HONEY. VANESSA (O.C.) Donâ€™t forget these. Have a great afternoon. Yes, can I help whoâ€™s next? Whoâ€™s next? Would you like some honey with that? It is beeapproved. SIGN ON THE BACK ROOM DOOR READS: â€œBarry Benson: Insects at Lawâ€. \"Bee Movie\" - JS REVISIONS 8/13/07 128. Camera moves into the back room. ANGLE ON: Barry. ANGLE ON: Barryâ€™s COW CLIENT. COW Milk, cream, cheese...itâ€™s all me. And I donâ€™t see a nickel. BARRY Uh huh? Uh huh? COW (breaking down) Sometimes I just feel like a piece of meat. BARRY I had no idea. VANESSA Barry? Iâ€™m sorry, have you got a moment? BARRY Would you excuse me? My mosquito associate here will be able to help you. Mooseblood ENTERS. MOOSEBLOOD Sorry Iâ€™m late. COW Heâ€™s a lawyer too? MOOSEBLOOD Maâ€™am, I was already a bloodsucking parasite. All I needed was * a briefcase. * ANGLE ON: Flower Counter. VANESSA (to customer) Have a great afternoon! (to Barry) Barry, I just got this huge tulip order for a wedding, and I canâ€™t get them anywhere. \"Bee Movie\" - JS REVISIONS 8/13/07 129. BARRY Not a problem, Vannie. Just leave it to me. Vanessa turns back to deal with a customer. VANESSA Youâ€™re a life-saver, Barry. (to the next customer) Can I help whoâ€™s next? Whoâ€™s next? ANGLE ON: Vanessa smiling back at Barry. Barry smiles too, then snaps himself out of it. BARRY (speaks into his antennae) Alright. Scramble jocks, itâ€™s time to fly! VANESSA Thank you, Barry! EXT. FLOWER SHOP - CONTINUOUS ANGLE ON: Ken and Andy walking down the street. KEN (noticing the new sign) Augh! What in the world? Itâ€™s that bee again! ANDY (guiding Ken protectively) Let it go, Kenny. KEN That bee is living my life! When will this nightmare end? ANDY Let it all go. They donâ€™t break stride. ANGLE ON: Camera in front of Barry as he flies out the door and up into the sky. Pollen jocks fold in formation behind him as they zoom into the park. BARRY (to Splitz) Beautiful day to fly. \"Bee Movie\" - JS REVISIONS 8/13/07 130. JACKSON Sure is. BARRY Between you and me, I was dying to get out of that office. FADE OUT: \"Bee Movie\" - JS REVISIONS 8/13/07 131.",
    "stars": 161,
    "forks": 31,
    "language": "Python",
    "url": "https://github.com/danderfer/Comp_Sci_Sem_2",
    "topics": [],
    "created_at": "2022-06-07T20:15:11Z",
    "updated_at": "2025-11-24T16:42:09Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# Comp_Sci_Sem_2\nAccording to all known laws of aviation, there is no way that a bee should be able to fly. Its wings are too small to get its fat little body off the ground. The bee, of course, flies anyway. Because bees donâ€™t care what humans think is impossible.â€ SEQ. 75 - â€œINTRO TO BARRYâ€ INT. BENSON HOUSE - DAY ANGLE ON: Sneakers on the ground. Camera PANS UP to reveal BARRY BENSONâ€™S BEDROOM ANGLE ON: Barryâ€™s hand flipping through different sweaters in his closet. BARRY Yellow black, yellow black, yellow black, yellow black, yellow black, yellow black...oohh, black and yellow... ANGLE ON: Barry wearing the sweater he picked, looking in the mirror. BARRY (CONTâ€™D) Yeah, letâ€™s shake it up a little. He picks the black and yellow one. He then goes to the sink, takes the top off a CONTAINER OF HONEY, and puts some honey into his hair. He squirts some in his mouth and gargles. Then he takes the lid off the bottle, and rolls some on like deodorant. CUT TO: INT. BENSON HOUSE KITCHEN - CONTINUOUS Barryâ€™s mother, JANET BENSON, yells up at Barry. JANET BENSON Barry, breakfast is ready! CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 1. INT. BARRYâ€™S ROOM - CONTINUOUS BARRY Coming! SFX: Phone RINGING. Barryâ€™s antennae vibrate as they RING like a phone. Barryâ€™s hands are wet. He looks around for a towel. BARRY (CONTâ€™D) Hang on a second! He wipes his hands on his sweater, and pulls his antennae down to his ear and mouth. BARRY (CONT'D) Hello? His best friend, ADAM FLAYMAN, is on the other end. ADAM Barry? BARRY Adam? ADAM Can you believe this is happening? BARRY Canâ€™t believe it. Iâ€™ll pick you up. Barry sticks his stinger in a sharpener. SFX: BUZZING AS HIS STINGER IS SHARPENED. He tests the sharpness with his finger. SFX: Bing. BARRY (CONTâ€™D) Looking sharp. ANGLE ON: Barry hovering down the hall, sliding down the staircase bannister. Barryâ€™s mother, JANET BENSON, is in the kitchen. JANET BENSON Barry, why donâ€™t you use the stairs? Your father paid good money for those. \"Bee Movie\" - JS REVISIONS 8/13/07 2. BARRY Sorry, Iâ€™m excited. Barryâ€™s father, MARTIN BENSON, ENTERS. Heâ€™s reading a NEWSPAPER with the HEADLINE, â€œQueen gives birth to thousandtuplets: Resting Comfortably.â€ MARTIN BENSON Hereâ€™s the graduate. Weâ€™re very proud of you, Son. And a perfect report card, all Bâ€™s. JANET BENSON (mushing Barryâ€™s hair) Very proud. BARRY Ma! Iâ€™ve got a thing going here. Barry re-adjusts his hair, starts to leave. JANET BENSON Youâ€™ve got some lint on your fuzz. She picks it off. BARRY Ow, thatâ€™s me! MARTIN BENSON Wave to us. Weâ€™ll be in row 118,000. Barry zips off. BARRY Bye! JANET BENSON Barry, I told you, stop flying in the house! CUT TO: SEQ. 750 - DRIVING TO GRADUATION EXT. BEE SUBURB - MORNING A GARAGE DOOR OPENS. Barry drives out in his CAR. \"Bee Movie\" - JS REVISIONS 8/13/07 3. ANGLE ON: Barryâ€™s friend, ADAM FLAYMAN, standing by the curb. Heâ€™s reading a NEWSPAPER with the HEADLINE: â€œFrisbee Hits Hive: Internet Down. Bee-stander: â€œI heard a sound, and next thing I knew...wham-o!.â€ Barry drives up, stops in front of Adam. Adam jumps in. BARRY Hey, Adam. ADAM Hey, Barry. (pointing at Barryâ€™s hair) Is that fuzz gel? BARRY A little. Itâ€™s a special day. Finally graduating. ADAM I never thought Iâ€™d make it. BARRY Yeah, three days of grade school, three days of high school. ADAM Those were so awkward. BARRY Three days of college. Iâ€™m glad I took off one day in the middle and just hitchhiked around the hive. ADAM You did come back different. They drive by a bee whoâ€™s jogging. ARTIE Hi Barry! BARRY (to a bee pedestrian) Hey Artie, growing a mustache? Looks good. Barry and Adam drive from the suburbs into the city. ADAM Hey, did you hear about Frankie? \"Bee Movie\" - JS REVISIONS 8/13/07 4. BARRY Yeah. ADAM You going to his funeral? BARRY No, Iâ€™m not going to his funeral. Everybody knows you sting someone you die, you donâ€™t waste it on a squirrel. He was such a hot head. ADAM Yeah, I guess he couldâ€™ve just gotten out of the way. The DRIVE through a loop de loop. BARRY AND ADAM Whoa...Whooo...wheee!! ADAM I love this incorporating the amusement park right into our regular day. BARRY I guess thatâ€™s why they say we donâ€™t need vacations. CUT TO: SEQ. 95 - GRADUATION EXT. GRADUATION CEREMONY - CONTINUOUS Barry and Adam come to a stop. They exit the car, and fly over the crowd to their seats. * BARRY * (re: graduation ceremony) * Boy, quite a bit of pomp...under * the circumstances. * They land in their seats. BARRY (CONTâ€™D) Well Adam, today we are men. \"Bee Movie\" - JS REVISIONS 8/13/07 5. ADAM We are. BARRY Bee-men. ADAM Amen! BARRY Hallelujah. Barry hits Adamâ€™s forehead. Adam goes into the rapture. An announcement comes over the PA. ANNOUNCER (V.O) Students, faculty, distinguished bees...please welcome, Dean Buzzwell. ANGLE ON: DEAN BUZZWELL steps up to the podium. The podium has a sign that reads: â€œWelcome Graduating Class of:â€, with train-station style flipping numbers after it. BUZZWELL Welcome New Hive City graduating class of... The numbers on the podium change to 9:15. BUZZWELL (CONTâ€™D) ...9:15. (he clears his throat) And that concludes our graduation ceremonies. And begins your career at Honex Industries. BARRY Are we going to pick our job today? ADAM I heard itâ€™s just orientation. The rows of chairs change in transformer-like mechanical motion to Universal Studios type tour trams. Buzzwell walks off stage. BARRY (re: trams) Whoa, heads up! Here we go. \"Bee Movie\" - JS REVISIONS 8/13/07 6. SEQ. 125 - â€œFACTORYâ€ FEMALE VOICE (V.O) Keep your hands and antennas inside the tram at all times. (in Spanish) Dejen las manos y antennas adentro del tram a todos tiempos. BARRY I wonder what itâ€™s going to be like? ADAM A little scary. Barry shakes Adam. BARRY AND ADAM AAHHHH! The tram passes under SIGNS READING: â€œHonex: A Division of Honesco: A Part of the Hexagon Group.â€ TRUDY Welcome to Honex, a division of Honesco, and a part of the Hexagon group. BARRY This is it! The Honex doors OPEN, revealing the factory. BARRY (CONTâ€™D) Wow. TRUDY We know that you, as a bee, have worked your whole life to get to the point where you can work for your whole life. Honey begins when our valiant pollen jocks bring the nectar to the hive where our top secret formula is automatically color-corrected, scent adjusted and bubble contoured into this... Trudy GRABS a TEST TUBE OF HONEY from a technician. \"Bee Movie\" - JS REVISIONS 8/13/07 7. TRUDY (CONTâ€™D) ...soothing, sweet syrup with its distinctive golden glow, you all know as... EVERYONE ON THE TRAM (in unison) H-o-n-e-y. Trudy flips the flask into the crowd, and laughs as they all scramble for it. ANGLE ON: A GIRL BEE catching the honey. ADAM (sotto) That girl was hot. BARRY (sotto) Sheâ€™s my cousin. ADAM She is? BARRY Yes, weâ€™re all cousins. ADAM Right. Youâ€™re right. TRUDY At Honex, we also constantly strive to improve every aspect of bee existence. These bees are stress testing a new helmet technology. ANGLE ON: A STUNT BEE in a HELMET getting hit with a NEWSPAPER, then a SHOE, then a FLYSWATTER. He gets up, and gives a â€œthumbâ€™s upâ€. The graduate bees APPLAUD. ADAM (re: stunt bee) What do you think he makes? BARRY Not enough. TRUDY And here we have our latest advancement, the Krelman. \"Bee Movie\" - JS REVISIONS 8/13/07 8. BARRY Wow, what does that do? TRUDY Catches that little strand of honey that hangs after you pour it. Saves us millions. ANGLE ON: The Krelman machine. Bees with hand-shaped hats on, rotating around a wheel to catch drips of honey. Adamâ€™s hand shoots up. ADAM Can anyone work on the Krelman? TRUDY Of course. Most bee jobs are small ones. But bees know that every small job, if itâ€™s done well, means a lot. There are over 3000 different bee occupations. But choose carefully, because youâ€™ll stay in the job that you pick for the rest of your life. The bees CHEER. ANGLE ON: Barryâ€™s smile dropping slightly. BARRY The same job for the rest of your life? I didnâ€™t know that. ADAM Whatâ€™s the difference? TRUDY And youâ€™ll be happy to know that bees as a species havenâ€™t had one day off in 27 million years. BARRY So youâ€™ll just work us to death? TRUDY (laughing) Weâ€™ll sure try. Everyone LAUGHS except Barry. \"Bee Movie\" - JS REVISIONS 8/13/07 9. The tram drops down a log-flume type steep drop. Cameras flash, as all the bees throw up their hands. The frame freezes into a snapshot. Barry looks concerned. The tram continues through 2 doors. FORM DISSOLVE TO: SEQ. 175 - â€œWALKING THE HIVEâ€ INT. HONEX LOBBY ANGLE ON: The log-flume photo, as Barry looks at it. ADAM Wow. That blew my mind. BARRY (annoyed) â€œWhatâ€™s the difference?â€ Adam, how could you say that? One job forever? Thatâ€™s an insane choice to have to make. ADAM Well, Iâ€™m relieved. Now we only have to make one decision in life. BARRY But Adam, how could they never have told us that? ADAM Barry, why would you question anything? Weâ€™re bees. Weâ€™re the most perfectly functioning society on Earth. They walk by a newspaper stand with A SANDWICH BOARD READING: â€œBee Goes Berserk: Stings Seven Then Self.â€ ANGLE ON: A BEE filling his carâ€™s gas tank from a honey pump. He fills his car some, then takes a swig for himself. NEWSPAPER BEE (to the bee guzzling gas) Hey! Barry and Adam begin to cross the street. \"Bee Movie\" - JS REVISIONS 8/13/07 10. BARRY Yeah but Adam, did you ever think that maybe things work a little too well around here? They stop in the middle of the street. The traffic moves perfectly around them. ADAM Like what? Give me one example. BARRY (thinks) ...I donâ€™t know. But you know what Iâ€™m talking about. They walk off. SEQ. 400 - â€œMEET THE JOCKSâ€ SFX: The SOUND of Pollen Jocks. PAN DOWN from the Honex statue. J-GATE ANNOUNCER Please clear the gate. Royal Nectar Force on approach. Royal Nectar Force on approach. BARRY Wait a second. Check it out. Hey, hey, those are Pollen jocks. ADAM Wow. FOUR PATROL BEES FLY in through the hiveâ€™s giant Gothic entrance. The Patrol Bees are wearing fighter pilot helmets with black visors. ADAM (CONTâ€™D) Iâ€™ve never seen them this close. BARRY They know what itâ€™s like to go outside the hive. ADAM Yeah, but some of them donâ€™t come back. \"Bee Movie\" - JS REVISIONS 8/13/07 11. The nectar from the pollen jocks is removed from their backpacks, and loaded into trucks on their way to Honex. A SMALL CROWD forms around the Patrol Bees. Each one has a PIT CREW that takes their nectar. Lou Loduca hurries a pit crew along: LOU LODUCA You guys did great! Youâ€™re monsters. Youâ€™re sky freaks! I love it! I love it! SCHOOL GIRLS are jumping up and down and squealing nearby. BARRY I wonder where those guys have just been? ADAM I donâ€™t know. BARRY Their dayâ€™s not planned. Outside the hive, flying who-knows-where, doing who-knows-what. ADAM You canâ€™t just decide one day to be a Pollen Jock. You have to be bred for that. BARRY Right. Pollen Jocks cross in close proximity to Barry and Adam. Some pollen falls off, onto Barry and Adam. BARRY (CONTâ€™D) Look at that. Thatâ€™s more pollen than you and I will ever see in a lifetime. ADAM (playing with the pollen) Itâ€™s just a status symbol. I think bees make too big a deal out of it. BARRY Perhaps, unless youâ€™re wearing it, and the ladies see you wearing it. ANGLE ON: Two girl bees. \"Bee Movie\" - JS REVISIONS 8/13/07 12. ADAM Those ladies? Arenâ€™t they our cousins too? BARRY Distant, distant. ANGLE ON: TWO POLLEN JOCKS. JACKSON Look at these two. SPLITZ Couple of Hive Harrys. JACKSON Letâ€™s have some fun with them. The pollen jocks approach. Barry and Adam continue to talk to the girls. GIRL 1 It must be so dangerous being a pollen jock. BARRY Oh yeah, one time a bear had me pinned up against a mushroom. He had one paw on my throat, and with the other he was slapping me back and forth across the face. GIRL 1 Oh my. BARRY I never thought Iâ€™d knock him out. GIRL 2 (to Adam) And what were you doing during all of this? ADAM Obviously I was trying to alert the authorities. The girl swipes some pollen off of Adam with a finger. BARRY (re: pollen) I can autograph that if you want. \"Bee Movie\" - JS REVISIONS 8/13/07 13. JACKSON Little gusty out there today, wasnâ€™t it, comrades? BARRY Yeah. Gusty. BUZZ You know, weâ€™re going to hit a sunflower patch about six miles from here tomorrow. BARRY Six miles, huh? ADAM (whispering) Barry. BUZZ Itâ€™s a puddle-jump for us. But maybe youâ€™re not up for it. BARRY Maybe I am. ADAM (whispering louder) You are not! BUZZ Weâ€™re going, oh-nine hundred at JGate. ADAM (re: j-gate) Whoa. BUZZ (leaning in, on top of Barry) What do you think, Buzzy Boy? Are you bee enough? BARRY I might be. It all depends on what oh-nine hundred means. CUT TO: SEQ. 450 - â€œTHE BALCONYâ€ \"Bee Movie\" - JS REVISIONS 8/13/07 14. INT. BENSON HOUSE BALCONY - LATER Barry is standing on the balcony alone, looking out over the city. Martin Benson ENTERS, sneaks up behind Barry and gooses him in his ribs. MARTIN BENSON Honex! BARRY Oh, Dad. You surprised me. MARTIN BENSON (laughing) Have you decided what youâ€™re interested in, Son? BARRY Well, thereâ€™s a lot of choices. MARTIN BENSON But you only get one. Martin LAUGHS. BARRY Dad, do you ever get bored doing the same job every day? MARTIN BENSON Son, let me tell you something about stirring. (making the stirring motion) You grab that stick and you just move it around, and you stir it around. You get yourself into a rhythm, itâ€™s a beautiful thing. BARRY You know dad, the more I think about it, maybe the honey field just isnâ€™t right for me. MARTIN BENSON And you were thinking of what, making balloon animals? Thatâ€™s a bad job for a guy with a stinger. \"Bee Movie\" - JS REVISIONS 8/13/07 15. BARRY Well no... MARTIN BENSON Janet, your sonâ€™s not sure he wants to go into honey. JANET BENSON Oh Barry, you are so funny sometimes. BARRY Iâ€™m not trying to be funny. MARTIN BENSON Youâ€™re not funny, youâ€™re going into honey. Our son, the stirrer. JANET BENSON Youâ€™re going to be a stirrer?! BARRY No oneâ€™s listening to me. MARTIN BENSON Wait until you see the sticks I have for you. BARRY I can say anything I want right now. Iâ€™m going to get an ant tattoo. JANET BENSON Letâ€™s open some fresh honey and celebrate. BARRY Maybe Iâ€™ll pierce my thorax! MARTIN BENSON (toasting) To honey! BARRY Shave my antennae! JANET BENSON To honey! \"Bee Movie\" - JS REVISIONS 8/13/07 16. BARRY Shack up with a grasshopper, get a gold tooth, and start calling everybody â€œDawg.â€ CUT TO: SEQ. 760 - â€œJOB PLACEMENTâ€ EXT. HONEX LOBBY - CONTINUOUS ANGLE ON: A BEE BUS STOP. One group of bees stands on the pavement, as another group hovers above them. A doubledecker bus pulls up. The hovering bees get on the top level, and the standing bees get on the bottom. Barry and Adam pull up outside of Honex. ADAM I canâ€™t believe weâ€™re starting work today. BARRY Todayâ€™s the day. Adam jumps out of the car. ADAM (O.C) Come on. All the good jobs will be gone. BARRY Yeah, right... ANGLE ON: A BOARD READING: â€œJOB PLACEMENT BOARDâ€. Buzzwell, the Bee Processor, is at the counter. Another BEE APPLICANT, SANDY SHRIMPKIN is EXITING. SANDY SHRIMPKIN Is it still available? BUZZWELL Hang on. (he looks at changing numbers on the board) Two left. And...one of themâ€™s yours. Congratulations Son, step to the side please. \"Bee Movie\" - JS REVISIONS 8/13/07 17. SANDY SHRIMPKIN Yeah! ADAM (to Sandy, leaving) What did you get? SANDY SHRIMPKIN Picking the crud out. That is stellar! ADAM Wow. BUZZWELL (to Adam and Barry) Couple of newbies? ADAM Yes Sir. Our first day. We are ready. BUZZWELL Well, step up and make your choice. ANGLE ON: A CHART listing the different sectors of Honex. Heating, Cooling, Viscosity, Krelman, Pollen Counting, Stunt Bee, Pouring, Stirrer, Humming, Regurgitating, Front Desk, Hair Removal, Inspector No. 7, Chef, Lint Coordinator, Stripe Supervisor, Antennae-ball polisher, Mite Wrangler, Swatting Counselor, Wax Monkey, Wing Brusher, Hive Keeper, Restroom Attendant. ADAM (to Barry) You want to go first? BARRY No, you go. ADAM Oh my. Whatâ€™s available? BUZZWELL Restroom attendant is always open, and not for the reason you think. ADAM Any chance of getting on to the Krelman, Sir? BUZZWELL Sure, youâ€™re on. \"Bee Movie\" - JS REVISIONS 8/13/07 18. He plops the KRELMAN HAT onto Adamâ€™s head. ANGLE ON: The job board. THE COLUMNS READ: â€œOCCUPATIONâ€ â€œPOSITIONS AVAILABLEâ€, and â€œSTATUSâ€. The middle column has numbers, and the right column has job openings flipping between â€œopenâ€, â€œpendingâ€, and â€œclosedâ€. BUZZWELL (CONTâ€™D) Oh, Iâ€™m sorry. The Krelman just closed out. ADAM Oh! He takes the hat off Adam. BUZZWELL Wax Monkeyâ€™s always open. The Krelman goes from â€œClosedâ€ to â€œOpenâ€. BUZZWELL (CONTâ€™D) And the Krelman just opened up again. ADAM What happened? BUZZWELL Well, whenever a bee dies, thatâ€™s an opening. (pointing at the board) See that? Heâ€™s dead, dead, another dead one, deady, deadified, two more dead. Dead from the neck up, dead from the neck down. But, thatâ€™s life. ANGLE ON: Barryâ€™s disturbed expression. ADAM (feeling pressure to decide) Oh, this is so hard. Heating, cooling, stunt bee, pourer, stirrer, humming, inspector no. 7, lint coordinator, stripe supervisor, antenna-ball polisher, mite wrangler-- Barry, Barry, what do you think I should-- Barry? Barry? \"Bee Movie\" - JS REVISIONS 8/13/07 19. Barry is gone. CUT TO: SEQ. 775 - â€œLOU LODUCA SPEECHâ€ EXT. J-GATE - SAME TIME Splitz, Jackson, Buzz, Lou and two other BEES are going through final pre-flight checks. Barry ENTERS. LOU LODUCA Alright, weâ€™ve got the sunflower patch in quadrant nine. Geranium window box on Sutton Place... Barryâ€™s antennae rings, like a phone. ADAM (V.O) What happened to you? Where are you? Barry whispers throughout. BARRY Iâ€™m going out. ADAM (V.O) Out? Out where? BARRY Out there. ADAM (V.O) (putting it together) Oh no. BARRY I have to, before I go to work for the rest of my life. ADAM (V.O) Youâ€™re going to die! Youâ€™re crazy! Hello? BARRY Oh, another call coming in. \"Bee Movie\" - JS REVISIONS 8/13/07 20. ADAM (V.O) Youâ€™re cra-- Barry HANGS UP. ANGLE ON: Lou Loduca. LOU LODUCA If anyoneâ€™s feeling brave, thereâ€™s a Korean Deli on 83rd that gets their roses today. BARRY (timidly) Hey guys. BUZZ Well, look at that. SPLITZ Isnâ€™t that the kid we saw yesterday? LOU LODUCA (to Barry) Hold it son, flight deckâ€™s restricted. JACKSON Itâ€™s okay Lou, weâ€™re going to take him up. Splitz and Jackson CHUCKLE. LOU LODUCA Really? Feeling lucky, are ya? A YOUNGER SMALLER BEE THAN BARRY, CHET, runs up with a release waiver for Barry to sign. CHET Sign here. Here. Just initial that. Thank you. LOU LODUCA Okay, you got a rain advisory today and as you all know, bees cannot fly in rain. So be careful. As always, (reading off clipboard) watch your brooms, hockey sticks, dogs, birds, bears, and bats. \"Bee Movie\" - JS REVISIONS 8/13/07 21. Also, I got a couple reports of root beer being poured on us. Murphyâ€™s in a home because of it, just babbling like a cicada. BARRY Thatâ€™s awful. LOU LODUCA And a reminder for all you rookies, bee law number one, absolutely no talking to humans. Alright, launch positions! The Jocks get into formation, chanting as they move. LOU LODUCA (CONTâ€™D) Black and Yellow! JOCKS Hello! SPLITZ (to Barry) Are you ready for this, hot shot? BARRY Yeah. Yeah, bring it on. Barry NODS, terrified. BUZZ Wind! - CHECK! JOCK #1 Antennae! - CHECK! JOCK #2 Nectar pack! - CHECK! JACKSON Wings! - CHECK! SPLITZ Stinger! - CHECK! BARRY Scared out of my shorts - CHECK. LOU LODUCA Okay ladies, letâ€™s move it out. Everyone FLIPS their goggles down. Pit crew bees CRANK their wings, and remove the starting blocks. We hear loud HUMMING. \"Bee Movie\" - JS REVISIONS 8/13/07 22. LOU LODUCA (CONT'D) LOU LODUCA (CONTâ€™D) Pound those petunia's, you striped stem-suckers! All of you, drain those flowers! A FLIGHT DECK GUY in deep crouch hand-signals them out the archway as the backwash from the bee wings FLUTTERS his jump suit. Barry follows everyone. SEQ. 800 - â€œFLYING WITH THE JOCKSâ€ The bees climb above tree tops in formation. Barry is euphoric. BARRY Whoa! Iâ€™m out! I canâ€™t believe Iâ€™m out! So blue. Ha ha ha! (a beat) I feel so fast...and free. (re: kites in the sky) Box kite! Wow! They fly by several bicyclists, and approach a patch of flowers. BARRY (CONT'D) Flowers! SPLITZ This is blue leader. We have roses visual. Bring it around thirty degrees and hold. BARRY (sotto) Roses. JACKSON Thirty degrees, roger, bringing it around. Many pollen jocks break off from the main group. They use their equipment to collect nectar from flowers. Barry flies down to watch the jocks collect the nectar. JOCK Stand to the side kid, itâ€™s got a bit of a kick. The jock fires the gun, and recoils. Barry watches the gun fill up with nectar. \"Bee Movie\" - JS REVISIONS 8/13/07 23. BARRY Oh, that is one Nectar Collector. JOCK You ever see pollination up close? BARRY No, Sir. He takes off, and the excess pollen dust falls causing the flowers to come back to life. JOCK (as he pollinates) I pick some pollen up over here, sprinkle it over here, maybe a dash over there, pinch on that one...see that? Itâ€™s a little bit of magic, ainâ€™t it? The FLOWERS PERK UP as he pollinates. BARRY Wow. Thatâ€™s amazing. Why do we do that? JOCK ...thatâ€™s pollen power, Kid. More pollen, more flowers, more nectar, more honey for us. BARRY Cool. The Jock WINKS at Barry. Barry rejoins the other jocks in the sky. They swoop in over a pond, kissing the surface. We see their image reflected in the water; theyâ€™re really moving. They fly over a fountain. BUZZ Iâ€™m picking up a lot of bright yellow, could be daisies. Donâ€™t we need those? SPLITZ Copy that visual. We see what appear to be yellow flowers on a green field. \"Bee Movie\" - JS REVISIONS 8/13/07 24. They go into a deep bank and dive. BUZZ Hold on, one of these flowers seems to be on the move. SPLITZ Say again...Are you reporting a moving flower? BUZZ Affirmative. SEQ. 900 - â€œTENNIS GAMEâ€ The pollen jocks land. It is a tennis court with dozens of tennis balls. A COUPLE, VANESSA and KEN, plays tennis. The bees land right in the midst of a group of balls. KEN (O.C) That was on the line! The other bees start walking around amongst the immense, yellow globes. SPLITZ This is the coolest. What is it? They stop at a BALL on a white line and look up at it. JACKSON I donâ€™t know, but Iâ€™m loving this color. SPLITZ (smelling tennis ball) Smells good. Not like a flower. But I like it. JACKSON Yeah, fuzzy. BUZZ Chemical-y. JACKSON Careful, guys, itâ€™s a little grabby. Barry LANDS on a ball and COLLAPSES. \"Bee Movie\" - JS REVISIONS 8/13/07 25. BARRY Oh my sweet lord of bees. JACKSON Hey, candy brain, get off there! Barry attempts to pulls his legs off, but they stick. BARRY Problem! A tennis shoe and a hand ENTER FRAME. The hand picks up the ball with Barry underneath it. BARRY (CONT'D) Guys! BUZZ This could be bad. JACKSON Affirmative. Vanessa walks back to the service line, BOUNCES the ball. Each time it BOUNCES, the other bees cringe and GASP. ANGLE ON: Barry, terrified. Pure dumb luck, heâ€™s not getting squished. BARRY (with each bounce) Very close...Gonna Hurt...Mammaâ€™s little boy. SPLITZ You are way out of position, rookie. ANGLE ON: Vanessa serving. We see Barry and the ball up against the racket as she brings it back. She tosses the ball into the air; Barryâ€™s eyes widen. The ball is STRUCK, and the rally is on. KEN Coming in at you like a missile! Ken HITS the ball back. Barry feels the g-forces. ANGLE ON: The Pollen Jocks watching Barry pass by them in SLOW MOTION. \"Bee Movie\" - JS REVISIONS 8/13/07 26. BARRY (in slow motion) Help me! JACKSON You know, I don't think these are flowers. SPLITZ Should we tell him? JACKSON I think he knows. BARRY (O.S) What is this?! Vanessa HITS a high arcing lob. Ken waits, poised for the return. We see Barry having trouble maneuvering the ball from fatigue. KEN (overly confident) Match point! ANGLE ON: Ken running up. He has a killer look in his eyes. Heâ€™s going to hit the ultimate overhead smash. KEN (CONT'D) You can just start packing up Honey, because I believe youâ€™re about to eat it! ANGLE ON: Pollen Jocks. JACKSON Ahem! Ken is distracted by the jock. KEN What? No! He misses badly. The ball rockets into oblivion. Barry is still hanging on. ANGLE ON: Ken, berating himself. KEN (CONTâ€™D) Oh, you cannot be serious. We hear the ball WHISTLING, and Barry SCREAMING. \"Bee Movie\" - JS REVISIONS 8/13/07 27. BARRY Yowser!!! SEQ. 1000 - â€œSUVâ€ The ball flies through the air, and lands in the middle of the street. It bounces into the street again, and sticks in the grille of an SUV. INT. CAR ENGINE - CONTINUOUS BARRYâ€™S POV: the grille of the SUV sucks him up. He tumbles through a black tunnel, whirling vanes, and pistons. BARRY AHHHHHHHHHHH!! OHHHH!! EECHHH!! AHHHHHH!! Barry gets chilled by the A/C system, and sees a frozen grasshopper. BARRY (CONTâ€™D) (re: grasshopper) Eww, gross. CUT TO: INT. CAR - CONTINUOUS The car is packed with a typical suburban family: MOTHER, FATHER, eight-year old BOY, LITTLE GIRL in a car seat and a GRANDMOTHER. A big slobbery DOG is behind a grate. Barry pops into the passenger compartment, hitting the Motherâ€™s magazine. MOTHER Thereâ€™s a bee in the car! They all notice the bee and start SCREAMING. BARRY Aaahhhh! Barry tumbles around the car. We see the faces from his POV. MOTHER Do something! \"Bee Movie\" - JS REVISIONS 8/13/07 28. FATHER Iâ€™m driving! Barry flies by the little girl in her CAR SEAT. She waves hello. LITTLE GIRL Hi, bee. SON Heâ€™s back here! Heâ€™s going to sting me! The car SWERVES around the road. Barry flies into the back, where the slobbery dog SNAPS at him. Barry deftly avoids the jaws and gross, flying SPITTLE. MOTHER Nobody move. If you donâ€™t move, he wonâ€™t sting you. Freeze! Everyone in the car freezes. Barry freezes. They stare at each other, eyes going back and forth, waiting to see who will make the first move. Barry blinks. GRANNY He blinked! Granny pulls out a can of HAIR SPRAY. SON Spray him, Granny! Granny sprays the hair spray everywhere. FATHER What are you doing? GRANNY Itâ€™s hair spray! Extra hold! MOTHER Kill it! Barry gets sprayed back by the hair spray, then sucked out of the sunroof. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 29. EXT. CITY STREET - CONTINUOUS BARRY Wow. The tension level out here is unbelievable. Iâ€™ve got to get home. As Barry flies down the street, it starts to RAIN. He nimbly avoids the rain at first. BARRY (CONTâ€™D) Whoa. Whoa! Canâ€™t fly in rain! Canâ€™t fly in rain! Canâ€™t fly in-- A couple of drops hit him, his wings go limp and he starts falling. BARRY (CONT'D) Mayday! Mayday! Bee going down! Barry sees a window ledge and aims for it and just makes it. Shivering and exhausted, he crawls into an open window as it CLOSES. SEQ. 1100 - â€œVANESSA SAVES BARRYâ€ INT. VANESSAâ€™S APARTMENT - CONTINUOUS Inside the window, Barry SHAKES off the rain like a dog. Vanessa, Ken, Andy, and Anna ENTER the apartment. VANESSA Ken, can you close the window please? KEN Huh? Oh. (to Andy) Hey, check out my new resume. I made it into a fold-out brochure. You see? It folds out. Ken holds up his brochure, with photos of himself, and a resume in the middle. ANGLE ON: Barry hiding behind the curtains, as Ken CLOSES THE WINDOW. \"Bee Movie\" - JS REVISIONS 8/13/07 30. BARRY Oh no, more humans. I donâ€™t need this. Barry HOVERS up into the air and THROWS himself into the glass. BARRY (CONTâ€™D) (dazed) Ow! What was that? He does it again, and then multiple more times. BARRY (CONT'D) Maybe this time...this time, this time, this time, this time, this time, this time, this time. Barry JUMPS onto the drapes. BARRY (CONT'D) (out of breath) Drapes! (then, re: glass) That is diabolical. KEN Itâ€™s fantastic. Itâ€™s got all my special skills, even my top ten favorite movies. ANDY Whatâ€™s your number one? Star Wars? KEN Ah, I donâ€™t go for that, (makes Star Wars noises), kind of stuff. ANGLE ON: Barry. BARRY No wonder weâ€™re not supposed to talk to them. Theyâ€™re out of their minds. KEN When I walk out of a job interview theyâ€™re flabbergasted. They canâ€™t believe the things I say. Barry looks around and sees the LIGHT BULB FIXTURE in the middle of the ceiling. \"Bee Movie\" - JS REVISIONS 8/13/07 31. BARRY (re: light bulb) Oh, thereâ€™s the sun. Maybe thatâ€™s a way out. Barry takes off and heads straight for the light bulb. His POV: The seventy-five watt label grows as he gets closer. BARRY (CONTâ€™D) I donâ€™t remember the sun having a big seventy five on it. Barry HITS the bulb and is KNOCKED SILLY. He falls into a BOWL OF GUACAMOLE. Andy dips his chip in the guacamole, taking Barry with it. ANGLE ON: Ken and Andy. KEN Iâ€™ll tell you what. You know what? I predicted global warming. I could feel it getting hotter. At first I thought it was just me. Barryâ€™s POV: Giant human mouth opening. KEN (CONTâ€™D) Wait! Stop! Beeeeeee! ANNA Kill it! Kill it! They all JUMP up from their chairs. Andy looks around for something to use. Ken comes in for the kill with a big TIMBERLAND BOOT on each hand. KEN Stand back. These are winter boots. Vanessa ENTERS, and stops Ken from squashing Barry. VANESSA (grabs Kenâ€™s arm) Wait. Donâ€™t kill him. CLOSE UP: on Barryâ€™s puzzled face. KEN You know Iâ€™m allergic to them. This thing could kill me. \"Bee Movie\" - JS REVISIONS 8/13/07 32. VANESSA Why does his life have any less value than yours? She takes a GLASS TUMBLER and places it over Barry. KEN Why does his life have any less value than mine? Is that your statement? VANESSA Iâ€™m just saying, all life has value. You donâ€™t know what heâ€™s capable of feeling. Barry looks up through the glass and watches this conversation, astounded. Vanessa RIPS Kenâ€™s resume in half and SLIDES it under the glass. KEN (wistful) My brochure. Thereâ€™s a moment of eye contact as she carries Barry to the window. She opens it and sets him free. VANESSA There you go, little guy. KEN (O.C) Iâ€™m not scared of them. But, you know, itâ€™s an allergic thing. ANDY (O.C) * Hey, why donâ€™t you put that on your * resume-brochure? * KEN (O.C) Itâ€™s not funny, my whole face could puff up. ANDY (O.C) Make it one of your â€œSpecial Skills.â€ KEN (O.C) You know, knocking someone out is also a special skill. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 33. EXT. WINDOWSILL - CONTINUOUS Barry stares over the window frame. He canâ€™t believe whatâ€™s just happened. It is still RAINING. DISSOLVE TO: SEQ. 1200 - â€œBARRY SPEAKSâ€ EXT. WINDOWSILL - LATER Barry is still staring through the window. Inside, everyoneâ€™s saying their good-byes. KEN Vanessa, next week? Yogurt night? VANESSA Uh, yeah sure Ken. You know, whatever. KEN You can put carob chips on there. VANESSA Good night. KEN (as he exits) Supposed to be less calories, or something. VANESSA Bye. She shuts the door. Vanessa starts cleaning up. BARRY Iâ€™ve got to say something. She saved my life. Iâ€™ve got to say something. Alright, here it goes. Barry flies in. \"Bee Movie\" - JS REVISIONS 8/13/07 34. INT. VANESSAâ€™S APARTMENT - CONTINUOUS Barry hides himself on different PRODUCTS placed along the kitchen shelves. He hides on a Bumblebee Tuna can, and a â€œGreetings From Coney Islandâ€ MUSCLE-MAN POSTCARD on the fridge. BARRY (on fridge) What would I say? (landing on a bottle) I could really get in trouble. He stands looking at Vanessa. BARRY (CONT'D) Itâ€™s a bee law. Youâ€™re not supposed to talk to a human. I canâ€™t believe Iâ€™m doing this. Iâ€™ve got to. Oh, I canâ€™t do it! Come on! No, yes, no, do it! I canâ€™t. How should I start it? You like jazz? No, thatâ€™s no good. Here she comes. Speak, you fool. As Vanessa walks by, Barry takes a DEEP BREATH. BARRY (CONTâ€™D) (cheerful) Umm...hi. Vanessa DROPS A STACK OF DISHES, and HOPS BACK. BARRY (CONTâ€™D) Iâ€™m sorry. VANESSA Youâ€™re talking. BARRY Yes, I know, I know. VANESSA Youâ€™re talking. BARRY I know, Iâ€™m sorry. Iâ€™m so sorry. VANESSA Itâ€™s okay. Itâ€™s fine. Itâ€™s just, I know Iâ€™m dreaming, but I donâ€™t recall going to bed. \"Bee Movie\" - JS REVISIONS 8/13/07 35. BARRY Well, you know Iâ€™m sure this is very disconcerting. VANESSA Well yeah. I mean this is a bit of a surprise to me. I mean...youâ€™re a bee. BARRY Yeah, I am a bee, and you know Iâ€™m not supposed to be doing this, but they were all trying to kill me and if it wasnâ€™t for you...I mean, I had to thank you. Itâ€™s just the way I was raised. Vanessa intentionally JABS her hand with a FORK. VANESSA Ow! BARRY That was a little weird. VANESSA (to herself) Iâ€™m talking to a bee. BARRY Yeah. VANESSA Iâ€™m talking to a bee. BARRY Anyway... VANESSA And a bee is talking to me... BARRY I just want you to know that Iâ€™m grateful, and Iâ€™m going to leave now. VANESSA Wait, wait, wait, wait, how did you learn to do that? BARRY What? \"Bee Movie\" - JS REVISIONS 8/13/07 36. VANESSA The talking thing. BARRY Same way you did, I guess. Mama, Dada, honey, you pick it up. VANESSA Thatâ€™s very funny. BARRY Yeah. Bees are funny. If we didnâ€™t laugh, weâ€™d cry. With what we have to deal with. Vanessa LAUGHS. BARRY (CONTâ€™D) Anyway. VANESSA Can I, uh, get you something? BARRY Like what? VANESSA I donâ€™t know. I mean, I donâ€™t know. Coffee? BARRY Well, uh, I donâ€™t want to put you out. VANESSA Itâ€™s no trouble. BARRY Unless youâ€™re making anyway. VANESSA Oh, it takes two minutes. BARRY Really? VANESSA Itâ€™s just coffee. BARRY I hate to impose. \"Bee Movie\" - JS REVISIONS 8/13/07 37. VANESSA Donâ€™t be ridiculous. BARRY Actually, I would love a cup. VANESSA Hey, you want a little rum cake? BARRY I really shouldnâ€™t. VANESSA Have a little rum cake. BARRY No, no, no, I canâ€™t. VANESSA Oh, come on. BARRY You know, Iâ€™m trying to lose a couple micrograms here. VANESSA Where? BARRY Well... These stripes donâ€™t help. VANESSA You look great. BARRY I donâ€™t know if you know anything about fashion. Vanessa starts POURING the coffee through an imaginary cup and directly onto the floor. BARRY (CONT'D) Are you alright? VANESSA No. DISSOLVE TO: SEQ. 1300 - â€œROOFTOP COFFEEâ€ \"Bee Movie\" - JS REVISIONS 8/13/07 38. EXT. VANESSAâ€™S ROOF - LATER Barry and Vanessa are drinking coffee on her roof terrace. He is perched on her keychain. BARRY ...He canâ€™t get a taxi. Heâ€™s making the tie in the cab, as theyâ€™re flying up Madison. So he finally gets there. VANESSA Uh huh? BARRY He runs up the steps into the church, the wedding is on... VANESSA Yeah? BARRY ...and he says, watermelon? I thought you said Guatemalan. VANESSA Uh huh? BARRY Why would I marry a watermelon? Barry laughs. Vanessa doesnâ€™t. VANESSA Oh! Is that, uh, a bee joke? BARRY Yeah, thatâ€™s the kind of stuff that we do. VANESSA Yeah, different. A BEAT. VANESSA (CONTâ€™D) So anyway...what are you going to do, Barry? \"Bee Movie\" - JS REVISIONS 8/13/07 39. BARRY About work? I donâ€™t know. I want to do my part for the hive, but I canâ€™t do it the way they want. VANESSA I know how you feel. BARRY You do? VANESSA Sure, my parents wanted me to be a lawyer or doctor, but I wanted to be a florist. BARRY Really? VANESSA My only interest is flowers. BARRY Our new queen was just elected with that same campaign slogan. VANESSA Oh. BARRY Anyway, see thereâ€™s my hive, right there. You can see it. VANESSA Oh, youâ€™re in Sheep Meadow. BARRY (excited) Yes! You know the turtle pond? VANESSA Yes? BARRY Iâ€™m right off of that. VANESSA Oh, no way. I know that area. Do you know I lost a toe-ring there once? BARRY Really? \"Bee Movie\" - JS REVISIONS 8/13/07 40. VANESSA Yes. BARRY Why do girls put rings on their toes? VANESSA Why not? BARRY I donâ€™t know. Itâ€™s like putting a hat on your knee. VANESSA Really? Okay. A JANITOR in the background changes a LIGHTBULB. To him, it appears that Vanessa is talking to an imaginary friend. JANITOR You all right, maâ€™am? VANESSA Oh, yeah, fine. Just having two cups of coffee. BARRY Anyway, this has been great. (wiping his mouth) Thanks for the coffee. Barry gazes at Vanessa. VANESSA Oh yeah, itâ€™s no trouble. BARRY Sorry I couldnâ€™t finish it. Vanessa giggles. BARRY (CONT'D) (re: coffee) If I did, Iâ€™d be up the rest of my life. Ummm. Can I take a piece of this with me? VANESSA Sure. Here, have a crumb. She takes a CRUMB from the plate and hands it to Barry. \"Bee Movie\" - JS REVISIONS 8/13/07 41. BARRY (a little dreamy) Oh, thanks. VANESSA Yeah. There is an awkward pause. BARRY Alright, well then, I guess Iâ€™ll see you around, or not, or... VANESSA Okay Barry. BARRY And thank you so much again, for before. VANESSA Oh that? BARRY Yeah. VANESSA Oh, that was nothing. BARRY Well, not nothing, but, anyway... Vanessa extends her hand, and shakes Barryâ€™s gingerly. The Janitor watches. The lightbulb shorts out. The Janitor FALLS. CUT TO: SEQ. 1400 - â€œHONEXâ€ INT. HONEX BUILDING - NEXT DAY ANGLE ON: A TEST BEE WEARING A PARACHUTE is in a wind tunnel, hovering through increasingly heavy wind. SIGNS UNDER A FLASHING LIGHT READ: â€œTest In Progressâ€ &amp; â€œHurricane Survival Testâ€. 2 BEES IN A LAB COATS are observing behind glass. \"Bee Movie\" - JS REVISIONS 8/13/07 42. LAB COAT BEE 1 This canâ€™t possibly work. LAB COAT BEE 2 Well, heâ€™s all set to go, we may as well try it. (into the mic) Okay Dave, pull the chute. The test bee opens his parachute. Heâ€™s instantly blown against the rear wall. Adam and Barry ENTER. ADAM Sounds amazing. BARRY Oh, it was amazing. It was the scariest, happiest moment of my life. ADAM Humans! Humans! I canâ€™t believe you were with humans! Giant scary humans! What were they like? BARRY Huge and crazy. They talk crazy, they eat crazy giant things. They drive around real crazy. ADAM And do they try and kill you like on TV? BARRY Some of them. But some of them donâ€™t. ADAM Howâ€™d you get back? BARRY Poodle. ADAM Look, you did it. And Iâ€™m glad. You saw whatever you wanted to see out there, you had your â€œexperienceâ€, and now youâ€™re back, you can pick out your job, and everything can be normal. \"Bee Movie\" - JS REVISIONS 8/13/07 43. ANGLE ON: LAB BEES examining a CANDY CORN through a microscope. BARRY Well... ADAM Well? BARRY Well, I met someone. ADAM You met someone? Was she Bee-ish? BARRY Mmm. ADAM Not a WASP? Your parents will kill you. BARRY No, no, no, not a wasp. ADAM Spider? BARRY You know, Iâ€™m not attracted to the spiders. I know to everyone else itâ€™s like the hottest thing with the eight legs and all. I canâ€™t get by that face. Barry makes a spider face. ADAM So, who is she? BARRY Sheâ€™s a human. ADAM Oh no, no, no, no. That didnâ€™t happen. You didnâ€™t do that. That is a bee law. You wouldnâ€™t break a bee law. BARRY Her nameâ€™s Vanessa. \"Bee Movie\" - JS REVISIONS 8/13/07 44. ADAM Oh, oh boy! BARRY Sheâ€™s so-o nice. And sheâ€™s a florist! ADAM Oh, no. No, no, no! Youâ€™re dating a human florist? BARRY Weâ€™re not dating. ADAM Youâ€™re flying outside the hive. Youâ€™re talking to human beings that attack our homes with power washers and M-80â€™s. Thatâ€™s 1/8 of a stick of dynamite. BARRY She saved my life. And she understands me. ADAM This is over. Barry pulls out the crumb. BARRY Eat this. Barry stuffs the crumb into Adamâ€™s face. ADAM This is not over. What was that? BARRY They call it a crumb. ADAM That was SO STINGING STRIPEY! BARRY And thatâ€™s not even what they eat. That just falls off what they eat. Do you know what a Cinnabon is? ADAM No. \"Bee Movie\" - JS REVISIONS 8/13/07 45. BARRY Itâ€™s bread... ADAM Come in here! BARRY and cinnamon, ADAM Be quiet! BARRY and frosting...they heat it up-- ADAM Sit down! INT. ADAMâ€™S OFFICE - CONTINUOUS BARRY Really hot! ADAM Listen to me! We are not them. Weâ€™re us. Thereâ€™s us and thereâ€™s them. BARRY Yes, but who can deny the heart that is yearning... Barry rolls his chair down the corridor. ADAM Thereâ€™s no yearning. Stop yearning. Listen to me. You have got to start thinking bee, my friend. ANOTHER BEE JOINS IN. ANOTHER BEE Thinking bee. WIDER SHOT AS A 3RD BEE ENTERS, popping up over the cubicle wall. 3RD BEE Thinking bee. EVEN WIDER SHOT AS ALL THE BEES JOIN IN. \"Bee Movie\" - JS REVISIONS 8/13/07 46. OTHER BEES Thinking bee. Thinking bee. Thinking bee. CUT TO: SEQ. 1500 - â€œPOOLSIDE NAGGINGâ€ EXT. BACKYARD PARENTâ€™S HOUSE - DAY Barry sits on a RAFT in a hexagon honey pool, legs dangling into the water. Janet Benson and Martin Benson stand over him wearing big, sixties sunglasses and cabana-type outfits. The sun shines brightly behind their heads. JANET BENSON (O.C) There he is. Heâ€™s in the pool. MARTIN BENSON You know what your problem is, Barry? BARRY Iâ€™ve got to start thinking bee? MARTIN BENSON Barry, how much longer is this going to go on? Itâ€™s been three days. I donâ€™t understand why youâ€™re not working. BARRY Well, Iâ€™ve got a lot of big life decisions Iâ€™m thinking about. MARTIN BENSON What life? You have no life! You have no job! Youâ€™re barely a bee! Barry throws his hands in the air. BARRY Augh. JANET BENSON Would it kill you to just make a little honey? Barry ROLLS off the raft and SINKS to the bottom of the pool. We hear his parentsâ€™ MUFFLED VOICES from above the surface. \"Bee Movie\" - JS REVISIONS 8/13/07 47. JANET BENSON (CONT'D) (muffled) Barry, come out from under there. Your fatherâ€™s talking to you. Martin, would you talk to him? MARTIN BENSON Barry, Iâ€™m talking to you. DISSOLVE TO: EXT. PICNIC AREA - DAY MUSIC: â€œSugar Sugarâ€ by the Archies. Barry and Vanessa are having a picnic. A MOSQUITO lands on Vanessaâ€™s leg. She SWATS it violently. Barryâ€™s head whips around, aghast. They stare at each other awkwardly in a frozen moment, then BURST INTO HYSTERICAL LAUGHTER. Vanessa GETS UP. VANESSA You coming? BARRY Got everything? VANESSA All set. Vanessa gets into a one-man Ultra Light plane with a black and yellow paint scheme. She puts on her helmet. BARRY You go ahead, Iâ€™ll catch up. VANESSA (come hither wink) Donâ€™t be too long. The Ultra Light takes off. Barry catches up. They fly sideby-side. VANESSA (CONTâ€™D) Watch this! Vanessa does a loop, and FLIES right into the side of a mountain, BURSTING into a huge ball of flames. \"Bee Movie\" - JS REVISIONS 8/13/07 48. BARRY (yelling, anguished) Vanessa! EXT. BARRYâ€™S PARENTâ€™S HOUSE - CONTINUOUS ANGLE ON: Barryâ€™s face bursting through the surface of the pool, GASPING for air, eyes opening in horror. MARTIN BENSON Weâ€™re still here, Barry. JANET BENSON I told you not to yell at him. He doesnâ€™t respond when you yell at him. MARTIN BENSON Then why are you yelling at me? JANET BENSON Because you donâ€™t listen. MARTIN BENSON Iâ€™m not listening to this. Barry is toweling off, putting on his sweater. BARRY Sorry Mom, Iâ€™ve got to go. JANET BENSON Where are you going? BARRY Nowhere. Iâ€™m meeting a friend. Barry JUMPS off the balcony and EXITS. JANET BENSON (calling after him) A girl? Is this why you canâ€™t decide? BARRY Bye! JANET BENSON I just hope sheâ€™s Bee-ish. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 49. SEQ. 1700 - â€œSTREETWALK/SUPERMARKETâ€ EXT. VANESSAâ€™S FLORIST SHOP - DAY Vanessa FLIPS the sign to say â€œSorry We Missed Youâ€, and locks the door. ANGLE ON: A POSTER on Vanessaâ€™s door for the Tournament of Roses Parade in Pasadena. BARRY So they have a huge parade of just flowers every year in Pasadena? VANESSA Oh, to be in the Tournament of Roses, thatâ€™s every floristâ€™s dream. Up on a float, surrounded by flowers, crowds cheering. BARRY Wow, a tournament. Do the roses actually compete in athletic events? VANESSA No. Alright, Iâ€™ve got one. How come you donâ€™t fly everywhere? BARRY Itâ€™s exhausting. Why donâ€™t you run everywhere? VANESSA Hmmm. BARRY Isnâ€™t that faster? VANESSA Yeah, okay. I see, I see. Alright, your turn. Barry and Vanessa walk/fly down a New York side street, no other pedestrians near them. BARRY Ah! Tivo. You can just freeze live TV? Thatâ€™s insane. \"Bee Movie\" - JS REVISIONS 8/13/07 50. VANESSA What, you donâ€™t have anything like that? BARRY We have Hivo, but itâ€™s a disease. Itâ€™s a horrible, horrible disease. VANESSA Oh my. They turn the corner onto a busier avenue and people start to swat at Barry. MAN Dumb bees! VANESSA You must just want to sting all those jerks. BARRY We really try not to sting. Itâ€™s usually fatal for us. VANESSA So you really have to watch your temper? They ENTER a SUPERMARKET. CUT TO: INT. SUPERMARKET BARRY Oh yeah, very carefully. You kick a wall, take a walk, write an angry letter and throw it out. You work through it like any emotion-- anger, jealousy, (under his breath) lust. Barry hops on top of some cardboard boxes in the middle of an aisle. A stock boy, HECTOR, whacks him with a rolled up magazine. VANESSA (to Barry) Oh my goodness. Are you okay? \"Bee Movie\" - JS REVISIONS 8/13/07 51. BARRY Yeah. Whew! Vanessa WHACKS Hector over the head with the magazine. VANESSA (to Hector) What is wrong with you?! HECTOR Itâ€™s a bug. VANESSA Well heâ€™s not bothering anybody. Get out of here, you creep. Vanessa pushes him, and Hector EXITS, muttering. BARRY (shaking it off) What was that, a Pick and Save circular? VANESSA Yeah, it was. How did you know? BARRY It felt like about ten pages. Seventy-fiveâ€™s pretty much our limit. VANESSA Boy, youâ€™ve really got that down to a science. BARRY Oh, we have to. I lost a cousin to Italian Vogue. VANESSA Iâ€™ll bet. Barry stops, sees the wall of honey jars. BARRY What, in the name of Mighty Hercules, is this? How did this get here? Cute Bee? Golden Blossom? Ray Liotta Private Select? VANESSA Is he that actor? \"Bee Movie\" - JS REVISIONS 8/13/07 52. BARRY I never heard of him. Why is this here? VANESSA For people. We eat it. BARRY Why? (gesturing around the market) You donâ€™t have enough food of your own? VANESSA Well yes, we-- BARRY How do you even get it? VANESSA Well, bees make it... BARRY I know who makes it! And itâ€™s hard to make it! Thereâ€™s Heating and Cooling, and Stirring...you need a whole Krelman thing. VANESSA Itâ€™s organic. BARRY Itâ€™s our-ganic! VANESSA Itâ€™s just honey, Barry. BARRY Just...what?! Bees donâ€™t know about this. This is stealing. A lot of stealing! Youâ€™ve taken our homes, our schools, our hospitals. This is all we have. And itâ€™s on sale? Iâ€™m going to get to the bottom of this. Iâ€™m going to get to the bottom of all of this! He RIPS the label off the Ray Liotta Private Select. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 53. SEQ. 1800 - â€œWINDSHIELDâ€ EXT. BACK OF SUPERMARKET LOADING DOCK - LATER THAT DAY Barry disguises himself by blacking out his yellow lines with a MAGIC MARKER and putting on some war paint. He sees Hector, the stock boy, with a knife CUTTING open cardboard boxes filled with honey jars. MAN You almost done? HECTOR Almost. Barry steps in some honey, making a SNAPPING noise. Hector stops and turns. HECTOR (CONTâ€™D) He is here. I sense it. Hector grabs his BOX CUTTER. Barry REACTS, hides himself behind the box again. HECTOR (CONTâ€™D) (talking too loud, to no one in particular) Well, I guess Iâ€™ll go home now, and just leave this nice honey out, with no one around. A BEAT. Hector pretends to exit. He takes a couple of steps in place. ANGLE ON: The honey jar. Barry steps out into a moody spotlight. BARRY Youâ€™re busted, box boy! HECTOR Ah ha! I knew I heard something. So, you can talk. Barry flies up, stinger out, pushing Hector up against the wall. As Hector backs up, he drops his knife. BARRY Oh, I can talk. And now youâ€™re going to start talking. \"Bee Movie\" - JS REVISIONS 8/13/07 54. Where are you getting all the sweet stuff? Whoâ€™s your supplier?! HECTOR I donâ€™t know what youâ€™re talking about. I thought we were all friends. The last thing we want to do is upset any of you...bees! Hector grabs a PUSHPIN. Barry fences with his stinger. HECTOR (CONTâ€™D) Youâ€™re too late. Itâ€™s ours now! BARRY You, sir, have crossed the wrong sword. HECTOR You, sir, are about to be lunch for my iguana, Ignacio! Barry and Hector get into a cross-swords, nose-to-nose confrontation. BARRY Where is the honey coming from? Barry knocks the pushpin out of his hand. Barry puts his stinger up to Hectorâ€™s nose. BARRY (CONT'D) Tell me where?! HECTOR (pointing to a truck) Honey Farms. It comes from Honey Farms. ANGLE ON: A Honey Farms truck leaving the parking lot. Barry turns, takes off after the truck through an alley. He follows the truck out onto a busy street, dodging a bus, and several cabs. CABBIE Crazy person! He flies through a metal pipe on the top of a truck. BARRY OOOHHH! \"Bee Movie\" - JS REVISIONS 8/13/07 55. BARRY (CONT'D) Barry grabs onto a bicycle messengerâ€™s backpack. The honey farms truck starts to pull away. Barry uses the bungee cord to slingshot himself towards the truck. He lands on the windshield, where the wind plasters him to the glass. He looks up to find himself surrounded by what appear to be DEAD BUGS. He climbs across, working his way around the bodies. BARRY (CONTâ€™D) Oh my. What horrible thing has happened here? Look at these faces. They never knew what hit them. And now theyâ€™re on the road to nowhere. A MOSQUITO opens his eyes. MOOSEBLOOD Pssst! Just keep still. BARRY What? Youâ€™re not dead? MOOSEBLOOD Do I look dead? Hey man, they will wipe anything that moves. Now, where are you headed? BARRY To Honey Farms. I am onto something huge here. MOOSEBLOOD Iâ€™m going to Alaska. Moose blood. Crazy stuff. Blows your head off. LADYBUG Iâ€™m going to Tacoma. BARRY (to fly) What about you? MOOSEBLOOD He really is dead. BARRY Alright. The WIPER comes towards them. \"Bee Movie\" - JS REVISIONS 8/13/07 56. MOOSEBLOOD Uh oh. BARRY What is that? MOOSEBLOOD Oh no! Itâ€™s a wiper, triple blade! BARRY Triple blade? MOOSEBLOOD Jump on. Itâ€™s your only chance, bee. They hang on as the wiper goes back and forth. MOOSEBLOOD (CONT'D) (yelling to the truck driver through the glass) Why does everything have to be so dog-gone clean?! How much do you people need to see? Open your eyes! Stick your head out the window! CUT TO: INT. TRUCK CAB SFX: Radio. RADIO VOICE For NPR News in Washington, Iâ€™m Carl Kasell. EXT. TRUCK WINDSHIELD MOOSEBLOOD But donâ€™t kill no more bugs! The Mosquito is FLUNG off of the wiper. MOOSEBLOOD (CONT'D) Beeeeeeeeeeeeee! BARRY Moose blood guy! \"Bee Movie\" - JS REVISIONS 8/13/07 57. Barry slides toward the end of the wiper, is thrown off, but he grabs the AERIAL and hangs on for dear life. Barry looks across and sees a CRICKET on another vehicle in the exact same predicament. They look at each other and SCREAM in unison. BARRY AND CRICKET Aaaaaaaaaah! ANOTHER BUG grabs onto the aerial, and screams as well. INT. TRUCK CAB - SAME TIME DRIVER You hear something? TRUCKER PASSENGER Like what? DRIVER Like tiny screaming. TRUCKER PASSENGER Turn off the radio. The driver reaches down and PRESSES a button, lowering the aerial. EXT. TRUCK WINDSHIELD - SAME TIME Barry and the other bug do a â€œchoose upâ€ to the bottom, Barry wins. BARRY Aha! Then he finally has to let go and gets thrown into the truck horn atop cab. Mooseblood is inside. MOOSEBLOOD Hey, whatâ€™s up bee boy? BARRY Hey, Blood! DISSOLVE TO: \"Bee Movie\" - JS REVISIONS 8/13/07 58. INT. TRUCK HORN - LATER BARRY ...and it was just an endless row of honey jars as far as the eye could see. MOOSEBLOOD Wow. BARRY So Iâ€™m just assuming wherever this honey truck goes, thatâ€™s where theyâ€™re getting it. I mean, that honeyâ€™s ours! MOOSEBLOOD Bees hang tight. BARRY Well, weâ€™re all jammed in there. Itâ€™s a close community. MOOSEBLOOD Not us, man. Weâ€™re on our own. Every mosquito is on his own. BARRY But what if you get in trouble? MOOSEBLOOD Trouble? You're a mosquito. You're in trouble! Nobody likes us. Theyâ€™re just all smacking. People see a mosquito, smack, smack! BARRY At least youâ€™re out in the world. You must meet a lot of girls. MOOSEBLOOD Mosquito girls try to trade up; get with a moth, dragonfly...mosquito girl donâ€™t want no mosquito. A BLOOD MOBILE pulls up alongside. MOOSEBLOOD (CONT'D) Whoa, you have got to be kidding me. Moosebloodâ€™s about to leave the building. So long bee. \"Bee Movie\" - JS REVISIONS 8/13/07 59. Mooseblood EXITS the horn, and jumps onto the blood mobile. MOOSEBLOOD (CONT'D) Hey guys. I knew Iâ€™d catch you all down here. Did you bring your crazy straws? CUT TO: SEQ. 1900 - â€œTHE APIARYâ€ EXT. APIARY - LATER Barry sees a SIGN, â€œHoney Farmsâ€ The truck comes to a stop. SFX: The Honey farms truck blares its horn. Barry flies out, lands on the hood. ANGLE ON: Two BEEKEEPERS, FREDDY and ELMO, walking around to the back of the gift shop. Barry follows them, and lands in a nearby tree FREDDY ...then we throw it in some jars, slap a label on it, and itâ€™s pretty much pure profit. BARRY What is this place? ELMO Bees got a brain the size of a pinhead. FREDDY They are pinheads. The both LAUGH. ANGLE ON: Barry REACTING. They arrive at the back of the shop where one of them opens a SMOKER BOX. FREDDY (CONTâ€™D) Hey, check out the new smoker. \"Bee Movie\" - JS REVISIONS 8/13/07 60. ELMO Oh, Sweet. Thatâ€™s the one you want. FREDDY The Thomas 3000. BARRY Smoker? FREDDY 90 puffs a minute, semi-automatic. Twice the nicotine, all the tar. They LAUGH again, nefariously. FREDDY (CONTâ€™D) Couple of breaths of this, and it knocks them right out. They make the honey, and we make the money. BARRY â€œThey make the honey, and we make the money?â€ Barry climbs onto the netting of Freddyâ€™s hat. He climbs up to the brim and looks over the edge. He sees the apiary boxes as Freddy SMOKES them. BARRY (CONT'D) Oh my. As Freddy turns around, Barry jumps into an open apiary box, and into an apartment. HOWARD and FRAN are just coming to from the smoking. BARRY (CONTâ€™D) Whatâ€™s going on? Are you okay? HOWARD Yeah, it doesnâ€™t last too long. HE COUGHS a few times. BARRY How did you two get here? Do you know youâ€™re in a fake hive with fake walls? HOWARD (pointing to a picture on the wall) \"Bee Movie\" - JS REVISIONS 8/13/07 61. Our queen was moved here, we had no choice. BARRY (looking at a picture on the wall) This is your queen? Thatâ€™s a man in womenâ€™s clothes. Thatâ€™s a dragqueen! The other wall opens. Barry sees the hundreds of apiary boxes. BARRY (CONT'D) What is this? Barry pulls out his camera, and starts snapping. BARRY (CONTâ€™D) Oh no. Thereâ€™s hundreds of them. (V.O, as Barry takes pictures) Bee honey, our honey, is being brazenly stolen on a massive scale. CUT TO: SEQ. 2100 - â€œBARRY TELLS FAMILYâ€ INT. BARRYâ€™S PARENTâ€™S HOUSE - LIVING ROOM - LATER Barry has assembled his parents, Adam, and Uncle Carl. BARRY This is worse than anything the bears have done to us. And I intend to do something about it. JANET BENSON Oh Barry, stop. MARTIN BENSON Who told you that humans are taking our honey? Thatâ€™s just a rumor. BARRY Do these look like rumors? Barry throws the PICTURES on the table. Uncle Carl, cleaning his glasses with his shirt tail, digs through a bowl of nuts with his finger. \"Bee Movie\" - JS REVISIONS 8/13/07 62. HOWARD (CONT'D) UNCLE CARL Thatâ€™s a conspiracy theory. These are obviously doctored photos. JANET BENSON Barry, how did you get mixed up in all this? ADAM (jumping up) Because heâ€™s been talking to humans! JANET BENSON Whaaat? MARTIN BENSON Talking to humans?! Oh Barry. ADAM He has a human girlfriend and they make out! JANET BENSON Make out? Barry? BARRY We do not. ADAM You wish you could. BARRY Whoâ€™s side are you on? ADAM The bees! Uncle Carl stands up and pulls his pants up to his chest. UNCLE CARL I dated a cricket once in San Antonio. Man, those crazy legs kept me up all night. Hotcheewah! JANET BENSON Barry, this is what you want to do with your life? BARRY This is what I want to do for all our lives. Nobody works harder than bees. \"Bee Movie\" - JS REVISIONS 8/13/07 63. Dad, I remember you coming home some nights so overworked, your hands were still stirring. You couldnâ€™t stop them. MARTIN BENSON Ehhh... JANET BENSON (to Martin) I remember that. BARRY What right do they have to our hardearned honey? Weâ€™re living on two cups a year. Theyâ€™re putting it in lip balm for no reason what-soever. MARTIN BENSON Even if itâ€™s true, Barry, what could one bee do? BARRY Iâ€™m going to sting them where it really hurts. MARTIN BENSON In the face? BARRY No. MARTIN BENSON In the eye? That would really hurt. BARRY No. MARTIN BENSON Up the nose? Thatâ€™s a killer. BARRY No. Thereâ€™s only one place you can sting the humans. One place where it really matters. CUT TO: SEQ. 2300 - â€œHIVE AT 5 NEWS/BEE LARRY KINGâ€ \"Bee Movie\" - JS REVISIONS 8/13/07 64. BARRY (CONT'D) INT. NEWS STUDIO - DAY DRAMATIC NEWS MUSIC plays as the opening news sequence rolls. We see the â€œHive at Fiveâ€ logo, followed by shots of past news events: A BEE freeway chase, a BEE BEARD protest rally, and a BEAR pawing at the hive as the BEES flee in panic. BOB BUMBLE (V.O.) Hive at Five, the hiveâ€™s only full hour action news source... SHOTS of NEWSCASTERS flash up on screen. BOB BUMBLE (V.O.) (CONT'D) With Bob Bumble at the anchor desk... BOB has a big shock of anchorman hair, gray temples and overly white teeth. BOB BUMBLE (V.O.) (CONT'D) ...weather with Storm Stinger, sports with Buzz Larvi, and Jeanette Chung. JEANETTE is an Asian bee. BOB BUMBLE (CONT'D) Good evening, Iâ€™m Bob Bumble. JEANETTE CHUNG And Iâ€™m Jeanette Chung. BOB BUMBLE Our top story, a tri-county bee, Barry Benson... INSERT: Barryâ€™s graduation picture. BOB BUMBLE (CONT'D) ...is saying he intends to sue the human race for stealing our honey, packaging it, and profiting from it illegally. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 65. INT. BEENN STUDIO - BEE LARRY KING LIVE BEE LARRY KING, wearing suspenders and glasses, is interviewing Barry. A LOWER-THIRD CHYRON reads: â€œBee Larry King Live.â€ BEE LARRY KING Donâ€™t forget, tomorrow night on Bee Larry King, we are going to have three former Queens all right here in our studio discussing their new book, â€œClassy Ladies,â€ out this week on Hexagon. (to Barry) Tonight, weâ€™re talking to Barry Benson. Did you ever think, Iâ€™m just a kid from the hive, I canâ€™t do this? BARRY Larry, bees have never been afraid to change the world. I mean, what about Bee-Columbus? Bee-Ghandi? Be-geesus? BEE LARRY KING Well, where Iâ€™m from you wouldnâ€™t think of suing humans. We were thinking more like stick ball, candy stores. BARRY How old are you? BEE LARRY KING I want you to know that the entire bee community is supporting you in this case, which is certain to be the trial of the bee century. BARRY Thank you, Larry. You know, they have a Larry King in the human world, too. BEE LARRY KING Itâ€™s a common name. Next week on Bee Larry King... \"Bee Movie\" - JS REVISIONS 8/13/07 66. BARRY No, I mean he looks like you. And he has a show with suspenders and different colored dots behind him. BEE LARRY KING Next week on Bee Larry King... BARRY Old guy glasses, and thereâ€™s quotes along the bottom from the guest youâ€™re watching even though you just heard them... BEE LARRY KING Bear week next week! Theyâ€™re scary, theyâ€™re hairy, and theyâ€™re here live. Bee Larry King EXITS. BARRY Always leans forward, pointy shoulders, squinty eyes... (lights go out) Very Jewish. CUT TO: SEQ. 2400 - â€œFLOWER SHOPâ€ INT. VANESSAâ€™S FLOWER SHOP - NIGHT Stacks of law books are piled up, legal forms, etc. Vanessa is talking with Ken in the other room. KEN Look, in tennis, you attack at the point of weakness. VANESSA But it was my grandmother, Ken. Sheâ€™s 81. KEN Honey, her backhandâ€™s a joke. Iâ€™m not going to take advantage of that? \"Bee Movie\" - JS REVISIONS 8/13/07 67. BARRY (O.C) Quiet please. Actual work going on here. KEN Is that that same bee? BARRY (O.C) Yes it is. VANESSA Iâ€™m helping him sue the human race. KEN What? Barry ENTERS. BARRY Oh, hello. KEN Hello Bee. Barry flies over to Vanessa. VANESSA This is Ken. BARRY Yeah, I remember you. Timberland, size 10 1/2, Vibram sole I believe. KEN Why does he talk again, Hun? VANESSA (to Ken, sensing the tension) Listen, youâ€™d better go because weâ€™re really busy working. KEN But itâ€™s our yogurt night. VANESSA (pushing him out the door) Oh...bye bye. She CLOSES the door. KEN Why is yogurt night so difficult?! \"Bee Movie\" - JS REVISIONS 8/13/07 68. Vanessa ENTERS the back room carrying coffee. VANESSA Oh you poor thing, you two have been at this for hours. BARRY Yes, and Adam here has been a huge help. ANGLE ON: A EMPTY CINNABON BOX with Adam asleep inside, covered in frosting. VANESSA How many sugars? BARRY Just one. I try not to use the competition. So, why are you helping me, anyway? VANESSA Bees have good qualities. BARRY (rowing on the sugar cube like a gondola) Si, Certo. VANESSA And it feels good to take my mind off the shop. I donâ€™t know why, instead of flowers, people are giving balloon bouquets now. BARRY Yeah, those are great...if youâ€™re 3. VANESSA And artificial flowers. BARRY (re: plastic flowers) Oh, they just get me psychotic! VANESSA Yeah, me too. BARRY The bent stingers, the pointless pollination. \"Bee Movie\" - JS REVISIONS 8/13/07 69. VANESSA Bees must hate those fake plastic things. BARRY Thereâ€™s nothing worse than a daffodil thatâ€™s had work done. VANESSA (holding up the lawsuit documents) Well, maybe this can make up for it a little bit. CUT TO: EXT. VANESSAâ€™S FLORIST SHOP They EXIT the store, and cross to the mailbox. VANESSA You know Barry, this lawsuit is a pretty big deal. BARRY I guess. VANESSA Are you sure that you want to go through with it? BARRY Am I sure? (kicking the envelope into the mailbox) When Iâ€™m done with the humans, they wonâ€™t be able to say, â€œHoney, Iâ€™m home,â€ without paying a royalty. CUT TO: SEQ. 2700 - â€œMEET MONTGOMERYâ€ EXT. MANHATTAN COURTHOUSE - DAY P.O.V SHOT - A camera feed turns on, revealing a newsperson. \"Bee Movie\" - JS REVISIONS 8/13/07 70. PRESS PERSON #2 (talking to camera) Sarah, itâ€™s an incredible scene here in downtown Manhattan where all eyes and ears of the world are anxiously waiting, because for the first time in history, weâ€™re going to hear for ourselves if a honey bee can actually speak. ANGLE ON: Barry, Vanessa, and Adam getting out of the cab. The press spots Barry and Vanessa and pushes in. Adam sits on Vanessaâ€™s shoulder. INT. COURTHOUSE - CONTINUOUS Barry, Vanessa, and Adam sit at the Plaintiffâ€™s Table. VANESSA (turns to Barry) What have we gotten into here, Barry? BARRY I donâ€™t know, but itâ€™s pretty big, isnâ€™t it? ADAM I canâ€™t believe how many humans donâ€™t have to be at work during the day. BARRY Hey, you think these billion dollar multinational food companies have good lawyers? CUT TO: EXT. COURTHOUSE STEPS - CONTINUOUS A BIG BLACK CAR pulls up. ANGLE ON: the grill filling the frame. We see the â€œL.T.Mâ€ monogram on the hood ornament. The defense lawyer, LAYTON T. MONTGOMERY comes out, squashing a bug on the pavement. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 71. INT. COURTHOUSE - CONTINUOUS Barry SHUDDERS. VANESSA Whatâ€™s the matter? BARRY I donâ€™t know. I just got a chill. Montgomery ENTERS. He walks by Barryâ€™s table shaking a honey packet. MONTGOMERY Well, if it isnâ€™t the B-Team. (re: the honey packet) Any of you boys work on this? He CHUCKLES. The JUDGE ENTERS. SEQ. 3000 - â€œWITNESSESâ€ BAILIFF All rise! The Honorable Judge Bumbleton presiding. JUDGE (shuffling papers) Alright...Case number 4475, Superior Court of New York. Barry Bee Benson vs. the honey industry, is now in session. Mr. Montgomery, you are representing the five major food companies, collectively. ANGLE ON: Montgomeryâ€™s BRIEFCASE. It has an embossed emblem of an EAGLE, holding a gavel in one talon and a briefcase in the other. MONTGOMERY A privilege. JUDGE Mr. Benson. Barry STANDS. JUDGE (CONTâ€™D) You are representing all bees of the world? \"Bee Movie\" - JS REVISIONS 8/13/07 72. Montgomery, the stenographer, and the jury lean in. CUT TO: EXT. COURTHOUSE - CONTINUOUS The spectators outside freeze. The helicopters angle forward to listen closely. CUT TO: INT. COURTHOUSE BARRY Bzzz bzzz bzzz...Ahh, Iâ€™m kidding, Iâ€™m kidding. Yes, your honor. We are ready to proceed. ANGLE ON: Courtroom hub-bub. JUDGE And Mr. Montgomery, your opening statement, please. Montgomery rises. MONTGOMERY (grumbles, clears his throat) Ladies and gentlemen of the jury. My grandmother was a simple woman. Born on a farm, she believed it was man's divine right to benefit from the bounty of nature God put before us. If we were to live in the topsy-turvy world Mr. Benson imagines, just think of what it would mean. Maybe I would have to negotiate with the silk worm for the elastic in my britches. Talking bee. How do we know this isnâ€™t some sort of holographic motion picture capture Hollywood wizardry? They could be using laser beams, robotics, ventriloquism, cloning...for all we know he could be on steroids! Montgomery leers at Barry, who moves to the stand. \"Bee Movie\" - JS REVISIONS 8/13/07 73. JUDGE Mr. Benson? Barry makes his opening statement. BARRY Ladies and Gentlemen of the jury, thereâ€™s no trickery here. Iâ€™m just an ordinary bee. And as a bee, honeyâ€™s pretty important to me. Itâ€™s important to all bees. We invented it, we make it, and we protect it with our lives. Unfortunately, there are some people in this room who think they can take whatever they want from us cause weâ€™re the little guys. And what Iâ€™m hoping is that after this is all over, youâ€™ll see how by taking our honey, youâ€™re not only taking away everything we have, but everything we are. ANGLE ON: Vanessa smiling. ANGLE ON: The BEE GALLERY wiping tears away. CUT TO: INT. BENSON HOUSE Barryâ€™s family is watching the case on TV. JANET BENSON Oh, I wish he would dress like that all the time. So nice... CUT TO: INT. COURTROOM - LATER JUDGE Call your first witness. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 74. INT. COURTHOUSE - LATER BARRY So, Mr. Klauss Vanderhayden of Honey Farms. Pretty big company you have there? MR. VANDERHAYDEN I suppose so. BARRY And I see you also own HoneyBurton, and Hon-Ron. MR. VANDERHAYDEN Yes. They provide beekeepers for our farms. BARRY Beekeeper. I find that to be a very disturbing term, I have to say. I donâ€™t imagine you employ any bee free-ers, do you? MR. VANDERHAYDEN No. BARRY Iâ€™m sorry. I couldnâ€™t hear you. MR. VANDERHAYDEN (louder) No. BARRY No. Because you donâ€™t free bees. You keep bees. And not only that, it seems you thought a bear would be an appropriate image for a jar of honey? MR. VANDERHAYDEN Well, theyâ€™re very lovable creatures. Yogi-bear, Fozzy-bear, Build-a-bear. BARRY Yeah, you mean like this?! Vanessa and the SUPERINTENDANT from her building ENTER with a GIANT FEROCIOUS GRIZZLY BEAR. He has a neck collar and chains extending from either side. \"Bee Movie\" - JS REVISIONS 8/13/07 75. By pulling the chains, they bring him directly in front of Vanderhayden. The bear LUNGES and ROARS. BARRY (CONT'D) Bears kill bees! How would you like his big hairy head crashing into your living room? Biting into your couch, spitting out your throwpillows...rowr, rowr! The bear REACTS. BEAR Rowr!! BARRY Okay, thatâ€™s enough. Take him away. Vanessa and the Superintendant pull the bear out of the courtroom. Vanderhayden TREMBLES. The judge GLARES at him. CUT TO: INT. COURTROOM- A LITTLE LATER Barry questions STING. BARRY So, Mr. Sting. Thank you for being here. Your name intrigues me, I have to say. Where have I heard it before? STING I was with a band called \"The Police\". BARRY But you've never been a police officer of any kind, have you? STING No, I haven't. \"Bee Movie\" - JS REVISIONS 8/13/07 76. BARRY No, you havenâ€™t. And so, here we have yet another example of bee culture being casually stolen by a human for nothing more than a prance-about stage name. STING Oh please. BARRY Have you ever been stung, Mr. Sting? Because I'm feeling a little stung, Sting. Or should I say, (looking in folder) Mr. Gordon M. Sumner? The jury GASPS. MONTGOMERY (to his aides) Thatâ€™s not his real name? You idiots! CUT TO: INT. COURTHOUSE- LATER BARRY Mr. Liotta, first may I offer my belated congratulations on your Emmy win for a guest spot on E.R. in 2005. LIOTTA Thank you. Thank you. Liotta LAUGHS MANIACALLY. BARRY I also see from your resume that youâ€™re devilishly handsome, but with a churning inner turmoil thatâ€™s always ready to blow. LIOTTA I enjoy what I do. Is that a crime? \"Bee Movie\" - JS REVISIONS 8/13/07 77. BARRY Not yet it isnâ€™t. But is this what itâ€™s come to for you, Mr. Liotta? Exploiting tiny helpless bees so you donâ€™t have to rehearse your part, and learn your lines, Sir? LIOTTA Watch it Benson, I could blow right now. BARRY This isnâ€™t a goodfella. This is a badfella! LIOTTA (exploding, trying to smash Barry with the Emmy) Why doesnâ€™t someone just step on this little creep and we can all go home? Youâ€™re all thinking it. Say it! JUDGE Order! Order in this courtroom! A MONTAGE OF NEWSPAPER HEADLINES FOLLOWS: NEW YORK POST: â€œBees to Humans: Buzz Offâ€. NEW YORK TELEGRAM: â€œSue Beeâ€. DAILY VARIETY: â€œStudio Dumps Liotta Project. Slams Door on Unlawful Entry 2.â€ CUT TO: SEQ. 3175 - â€œCANDLELIGHT DINNERâ€ INT. VANESSAâ€™S APARTMENT Barry and Vanessa are having a candle light dinner. Visible behind Barry is a â€œLITTLE MISSYâ€ SET BOX, with the flaps open. BARRY Well, I just think that was awfully nice of that bear to pitch in like that. \"Bee Movie\" - JS REVISIONS 8/13/07 78. VANESSA Iâ€™m telling you, I think the juryâ€™s on our side. BARRY Are we doing everything right...you know, legally? VANESSA Iâ€™m a florist. BARRY Right, right. Barry raises his glass. BARRY (CONTâ€™D) Well, hereâ€™s to a great team. VANESSA To a great team. They toast. Ken ENTERS KEN Well hello. VANESSA Oh...Ken. BARRY Hello. VANESSA I didnâ€™t think you were coming. KEN No, I was just late. I tried to call. But, (holding his cell phone) the battery... VANESSA I didnâ€™t want all this to go to waste, so I called Barry. Luckily he was free. BARRY Yeah. KEN (gritting his teeth) Oh, that was lucky. \"Bee Movie\" - JS REVISIONS 8/13/07 79. VANESSA Well, thereâ€™s still a little left. I could heat it up. KEN Yeah, heat it up. Sure, whatever. Vanessa EXITS. Ken and Barry look at each other as Barry eats. BARRY So, I hear youâ€™re quite a tennis player. Iâ€™m not much for the game myself. I find the ball a little grabby. KEN Thatâ€™s where I usually sit. Right there. VANESSA (O.C) Ken, Barry was looking at your resume, and he agreed with me that â€œeating with chopsticksâ€ isnâ€™t really a special skill. KEN (to Barry) You think I donâ€™t see what youâ€™re doing? BARRY Hey look, I know how hard it is trying to find the right job. We certainly have that in common. KEN Do we? BARRY Well, bees have 100% employment, of course. But we do jobs like taking the crud out. KEN Thatâ€™s just what I was thinking about doing. Ken holds his table knife up. It slips out of his hand. He goes under the table to pick it up. \"Bee Movie\" - JS REVISIONS 8/13/07 80. VANESSA Ken, I let Barry borrow your razor for his fuzz. I hope that was alright. Ken hits his head on the table. BARRY Iâ€™m going to go drain the old stinger. KEN Yeah, you do that. Barry EXITS to the bathroom, grabbing a small piece of a VARIETY MAGAZINE on the way. BARRY Oh, look at that. Ken slams the champagne down on the table. Ken closes his eyes and buries his face in his hands. He grabs a magazine on the way into the bathroom. SEQ. 2800 - â€œBARRY FIGHTS KENâ€ INT. BATHROOM - CONTINUOUS Ken ENTERS, closes the door behind him. Heâ€™s not happy. Barry is washing his hands. He glances back at Ken. KEN You know, Iâ€™ve just about had it with your little mind games. BARRY Whatâ€™s that? KEN Italian Vogue. BARRY Mamma Mia, thatâ€™s a lot of pages. KEN Itâ€™s a lot of ads. BARRY Remember what Van said. Why is your life any more valuable than mine? \"Bee Movie\" - JS REVISIONS 8/13/07 81. KEN Itâ€™s funny, I just canâ€™t seem to recall that! Ken WHACKS at Barry with the magazine. He misses and KNOCKS EVERYTHING OFF THE VANITY. Ken grabs a can of AIR FRESHENER. KEN (CONT'D) I think something stinks in here. He sprays at Barry. BARRY I love the smell of flowers. KEN Yeah? How do you like the smell of flames? Ken lights the stream. BARRY Not as much. Barry flies in a circle. Ken, trying to stay with him, spins in place. ANGLE ON: Flames outside the bathroom door. Ken slips on the Italian Vogue, falls backward into the shower, pulling down the shower curtain. The can hits him in the head, followed by the shower curtain rod, and the rubber duck. Ken reaches back, grabs the handheld shower head. He whips around, looking for Barry. ANGLE ON: A WATERBUG near the drain. WATERBUG Waterbug. Not taking sides. Barry is on the toilet tank. He comes out from behind a shampoo bottle, wearing a chapstick cap as a helmet. BARRY Ken, look at me! Iâ€™m wearing a chapstick hat. This is pathetic. ANGLE ON: Ken turning the hand shower nozzle from â€œGENTLEâ€, to â€œTURBOâ€, to â€œLETHALâ€. \"Bee Movie\" - JS REVISIONS 8/13/07 82. KEN Iâ€™ve got issues! Ken fires the water at Barry, knocking him into the toilet. The items from the vanity (emory board, lipstick, eye curler, etc.) are on the toilet seat. Ken looks down at Barry. KEN (CONT'D) Well well well, a royal flush. BARRY Youâ€™re bluffing. KEN Am I? Ken flushes the toilet. Barry grabs the Emory board and uses it to surf. He puts his hand in the water while heâ€™s surfing. Some water splashes on Ken. BARRY Surfâ€™s up, dude! KEN Awww, poo water! He does some skate board-style half-pipe riding. Barry surfs out of the toilet. BARRY That bowl is gnarly. Ken tries to get a shot at him with the toilet brush. KEN Except for those dirty yellow rings. Vanessa ENTERS. VANESSA Kenneth! What are you doing? KEN You know what? I donâ€™t even like honey! I donâ€™t eat it! VANESSA We need to talk! \"Bee Movie\" - JS REVISIONS 8/13/07 83. She pulls Ken out by his ear. Ken glares at Barry. CUT TO: INT. HALLWAY - CONTINUOUS VANESSA Heâ€™s just a little bee. And he happens to be the nicest bee Iâ€™ve met in a long time. KEN Long time? What are you talking about? Are there other bugs in your life? VANESSA No, but there are other things bugging me in life. And youâ€™re one of them! KEN Fine! Talking bees, no yogurt night...my nerves are fried from riding on this emotional rollercoaster. VANESSA Goodbye, Ken. KEN Augh! VANESSA Whew! Ken EXITS, then re-enters frame. KEN And for your information, I prefer sugar-free, artificial sweeteners, made by man! He EXITS again. The DOOR SLAMS behind him. VANESSA (to Barry) Iâ€™m sorry about all that. Ken RE-ENTERS. \"Bee Movie\" - JS REVISIONS 8/13/07 84. KEN I know itâ€™s got an aftertaste! I like it! BARRY (re: Ken) I always felt there was some kind of barrier between Ken and me. (puts his hands in his pockets) I couldnâ€™t overcome it. Oh well. VANESSA Are you going to be okay for the trial tomorrow? BARRY Oh, I believe Mr. Montgomery is about out of ideas. CUT TO: SEQ. 3300 - â€œADAM STINGS MONTYâ€ INT. COURTROOM - NEXT DAY ANGLE ON: Medium shot of Montgomery standing at his table. MONTGOMERY We would like to call Mr. Barry Benson Bee to the stand. ADAM (whispering to Vanessa) Now thatâ€™s a good idea. (to Barry) You can really see why heâ€™s considered one of the very best lawyers-- Oh. Barry rolls his eyes. He gets up, takes the stand. A juror in a striped shirt APPLAUDS. MR. GAMMIL (whispering) Layton, youâ€™ve got to weave some magic with this jury, or itâ€™s going to be all over. Montgomery is holding a BOOK, â€œThe Secret Life of Beesâ€. \"Bee Movie\" - JS REVISIONS 8/13/07 85. MONTGOMERY (confidently whispering) Oh, donâ€™t worry Mr. Gammil. The only thing I have to do to turn this jury around is to remind them of what they donâ€™t like about bees. (to Gammil) You got the tweezers? Mr. Gammil NODS, and pats his breast pocket. MR. GAMMIL Are you allergic? MONTGOMERY Only to losing, son. Only to losing. Montgomery approaches the stand. MONTGOMERY (CONTâ€™D) Mr. Benson Bee. Iâ€™ll ask you what I think weâ€™d all like to know. What exactly is your relationship to that woman? Montgomery points to Vanessa. BARRY Weâ€™re friends. MONTGOMERY Good friends? BARRY Yes. MONTGOMERY (softly in Barryâ€™s face) How good? BARRY What? MONTGOMERY Do you live together? BARRY Wait a minute, this isnâ€™t about-- \"Bee Movie\" - JS REVISIONS 8/13/07 86. MONTGOMERY Are you her little... (clearing throat) ... bed bug? BARRY (flustered) Hey, thatâ€™s not the kind of-- MONTGOMERY Iâ€™ve seen a bee documentary or two. Now, from what I understand, doesnâ€™t your Queen give birth to all the bee children in the hive? BARRY Yeah, but-- MONTGOMERY So those arenâ€™t even your real parents! ANGLE ON: Barryâ€™s parents. MARTIN BENSON Oh, Barry. BARRY Yes they are! ADAM Hold me back! Vanessa holds him back with a COFFEE STIRRER. Montgomery points to Barryâ€™s parents. MONTGOMERY Youâ€™re an illegitimate bee, arenâ€™t you Benson? ADAM Heâ€™s denouncing bees! All the bees in the courtroom start to HUM. Theyâ€™re agitated. MONTGOMERY And donâ€™t yâ€™all date your cousins? \"Bee Movie\" - JS REVISIONS 8/13/07 87. VANESSA (standing, letting go of Adam) Objection! Adam explodes from the table and flies towards Montgomery. ADAM Iâ€™m going to pin cushion this guy! Montgomery turns around and positions himself by the judgeâ€™s bench. He sticks his butt out. Montgomery winks at his team. BARRY Adam, donâ€™t! Itâ€™s what he wants! Adam shoves Barry out of the way. Adam STINGS Montgomery in the butt. The jury REACTS, aghast. MONTGOMERY Ow! Iâ€™m hit! Oh, lordy, I am hit! The judge BANGS her gavel. JUDGE Order! Order! Please, Mr. Montgomery. MONTGOMERY The venom! The venom is coursing through my veins! I have been felled by a wing-ed beast of destruction. You see? You canâ€™t treat them like equals. Theyâ€™re strip-ed savages! Stingingâ€™s the only thing they know! Itâ€™s their way! ANGLE ON: Adam, collapsed on the floor. Barry rushes to his side. BARRY Adam, stay with me. ADAM I canâ€™t feel my legs. Montgomery falls on the Bailiff. BAILIFF Take it easy. \"Bee Movie\" - JS REVISIONS 8/13/07 88. MONTGOMERY Oh, what angel of mercy will come forward to suck the poison from my heaving buttocks? The JURY recoils. JUDGE Please, I will have order in this court. Order! Order, please! FADE TO: SEQ. 3400 - â€œADAM AT HOSPITALâ€ INT. HOSPITAL - STREET LEVEL ROOM - DAY PRESS PERSON #1 (V.O) The case of the honey bees versus the human race took a pointed turn against the bees yesterday, when one of their legal team stung Layton T. Montgomery. Now hereâ€™s Don with the 5-day. A NURSE lets Barry into the room. Barry CARRIES a FLOWER. BARRY Thank you. Barry stands over Adam, in a bed. Barry lays the flower down next to him. The TV is on. BARRY (CONT'D) Hey buddy. ADAM Hey. BARRY Is there much pain? Adam has a BEE-SIZED PAINKILLER HONEY BUTTON near his head that he presses. ADAM (pressing the button) Yeah...I blew the whole case, didnâ€™t I? \"Bee Movie\" - JS REVISIONS 8/13/07 89. BARRY Oh, it doesnâ€™t matter. The important thing is youâ€™re alive. You could have died. ADAM Iâ€™d be better off dead. Look at me. Adam THROWS the blanket off his lap, revealing a GREEN SANDWICH SWORD STINGER. ADAM (CONTâ€™D) (voice cracking) They got it from the cafeteria, they got it from downstairs. In a tuna sandwich. Look, thereâ€™s a little celery still on it. BARRY What was it like to sting someone? ADAM I canâ€™t explain it. It was all adrenaline...and then...ecstasy. Barry looks at Adam. BARRY Alright. ADAM You think that was all a trap? BARRY Of course. Iâ€™m sorry. I flew us right into this. What were we thinking? Look at us, weâ€™re just a couple of bugs in this world. ADAM What do you think the humans will do to us if they win? BARRY I donâ€™t know. ADAM I hear they put the roaches in motels. That doesnâ€™t sound so bad. \"Bee Movie\" - JS REVISIONS 8/13/07 90. BARRY Adam, they check in, but they donâ€™t check out. Adam GULPS. ADAM Oh my. ANGLE ON: the hospital window. We see THREE PEOPLE smoking outside on the sidewalk. The smoke drifts in. Adam COUGHS. ADAM (CONTâ€™D) Say, could you get a nurse to close that window? BARRY Why? ADAM The smoke. Bees donâ€™t smoke. BARRY Right. Bees donâ€™t smoke. Bees donâ€™t smoke! But some bees are smoking. Adam, thatâ€™s it! Thatâ€™s our case. Adam starts putting his clothes on. ADAM It is? Itâ€™s not over? BARRY No. Get up. Get dressed. Iâ€™ve got to go somewhere. You get back the court and stall. Stall anyway you can. CUT TO: SEQ. 3500 - â€œSMOKING GUNâ€ INT. COURTROOM - THE NEXT DAY Adam is folding a piece of paper into a boat. ADAM ...and assuming youâ€™ve done step 29 correctly, youâ€™re ready for the tub. \"Bee Movie\" - JS REVISIONS 8/13/07 91. ANGLE ON: The jury, all with paper boats of their own. JURORS Ooh. ANGLE ON: Montgomery frustrated with Gammil, whoâ€™s making a boat also. Monty crumples Gammilâ€™s boat, and throws it at him. JUDGE Mr. Flayman? ADAM Yes? Yes, Your Honor? JUDGE Where is the rest of your team? ADAM (fumbling with his swordstinger) Well, your honor, itâ€™s interesting. You know Bees are trained to fly kind of haphazardly and as a result quite often we donâ€™t make very good time. I actually once heard a pretty funny story about a bee-- MONTGOMERY Your Honor, havenâ€™t these ridiculous bugs taken up enough of this courtâ€™s valuable time? Montgomery rolls out from behind his table. Heâ€™s suspended in a LARGE BABY CHAIR with wheels. MONTGOMERY (CONT'D) How much longer are we going to allow these absurd shenanigans to go on? They have presented no compelling evidence to support their charges against my clients who have all run perfectly legitimate businesses. I move for a complete dismissal of this entire case. JUDGE Mr. Flayman, I am afraid I am going to have to consider Mr. Montgomeryâ€™s motion. \"Bee Movie\" - JS REVISIONS 8/13/07 92. ADAM But you canâ€™t. We have a terrific case. MONTGOMERY Where is your proof? Where is the evidence? Show me the smoking gun. Barry bursts through the door. BARRY Hold it, your honor. You want a smoking gun? Here is your smoking gun. Vanessa ENTERS, holding a bee smoker Vanessa slams the beekeeper's SMOKER onto the judgeâ€™s bench. JUDGE What is that? BARRY Itâ€™s a Bee smoker. Montgomery GRABS the smoker. MONTGOMERY What, this? This harmless little contraption? This couldnâ€™t hurt a fly, let alone a bee. He unintentionally points it towards the bee gallery, KNOCKING THEM ALL OUT. The jury GASPS. The press SNAPS pictures of them. BARRY Members of the jury, look at what has happened to bees who have never been asked, \"Smoking or Non?\" Is this what nature intended for us? To be forcibly addicted to these smoke machines in man-made wooden slat work camps? Living out our lives as honey slaves to the white man? Barry gestures dramatically towards Montgomery's racially mixed table. The BLACK LAWYER slowly moves his chair away. GAMMIL What are we going to do? \"Bee Movie\" - JS REVISIONS 8/13/07 93. MONTGOMERY (to Pross) He's playing the species card. Barry lands on the scale of justice, by the judgeâ€™s bench. It balances as he lands. BARRY Ladies and gentlemen, please, FreeThese-Bees! ANGLE ON: Jury, chanting \"Free the bees\". JUDGE The court finds in favor of the bees. The chaos continues. Barry flies over to Vanessa, with his hand up for a â€œhigh 5â€. BARRY Vanessa, we won! VANESSA Yay! I knew you could do it. Highfive! She high 5â€™s Barry, sending him crashing to the table. He bounces right back up. VANESSA (CONT'D) Oh, sorry. BARRY Ow!! Iâ€™m okay. Vanessa, do you know what this means? All the honey is finally going to belong to the bees. Now we wonâ€™t have to work so hard all the time. Montgomery approaches Barry, surrounded by the press. The cameras and microphones go to Montgomery. MONTGOMERY (waving a finger) This is an unholy perversion of the balance of nature, Benson! Youâ€™ll regret this. ANGLE ON: Barryâ€™s â€˜deer in headlightsâ€™ expression, as the press pushes microphones in his face. \"Bee Movie\" - JS REVISIONS 8/13/07 94. PRESS PERSON 1 Barry, how much honey do you think is out there? BARRY Alright, alright, one at a time... SARAH Barry, who are you wearing? BARRY Uhhh, my sweater is Ralph Lauren, and I have no pants. The Press follows Barry as he EXITS. ANGLE ON: Adam and Vanessa. ADAM (putting papers away) What if Montgomeryâ€™s right? VANESSA What do you mean? ADAM Weâ€™ve been living the bee way a long time. 27 million years. DISSOLVE TO: SEQ. 3600 - â€œHONEY ROUNDUPâ€ EXT. HONEY FARMS APIARY - MONTAGE SARAH (V.O) Congratulations on your victory. What are you going to demand as a settlement? BARRY (V.O) (over montage) First, weâ€™re going to demand a complete shutdown of all bee work camps. Then, we want to get back all the honey that was ours to begin with. Every last drop. We demand an end to the glorification of the bear as anything more than a filthy, smelly, big-headed, bad breath, stink-machine. \"Bee Movie\" - JS REVISIONS 8/13/07 95. I believe weâ€™re all aware of what they do in the woods. We will no longer tolerate derogatory beenegative nick-names, unnecessary inclusion of honey in bogus health products, and la-dee-da tea-time human snack garnishments. MONTAGE IMAGES: Close-up on an ATF JACKET, with the YELLOW LETTERS. Camera pulls back. We see an ARMY OF BEE AND HUMAN AGENTS wearing hastily made â€œAlcohol, Tobacco, Firearms, and Honeyâ€ jackets. Barry supervises. The gate to Honey Farms is locked permanently. All the smokers are collected and locked up. All the bees leave the Apiary. CUT TO: EXT. ATF OUTSIDE OF SUPERMARKET - MONTAGE Agents begin YANKING honey off the supermarket shelves, and out of shopping baskets. CUT TO: EXT. NEW HIVE CITY - MONTAGE The bees tear down a honey-bear statue. CUT TO: EXT. YELLOWSTONE FOREST - MONTAGE POV of a sniperâ€™s crosshairs. An animated BEAR character looka-like, turns his head towards camera. BARRY Wait for my signal. ANGLE ON: Barry lowering his binoculars. BARRY (CONT'D) Take him out. The sniper SHOOTS the bear. It hits him in the shoulder. The bear looks at it. He gets woozy and the honey jar falls out of his lap, an ATF&amp;H agent catches it. \"Bee Movie\" - JS REVISIONS 8/13/07 96. BARRY (V.O) (CONT'D) ATF&amp;H AGENT (to the bearâ€™s pig friend) Heâ€™ll have a little nausea for a few hours, then heâ€™ll be fine. CUT TO: EXT. STINGâ€™S HOUSE - MONTAGE ATF&amp;H agents SLAP CUFFS on Sting, who is meditating. STING But itâ€™s just a prance-about stage name! CUT TO: INT. A WOMANâ€™S SHOWER - MONTAGE A WOMAN is taking a shower, and using honey shampoo. An ATF&amp;H agent pulls the shower curtain aside, and grabs her bottle of shampoo. The woman SCREAMS. The agent turns to the 3 other agents, and Barry. ANGLE ON: Barry looking at the label on the shampoo bottle, shaking his head and writing in his clipboard. CUT TO: EXT. SUPERMARKET CAFE - MONTAGE Another customer, an old lady having her tea with a little jar of honey, gets her face pushed down onto the table and turned to the side by two agents. One of the agents has a gun on her. OLD LADY Canâ€™t breathe. CUT TO: EXT. CENTRAL PARK - MONTAGE An OIL DRUM of honey is connected to Barryâ€™s hive. \"Bee Movie\" - JS REVISIONS 8/13/07 97. BARRY Bring it in, boys. CUT TO: SEQ. 3650 - â€œNO MORE WORKâ€ INT. HONEX - MONTAGE ANGLE ON: The honey goes past the 3-cup hash-mark, and begins to overflow. A WORKER BEE runs up to Buzzwell. WORKER BEE 1 Mr. Buzzwell, we just passed 3 cups, and thereâ€™s gallons mores coming. I think we need to shutdown. KEYCHAIN BEE (to Buzzwell) Shutdown? Weâ€™ve never shutdown. ANGLE ON: Buzzwell overlooking the factory floor. BUZZWELL Shutdown honey production! Stop making honey! ANGLE ON: TWO BEES, each with a KEY. BUZZWELL (CONTâ€™D) Turn your key, Sir! They turn the keys simultaneously, War Games-style, shutting down the honey machines. ANGLE ON: the Taffy-Pull machine, Centrifuge, and Krelman all slowly come to a stop. The bees look around, bewildered. WORKER BEE 5 What do we do now? A BEAT. WORKER BEE 6 Cannon ball!! He jumps into a HONEY VAT, doesnâ€™t penetrate the surface. He looks around, and slowly sinks down to his waist. \"Bee Movie\" - JS REVISIONS 8/13/07 98. EXT. HONEX FACTORY THE WHISTLE BLOWS, and the bees all stream out the exit. CUT TO: INT. J-GATE - CONTINUOUS Lou Loduca gives orders to the pollen jocks. LOU LODUCA Weâ€™re shutting down honey production. Mission abort. CUT TO: EXT. CENTRAL PARK Jackson receives the orders, mid-pollination. JACKSON Aborting pollination and nectar detail. Returning to base. CUT TO: EXT. NEW HIVE CITY ANGLE ON: Bees, putting sun-tan lotion on their noses and antennae, and sunning themselves on the balconies of the gyms. CUT TO: EXT. CENTRAL PARK ANGLE ON: THE FLOWERS starting to DROOP. CUT TO: INT. J-GATE J-Gate is deserted. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 99. EXT. NEW HIVE CITY ANGLE ON: Bees sunning themselves. A TIMER DINGS, and they all turn over. CUT TO: EXT. CENTRAL PARK TIME LAPSE of Central Park turning brown. CUT TO: EXT. VANESSAâ€™S FLORIST SHOP CLOSE-UP SHOT: Vanessa writes â€œSorry. No more flowers.â€ on a â€œClosedâ€ sign, an turns it facing out. CUT TO: SEQ. 3700 - â€œIDLE HIVEâ€ EXT. NEW HIVE CITY - DAY Barry flies at high speed. TRACKING SHOT into the hive, through the lobby of Honex, and into Adamâ€™s office. CUT TO: INT. ADAMâ€™S OFFICE - CONTINUOUS Barry meets Adam in his office. Adamâ€™s office is in disarray. There are papers everywhere. Heâ€™s filling up his cardboard hexagon box. BARRY (out of breath) Adam, you wouldnâ€™t believe how much honey was out there. ADAM Oh yeah? BARRY Whatâ€™s going on around here? Where is everybody? Are they out celebrating? \"Bee Movie\" - JS REVISIONS 8/13/07 100. ADAM (exiting with a cardboard box of belongings) No, theyâ€™re just home. They donâ€™t know what to do. BARRY Hmmm. ADAM Theyâ€™re laying out, theyâ€™re sleeping in. I heard your Uncle Carl was on his way to San Antonio with a cricket. BARRY At least we got our honey back. They walk through the empty factory. ADAM Yeah, but sometimes I think, so what if the humans liked our honey? Who wouldnâ€™t? Itâ€™s the greatest thing in the world. I was excited to be a part of making it. ANGLE ON: Adamâ€™s desk on itâ€™s side in the hall. ADAM (CONTâ€™D) This was my new desk. This was my new job. I wanted to do it really well. And now...and now I canâ€™t. Adam EXITS. CUT TO: SEQ. 3900 - â€œWORLD WITHOUT BEESâ€ INT. STAIRWELL Vanessa and Barry are walking up the stairs to the roof. BARRY I donâ€™t understand why theyâ€™re not happy. We have so much now. I thought their lives would be better. \"Bee Movie\" - JS REVISIONS 8/13/07 101. VANESSA Hmmm. BARRY Theyâ€™re doing nothing. Itâ€™s amazing, honey really changes people. VANESSA You donâ€™t have any idea whatâ€™s going on, do you? BARRY What did you want to show me? VANESSA This. They reach the top of the stairs. Vanessa opens the door. CUT TO: EXT. VANESSAâ€™S ROOFTOP - CONTINUOUS Barry sees Vanessaâ€™s flower pots and small garden have all turned brown. BARRY What happened here? VANESSA That is not the half of it... Vanessa turns Barry around with her two fingers, revealing the view of Central Park, which is also all brown. BARRY Oh no. Oh my. Theyâ€™re all wilting. VANESSA Doesnâ€™t look very good, does it? BARRY No. VANESSA And whoâ€™s fault do you think that is? \"Bee Movie\" - JS REVISIONS 8/13/07 102. BARRY Mmmm...you know, Iâ€™m going to guess, bees. VANESSA Bees? BARRY Specifically me. I guess I didnâ€™t think that bees not needing to make honey would affect all these other things. VANESSA And itâ€™s not just flowers. Fruits, vegetables...they all need bees. BARRY Well, thatâ€™s our whole SAT test right there. VANESSA So, you take away the produce, that affects the entire animal kingdom. And then, of course... BARRY The human species? VANESSA (clearing throat) Ahem! BARRY Oh. So, if thereâ€™s no more pollination, it could all just go south here, couldnâ€™t it? VANESSA And I know this is also partly my fault. Barry takes a long SIGH. BARRY How about a suicide pact? VANESSA (not sure if heâ€™s joking) How would we do it? BARRY Iâ€™ll sting you, you step on me. \"Bee Movie\" - JS REVISIONS 8/13/07 103. VANESSA That just kills you twice. BARRY Right, right. VANESSA Listen Barry. Sorry but Iâ€™ve got to get going. She EXITS. BARRY (looking out over the park) Had to open my mouth and talk... (looking back) Vanessa..? Vanessa is gone. CUT TO: SEQ. 3935 - â€œGOING TO PASADENAâ€ EXT. NY STREET - CONTINUOUS Vanessa gets into a cab. Barry ENTERS. BARRY Vanessa. Why are you leaving? Where are you going? VANESSA To the final Tournament of Roses parade in Pasadena. They moved it up to this weekend because all the flowers are dying. Itâ€™s the last chance Iâ€™ll ever have to see it. BARRY Vanessa, I just want to say Iâ€™m sorry. I never meant it to turn out like this. VANESSA I know. Me neither. Vanessa cab drives away. \"Bee Movie\" - JS REVISIONS 8/13/07 104. BARRY (chuckling to himself) Tournament of Roses. Roses canâ€™t do sports. Wait a minute...roses. Roses? Roses!? Vanessa! Barry follows shortly after. He catches up to it, and he pounds on the window. Barry follows shortly after Vanessaâ€™s cab. He catches up to it, and he pounds on the window. INT. TAXI - CONTINUOUS Barry motions for her to roll the window down. She does so. BARRY Roses?! VANESSA Barry? BARRY (as he flies next to the cab) Roses are flowers. VANESSA Yes, they are. BARRY Flowers, bees, pollen! VANESSA I know. Thatâ€™s why this is the last parade. BARRY Maybe not. The cab starts pulling ahead of Barry. BARRY (CONT'D) (re: driver) Could you ask him to slow down? VANESSA Could you slow down? The cabs slows. Barry flies in the window, and lands in the change box, which closes on him. \"Bee Movie\" - JS REVISIONS 8/13/07 105. VANESSA (CONT'D) Barry! Vanessa lets him out. Barry stands on the change box, in front of the driverâ€™s license. BARRY Okay, I made a huge mistake! This is a total disaster, and itâ€™s all my fault! VANESSA Yes, it kind of is. BARRY Iâ€™ve ruined the planet. And, I wanted to help with your flower shop. Instead, Iâ€™ve made it worse. VANESSA Actually, itâ€™s completely closed down. BARRY Oh, I thought maybe you were remodeling. Nonetheless, I have another idea. And itâ€™s greater than all my previous great ideas combined. VANESSA I donâ€™t want to hear it. Vanessa closes the change box on Barry. BARRY (opening it again) Alright, hereâ€™s what Iâ€™m thinking. They have the roses, the roses have the pollen. I know every bee, plant, and flower bud in this park. All weâ€™ve got to do is get what theyâ€™ve got back here with what weâ€™ve got. VANESSA Bees... BARRY Park... VANESSA Pollen... \"Bee Movie\" - JS REVISIONS 8/13/07 106. BARRY Flowers... VANESSA Repollination! BARRY (on luggage handle, going up) Across the nation! CUT TO: SEQ. 3950 - â€œROSE PARADEâ€ EXT. PASADENA PARADE BARRY (V.O) Alright. Tournament of Roses. Pasadena, California. Theyâ€™ve got nothing but flowers, floats, and cotton candy. Security will be tight. VANESSA I have an idea. CUT TO: EXT. FLOAT STAGING AREA ANGLE ON: Barry and Vanessa approaching a HEAVILY ARMED GUARD in front of the staging area. VANESSA Vanessa Bloome, FTD. Official floral business. He leans in to look at her badge. She SNAPS IT SHUT, VANESSA (CONTâ€™D) Oh, itâ€™s real. HEAVILY ARMED GUARD Sorry maâ€™am. Thatâ€™s a nice brooch, by the way. VANESSA Thank you. It was a gift. \"Bee Movie\" - JS REVISIONS 8/13/07 107. They ENTER the staging area. BARRY (V.O) Then, once weâ€™re inside, we just pick the right float. VANESSA How about the Princess and the Pea? BARRY Yeah. VANESSA I can be the princess, and-- BARRY ...yes, I think-- VANESSA You could be-- BARRY Iâ€™ve-- VANESSA The pea. BARRY Got it. CUT TO: EXT. FLOAT STAGING AREA - A FEW MOMENTS LATER Barry, dressed as a PEA, flies up and hovers in front of the princess on the â€œPrincess and the Peaâ€ float. The float is sponsored by Inflat-a-bed and a SIGN READS: â€œInflat-a-bed: If it blows, itâ€™s ours.â€ BARRY Sorry Iâ€™m late. Where should I sit? PRINCESS What are you? BARRY I believe Iâ€™m the pea. PRINCESS The pea? Itâ€™s supposed to be under the mattresses. \"Bee Movie\" - JS REVISIONS 8/13/07 108. BARRY Not in this fairy tale, sweetheart. PRINCESS Iâ€™m going to go talk to the marshall. BARRY You do that. This whole parade is a fiasco! She EXITS. Vanessa removes the step-ladder. The princess FALLS. Barry and Vanessa take off in the float. BARRY (CONTâ€™D) Letâ€™s see what this baby will do. ANGLE ON: Guy with headset talking to drivers. HEADSET GUY Hey! The float ZOOMS by. A young CHILD in the stands, TIMMY, cries. CUT TO: EXT. FLOAT STAGING AREA - A FEW MOMENTS LATER ANGLE ON: Vanessa putting the princess hat on. BARRY (V.O) Then all we do is blend in with traffic, without arousing suspicion. CUT TO: EXT. THE PARADE ROUTE - CONTINUOUS The floats go flying by the crowds. Barry and Vanessaâ€™s float CRASHES through the fence. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 109. EXT. LA FREEWAY Vanessa and Barry speed, dodging and weaving, down the freeway. BARRY (V.O) And once weâ€™re at the airport, thereâ€™s no stopping us. CUT TO: EXT. LAX AIRPORT Barry and Vanessa pull up to the curb, in front of an TSA AGENT WITH CLIPBOARD. TSA AGENT Stop. Security. Did you and your insect pack your own float? VANESSA (O.C) Yes. TSA AGENT Has this float been in your possession the entire time? VANESSA (O.C) Since the parade...yes. ANGLE ON: Barry holding his shoes. TSA AGENT Would you remove your shoes and everything in your pockets? Can you remove your stinger, Sir? BARRY Thatâ€™s part of me. TSA AGENT I know. Just having some fun. Enjoy your flight. CUT TO: EXT. RUNWAY Barry and Vanessaâ€™s airplane TAKES OFF. \"Bee Movie\" - JS REVISIONS 8/13/07 110. BARRY (O.C) Then, if weâ€™re lucky, weâ€™ll have just enough pollen to do the job. DISSOLVE TO: SEQ. 4025 - â€œCOCKPIT FIGHTâ€ INT. AIRPLANE Vanessa is on the aisle. Barry is on a laptop calculating flowers, pollen, number of bees, airspeed, etc. He does a â€œStompâ€ dance on the keyboard. BARRY Can you believe how lucky we are? We have just enough pollen to do the job. I think this is going to work, Vanessa. VANESSA Itâ€™s got to work. PILOT (V.O) Attention passengers. This is Captain Scott. Iâ€™m afraid we have a bit of bad weather in the New York area. And looks like weâ€™re going to be experiencing a couple of hours delay. VANESSA Barry, these are cut flowers with no water. Theyâ€™ll never make it. BARRY Iâ€™ve got to get up there and talk to these guys. VANESSA Be careful. Barry flies up to the cockpit door. CUT TO: INT. COCKPIT - CONTINUOUS A female flight attendant, ANGELA, is in the cockpit with the pilots. \"Bee Movie\" - JS REVISIONS 8/13/07 111. Thereâ€™s a KNOCK at the door. BARRY (C.O) Hey, can I get some help with this Sky Mall Magazine? Iâ€™d like to order the talking inflatable travel pool filter. ANGELA (to the pilots, irritated) Excuse me. CUT TO: EXT. CABIN - CONTINUOUS Angela opens the cockpit door and looks around. She doesnâ€™t see anybody. ANGLE ON: Barry hidden on the yellow and black â€œcautionâ€ stripe. As Angela looks around, Barry zips into the cockpit. CUT TO: INT. COCKPIT BARRY Excuse me, Captain. I am in a real situation here... PILOT (pulling an earphone back, to the co-pilot) What did you say, Hal? CO-PILOT I didnâ€™t say anything. PILOT (he sees Barry) Ahhh! Bee! BARRY No, no! Donâ€™t freak out! Thereâ€™s a chance my entire species-- CO-PILOT (taking off his earphones) Ahhh! \"Bee Movie\" - JS REVISIONS 8/13/07 112. The pilot grabs a â€œDUSTBUSTERâ€ vacuum cleaner. He aims it around trying to vacuum up Barry. The co-pilot faces camera, as the pilot tries to suck Barry up. Barry is on the other side of the co-pilot. As they dosey-do, the toupee of the co-pilot begins to come up, still attached to the front. CO-PILOT (CONT'D) What are you doing? Stop! The toupee comes off the co-pilotâ€™s head, and sticks in the Dustbuster. Barry runs across the bald head. BARRY Wait a minute! Iâ€™m an attorney! CO-PILOT Whoâ€™s an attorney? PILOT Donâ€™t move. The pilot uses the Dustbuster to try and mash Barry, who is hovering in front of the co-pilotâ€™s nose, and knocks out the co-pilot who falls out of his chair, hitting the life raft release button. The life raft inflates, hitting the pilot, knocking him into a wall and out cold. Barry surveys the situation. BARRY Oh, Barry. CUT TO: INT. AIRPLANE CABIN Vanessa studies her laptop, looking serious. SFX: PA CRACKLE. BARRY (V.O) (in captain voice) Good afternoon passengers, this is your captain speaking. Would a Miss Vanessa Bloome in 24F please report to the cockpit. And please hurry! \"Bee Movie\" - JS REVISIONS 8/13/07 113. ANGLE ON: The aisle, and Vanessa head popping up. CUT TO: INT. COCKPIT Vanessa ENTERS. VANESSA What happened here? BARRY I tried to talk to them, but then there was a Dustbuster, a toupee, a life raft exploded...Now oneâ€™s bald, oneâ€™s in a boat, and theyâ€™re both unconscious. VANESSA Is that another bee joke? BARRY No. No oneâ€™s flying the plane. The AIR TRAFFIC CONTROLLER, BUD, speaks over the radio. BUD This is JFK control tower. Flight 356, whatâ€™s your status? Vanessa presses a button, and the intercom comes on. VANESSA This is Vanessa Bloome. Iâ€™m a florist from New York. BUD Whereâ€™s the pilot? VANESSA Heâ€™s unconscious and so is the copilot. BUD Not good. Is there anyone onboard who has flight experience? A BEAT. BARRY As a matter of fact, there is. \"Bee Movie\" - JS REVISIONS 8/13/07 114. BUD Whoâ€™s that? VANESSA Barry Benson. BUD From the honey trial? Oh great. BARRY Vanessa, this is nothing more than a big metal bee. Itâ€™s got giant wings, huge engines. VANESSA I canâ€™t fly a plane. BARRY Why not? Isnâ€™t John Travolta a pilot? VANESSA Yes? BARRY How hard could it be? VANESSA Wait a minute. Barry, weâ€™re headed into some lightning. CUT TO: Vanessa shrugs, and takes the controls. SEQ. 4150 - â€œBARRY FLIES PLANEâ€ INT. BENSON HOUSE The family is all huddled around the TV at the Benson house. ANGLE ON: TV. Bob Bumble is broadcasting. BOB BUMBLE This is Bob Bumble. We have some late-breaking news from JFK airport, where a very suspenseful scene is developing. Barry Benson, fresh off his stunning legal victory... \"Bee Movie\" - JS REVISIONS 8/13/07 115. Adam SPRAYS a can of HONEY-WHIP into his mouth. ADAM Thatâ€™s Barry. BOB BUMBLE ...is now attempting to land a plane, loaded with people, flowers, and an incapacitated flight crew. EVERYONE Flowers?! CUT TO: INT. AIR TRAFFIC CONTROL TOWER BUD Well, we have an electrical storm in the area, and two individuals at the controls of a jumbo jet with absolutely no flight experience. JEANETTE CHUNG Just a minute, Mr. Ditchwater, thereâ€™s a honey bee on that plane. BUD Oh, Iâ€™m quite familiar with Mr. Bensonâ€™s work, and his no-account compadres. Havenâ€™t they done enough damage already? JEANETTE CHUNG But isnâ€™t he your only hope right now? BUD Come on, technically a bee shouldnâ€™t be able to fly at all. CUT TO: INT. COCKPIT. Barry REACTS BUD The wings are too small, their bodies are too big-- \"Bee Movie\" - JS REVISIONS 8/13/07 116. BARRY (over PA) Hey, hold on a second. Havenâ€™t we heard this million times? The surface area of the wings, and the body mass doesnâ€™t make sense? JEANETTE CHUNG Get this on the air. CAMERAMAN You got it! CUT TO: INT. BEE TV CONTROL ROOM An engineer throws a switch. BEE ENGINEER Stand by. Weâ€™re going live. The â€œON AIRâ€ sign illuminates. CUT TO: INT. VARIOUS SHOTS OF NEW HIVE CITY The news report plays on TV. The pollen jocks are sitting around, playing paddle-ball, Wheel-o, and one of them is spinning his helmet on his finger. Buzzwell is in an office cubicle, playing computer solitaire. Barryâ€™s family and Adam watch from their living room. Bees sitting on the street curb turn around to watch the TV. BARRY Mr. Ditchwater, the way we work may be a mystery to you, because making honey takes a lot of bees doing a lot of small jobs. But let me tell you something about a small job. If you do it really well, it makes a big difference. More than we realized. To us, to everyone. Thatâ€™s why I want to get bees back to doing what we do best. \"Bee Movie\" - JS REVISIONS 8/13/07 117. Working together. Thatâ€™s the bee way. Weâ€™re not made of Jello. We get behind a fellow. Black and yellow. CROWD OF BEES Hello! CUT TO: INT. COCKPIT Barry is giving orders to Vanessa. BARRY Left, right, down, hover. VANESSA Hover? BARRY Forget hover. VANESSA You know what? This isnâ€™t so hard. Vanessa pretends to HONK THE HORN. VANESSA (CONTâ€™D) Beep, beep! Beep, beep! A BOLT OF LIGHTNING HITS the plane. The plane takes a sharp dip. VANESSA (CONTâ€™D) Barry, what happened? BARRY (noticing the control panel) Wait a minute. I think we were on autopilot that whole time. VANESSA That may have been helping me. BARRY And now weâ€™re not! VANESSA (V.O.) (folding her arms) Well, then it turns out I cannot fly a plane. \"Bee Movie\" - JS REVISIONS 8/13/07 118. BARRY (CONT'D) Vanessa struggles with the yoke. CUT TO: EXT. AIRPLANE The airplane goes into a steep dive. CUT TO: SEQ. 4175 - â€œCRASH LANDINGâ€ INT. J-GATE An ALERT SIGN READING: â€œHive Alert. We Need:â€ Then the SIGNAL goes from â€œTwo Beesâ€ â€œSome Beesâ€ â€œEvery Bee There Isâ€ Lou Loduca gathers the pollen jocks at J-Gate. LOU LODUCA All of you, letâ€™s get behind this fellow. Move it out! The bees follow Lou Loduca, and EXIT J-Gate. CUT TO: INT. AIRPLANE COCKPIT BARRY Our only chance is if I do what I would do, and you copy me with the wings of the plane! VANESSA You donâ€™t have to yell. BARRY Iâ€™m not yelling. We happen to be in a lot of trouble here. VANESSA Itâ€™s very hard to concentrate with that panicky tone in your voice. BARRY Itâ€™s not a tone. Iâ€™m panicking! CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 119. EXT. JFK AIRPORT ANGLE ON: The bees arriving and massing at the airport. CUT TO: INT. COCKPIT Barry and Vanessa alternately SLAP EACH OTHER IN THE FACE. VANESSA I donâ€™t think I can do this. BARRY Vanessa, pull yourself together. Listen to me, you have got to snap out of it! VANESSA You snap out of it! BARRY You snap out of it! VANESSA You snap out of it! BARRY You snap out of it! VANESSA You snap out of it! CUT TO: EXT. AIRPLANE A GIGANTIC SWARM OF BEES flies in to hold the plane up. CUT TO: INT. COCKPIT - CONTINUOUS BARRY You snap out of it! VANESSA You snap out of it! \"Bee Movie\" - JS REVISIONS 8/13/07 120. BARRY You snap-- VANESSA Hold it! BARRY (about to slap her again) Why? Come on, itâ€™s my turn. VANESSA How is the plane flying? Barryâ€™s antennae ring. BARRY I donâ€™t know. (answering) Hello? CUT TO: EXT. AIRPLANE ANGLE ON: The underside of the plane. The pollen jocks have massed all around the underbelly of the plane, and are holding it up. LOU LODUCA Hey Benson, have you got any flowers for a happy occasion in there? CUT TO: INT. COCKPIT Lou, Buzz, Splitz, and Jackson come up alongside the cockpit. BARRY The pollen jocks! VANESSA They do get behind a fellow. BARRY Black and yellow. LOU LODUCA (over headset) Hello. \"Bee Movie\" - JS REVISIONS 8/13/07 121. Alright you two, what do you say we drop this tin can on the blacktop? VANESSA What blacktop? Where? I canâ€™t see anything. Can you? BARRY No, nothing. Itâ€™s all cloudy. CUT TO: EXT. RUNWAY Adam SHOUTS. ADAM Come on, youâ€™ve got to think bee, Barry. Thinking bee, thinking bee. ANGLE ON: Overhead shot of runway. The bees are in the formation of a flower. In unison they move, causing the flower to FLASH YELLOW AND BLACK. BEES (chanting) Thinking bee, thinking bee. CUT TO: INT. COCKPIT We see through the swirling mist and clouds. A GIANT SHAPE OF A FLOWER is forming in the middle of the runway. BARRY Wait a minute. I think Iâ€™m feeling something. VANESSA What? BARRY I donâ€™t know, but itâ€™s strong. And itâ€™s pulling me, like a 27 million year old instinct. Bring the nose of the plane down. \"Bee Movie\" - JS REVISIONS 8/13/07 122. LOU LODUCA (CONT'D) EXT. RUNWAY All the bees are on the runway chanting â€œThinking Beeâ€. CUT TO: INT. CONTROL TOWER RICK What in the world is on the tarmac? ANGLE ON: Dave OTS onto runway seeing a flower being formed by millions of bees. BUD Get some lights on that! CUT TO: EXT. RUNWAY ANGLE ON: AIRCRAFT LANDING LIGHT SCAFFOLD by the side of the runway, illuminating the bees in their flower formation. INT. COCKPIT BARRY Vanessa, aim for the flower! VANESSA Oh, okay? BARRY Cut the engines! VANESSA Cut the engines? BARRY Weâ€™re going in on bee power. Ready boys? LOU LODUCA Affirmative. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 123. INT. AIRPLANE COCKPIT BARRY Good, good, easy now. Land on that flower! Ready boys? Give me full reverse. LOU LODUCA Spin it around! The plane attempts to land on top of an â€œAloha Airlinesâ€ plane with flowers painted on it. BARRY (V.O) I mean the giant black and yellow pulsating flower made of millions of bees! VANESSA Which flower? BARRY That flower! VANESSA Iâ€™m aiming at the flower! The plane goes after a FAT GUY IN A HAWAIIAN SHIRT. BARRY (V.O) Thatâ€™s a fat guy in a flowered shirt! The other other flower! The big one. He snaps a photo and runs away. BARRY (CONT'D) Full forward. Ready boys? Nose down. Bring your tail up. Rotate around it. VANESSA Oh, this is insane, Barry. BARRY This is the only way I know how to fly. CUT TO: \"Bee Movie\" - JS REVISIONS 8/13/07 124. AIR TRAFFIC CONTROL TOWER BUD Am I koo-koo kachoo, or is this plane flying in an insect-like pattern? CUT TO: EXT. RUNWAY BARRY (V.O) Get your nose in there. Donâ€™t be afraid of it. Smell it. Full reverse! Easy, just drop it. Be a part of it. Aim for the center! Now drop it in. Drop it in, woman! The plane HOVERS and MANEUVERS, landing in the center of the giant flower, like a bee. The FLOWERS from the cargo hold spill out onto the runway. INT. AIPLANE CABIN The passengers are motionless for a beat. PASSENGER Come on already! They hear the â€œding dingâ€, and all jump up to grab their luggage out of the overheads. SEQ. 4225 - â€œRUNWAY SPEECHâ€ EXT. RUNWAY - CONTINUOUS The INFLATABLE SLIDES pop out the side of the plane. The passengers escape. Barry and Vanessa slide down out of the cockpit. Barry and Vanessa exhale a huge breath. VANESSA Barry, we did it. You taught me how to fly. Vanessa raises her hand up for a high five. \"Bee Movie\" - JS REVISIONS 8/13/07 125. BARRY Yes. No high five. VANESSA Right. ADAM Barry, it worked. Did you see the giant flower? BARRY What giant flower? Where? Of course I saw the flower! That was genius, man. Genius! ADAM Thank you. BARRY But weâ€™re not done yet. Barry flies up to the wing of the plane, and addresses the bee crowd. BARRY (CONTâ€™D) Listen everyone. This runway is covered with the last pollen from the last flowers available anywhere on Earth. That means this is our last chance. Weâ€™re the only ones who make honey, pollinate flowers, and dress like this. If weâ€™re going to survive as a species, this is our moment. So what do you all say? Are we going to be bees, or just Museum of Natural History key chains? BEES Weâ€™re bees! KEYCHAIN BEE Keychain! BARRY Then follow me... Except Keychain. BUZZ Hold on Barry. Youâ€™ve earned this. Buzz puts a pollen jock jacket and helmet with Barryâ€™s name on it on Barry. \"Bee Movie\" - JS REVISIONS 8/13/07 126. BARRY Iâ€™m a pollen jock! (looking at the jacket. The sleeves are a little long) And itâ€™s a perfect fit. All Iâ€™ve got to do are the sleeves. The Pollen Jocks toss Barry a gun. BARRY (CONTâ€™D) Oh yeah! ANGLE ON: Martin and Janet Benson. JANET BENSON Thatâ€™s our Barry. All the bees descend upon the flowers on the tarmac, and start collecting pollen. CUT TO: SEQ. 4250 - â€œRE-POLLINATIONâ€ EXT. SKIES - CONTINUOUS The squadron FLIES over the city, REPOLLINATING trees and flowers as they go. Barry breaks off from the group, towards Vanessaâ€™s flower shop. CUT TO: EXT. VANESSAâ€™S FLOWER SHOP - CONTINUOUS Barry REPOLLINATES Vanessaâ€™s flowers. CUT TO: EXT. CENTRAL PARK - CONTINUOUS ANGLE ON: Timmy with a frisbee, as the bees fly by. TIMMY Mom, the bees are back! \"Bee Movie\" - JS REVISIONS 8/13/07 127. Central Park is completely repollinated by the bees. DISSOLVE TO: INT. HONEX - CONTINUOUS Honex is back to normal and everyone is busily working. ANGLE ON: Adam, putting his Krelman hat on. ADAM If anyone needs to make a call, nowâ€™s the time. Iâ€™ve got a feeling weâ€™ll be working late tonight! The bees CHEER. CUT TO: SEQ. 4355 EXT: VANESSAâ€™S FLOWER SHOP With a new sign out front. â€œVanessa &amp; Barry: Flowers, Honey, Legal Adviceâ€ DISSOLVE TO: INT: FLOWER COUNTER Vanessa doing a brisk trade with many customers. CUT TO: INT: FLOWER SHOP - CONTINUOUS Vanessa is selling flowers. In the background, there are SHELVES STOCKED WITH HONEY. VANESSA (O.C.) Donâ€™t forget these. Have a great afternoon. Yes, can I help whoâ€™s next? Whoâ€™s next? Would you like some honey with that? It is beeapproved. SIGN ON THE BACK ROOM DOOR READS: â€œBarry Benson: Insects at Lawâ€. \"Bee Movie\" - JS REVISIONS 8/13/07 128. Camera moves into the back room. ANGLE ON: Barry. ANGLE ON: Barryâ€™s COW CLIENT. COW Milk, cream, cheese...itâ€™s all me. And I donâ€™t see a nickel. BARRY Uh huh? Uh huh? COW (breaking down) Sometimes I just feel like a piece of meat. BARRY I had no idea. VANESSA Barry? Iâ€™m sorry, have you got a moment? BARRY Would you excuse me? My mosquito associate here will be able to help you. Mooseblood ENTERS. MOOSEBLOOD Sorry Iâ€™m late. COW Heâ€™s a lawyer too? MOOSEBLOOD Maâ€™am, I was already a bloodsucking parasite. All I needed was * a briefcase. * ANGLE ON: Flower Counter. VANESSA (to customer) Have a great afternoon! (to Barry) Barry, I just got this huge tulip order for a wedding, and I canâ€™t get them anywhere. \"Bee Movie\" - JS REVISIONS 8/13/07 129. BARRY Not a problem, Vannie. Just leave it to me. Vanessa turns back to deal with a customer. VANESSA Youâ€™re a life-saver, Barry. (to the next customer) Can I help whoâ€™s next? Whoâ€™s next? ANGLE ON: Vanessa smiling back at Barry. Barry smiles too, then snaps himself out of it. BARRY (speaks into his antennae) Alright. Scramble jocks, itâ€™s time to fly! VANESSA Thank you, Barry! EXT. FLOWER SHOP - CONTINUOUS ANGLE ON: Ken and Andy walking down the street. KEN (noticing the new sign) Augh! What in the world? Itâ€™s that bee again! ANDY (guiding Ken protectively) Let it go, Kenny. KEN That bee is living my life! When will this nightmare end? ANDY Let it all go. They donâ€™t break stride. ANGLE ON: Camera in front of Barry as he flies out the door and up into the sky. Pollen jocks fold in formation behind him as they zoom into the park. BARRY (to Splitz) Beautiful day to fly. \"Bee Movie\" - JS REVISIONS 8/13/07 130. JACKSON Sure is. BARRY Between you and me, I was dying to get out of that office. FADE OUT: \"Bee Movie\" - JS REVISIONS 8/13/07 131.\n",
    "readme_length": 113283
  },
  {
    "name": "LucaPCycle",
    "full_name": "LucaOne/LucaPCycle",
    "description": "We developed a dual-channel model named LucaPCycle, based on the raw sequence and protein language large models, to predict whether a protein sequence has phosphate-solubilizing functionality and its specific type among the 31 fine-grained functions.        ",
    "stars": 32,
    "forks": 7,
    "language": "Python",
    "url": "https://github.com/LucaOne/LucaPCycle",
    "topics": [
      "deep-learning",
      "deep-seq-mining",
      "embedding",
      "large-language-models",
      "lucapcycle",
      "phosphate-solubilizing",
      "protein-language-models"
    ],
    "created_at": "2024-07-18T07:19:18Z",
    "updated_at": "2025-11-20T03:08:05Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "# LucaPCycle   \nWe developed a dual-channel model named LucaPCycle, based on the raw sequence and protein language large models, to predict whether a protein sequence has phosphate-solubilizing functionality and its specific type among the 31 fine-grained functions.        \n\nWe constructed two models, including an identification model(binary classification) and a fine-grained classification of specific phosphate-solubilizing functional types(31 classification).\n\n# TimeLine\n* 2024/12/01: The latest branch is `V3`,  so use `V3` first instead of the `master` branch(i.e. `V2`). \n\n## 1. Model Architecture     \n\n<center>\n<img alt=\"LucaPCycle\" src=\"./pics/LucaPCycle.png\"/>\n\n**Fig.1 LucaPCycle.**  \n</center>\n\n\n## 2. Environment Installation\n### step1: update git\n#### 1) centos\nsudo yum update     \nsudo yum install git-all\n\n#### 2) ubuntu\nsudo apt-get update     \nsudo apt install git-all\n\n### step2: install python 3.9\n#### 1) download anaconda3\nwget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh\n\n#### 2) install conda\nsh Anaconda3-2022.05-Linux-x86_64.sh\n##### Notice: Select Yes to update ~/.bashrc\nsource ~/.bashrc\n\n#### 3) create a virtual environment: python=3.9.13\nconda create -n lucapcycle python=3.9.13\n\n\n#### 4) activate lucapcycle\nconda activate lucapcycle\n\n### step3:  install other requirements\npip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple    \n\n## 3. Inference     \n\n### TrainedCheckPoint   \nTrained LucaPCycle Checkpoint FTP: <a href='http://47.93.21.181/lucapcycle/TrainedCheckPoint/'>TrainedCheckPoint for LucaPCycle</a>\n\n**Notice**    \nThe project will download automatically LucaPCycle Trained-CheckPoint from **FTP**.\n\nWhen downloading automatically failed, you can manually download:\n\nCopy the **TrainedCheckPoint Files(`models/` + `logs/`)** from <href> http://47.93.21.181/lucapcycle/TrainedCheckPoint/* </href> into the project.  \n\n### Usage    \nFirstly, predict whether a sequence has phosphate-solubilizing functionality.  \nThe inference script: **`src/prediction.py`** or  **`src/prediction.sh`**    \n\n```python prediction.py -h``` for **help**\n#### Binary Classification  \n```shell\ncd src/\nexport CUDA_VISIBLE_DEVICES=\"0,1,2,3\"\npython prediction.py \\\n    --seq_type prot \\\n    --input_file ../test_data/examples.fasta \\\n    --llm_truncation_seq_length 4096 \\\n    --model_path .. \\\n    --save_path ../predicted_results/test_data/examples_predicted.csv \\\n    --dataset_name extra_p_2_class_v2 \\\n    --dataset_type protein \\\n    --task_type binary_class \\\n    --task_level_type seq_level \\\n    --model_type lucaprot \\\n    --input_type seq_matrix \\\n    --time_str 20240120061735 \\\n    --step 955872 \\\n    --threshold 0.2 \\\n    --per_num 1000 \\\n    --gpu_id 0\n```\n\n#### 31 Classification      \nThen, for the sequences predicted to be positive in the **2-classification** inference, the fine-grained classification of specific phosphate-solubilizing functional types(**31 classes**) is further predicted.      \n```shell\ncd src/\nexport CUDA_VISIBLE_DEVICES=\"0,1,2,3\"\npython prediction.py \\\n    --seq_type prot \\\n    --input_file ../test_data/example_positives.fasta \\\n    --llm_truncation_seq_length 4096 \\\n    --model_path .. \\\n    --save_path ../predicted_results/test_data/example_positives_fine_grained_predicted.csv \\\n    --dataset_name extra_p_31_class_v2 \\\n    --dataset_type protein \\\n    --task_type multi_class \\\n    --task_level_type seq_level \\\n    --model_type lucaprot \\\n    --input_type seq_matrix \\\n    --time_str 20240120061524 \\\n    --step 294536 \\\n    --per_num 1000 \\\n    --gpu_id 1\n```\n\n#### Parameters   \n1) Input data parameters:     \n   * seq_type: `str`, the input seq type(`gene` or `prot`)    \n   * input_file: `Path`, the input filepath(for a batch samples, format: `fasta` or `csv`(contain header, columns: `seq_id`, `seq`))   \n   * seq_id: `str`, the seq id(for one sample)     \n   * seq: `str`, the sequence(for one sample)      \n   * save_path: `Path`, the saved dir path of the batch samples predicted results(only for batch prediction)          \n\n2) Trained LucaPCycle checkpoint parameters:      \n   * model_path: `Path`, model dir pathï¼Œdefault: `../` (meaning the checkpoint in the project)        \n   * dataset_name: `str`, the checkpoint version: `extra_p_2_class_v2`(2-classification) or `extra_p_31_class_v2`(31-classification)        \n   * dataset_type: `str`, only `protein`, default: `protein`    \n   * task_type: `str`, the trained task type: `binary_class`(2-classification) or `multi_class`(31-classification)      \n   * task_level_type: `str`, sequence-level tasks, default: `seq-level`   \n   * model_type: `str`, the model type, default: `lucaprot`   \n   * input_type: `str`, the model channels, default: `seq_matrix`      \n   * time_str: `str`, the trained checkpoint running time str: `20240120061735`(2-classification) or `20240120061524`(31-classification)     \n   * step: `int`, the checkpoint step: `955872`(2-classification) or `294536`(31-classification)\n\n3) Running parameters: \n   * topk: `int`, the topk labels when inferring 31-classification, default: `None`(meaining k=1)     \n   * llm_truncation_seq_length: `int`, the max seq length to truncation(depends on the length of your sequence and the size of your GPU memory. default: `4096`           \n   * per_num: `int`, the print progress is determined by how many sequences are predicted.  default: `1000`          \n   * threshold: `float`, the threshold for binary-classification, default: `0.1`, (positive>=threshold, negative<threshold, small value leads to high recall, and large value to high precision)     \n   * gpu_id: `int`, the gpu id to use(-1 for cpu), default: `-1`          \n\n4) Protein LLM exists path(optional):\n   * torch_hub_dir: `Path`, the torch hub dir path for saving pretrained model(default: `~/.cache/torch/hub/`) \n\n\n## 4. Model Building Dataset    \nFor the two models, we divided the dataset into the training, validation, and testing sets, which were used for model fitting, model finalization (based on the best F1-score training iteration), and performance reporting, respectively.   \n\n### Binary Classification      \nThe training, validation, and testing sets of **binary-classification** in **`dataset/extra_p_2_class_v2/`**     \n \n\n### 31 Classification      \nThe training, validation, and testing sets of **fine-grained 31-classification** in **`dataset/extra_p_31_class_v2/`**      \n\n## 5. Model Building    \n### Training Binary Classification Model   \nThe script of **binary-classification** model building is **`src/training/run_extra_p_2_class_subword_v2.sh`**      \n\n### Training 31 Classification Model     \nThe script of **fine-grained 31-classification** model building is **`src/training/run_extra_p_31_class_subword_v2.sh`**      \n\n\n## 6. Data Availability    \n### 1) Data for Model Building     \nThe model building dataset in **`dataset/`** or <a href='http://47.93.21.181/lucapcycle/dataset'>Dataset FTP</a>.\n\nThe raw data of LucaPCycle building in **`data/`** or <a href='http://47.93.21.181/lucapcycle/data'>Raw Data FTP</a>, \nwhere folder **`31P_genes/`** is fasta for each of the 31 fine-grained phosphate-solubilizing types, \nand the file **`cold_spring_sample_50.csv`** is the non-redundancy sequences(including positives and negatives) using the CD-HIT tool with 50% sequence identity.  \n\n### 2) Large-scale Identification    \nThe large-scale unidentified data is in **`inference_data/`** or <a href='http://47.93.21.181/lucapcycle/inference_data'>Large-scale Unidentified Data FTP</a>, total of **151,187,265** sequences.    \nThe data includes 164 metagenomes and 33 metatranscriptomes,    \nwhich is sourced from sediment samples (sediment depths: 0-68.55 mbsf; water depths 860-3005 m) collected at 16 globally distributed cold seep sites.    \nThese samples encompass five types of cold seeps, namely gas hydrates (n = 39), mud volcanoes (n = 7), asphalt volcanoes (n = 7), oil and gas seeps (n = 15) and methane seeps (n = 96).\n\n\nThe predicted results of the large-scale data are list in **`results/`** or <a href='http://47.93.21.181/lucapcycle/results'>Results FTP</a>:  \nThe file in the format of **`*_init*`** is the unchecked results, and the file in the format of **`*_verified*`** is the result of the verification by through three distinct methods: **ECOD Domain Analysis**, **DeepFRI v1.0.0 (Deep Functional Residue Identification)**, and **CLEAN v1.0.1 (Contrastive Learning-Enabled Enzyme Annotation)**.   \n\n* **LucaPCycle**     \n  Results in **`results/LucaPCycle/`** or <a href='http://47.93.21.181/lucapcycle/results/LucaPCycle'>LucaPCycle Results FTP</a>:   \n  Resulting in **1,481,237** positive sequences.   \n  The detailed predicted numbers for each class are shown below.   \n  **Notice:** There may be **interesting findings**. Totaling 134,227 positive sequences(predicted by LucaPCycle) in file **`results/LucaPCycle/lucapcycle_unverifiable.fasta`** (9.06%) could not be confirmed using existing verified methods.      \n  `lucapcycle_details_init.csv`:  unchecked predicted details positives by LucaPCycle(include top1 prob and label, top10 prob and label)   \n  `lucapcycle_init.ids.labels` & `lucapcycle_init.fasta`: unchecked predicted positives by LucaPCycle.      \n  `lucapcycle_verified.ids.labels` & `lucapcycle_verified.fasta`: checked predicted positives by LucaPCycle.   \n  `lucapcycle_unverifiable.ids` & `lucapcycle_unverifiable.ids`: unverifiable predicted positives by LucaPCycle.         \n\n  <center>\n  <img alt=\"Benchmark\" src=\"./pics/Fig.S14-e.png\"/>\n\n  **Fig.2 The Predicted Details.**        \n  </center>\n\n\n* **Diamond Blastp**        \n  Results in **`results/Blastp/`** or <a href='http://47.93.21.181/lucapcycle/results/Blastp'>Blastp Results FTP</a>   \n  `blastp_init.ids.labels` & `blastp_init.fasta`: unchecked predicted positives by Blastp.    \n  `blastp_verified.ids.labels` & `blastp_verified.fasta`: checked predicted positives by Blastp.   \n\n\n* **KofamScan**       \n  Results in **`results/KofamScan/`** or <a href='http://47.93.21.181/lucapcycle/results/KofamScan'>KofamScan FTP</a>     \n  `kofamscan_init.ids.labels` & `kofamscan_init.fasta`: unchecked predicted positives by KofamScan.     \n  `kofamscan_verified.ids.labels` & `kofamscan_verified.fasta`: checked predicted positives by KofamScan.       \n\n<center>\n<img alt=\"Benchmark\" src=\"./pics/Fig.S14-g.png\"/>\n\n**Fig.3 Benchmark.**   \n</center>   \n\n### 3) Tree-Families     \n<a href='http://47.93.21.181/lucapcycle/results/Tree-Families/Sequence_tree_of_APL_newick.txt'>Sequence Tree</a>          \nPhylogenetic tree of alkaline phosphatase with remote homology based on protein sequences.\n\n<a href='http://47.93.21.181/lucapcycle/results/Tree-Families/Protein_structure_tree_of_APL_newick.txt'>Structural Tree</a>      \nStructure-based phylogeny of alkaline phosphatase with remote homology and reference proteins.\n\n<a href='http://47.93.21.181/lucapcycle/results/Tree-Families/non-singleton_cluster70.faa.gz'>Families</a>    \nRepresentatives from non-singleton P-solubilizing protein families.   \n\n\n## 7. Contributor    \n<a href=\"https://scholar.google.com.hk/citations?user=RDbqGTcAAAAJ&hl=en\" title=\"Yong He\">Yong He</a>,\n<a href=\"https://scholar.google.com/citations?user=lT3nelQAAAAJ&hl=en\" title=\"Zhaorong Li\">Zhaorong Li</a>, \n<a href=\"https://scholar.google.com/citations?user=a1Evzq4AAAAJ&hl=en\" title=\"Chuwen Zhang\">Chuwen Zhang</a>, \n<a href=\"https://scholar.google.com/citations?hl=zh-CN&user=7c3wH8sAAAAJ&hl=en\" title=\"Xiyang Dong\">Xiyang Dong</a>      \n\n## 8. Citation\n**<a href='https://www.biorxiv.org/content/early/2024/07/09/2024.07.09.602434'>LucaPCycle Biorxiv</a>**\n\n\n@article{LucaPCycle,\ntitle={LucaPCycle: Illuminating microbial phosphorus cycling in deep-sea cold seep sediments using protein language models},\nauthor={Zhang, Chuwen and He, Yong and Wang, Jieni and Chen, Tengkai and Baltar, Federico and Hu, Minjie and Liao, Jing and Xiao, Xi and Li, Zhao-Rong and Dong, Xiyang},\njournal={Nature Communications},\nvolume={16},\nnumber={1},\npages={1--16},\nyear={2025},\npublisher={Nature Publishing Group}\n}\n\n\n  \n",
    "readme_length": 12085
  },
  {
    "name": "nncf",
    "full_name": "openvinotoolkit/nncf",
    "description": "Neural Network Compression Framework for enhanced OpenVINOâ„¢ inference",
    "stars": 1109,
    "forks": 270,
    "language": "Python",
    "url": "https://github.com/openvinotoolkit/nncf",
    "topics": [
      "bert",
      "classification",
      "compression",
      "deep-learning",
      "genai",
      "llm",
      "mixed-precision-training",
      "nlp",
      "object-detection",
      "onnx",
      "openvino",
      "pruning",
      "pytorch",
      "quantization",
      "quantization-aware-training",
      "semantic-segmentation",
      "sparsity",
      "tensorflow",
      "transformers"
    ],
    "created_at": "2020-05-13T16:41:05Z",
    "updated_at": "2025-12-02T06:57:41Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "<div align=\"center\">\n\n# Neural Network Compression Framework (NNCF)\n\n[Key Features](#key-features) â€¢\n[Installation](#installation-guide) â€¢\n[Documentation](#documentation) â€¢\n[Usage](#usage) â€¢\n[Tutorials and Samples](#demos-tutorials-and-samples) â€¢\n[Third-party integration](#third-party-repository-integration) â€¢\n[Model Zoo](./docs/ModelZoo.md)\n\n[![GitHub Release](https://img.shields.io/github/v/release/openvinotoolkit/nncf?color=green)](https://github.com/openvinotoolkit/nncf/releases)\n[![Website](https://img.shields.io/website?up_color=blue&up_message=docs&url=https%3A%2F%2Fdocs.openvino.ai%2Fnncf)](https://docs.openvino.ai/nncf)\n[![Apache License Version 2.0](https://img.shields.io/badge/license-Apache_2.0-green.svg)](LICENSE)\n[![PyPI Downloads](https://static.pepy.tech/badge/nncf)](https://pypi.org/project/nncf/)\n\n![Python](https://img.shields.io/badge/python-3.10+-blue)\n![Backends](https://img.shields.io/badge/backends-openvino_|_pytorch_|_onnx_|_tensorflow-orange)\n![OS](https://img.shields.io/badge/OS-Linux_|_Windows_|_MacOS-blue)\n\n</div>\n\nNeural Network Compression Framework (NNCF) provides a suite of post-training and training-time algorithms for\noptimizing inference of neural networks in [OpenVINO&trade;](https://docs.openvino.ai) with a minimal accuracy drop.\n\nNNCF is designed to work with models from [PyTorch](https://pytorch.org/),\n[TorchFX](https://pytorch.org/docs/stable/fx.html), [TensorFlow](https://www.tensorflow.org/),\n[ONNX](https://onnx.ai/) and [OpenVINO&trade;](https://docs.openvino.ai).\n\nNNCF provides [samples](#demos-tutorials-and-samples) that demonstrate the usage of compression algorithms for different\nuse cases and models. See compression results achievable with the NNCF-powered samples on the [NNCF Model Zoo page](./docs/ModelZoo.md).\n\nThe framework is organized as a Python\\* package that can be built and used in a standalone mode. The framework\narchitecture is unified to make it easy to add different compression algorithms for both PyTorch and TensorFlow deep\nlearning frameworks.\n\n<a id=\"key-features\"></a>\n\n## Key Features\n\n### Post-Training Compression Algorithms\n\n| Compression algorithm                                                                                    | OpenVINO  | PyTorch   | TorchFX   | TensorFlow    | ONNX          |\n| :------------------------------------------------------------------------------------------------------- | :-------: | :-------: | :-----------: | :-----------: | :-----------: |\n| [Post-Training Quantization](./docs/usage/post_training_compression/post_training_quantization/Usage.md) | Supported | Supported | Experimental | Supported     | Supported     |\n| [Weights Compression](./docs/usage/post_training_compression/weights_compression/Usage.md)               | Supported | Supported | Experimental | Not supported | Supported |\n| [Activation Sparsity](./src/nncf/experimental/torch/sparsify_activations/ActivationSparsity.md)          | Not supported | Experimental | Not supported| Not supported| Not supported |\n\n### Training-Time Compression Algorithms\n\n| Compression algorithm                                                                                      | PyTorch      | TensorFlow    |\n| :--------------------------------------------------------------------------------------------------------- | :----------: | :-----------: |\n| [Quantization Aware Training](./docs/usage/training_time_compression/quantization_aware_training/Usage.md) | Supported    | Supported     |\n| [Weight-Only Quantization Aware Training with LoRA and NLS](./docs/usage/training_time_compression/quantization_aware_training_lora/Usage.md) | Supported    | Not Supported     |\n| [Mixed-Precision Quantization](./docs/usage/training_time_compression/other_algorithms/LegacyQuantization.md#mixed-precision-quantization) | Supported | Not supported |\n| [Sparsity](./docs/usage/training_time_compression/other_algorithms/Sparsity.md)                            | Supported    | Supported     |\n| [Filter pruning](./docs/usage/training_time_compression/other_algorithms/Pruning.md)                       | Supported    | Supported     |\n| [Movement pruning](./src/nncf/experimental/torch/sparsity/movement/MovementSparsity.md)                    | Experimental | Not supported |\n\n- Automatic, configurable model graph transformation to obtain the compressed model.\n  > **NOTE**: Limited support for TensorFlow models. Only models created using Sequential or Keras Functional API are supported.\n- Common interface for compression methods.\n- GPU-accelerated layers for faster compressed model fine-tuning.\n- Distributed training support.\n- Git patch for prominent third-party repository ([huggingface-transformers](https://github.com/huggingface/transformers)) demonstrating the process of integrating NNCF into custom training pipelines.\n- Exporting PyTorch compressed models to ONNX\\* checkpoints and TensorFlow compressed models to SavedModel or Frozen Graph format, ready to use with [OpenVINO&trade; toolkit](https://docs.openvino.ai).\n- Support for [Accuracy-Aware model training](./docs/usage/training_time_compression/other_algorithms/Usage.md#accuracy-aware-model-training) pipelines via the [Adaptive Compression Level Training](./docs/accuracy_aware_model_training/AdaptiveCompressionLevelTraining.md) and [Early Exit Training](./docs/accuracy_aware_model_training/EarlyExitTraining.md).\n\n<a id=\"documentation\"></a>\n\n## Documentation\n\nThis documentation covers detailed information about NNCF algorithms and functions needed for the contribution to NNCF.\n\nThe latest user documentation for NNCF is available [here](https://docs.openvino.ai/nncf).\n\nNNCF API documentation can be found [here](https://openvinotoolkit.github.io/nncf/autoapi/nncf/).\n\n<a id=\"usage\"></a>\n\n## Usage\n\n### Post-Training Quantization\n\nThe NNCF PTQ is the simplest way to apply 8-bit quantization. To run the algorithm you only need your model and a small (~300 samples) calibration dataset.\n\n[OpenVINO](https://github.com/openvinotoolkit/openvino) is the preferred backend to run PTQ with, while PyTorch, TensorFlow, and ONNX are also supported.\n\n<details open><summary><b>OpenVINO</b></summary>\n\n```python\nimport nncf\nimport openvino as ov\nimport torch\nfrom torchvision import datasets, transforms\n\n# Instantiate your uncompressed model\nmodel = ov.Core().read_model(\"/model_path\")\n\n# Provide validation part of the dataset to collect statistics needed for the compression algorithm\nval_dataset = datasets.ImageFolder(\"/path\", transform=transforms.Compose([transforms.ToTensor()]))\ndataset_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1)\n\n# Step 1: Initialize transformation function\ndef transform_fn(data_item):\n    images, _ = data_item\n    return images\n\n# Step 2: Initialize NNCF Dataset\ncalibration_dataset = nncf.Dataset(dataset_loader, transform_fn)\n# Step 3: Run the quantization pipeline\nquantized_model = nncf.quantize(model, calibration_dataset)\n```\n\n</details>\n\n<details><summary><b>PyTorch</b></summary>\n\n```python\nimport nncf\nimport torch\nfrom torchvision import datasets, models\n\n# Instantiate your uncompressed model\nmodel = models.mobilenet_v2()\n\n# Provide validation part of the dataset to collect statistics needed for the compression algorithm\nval_dataset = datasets.ImageFolder(\"/path\", transform=transforms.Compose([transforms.ToTensor()]))\ndataset_loader = torch.utils.data.DataLoader(val_dataset)\n\n# Step 1: Initialize the transformation function\ndef transform_fn(data_item):\n    images, _ = data_item\n    return images\n\n# Step 2: Initialize NNCF Dataset\ncalibration_dataset = nncf.Dataset(dataset_loader, transform_fn)\n# Step 3: Run the quantization pipeline\nquantized_model = nncf.quantize(model, calibration_dataset)\n\n```\n\n**NOTE** If the Post-Training Quantization algorithm does not meet quality requirements you can fine-tune the quantized pytorch model. You can find an example of the Quantization-Aware training pipeline for a pytorch model [here](examples/quantization_aware_training/torch/resnet18/README.md).\n\n</details>\n\n<details><summary><b>TorchFX</b></summary>\n\n```python\nimport nncf\nimport torch.fx\nfrom torchvision import datasets, models\n\n# Instantiate your uncompressed model\nmodel = models.mobilenet_v2()\n\n# Provide validation part of the dataset to collect statistics needed for the compression algorithm\nval_dataset = datasets.ImageFolder(\"/path\", transform=transforms.Compose([transforms.ToTensor()]))\ndataset_loader = torch.utils.data.DataLoader(val_dataset)\n\n# Step 1: Initialize the transformation function\ndef transform_fn(data_item):\n    images, _ = data_item\n    return images\n\n# Step 2: Initialize NNCF Dataset\ncalibration_dataset = nncf.Dataset(dataset_loader, transform_fn)\n\n# Step 3: Export model to TorchFX\ninput_shape = (1, 3, 224, 224)\nfx_model = torch.export.export_for_training(model, args=(ex_input,)).module()\n# or\n# fx_model = torch.export.export(model, args=(ex_input,)).module()\n\n# Step 4: Run the quantization pipeline\nquantized_fx_model = nncf.quantize(fx_model, calibration_dataset)\n```\n\n</details>\n<details><summary><b>TensorFlow</b></summary>\n\n```python\nimport nncf\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# Instantiate your uncompressed model\nmodel = tf.keras.applications.MobileNetV2()\n\n# Provide validation part of the dataset to collect statistics needed for the compression algorithm\nval_dataset = tfds.load(\"/path\", split=\"validation\",\n                        shuffle_files=False, as_supervised=True)\n\n# Step 1: Initialize transformation function\ndef transform_fn(data_item):\n    images, _ = data_item\n    return images\n\n# Step 2: Initialize NNCF Dataset\ncalibration_dataset = nncf.Dataset(val_dataset, transform_fn)\n# Step 3: Run the quantization pipeline\nquantized_model = nncf.quantize(model, calibration_dataset)\n```\n\n</details>\n\n<details><summary><b>ONNX</b></summary>\n\n```python\nimport onnx\nimport nncf\nimport torch\nfrom torchvision import datasets\n\n# Instantiate your uncompressed model\nonnx_model = onnx.load_model(\"/model_path\")\n\n# Provide validation part of the dataset to collect statistics needed for the compression algorithm\nval_dataset = datasets.ImageFolder(\"/path\", transform=transforms.Compose([transforms.ToTensor()]))\ndataset_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1)\n\n# Step 1: Initialize transformation function\ninput_name = onnx_model.graph.input[0].name\ndef transform_fn(data_item):\n    images, _ = data_item\n    return {input_name: images.numpy()}\n\n# Step 2: Initialize NNCF Dataset\ncalibration_dataset = nncf.Dataset(dataset_loader, transform_fn)\n# Step 3: Run the quantization pipeline\nquantized_model = nncf.quantize(onnx_model, calibration_dataset)\n```\n\n</details>\n\n[//]: # (NNCF provides full  [samples]&#40;#post-training-quantization-samples&#41;, which demonstrate Post-Training Quantization usage for PyTorch, TensorFlow, ONNX, and OpenVINO.)\n\n### Training-Time Quantization\n\nHere is an example of Accuracy Aware Quantization pipeline where model weights and compression parameters may be fine-tuned to achieve a higher accuracy.\n\n<details><summary><b>PyTorch</b></summary>\n\n```python\nimport nncf\nimport nncf.torch\nimport torch\nfrom torchvision import datasets, models\n\n# Instantiate your uncompressed model\nmodel = models.mobilenet_v2()\n\n# Provide validation part of the dataset to collect statistics needed for the compression algorithm\nval_dataset = datasets.ImageFolder(\"/path\", transform=transforms.Compose([transforms.ToTensor()]))\ndataset_loader = torch.utils.data.DataLoader(val_dataset)\n\n# Step 1: Initialize the transformation function\ndef transform_fn(data_item):\n    images, _ = data_item\n    return images\n\n# Step 2: Initialize NNCF Dataset\ncalibration_dataset = nncf.Dataset(dataset_loader, transform_fn)\n# Step 3: Run the quantization pipeline\nquantized_model = nncf.quantize(model, calibration_dataset)\n\n# Now use compressed_model as a usual torch.nn.Module\n# to fine-tune compression parameters along with the model weights\n\n# Save quantization modules and the quantized model parameters\ncheckpoint = {\n    'state_dict': model.state_dict(),\n    'nncf_config': nncf.torch.get_config(model),\n    ... # the rest of the user-defined objects to save\n}\ntorch.save(checkpoint, path_to_checkpoint)\n\n# ...\n\n# Load quantization modules and the quantized model parameters\nresuming_checkpoint = torch.load(path_to_checkpoint)\nnncf_config = resuming_checkpoint['nncf_config']\nstate_dict = resuming_checkpoint['state_dict']\n\nquantized_model = nncf.torch.load_from_config(model, nncf_config, example_input)\nmodel.load_state_dict(state_dict)\n# ... the rest of the usual PyTorch-powered training pipeline\n```\n\n</details>\n\n### Training-Time Compression\n\nHere is an example of Accuracy Aware RB Sparsification pipeline where model weights and compression parameters may be fine-tuned to achieve a higher accuracy.\n\n<details><summary><b>PyTorch</b></summary>\n\n```python\nimport torch\nimport nncf.torch  # Important - must be imported before any other external package that depends on torch\n\nfrom nncf import NNCFConfig\nfrom nncf.torch import create_compressed_model, register_default_init_args\n\n# Instantiate your uncompressed model\nfrom torchvision.models.resnet import resnet50\nmodel = resnet50()\n\n# Load a configuration file to specify compression\nnncf_config = NNCFConfig.from_json(\"resnet50_imagenet_rb_sparsity.json\")\n\n# Provide data loaders for compression algorithm initialization, if necessary\nimport torchvision.datasets as datasets\nrepresentative_dataset = datasets.ImageFolder(\"/path\", transform=transforms.Compose([transforms.ToTensor()]))\ninit_loader = torch.utils.data.DataLoader(representative_dataset)\nnncf_config = register_default_init_args(nncf_config, init_loader)\n\n# Apply the specified compression algorithms to the model\ncompression_ctrl, compressed_model = create_compressed_model(model, nncf_config)\n\n# Now use compressed_model as a usual torch.nn.Module\n# to fine-tune compression parameters along with the model weights\n\n# ... the rest of the usual PyTorch-powered training pipeline\n\n# Export to ONNX or .pth when done fine-tuning\ncompression_ctrl.export_model(\"compressed_model.onnx\")\ntorch.save(compressed_model.state_dict(), \"compressed_model.pth\")\n```\n\n**NOTE (PyTorch)**: Due to the way NNCF works within the PyTorch backend, `import nncf` must be done before any other import of `torch` in your package _or_ in third-party packages that your code utilizes. Otherwise, the compression may be applied incompletely.\n\n</details>\n\n<details><summary><b>Tensorflow</b></summary>\n\n```python\nimport tensorflow as tf\n\nfrom nncf import NNCFConfig\nfrom nncf.tensorflow import create_compressed_model, register_default_init_args\n\n# Instantiate your uncompressed model\nfrom tensorflow.keras.applications import ResNet50\nmodel = ResNet50()\n\n# Load a configuration file to specify compression\nnncf_config = NNCFConfig.from_json(\"resnet50_imagenet_rb_sparsity.json\")\n\n# Provide dataset for compression algorithm initialization\nrepresentative_dataset = tf.data.Dataset.list_files(\"/path/*.jpeg\")\nnncf_config = register_default_init_args(nncf_config, representative_dataset, batch_size=1)\n\n# Apply the specified compression algorithms to the model\ncompression_ctrl, compressed_model = create_compressed_model(model, nncf_config)\n\n# Now use compressed_model as a usual Keras model\n# to fine-tune compression parameters along with the model weights\n\n# ... the rest of the usual TensorFlow-powered training pipeline\n\n# Export to Frozen Graph, TensorFlow SavedModel or .h5  when done fine-tuning\ncompression_ctrl.export_model(\"compressed_model.pb\", save_format=\"frozen_graph\")\n```\n\n</details>\n\nFor a more detailed description of NNCF usage in your training code, see [this tutorial](./docs/usage/training_time_compression/other_algorithms/Usage.md).\n\n<a id=\"demos-tutorials-and-samples\"></a>\n\n## Demos, Tutorials and Samples\n\nFor a quicker start with NNCF-powered compression, try sample notebooks and scripts presented below.\n\n### Jupyter* Notebook Tutorials and Demos\n\nReady-to-run Jupyter* notebook tutorials and demos are available to explain and display NNCF compression algorithms for optimizing models for inference with the OpenVINO Toolkit:\n\n| Notebook Tutorial Name                                                                                                                                                                                                                                                                                                                                 |                                  Compression Algorithm                                  |  Backend   |               Domain                |\n|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------:|:----------:|:-----------------------------------:|\n| [BERT Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/language-quantize-bert)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/language-quantize-bert/language-quantize-bert.ipynb) |                               Post-Training Quantization                                |  OpenVINO  |                 NLP                 |\n| [MONAI Segmentation Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/ct-segmentation-quantize)<br>[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/openvinotoolkit/openvino_notebooks/HEAD?filepath=notebooks%2Fct-segmentation-quantize%2Fct-scan-live-inference.ipynb)     |                               Post-Training Quantization                                |  OpenVINO  |            Segmentation             |\n| [PyTorch Model Quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/pytorch-post-training-quantization-nncf)                                                                                                                                                                                                      |                               Post-Training Quantization                                |  PyTorch   |        Image Classification         |\n| [YOLOv11 Quantization with Accuracy Control](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/yolov11-quantization-with-accuracy-control)                                                                                                                                                                                               |                    Post-Training Quantization with Accuracy Control                     |  OpenVINO  | Speech-to-Text,<br>Object Detection |\n| [PyTorch Training-Time Compression](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/pytorch-quantization-aware-training)                                                                                                                                                                                                   |                                Training-Time Compression                                |  PyTorch   |        Image Classification         |\n| [TensorFlow Training-Time Compression](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/tensorflow-quantization-aware-training)                                                                                                                                                                                                       |                                Training-Time Compression                                | Tensorflow |        Image Classification         |\n\nA list of notebooks demonstrating OpenVINO conversion and inference together with NNCF compression for models from various domains:\n\n| Demo Model                                                                                                                                                                                                                                                                                                                                        |               Compression Algorithm               |  Backend  |                                Domain                                |\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------:|:---------:|:--------------------------------------------------------------------:|\n| [YOLOv8](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/yolov8-optimization)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/yolov8-optimization/yolov8-object-detection.ipynb)            |            Post-Training Quantization             | OpenVINO  |  Object Detection,<br>KeyPoint Detection,<br>Instance Segmentation   |\n| [EfficientSAM](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/efficient-sam)                                                                                                                                                                                                                                         |            Post-Training Quantization             | OpenVINO  |                          Image Segmentation                          |\n| [Segment Anything Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/segment-anything)                                                                                                                                                                                                                            |            Post-Training Quantization             | OpenVINO  |                          Image Segmentation                          |\n| [OneFormer](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/oneformer-segmentation)                                                                                                                                                                                                                                   |            Post-Training Quantization             | OpenVINO  |                          Image Segmentation                          |\n| [CLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/clip-zero-shot-image-classification)                                                                                                                                                                                                                           |            Post-Training Quantization             | OpenVINO  |                            Image-to-Text                             |\n| [BLIP](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/blip-visual-language-processing)                                                                                                                                                                                                                               |            Post-Training Quantization             | OpenVINO  |                            Image-to-Text                             |\n| [Latent Consistency Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/latent-consistency-models-image-generation)                                                                                                                                                                                                |            Post-Training Quantization             | OpenVINO  |                            Text-to-Image                             |\n| [SDXL-turbo](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/sdxl-turbo)                                                                                                                                                                                                                                              |            Post-Training Quantization             | OpenVINO  |                   Text-to-Image,<br>Image-to-Image                   |\n| [Distil-Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/distil-whisper-asr)                                                                                                                                                                                                                                  |            Post-Training Quantization             | OpenVINO  |                            Speech-to-Text                            |\n| [Whisper](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/whisper-subtitles-generation)<br>[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/whisper-subtitles-generation/whisper-convert.ipynb) |            Post-Training Quantization             | OpenVINO  |                            Speech-to-Text                            |\n| [MMS Speech Recognition](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/mms-massively-multilingual-speech)                                                                                                                                                                                                           |            Post-Training Quantization             | OpenVINO  |                            Speech-to-Text                            |\n| [Grammar Error Correction](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/grammar-correction)                                                                                                                                                                                                                        |            Post-Training Quantization             | OpenVINO  |                       NLP, Grammar Correction                        |\n| [LLM Instruction Following](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/llm-question-answering)                                                                                                                                                                                                                   |                Weight Compression                 | OpenVINO  |                      NLP, Instruction Following                      |\n| [LLM Chat Bots](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/llm-chatbot)                                                                                                                                                                                                                                          |                Weight Compression                 | OpenVINO  |                            NLP, Chat Bot                             |\n\n### Post-Training Quantization and Weight Compression Examples\n\nCompact scripts demonstrating quantization/weight compression and corresponding inference speed boost:\n\n| Example Name                                                                                                                             |              Compression Algorithm               |  Backend   |         Domain         |\n|:-----------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------:|:----------:|:----------------------:|\n| [OpenVINO MobileNetV2](./examples/post_training_quantization/openvino/mobilenet_v2/README.md)                                            |            Post-Training Quantization            |  OpenVINO  |  Image Classification  |\n| [OpenVINO YOLOv8](./examples/post_training_quantization/openvino/yolov8/README.md)                                                       |            Post-Training Quantization            |  OpenVINO  |    Object Detection    |\n| [OpenVINO YOLOv8 QwAC](./examples/post_training_quantization/openvino/yolov8_quantize_with_accuracy_control/README.md)                   | Post-Training Quantization with Accuracy Control |  OpenVINO  |    Object Detection    |\n| [OpenVINO Anomaly Classification](./examples/post_training_quantization/openvino/anomaly_stfpm_quantize_with_accuracy_control/README.md) | Post-Training Quantization with Accuracy Control |  OpenVINO  | Anomaly Classification |\n| [PyTorch MobileNetV2](./examples/post_training_quantization/torch/mobilenet_v2/README.md)                                                |            Post-Training Quantization            |  PyTorch   |  Image Classification  |\n| [PyTorch SSD](./examples/post_training_quantization/torch/ssd300_vgg16/README.md)                                                        |            Post-Training Quantization            |  PyTorch   |    Object Detection    |\n| [TorchFX Resnet18](./examples/post_training_quantization/torch_fx/resnet18/README.md)                                                    |            Post-Training Quantization            |  TorchFX   |  Image Classification  |\n| [TensorFlow MobileNetV2](./examples/post_training_quantization/tensorflow/mobilenet_v2/README.md)                                        |            Post-Training Quantization            | TensorFlow |  Image Classification  |\n| [ONNX MobileNetV2](./examples/post_training_quantization/onnx/mobilenet_v2/README.md)                                                    |            Post-Training Quantization            |    ONNX    |  Image Classification  |\n| [ONNX YOLOv8 QwAC](./examples/post_training_quantization/onnx/yolov8_quantize_with_accuracy_control/README.md)                           | Post-Training Quantization with Accuracy Control |    ONNX    |    Object Detection    |\n| [ONNX TinyLlama WC](./examples/llm_compression/onnx/tiny_llama/README.md)                                                                |                Weight Compression                |    ONNX    |           LLM          |\n| [TorchFX TinyLlama WC](./examples/llm_compression/torch_fx/tiny_llama/README.md)                                                         |                Weight Compression                |  TorchFX   |           LLM          |\n| [OpenVINO TinyLlama WC](./examples/llm_compression/openvino/tiny_llama/README.md)                                                        |                Weight Compression                |  OpenVINO  |           LLM          |\n| [OpenVINO TinyLlama WC with HS](./examples/llm_compression/openvino/tiny_llama_find_hyperparams/README.md)                               |  Weight Compression with Hyperparameters Search  |  OpenVINO  |           LLM          |\n| [ONNX TinyLlama WC with SE](./examples/llm_compression/onnx/tiny_llama_scale_estimation/README.md)                                       |     Weight Compression with Scale Estimation     |    ONNX    |           LLM          |\n\n### Quantization-Aware Training Examples\n\n| Example Name                                                                        |   Compression Algorithm     | Backend |        Domain        |\n|:------------------------------------------------------------------------------------|:---------------------------:|:-------:|:--------------------:|\n| [PyTorch Resnet18](./examples/quantization_aware_training/torch/resnet18/README.md) | Quantization-Aware Training | PyTorch | Image Classification |\n| [PyTorch Anomalib](./examples/quantization_aware_training/torch/anomalib/README.md) | Quantization-Aware Training | PyTorch | Anomaly Detection    |\n\n<a id=\"third-party-repository-integration\"></a>\n\n## Third-party Repository Integration\n\nNNCF may be easily integrated into training/evaluation pipelines of third-party repositories.\n\n### Used by\n\n- [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov)\n\n  NNCF is used as a compression backend within the renowned `transformers` repository in HuggingFace Optimum Intel. For instance, the command below exports the [Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) model to OpenVINO format with INT4-quantized weights:\n\n  ```bash\n  optimum-cli export openvino -m meta-llama/Llama-3.2-3B-Instruct --weight-format int4 ./Llama-3.2-3B-Instruct-int4\n  ```\n\n- [Ultralytics](https://docs.ultralytics.com/integrations/openvino)\n\n  NNCF is integrated into the Intel OpenVINO export pipeline, enabling quantization for the exported models.\n\n- [ExecuTorch](https://github.com/pytorch/executorch/blob/main/examples/openvino/README.md)\n\n  NNCF is used as primary quantization framework for the [ExecuTorch OpenVINO integration](https://docs.pytorch.org/executorch/main/build-run-openvino.html).\n\n- [torch.compile](https://docs.pytorch.org/tutorials/prototype/openvino_quantizer.html)\n\n  NNCF is used as primary quantization framework for the [torch.compile OpenVINO integration](https://docs.openvino.ai/2025/openvino-workflow/torch-compile.html).\n\n- [OpenVINO Training Extensions](https://github.com/openvinotoolkit/training_extensions)\n\n  NNCF is integrated into OpenVINO Training Extensions as a model optimization backend. You can train, optimize, and\n  export new models based on available model templates as well as run the exported models with OpenVINO.\n\n<a id=\"installation-guide\"></a>\n\n## Installation Guide\n\nFor detailed installation instructions, refer to the [Installation](./docs/Installation.md) guide.\n\nNNCF can be installed as a regular PyPI package via pip:\n\n```bash\npip install nncf\n```\n\nNNCF is also available via [conda](https://anaconda.org/conda-forge/nncf):\n\n```bash\nconda install -c conda-forge nncf\n```\n\nSystem requirements of NNCF correspond to the used backend. System requirements for each backend and\nthe matrix of corresponding versions can be found in [installation.md](./docs/Installation.md).\n\n## NNCF Compressed Model Zoo\n\nList of models and compression results for them can be found at our [NNCF Model Zoo page](./docs/ModelZoo.md).\n\n## Citing\n\n```bi\n@article{kozlov2020neural,\n    title =   {Neural network compression framework for fast model inference},\n    author =  {Kozlov, Alexander and Lazarevich, Ivan and Shamporov, Vasily and Lyalyushkin, Nikolay and Gorbachev, Yury},\n    journal = {arXiv preprint arXiv:2002.08679},\n    year =    {2020}\n}\n```\n\n## Contributing Guide\n\nRefer to the [CONTRIBUTING.md](./CONTRIBUTING.md) file for guidelines on contributions to the NNCF repository.\n\n## Useful links\n\n- [Documentation](./docs)\n- [Examples](./examples)\n- [FAQ](./docs/FAQ.md)\n- [Notebooks](https://github.com/openvinotoolkit/openvino_notebooks#-model-training)\n- [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov)\n- [OpenVINO Model Optimization Guide](https://docs.openvino.ai/nncf)\n- [OpenVINO Hugging Face page](https://huggingface.co/OpenVINO#models)\n- [OpenVino Performance Benchmarks page](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)\n\n## Telemetry\n\nNNCF as part of the OpenVINOâ„¢ toolkit collects anonymous usage data for the purpose of improving OpenVINOâ„¢ tools.\nYou can opt-out at any time by running the following command in the Python environment where you have NNCF installed:\n\n`opt_in_out --opt_out`\n\nMore information available on [OpenVINO telemetry](https://docs.openvino.ai/2025/about-openvino/additional-resources/telemetry.html).\n",
    "readme_length": 36430
  },
  {
    "name": "STEP",
    "full_name": "GestaltCogTeam/STEP",
    "description": "Code for our SIGKDD'22 paper Pre-training-Enhanced Spatial-Temporal Graph Neural Network For Multivariate Time Series Forecasting.",
    "stars": 424,
    "forks": 47,
    "language": "Python",
    "url": "https://github.com/GestaltCogTeam/STEP",
    "topics": [
      "graph-neural-networks",
      "multivariate-time-series",
      "pre-training",
      "traffic-forecasting"
    ],
    "created_at": "2022-02-08T10:15:29Z",
    "updated_at": "2025-11-28T09:22:09Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "# <div align=\"center\"> Pre-training-Enhanced Spatial-Temporal Graph Neural Network For Multivariate Time Series Forecasting </div>\n\n<div align=\"center\">\n\n[![BasicTS](https://img.shields.io/badge/Developing%20with-BasicTS-blue)](https://github.com/zezhishao/BasicTS)\n[![EasyTorch](https://img.shields.io/badge/Developing%20with-EasyTorch-2077ff.svg)](https://github.com/cnstark/easytorch)\n[![LICENSE](https://img.shields.io/github/license/zezhishao/BasicTS.svg)](https://github.com/zezhishao/BasicTS/blob/master/LICENSE)\n\nCode for our SIGKDD'22 paper: \"[Pre-training-Enhanced Spatial-Temporal Graph Neural Network For Multivariate Time Series Forecasting](https://arxiv.org/abs/2206.09113)\".\n\nThe code is developed with [BasicTS](https://github.com/zezhishao/BasicTS), a PyTorch-based benchmark and toolbox for time series forecasting.\n\n</div>\n\n\n<img src=\"figure/STEP.png\" alt=\"TheTable\" style=\"zoom:42%;\" />\n\n> Multivariate Time Series (MTS) forecasting plays a vital role in a wide range of applications. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have become increasingly popular MTS forecasting methods. STGNNs jointly model the spatial and temporal patterns of MTS through graph neural networks and sequential models, significantly improving the prediction accuracy. But limited by model complexity, most STGNNs only consider short-term historical MTS data, such as data over the past one hour. However, the patterns of time series and the dependencies between them (i.e., the temporal and spatial patterns) need to be analyzed based on long-term historical MTS data. To address this issue, we propose a novel framework, in which STGNN is Enhanced by a scalable time series Pre-training model (STEP). Specifically, we design a pre-training model to efficiently learn temporal patterns from very long-term history time series (e.g., the past two weeks) and generate segment-level representations. These representations provide contextual information for short-term time series input to STGNNs and facilitate modeling dependencies between time series. Experiments on three public real-world datasets demonstrate that our framework is capable of significantly enhancing downstream STGNNs, and our pre-training model aptly captures temporal patterns.\n\n## ðŸ“š Table of Contents\n\n```text\nbasicts   --> The BasicTS, which provides standard pipelines for training MTS forecasting models. Don't worry if you don't know it, because it doesn't prevent you from understanding STEP's code.\n\ndatasets  --> Raw datasets and preprocessed data\n\nfigures   --> Some figures used in README.\n\nscripts   --> Data preprocessing scripts.\n\nstep      --> The implementation of STEP, including the architecture, dataloader, loss, and runner for STEP.\n\ntsformer_ckpt --> Pre-trained TSFormer for METR-LA, PEMS-BAY, and PEMS04 dataset.\n\ntraining_logs --> Training logs of STEP and TSFormer.\n\n```\n\n## ðŸ’¿ Requirements\n\nThe code is built based on Python 3.9, PyTorch 1.10.0, and [EasyTorch](https://github.com/cnstark/easytorch).\nYou can install PyTorch following the instruction in [PyTorch](https://pytorch.org/get-started/locally/). For example:\n\n```bash\npip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 torchaudio==0.10.0 -f https://download.pytorch.org/whl/torch_stable.html\n```\n\nAfter ensuring that PyTorch is installed correctly, you can install other dependencies via:\n\n```bash\npip install -r requirements.txt\n```\n\n## ðŸ“¦ Data Preparation\n\n### **Download Raw Data**\n\nYou can download all the raw datasets at [Google Drive](https://drive.google.com/file/d/1PY7IZ3SchpyXfNIXs71A2GEV29W5QCv2/view?usp=sharing) or [Baidu Yun](https://pan.baidu.com/s/1CXLxeHxHIMWLy3IKGFUq8g?pwd=blf8), and unzip them to `datasets/raw_data/`.\n\n### **Pre-process Data**\n\nYou can pre-process all data via:\n\n```bash\ncd /path/to/your/project\nbash scripts/data_preparation/all.sh\n```\n\nThen the `dataset` directory will look like this:\n\n```text\ndatasets\n   â”œâ”€METR-LA\n   â”œâ”€METR-BAY\n   â”œâ”€PEMS04\n   â”œâ”€raw_data\n   |    â”œâ”€PEMS04\n   |    â”œâ”€PEMS-BAY\n   |    â”œâ”€METR-LA\n   â”œâ”€README.md\n```\n\n## <span id=\"jump\"> ðŸŽ¯ Train STEP based on a Pre-trained TSFormer </span>\n\n```bash\npython step/run.py --cfg='step/STEP_$DATASET.py' --gpus='0, 1'\n# python step/run.py --cfg='step/STEP_METR-LA.py' --gpus='0, 1'\n# python step/run.py --cfg='step/STEP_PEMS-BAY.py' --gpus='0, 1'\n# python step/run.py --cfg='step/STEP_PEMS04.py' --gpus='0, 1'\n```\n\nReplace `$DATASET_NAME` with one of `METR-LA`, `PEMS-BAY`, `PEMS04` as shown in the code above. \nConfiguration file `step/STEP_$DATASET.py` describes the forecasting configurations.\nEdit `BATCH_SIZE` and `GPU_NUM` in the configuration file and `--gpu` in the command line to run on your own hardware.\nNote that different GPU number leads to different real batch sizes, affecting the learning rate setting and the forecasting accuracy.\n\nOur training logs are shown in [training_logs/STEP_METR-LA.log](./training_logs/STEP_METR-LA.log), [training_logs/STEP_METR-LA.log](./training_logs/STEP_METR-LA.log) and [training_logs/STEP_PEMS-BAY.log](./training_logs/STEP_PEMS-BAY.log).\n\n## âš’ Train STEP from Scratch\n\n### **Pre-training Stage**\n\n```bash\npython step/run.py --cfg='step/TSFormer_$DATASET.py' --gpus '0'\n# python step/run.py --cfg='step/TSFormer_METR-LA.py' --gpus='0'\n# python step/run.py --cfg='step/TSFormer_PEMS-BAY.py' --gpus='0, 1'\n# python step/run.py --cfg='step/TSFormer_PEMS04.py' --gpus='0, 1'\n```\n\nReplace `$DATASET_NAME` with one of `METR-LA`, `PEMS-BAY`, `PEMS04` as shown in the code above.\nConfiguration file `step/TSFormer_$DATASET.py` describes the pre-training configurations.\nEdit the `BATCH_SIZE` and `GPU_NUM` in the configuration file and `--gpu` in the command line to run on your own hardware.\n\nAll the training logs, including the config file, training log, and checkpoints, will be saved in `checkpoints/MODEL_EPOCH/MD5_of_config_file`.\nFor example, `checkpoints/TSFormer_100/5afe80b3e7a3dc055158bcfe99afbd7f`.\n\nOur training logs are shown in [training_logs/TSFormer_METR-LA.log](./training_logs/TSFormer_METR-LA.log), [training_logs/TSFormer_PEMS04.log](./training_logs/TSFormer_PEMS04.log), and [training_logs/TSFormer_PEMS04.log](./training_logs/TSFormer_PEMS-BAY.log), and the our pre-trained TSFormers for each datasets are placed in `tsformer_ckpt` folder.\n\n### **Forecasting Stage**\n\nAfter pre-training TSFormer, move your pre-trained best checkpoint to `tsformer_ckpt/`.\nFor example:\n\n```bash\ncp checkpoints/TSFormer_100/5afe80b3e7a3dc055158bcfe99afbd7f/TSFormer_best_val_MAE.pt tsformer_ckpt/TSFormer_$DATASET_NAME.pt\n```\n\nReplace `$DATASET_NAME` with one of `METR-LA`, `PEMS-BAY`, `PEMS04`.\n\nThen train the downstream STGNN (Graph WaveNet) like in section [\"ðŸŽ¯ Train STEP based on a Pre-trained TSFormer\"](#jump).\n\n## ðŸ“ˆ Performance and Visualization\n\n<img src=\"figure/MainResults.png\" alt=\"TheTable\" style=\"zoom:49.4%;\" />\n\n<img src=\"figure/Inspecting.jpg\" alt=\"Visualization\" style=\"zoom:25%;\" />\n\n## ðŸ”— More Related Works\n\n- [D2STGNN: Decoupled Dynamic Spatial-Temporal Graph Neural Network for Traffic Forecasting. VLDB'22.](https://github.com/zezhishao/D2STGNN)\n\n- [BasicTS: An Open Source Standard Time Series Forecasting Benchmark.](https://github.com/zezhishao/BasicTS)\n\n## QA:\n\nQ1: Why is the performance in the training log slightly different from the performance in the paper?\n\nA1: \nSTEP's code is now refactored based on [BasicTS](https://github.com/zezhishao/BasicTS), to provide fair comparisons with all baselines.\nBasicTS unifies the training pipeline and evaluation metrics, which is slightly different from the original implementation of STEP.\nTherefore, there may be small differences (better or worse) between training logs and papers that do not affect the conclusions of the paper.\nMoreover, you can refer to BasicTS to find the true performance of many baseline models.\n\n## Citing\n\nIf you find this repository useful for your work, please consider citing it as follows:\n\n```bibtex\n@inproceedings{DBLP:conf/kdd/ShaoZWX22,\n  author    = {Zezhi Shao and\n               Zhao Zhang and\n               Fei Wang and\n               Yongjun Xu},\n  title     = {Pre-training Enhanced Spatial-temporal Graph Neural Network for Multivariate\n               Time Series Forecasting},\n  booktitle = {{KDD} '22: The 28th {ACM} {SIGKDD} Conference on Knowledge Discovery\n               and Data Mining, Washington, DC, USA, August 14 - 18, 2022},\n  pages     = {1567--1577},\n  publisher = {{ACM}},\n  year      = {2022}\n}\n```\n",
    "readme_length": 8462
  },
  {
    "name": "A-Convolutional-Recurrent-Neural-Network-for-Real-Time-Speech-Enhancement",
    "full_name": "haoxiangsnr/A-Convolutional-Recurrent-Neural-Network-for-Real-Time-Speech-Enhancement",
    "description": "A minimum unofficial implementation of the \"A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement\" (CRN) using PyTorch",
    "stars": 339,
    "forks": 64,
    "language": "Python",
    "url": "https://github.com/haoxiangsnr/A-Convolutional-Recurrent-Neural-Network-for-Real-Time-Speech-Enhancement",
    "topics": [
      "cnn",
      "cnn-rnn",
      "pytorch",
      "real-time",
      "rnn",
      "speech-enhancement",
      "speech-processing"
    ],
    "created_at": "2019-08-12T08:02:05Z",
    "updated_at": "2025-11-23T14:43:20Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement\n\nA minimum unofficial implementation of the [A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement (CRN)](https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1405.pdf) using PyTorch.\n\n## ToDo\n- [x] Real-time version\n- [x] Update trainer\n- [x] Visualization of the spectrogram and the metrics (PESQ, STOI, SI-SDR) in the training\n- [ ] More docs\n\n## Usage\n\nTraining:\n\n```\npython train.py -C config/train/baseline_model.json5\n```\n\nInference:\n\n```\npython inference.py \\\n    -C config/inference/basic.json5 \\\n    -cp ~/Experiments/CRN/baseline_model/checkpoints/latest_model.tar \\\n    -dist ./enhanced\n```\n\nCheck out the README of [Wave-U-Net for SE](https://github.com/haoxiangsnr/Wave-U-Net-for-Speech-Enhancement) to learn more.\n\n## Performance\n\nPESQ, STOI, SI-SDR on DEMAND - Voice Bank test dataset, for reference only:\n\n| Experiment | PESQ | SI-SDR | STOI |\n| --- | --- | --- | --- |\n|Noisy | 1.979 | 8.511| 0.9258|\n|CRN | 2.528| 17.71| 0.9325|\n|CRN signal approximation  |2.606 |17.84 |0.9382|\n\n## Dependencies\n\n- Python==3.\\*.\\*\n- torch==1.\\*\n- librosa==0.7.0\n- tensorboard\n- pesq\n- pystoi\n- matplotlib\n- tqdm\n\n## References\n\n- [CRNN_mapping_baseline](https://github.com/YangYang/CRNN_mapping_baseline)\n- [A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement](https://web.cse.ohio-state.edu/~wang.77/papers/Tan-Wang1.interspeech18.pdf)\n- [EHNet](https://github.com/ododoyo/EHNet)\n- [Convolutional-Recurrent Neural Networks for Speech Enhancement](https://arxiv.org/abs/1805.00579)\n",
    "readme_length": 1608
  },
  {
    "name": "CARE-GNN",
    "full_name": "YingtongDou/CARE-GNN",
    "description": "Code for CIKM 2020 paper Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters",
    "stars": 295,
    "forks": 59,
    "language": "Python",
    "url": "https://github.com/YingtongDou/CARE-GNN",
    "topics": [
      "datamining",
      "deep-learning",
      "fraud-detection",
      "fraud-prevention",
      "graphneuralnetwork",
      "machine-learning",
      "reinforcement-learning",
      "security"
    ],
    "created_at": "2020-08-05T20:05:39Z",
    "updated_at": "2025-11-28T02:55:45Z",
    "homepage": "https://arxiv.org/abs/2008.08692",
    "license": "Apache License 2.0",
    "readme": "# CARE-GNN\n\nA PyTorch implementation for the [CIKM 2020](https://www.cikm2020.org/) paper below:  \n**Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters**.  \n[Yingtong Dou](http://ytongdou.com/), [Zhiwei Liu](https://sites.google.com/view/zhiwei-jim), [Li Sun](https://www.researchgate.net/profile/Li_Sun118), Yutong Deng, [Hao Peng](https://penghao-buaa.github.io/), [Philip S. Yu](https://www.cs.uic.edu/PSYu/).  \n\\[[Paper](https://arxiv.org/pdf/2008.08692.pdf)\\]\\[[Toolbox](https://github.com/safe-graph/DGFraud)\\]\\[[DGL Example](https://github.com/dmlc/dgl/tree/master/examples/pytorch/caregnn)\\]\\[[Benchmark](https://paperswithcode.com/paper/enhancing-graph-neural-network-based-fraud)\\]\n\n## Bug Fixes and Update (06/2021)\n\n### Similarity score\nThe feature and label similarity scores presented in Table 2 of the paper are incorrect. The updated equations for calculating two similarity scores are shown below:\n<p align=\"center\">\n    <br>\n    <a href=\"https://github.com/YingtongDou/CARE-GNN\">\n        <img src=\"https://github.com/YingtongDou/CARE-GNN/blob/master/eq_simi.png\" width=\"500\"/>\n    </a>\n    <br>\n<p>\n\nThe code for calculating the similarity scores is in [simi_comp.py](https://github.com/YingtongDou/CARE-GNN/blob/master/simi_comp.py).\n\nThe updated similarity scores for the two datasets are shown below. Note that we only compute the similarity scores for positive nodes to demonstrate the camouflage of fraudsters (positive nodes).\n\n| YelpChi  | rur  | rtr  | rsr  | homo  |\n|-------|--------|--------|--------|--------|\n| Avg. Feature Similarity | 0.991   |   0.988    |  0.988  | 0.988  |\n| Avg. Label Similarity |  0.909  |   0.176   |  0.186  | 0.184  |\n\n| Amazon  | upu  | usu  | uvu  | homo  |\n|-------|--------|--------|--------|--------|\n| Avg. Feature Similarity | 0.711   |   0.687    |  0.697  | 0.687  |\n| Avg. Label Similarity |  0.167  |   0.056   |  0.053  | 0.072  |\n\n### Relation weight in Figure 3\n\nAccording to this [issue](https://github.com/YingtongDou/CARE-GNN/issues/5), the weighted aggregation of CARE-Weight (a variant of CARE-GNN) has an error. After fixing it, the relation weight will not converge to the same value. Thus, the relation weight subfigure in Figure 3 and its associated conclusion are wrong.\n\n### Extended version CARE-GNN\n\nPlease check out [RioGNN](https://github.com/safe-graph/RioGNN), a GNN model extended based on CARE-GNN with more reinforcement learning modules integrated. We are actively developing an efficient multi-layer version of CARE-GNN. Stay tuned.\n\n\n## Overview\n\n<p align=\"center\">\n    <br>\n    <a href=\"https://github.com/YingtongDou/CARE-GNN\">\n        <img src=\"https://github.com/YingtongDou/CARE-GNN/blob/master/model.png\" width=\"900\"/>\n    </a>\n    <br>\n<p>\n\n**CA**mouflage-**RE**sistant **G**raph **N**eural **N**etwork **(CARE-GNN)** is a GNN-based fraud detector based on a multi-relation graph equipped with three modules that enhance its performance against camouflaged fraudsters.\n\nThree enhancement modules are:\n\n- **A label-aware similarity measure** which measures the similarity scores between a center node and its neighboring nodes;\n- **A similarity-aware neighbor selector** which leverages top-p sampling and reinforcement learning to select the optimal amount of neighbors under each relation;\n- **A relation-aware neighbor aggregator** which directly aggregates information from different relations using the optimal neighbor selection thresholds as weights.\n\nCARE-GNN has following advantages:\n\n- **Adaptability.** CARE-GNN adaptively selects best neighbors\nfor aggregation given arbitrary multi-relation graph;\n- **High-efficiency.** CARE-GNN has a high computational efficiency without attention and deep reinforcement learning;\n- **Flexibility.** Many other neural modules and external knowledge can be plugged into the CARE-GNN;\n\nWe have integrated more than **eight** GNN-based fraud detectors as a TensorFlow [toolbox](https://github.com/safe-graph/DGFraud). \n\n## Setup\n\nYou can download the project and install the required packages using the following commands:\n\n```bash\ngit clone https://github.com/YingtongDou/CARE-GNN.git\ncd CARE-GNN\npip3 install -r requirements.txt\n```\n\nTo run the code, you need to have at least **Python 3.6** or later versions. \n\n## Running\n\n1. In CARE-GNN directory, run `unzip /data/Amazon.zip` and `unzip /data/YelpChi.zip` to unzip the datasets; \n2. Run `python data_process.py` to generate adjacency lists used by CARE-GNN;\n3. Run `python train.py` to run CARE-GNN with default settings.\n\nFor other dataset and parameter settings, please refer to the arg parser in `train.py`. Our model supports both CPU and GPU mode.\n\n## Running on your datasets\n\nTo run CARE-GNN on your datasets, you need to prepare the following data:\n\n- Multiple-single relation graphs with the same nodes where each graph is stored in `scipy.sparse` matrix format, you can use `sparse_to_adjlist()` in `utils.py` to transfer the sparse matrix into adjacency lists used by CARE-GNN;\n- A numpy array with node labels. Currently, CARE-GNN only supports binary classification;\n- A node feature matrix stored in `scipy.sparse` matrix format. \n\n### Repo Structure\nThe repository is organized as follows:\n- `data/`: dataset files;\n- `data_process.py`: transfer sparse matrix to adjacency lists;\n- `graphsage.py`: model code for vanilla [GraphSAGE](https://github.com/williamleif/graphsage-simple/) model;\n- `layers.py`: CARE-GNN layers implementations;\n- `model.py`: CARE-GNN model implementations;\n- `train.py`: training and testing all models;\n- `utils.py`: utility functions for data i/o and model evaluation.\n\n## Citation\nIf you use our code, please cite the paper below:\n```bibtex\n@inproceedings{dou2020enhancing,\n  title={Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters},\n  author={Dou, Yingtong and Liu, Zhiwei and Sun, Li and Deng, Yutong and Peng, Hao and Yu, Philip S},\n  booktitle={Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM'20)},\n  year={2020}\n}\n```\n",
    "readme_length": 6092
  },
  {
    "name": "Wave-U-Net-For-Speech-Enhancement",
    "full_name": "craigmacartney/Wave-U-Net-For-Speech-Enhancement",
    "description": "Improved speech enhancement with the Wave-U-Net, a deep convolutional neural network architecture for audio source separation, implemented for the task of speech enhancement in the time-domain.",
    "stars": 221,
    "forks": 38,
    "language": "Python",
    "url": "https://github.com/craigmacartney/Wave-U-Net-For-Speech-Enhancement",
    "topics": [],
    "created_at": "2018-11-29T18:48:42Z",
    "updated_at": "2025-08-23T11:52:55Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# Improved Speech Enhancement with the Wave-U-Net\n\nThe [Wave-U-Net applied to speech enhancement](http://arxiv.org/abs/1811.11307) [1], an adaptation of the original implementation for music source separation by [Stoller et al](https://arxiv.org/abs/1806.03185) [2].\n\nThe Wave-U-Net is a convolutional neural network applicable to audio source separation tasks, recently introduced by Stoller et al for the separation of music vocals and accompaniment [2]. A 1D convolutional time domain variant of the 2D convolutions within the U-Net [3], this end-to-end learning method for audio source separation operates directly in the time domain, permitting the integrated modelling of phase information and being able to take large temporal contexts into account.\n\nExperiments on audio source separation for speech enhancement in [1] show that the proposed method rivals state-of-the-art architectures, improving upon various metrics, namely PESQ, CSIG, CBAK, COVL and SSNR, with respect to the single-channel speech enhancement task on the Voice Bank corpus (VCTK) dataset. Future experimentation will focus on increasing effectiveness and efficiency by further adapting the model size and other parameters, e.g. filter sizes, to the task and expanding to multi-channel audio and multi-source-separation.\n\n<br>\n\n## Architecture\nThe architecture is the same as that employed in [2] with the exception of the number of hidden layers and the validation set size. The number of hidden layers was experimented with and the results suggest the optimum size to be 9 layers.\n\nSee diagram below for a summary visual representation of the architecture:\n\n![alt text](https://github.com/craigmacartney/Wave-U-Net-For-Speech-Enhancement/blob/master/Wave-U-Net_Diagram-v1.png)\n\n## Initialisation\n<b>Requirements</b>\n\nUnder our implementation, training took c.36 hours using GeForce GTX 1080 Ti GPU with 11178 MiB, on Linux Ubuntu 16.04, with Python 2.7. In a new virtual environment, required Python 2.7 packages can be installed using `pip install -r requirements.txt`. N.B. this presumes the installation of <b>ffmpeg</b> and <b>libsndfile</b>.\n\n<br>\n\n<b>Data Preparation</b>\n\nTrain and test datasets provided by the 28-speaker [Voice Bank Corpus (VCTK)](https://datashare.is.ed.ac.uk/handle/10283/2791) [4] (30 speakers in total - 28 intended for training and 2 reserved for testing). The noisy training data were generated by mixing the clean data with various noise datasets, as per the instructions provided in [4, 5, 6].\n\nThe training dataset should then be prepared for being parsed as an XML file (not provided) using the ElementTree XML API in <b>Datasets.getAudioData</b>.\n\n<br>\n\n## Training\nTraining can be executed by running the command `python Training.py`, modyfing the parameters in <b>Config.py</b> as desired.\n\n## Testing\nTesting experiments can be performed by running the command `python Test_Predictions_VCTK.py`.\n\nSpeech source estimates should then be evaluated against the clean speech file they are estimating. This can be done using <b>Evaluate.m</b>, which selects multiple files and performs the <b>composite.m</b> script [7] (available [here](https://ecs.utdallas.edu/loizou/speech/software.htm)) upon each one, calculating the PESQ, SSNR, CSIG, CBAK and COVL.\n\nAudio examples of both speech and background noise estimates of the VCTK test set, alongside the noisy test files and clean speech for reference, are available for download in the <b>audio_examples</b> directory.\n\n<br>\n\n## References\n[1] Craig Macartney and Tillman Weyde. Improved Speech Enhancement with the Wave-U-Net. 2018. URL http://arxiv.org/abs/1811.11307\n\n[2] Daniel Stoller, Sebastian Ewert, and Simon Dixon. Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation. 6 2018. URL https://arxiv.org/abs/1806.03185.\n\n[3] Andreas Jansson, Eric J. Humphrey, Nicola Montecchio, Rachel M. Bittner, Aparna Kumar, and Tillman Weyde. Singing voice separation with deep u-net convolutional networks. In Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017, Suzhou, China, October 23-27, 2017, pages 745â€“751, 2017. URL https://ismir2017.smcnus.org/wp-content/uploads/2017/10/171_Paper.pdf.\n\n[4] Cassia Valentini-Botinhao. Noisy speech database for training speech enhancement algorithms and TTS models, 2016 [sound]. University of Edinburgh. School of Informatics. Centre for Speech Technology Research (CSTR), 2017. URL http://dx.doi.org/10.7488/ds/2117.\n\n[5] Santiago Pascual, Antonio Bonafonte, and Joan SerrÃ . SEGAN: Speech Enhancement Generative Adversarial Network. doi: 10.7488/ds/1356. URL http://dx.doi.org/10.7488/ds/1356.\n\n[6] Cassia Valentini-Botinhao, Xin Wang, Shinji Takaki, and Junichi Yamagishi. Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech. Technical report. URL https://www.research.ed.ac.uk/portal/files/26581510/SSW9_Cassia_1.pdf.\n\n[7] Philipos C Loizou. Speech Enhancement: Theory and Practice. CRC Press, Inc., Boca Raton, FL, USA, 2nd edition, 2013. ISBN 1466504218, 9781466504219.\n",
    "readme_length": 5085
  },
  {
    "name": "PM2.5-GNN",
    "full_name": "shuowang-ai/PM2.5-GNN",
    "description": "PM2.5-GNN: A Domain Knowledge Enhanced Graph Neural Network For PM2.5 Forecasting",
    "stars": 206,
    "forks": 67,
    "language": "Python",
    "url": "https://github.com/shuowang-ai/PM2.5-GNN",
    "topics": [],
    "created_at": "2020-10-11T12:04:23Z",
    "updated_at": "2025-12-02T07:47:29Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# PM2.5-GNN\n\nPM2.5-GNN: A Domain Knowledge Enhanced Graph Neural Network For PM2.5 Forecasting\n\n## Dataset\n\n- Download dataset **KnowAir** from [Google Drive](https://drive.google.com/open?id=1R6hS5VAgjJQ_wu8i5qoLjIxY0BG7RD1L) or [Baiduyun](https://pan.baidu.com/s/18D6Etl5Lm1E4vOLVrX0ZAw) with code `t82d`.\n\n## KnowAir-V2\n\nðŸš€ Dataset Update: Announcing KnowAir-V2! ðŸš€\n\nWe are excited to announce a major upgrade to the original KnowAir (PM2.5-GNN) dataset with the official release of KnowAir-V2! This is a brand-new, higher-quality benchmark dataset for air quality forecasting.\n\nKey improvements in KnowAir-V2 include:\n- Longer Temporal Span: Data covers from 2016 to 2023.\n- Richer Variables: Includes not only PM2.5 but also O3 and more related meteorological variables.\n- Higher Data Quality: The data has undergone rigorous preprocessing and imputation, reaching an operational-level standard.\n\nFor all new research and projects, we strongly recommend using KnowAir-V2. This dataset is designed to provide a powerful benchmarking platform for more advanced spatio-temporal prediction models that integrate physical-chemical knowledge, such as PCDCNet.\n\nHow to Access and Cite\nDataset Download (KnowAir-V2):\n\n- Wang, S., Cheng, Y., Meng, Q., Saukh, O., Zhang, J., Fan, J., Zhang, Y., Yuan, X., & Thiele, L. (2025). KnowAir-V2: A Benchmark Dataset for Air Quality Forecasting with PCDCNet [Data set]. Zenodo. https://doi.org/10.5281/zenodo.15614907\n\n- Related Paper (PCDCNet):\nPlease refer to the paper: \"PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints\" (arXiv:2505.19842). https://www.arxiv.org/abs/2505.19842\n\n## Requirements\n\n```\nPython 3.7.3\nPyTorch 1.7.0\nPyG: https://github.com/rusty1s/pytorch_geometric#pytorch-170\n```\n\n```bash\npip install -r requirements.txt\n```\n\n## Experiment Setup\n\nopen `config.yaml`, do the following setups.\n\n- set data path after your server name. Like mine.\n\n![](https://tva1.sinaimg.cn/large/0081Kckwly1gjy8kojsfmj30i202g746.jpg)\n\n```python\nfilepath:\n  GPU-Server:\n    knowair_fp: /data/wangshuo/haze/pm25gnn/KnowAir.npy\n    results_dir: /data/wangshuo/haze/pm25gnn/results\n\n```\n\n- Uncomment the model you want to run.\n\n```python\n#  model: MLP\n#  model: LSTM\n#  model: GRU\n#  model: GC_LSTM\n#  model: nodesFC_GRU\n   model: PM25_GNN\n#  model: PM25_GNN_nosub\n```\n\n- Choose the sub-datast number in [1,2,3].\n\n```python\n dataset_num: 3\n```\n\n- Set weather variables you wish to use. Following is the default setting in the paper. You can uncomment specific variables. Variables in dataset **KnowAir** is defined in `metero_var`.\n\n```python\n  metero_use: ['2m_temperature',\n               'boundary_layer_height',\n               'k_index',\n               'relative_humidity+950',\n               'surface_pressure',\n               'total_precipitation',\n               'u_component_of_wind+950',\n               'v_component_of_wind+950',]\n\n```\n\n## Run\n\n```bash\npython train.py\n```\n\n## Reference\n\nPaper: https://dl.acm.org/doi/10.1145/3397536.3422208\n\n```\n@inproceedings{10.1145/3397536.3422208,\nauthor = {Wang, Shuo and Li, Yanran and Zhang, Jiang and Meng, Qingye and Meng, Lingwei and Gao, Fei},\ntitle = {PM2.5-GNN: A Domain Knowledge Enhanced Graph Neural Network For PM2.5 Forecasting},\nyear = {2020},\nisbn = {9781450380195},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3397536.3422208},\ndoi = {10.1145/3397536.3422208},\nabstract = {When predicting PM2.5 concentrations, it is necessary to consider complex information sources since the concentrations are influenced by various factors within a long period. In this paper, we identify a set of critical domain knowledge for PM2.5 forecasting and develop a novel graph based model, PM2.5-GNN, being capable of capturing long-term dependencies. On a real-world dataset, we validate the effectiveness of the proposed model and examine its abilities of capturing both fine-grained and long-term influences in PM2.5 process. The proposed PM2.5-GNN has also been deployed online to provide free forecasting service.},\nbooktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},\npages = {163â€“166},\nnumpages = {4},\nkeywords = {air quality prediction, graph neural network, spatio-temporal prediction},\nlocation = {Seattle, WA, USA},\nseries = {SIGSPATIAL '20}\n}\n```\n",
    "readme_length": 4414
  },
  {
    "name": "bitorch-engine",
    "full_name": "GreenBitAI/bitorch-engine",
    "description": "A toolkit enhances PyTorch with specialized functions for low-bit quantized neural networks.",
    "stars": 195,
    "forks": 24,
    "language": "Python",
    "url": "https://github.com/GreenBitAI/bitorch-engine",
    "topics": [],
    "created_at": "2024-03-20T14:51:16Z",
    "updated_at": "2025-09-14T03:29:42Z",
    "homepage": "https://greenbitai.github.io/bitorch-engine/",
    "license": "Apache License 2.0",
    "readme": "# BITorch Engine (BIE)\n\nBitorch Engine is a cutting-edge computation library for neural networks that enhances PyTorch by integrating specialized\nlayers and functions tailored for **Low-Bit** quantized neural network operations.\nIt harnesses the robust capabilities of high-performance computing platforms, including GPUs and CPUs,\nand is designed with future adaptability in mind to extend support to emerging NPU hardware technologies.\n\n## More about BIE\n\nBitorch Engine offers a suite of optimized neural network components that are designed to leverage the full power of modern GPUs.\nThis includes custom CUDA kernels, quantization-aware training mechanisms, and a variety of layer types\nthat are specifically crafted to reduce computational overhead while maintaining high precision and accuracy in deep learning models.\n\nBuilding on the foundational strengths of Bitorch Engine, the technology has been employed in pioneering projects that\npush the boundaries of neural network training and inference.\nFor instance:\n\n- [green-bit-llm-trainer](https://github.com/GreenBitAI/green-bit-llm/tree/main/green_bit_llm/sft): In this project, BIE represents a significant leap in the field of Large Language Model (LLM) fine-tuning. Unlike traditional approaches that either quantize a fully trained model or introduce a few additional trainable parameters for [LoRA](https://github.com/microsoft/LoRA) style fine-tuning, this project innovates by directly fine-tuning the quantized parameters of LLMs. This paradigm shift allows for the full-scale quantization fine-tuning of LLMs, ensuring that the training process tightly integrates with the quantization schema from the outset.\n- [green-bit-llm-inference](https://github.com/GreenBitAI/green-bit-llm/tree/main/green_bit_llm/inference) also showcase the BIE's adeptness at supporting inference for models quantized from 4 to 2-bits without any significant loss in accuracy compared to the original 32 or 16-bits models. It stands as a testament to BIE's capability to maintain the delicate balance between model size, computational efficiency, and accuracy, addressing one of the key challenges in deploying sophisticated neural networks in resource-constrained environments.\n\nThese projects exemplify the practical applications of Bitorch Engine and underscore its flexibility and efficiency for modern AI research and development.\nHowever, keep in mind that BIE is still in an early beta stage, see our roadmap below. \n\n## Roadmap\n\nOur goals for BITorch engine in the future are (not necessarily in this order):\n\n- Add support for (Distributed) Data Parallel training strategies (for selected layers)\n- Provide better support for Metal kernels\n- Improve our existing code, so it becomes even faster, more memory-efficient and easier to use\n- Binary pip releases which include the built extensions\n\nWe are planning to release new features and improvements as they become available,\nbut this also means breaking changes can occur in the API during our beta stage.\n\n## Installation\n\nThe requirements are:\n\n- A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required, but gcc 12.x is not supported yet)\n- Python 3.9 or later\n- PyTorch 1.8 or later\n\nPlease check your operating system's options for the C++ compiler.\nFor more detailed information, you can check the [requirements to build PyTorch from source](https://github.com/pytorch/pytorch?tab=readme-ov-file#prerequisites).\nIn addition, for layers to speed up on specific hardware (such as CUDA devices, or MacOS M1/2/3 chips), we recommend installing:\n\n- CUDA Toolkit 11.8 or 12.1 for CUDA accelerated layers\n- **[MLX](https://github.com/ml-explore/mlx)** for mlx-based layers on MacOS\n- **[CUTLASS](https://github.com/NVIDIA/cutlass)** for cutlass-based layers\n\n### Binary Release\n\n**A first experimental binary release for Linux with CUDA 12.1 is ready.**\nIt only supports GPUs with CUDA compute capability with 8.6 or higher ([check here](https://developer.nvidia.com/cuda-gpus)).\nFor MacOS or lower compute capability, build the package from source (additional binary release options are planned in the future).\nWe recommend to create a conda environment to manage the installed CUDA version and other packages:\n\n1. Create Environment for Python 3.10 and activate it:\n```bash\nconda create -y --name bitorch-engine python=3.10\nconda activate bitorch-engine\n```\n\nAs an alternative, you can also store the environment in a relative path.\n\n<details><summary>Click to here to expand the instructions for this.</summary>\n\n```bash\nexport BITORCH_WORKSPACE=\"${HOME}/bitorch-workspace\"\nmkdir -p \"${BITORCH_WORKSPACE}\" && cd \"${BITORCH_WORKSPACE}\"\nconda create -y --prefix ./conda-env python=3.10\nconda activate ./conda-env\n```\n\n</details>\n\n2. Install CUDA (if it is not installed already on the system):\n```bash\nconda install -y -c \"nvidia/label/cuda-12.1.0\" cuda-toolkit\n```\n3. Install our customized torch that allows gradients on INT tensors and install it with pip (this URL is for CUDA 12.1\nand Python 3.10 - you can find other versions [here](https://packages.greenbit.ai/whl/)) together with bitorch engine:\n```bash\npip install \\\n  \"https://packages.greenbit.ai/whl/cu121/torch/torch-2.3.0-cp310-cp310-linux_x86_64.whl\" \\\n  \"https://packages.greenbit.ai/whl/cu121/bitorch-engine/bitorch_engine-0.2.6-cp310-cp310-linux_x86_64.whl\"\n```\n\n### Build From Source\n\nWe provide instructions for the following options:\n\n- [Conda + Linux](#conda-on-linux-with-cuda) (with CUDA and cutlass)\n- [Docker](#docker-with-cuda) (with CUDA and cutlass)\n- [Conda + MacOS](#conda-on-macos-with-mlx) (with MLX)\n\nWe recommend managing your BITorch Engine installation in a conda environment (otherwise you should adapt/remove certain variables, e.g. `CUDA_HOME`).\nYou may want to keep everything (environment, code, etc.) in one directory or use the default directory for conda environments.\nYou may wish to adapt the CUDA version to 12.1 where applicable.\n\n#### Conda on Linux (with CUDA)\n\nTo use these instructions, you need to have [conda](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html) and a suitable C++ compiler installed.\n\n1. Create Environment for Python 3.9 and activate it:\n```bash\nconda create -y --name bitorch-engine python=3.9\nconda activate bitorch-engine\n```\n2. Install CUDA\n```bash\nconda install -y -c \"nvidia/label/cuda-11.8.0\" cuda-toolkit\n```\n3. Install our customized torch that allows gradients on INT tensors and install it with pip (this URL is for CUDA 11.8\nand Python 3.9 - you can find other versions [here](https://packages.greenbit.ai/whl/)):\n```bash\npip install \"https://packages.greenbit.ai/whl/cu118/torch/torch-2.1.0-cp39-cp39-linux_x86_64.whl\"\n```\n4. To use cutlass layers, you should also install CUTLASS 2.8.0 (from source), adjust `CUTLASS_HOME` (this is where we clone and install cutlass)\n(if you have older or newer GPUs you may need to add your [CUDA compute capability](https://developer.nvidia.com/cuda-gpus) in `CUTLASS_NVCC_ARCHS`):\n```bash\nexport CUTLASS_HOME=\"/some/path\"\nmkdir -p \"${CUTLASS_HOME}\"\ngit clone --depth 1 --branch \"v2.8.0\" \"https://github.com/NVIDIA/cutlass.git\" --recursive ${CUTLASS_HOME}/source\nmkdir -p \"${CUTLASS_HOME}/build\" && mkdir -p \"${CUTLASS_HOME}/install\"\ncd \"${CUTLASS_HOME}/build\"\ncmake ../source -DCMAKE_INSTALL_PREFIX=\"${CUTLASS_HOME}/install\" -DCUTLASS_ENABLE_TESTS=OFF -DCUTLASS_ENABLE_EXAMPLES=OFF -DCUTLASS_NVCC_ARCHS='75;80;86'\nmake -j 4\ncmake --install .\n```\nIf you have difficulties installing cutlass, you can check the [official documentation](https://github.com/NVIDIA/cutlass/tree/v2.8.0),\nuse the other layers without installing it or try the docker installation.\n\nAs an alternative to the instructions above, you can also store the environment and clone all repositories within one \"root\" directory.\n\n<details><summary>Click to here to expand the instructions for this.</summary>\n\n0. Set workspace dir (use an absolute path!):\n```bash\nexport BITORCH_WORKSPACE=\"${HOME}/bitorch-workspace\"\nmkdir -p \"${BITORCH_WORKSPACE}\" && cd \"${BITORCH_WORKSPACE}\"\n```\n1. Create Environment for Python 3.9 and activate it:\n```bash\nconda create -y --prefix ./conda-env python=3.9\nconda activate ./conda-env\n```\n2. Install CUDA\n```bash\nconda install -y -c \"nvidia/label/cuda-11.8.0\" cuda-toolkit\n```\n3. Install our customized torch that allows gradients on INT tensors and install it with pip (this url is for CUDA 11.8\nand Python 3.9 - you can find other versions [here](https://packages.greenbit.ai/whl/)):\n```bash\npip install \"https://packages.greenbit.ai/whl/cu118/torch/torch-2.1.0-cp39-cp39-linux_x86_64.whl\"\n```\n4. To use cutlass layers, you should also install CUTLASS 2.8.0\n(if you have older or newer GPUs you may need to add your [CUDA compute capability](https://developer.nvidia.com/cuda-gpus) in `CUTLASS_NVCC_ARCHS`):\n```bash\nexport CUTLASS_HOME=\"${BITORCH_WORKSPACE}/cutlass\"\nmkdir -p \"${CUTLASS_HOME}\"\ngit clone --depth 1 --branch \"v2.8.0\" \"https://github.com/NVIDIA/cutlass.git\" --recursive ${CUTLASS_HOME}/source\nmkdir -p \"${CUTLASS_HOME}/build\" && mkdir -p \"${CUTLASS_HOME}/install\"\ncd \"${CUTLASS_HOME}/build\"\ncmake ../source -DCMAKE_INSTALL_PREFIX=\"${CUTLASS_HOME}/install\" -DCUTLASS_ENABLE_TESTS=OFF -DCUTLASS_ENABLE_EXAMPLES=OFF -DCUTLASS_NVCC_ARCHS='75;80;86'\nmake -j 4\ncmake --install .\ncd \"${BITORCH_WORKSPACE}\"\n```\nIf you have difficulties installing cutlass, you can check the [official documentation](https://github.com/NVIDIA/cutlass/tree/v2.8.0),\nuse the other layers without installing it or try the docker installation.\n</details>\n\nAfter setting up the environment, clone the code and build with pip (to hide the build output remove `-v`):\n\n```bash\n# make sure you are in a suitable directory, e.g. your bitorch workspace\ngit clone --recursive https://github.com/GreenBitAI/bitorch-engine\ncd bitorch-engine\n# only gcc versions 9.x, 10.x, 11.x are supported\n# to select the correct gcc, use:\n# export CC=gcc-11 CPP=g++-11 CXX=g++-11\nCPATH=\"${CUTLASS_HOME}/install/include\" CUDA_HOME=\"${CONDA_PREFIX}\" pip install -e . -v\n```\n\n#### Docker (with CUDA)\n\nYou can also use our prepared Dockerfile to build a docker image (which includes building the engine under `/bitorch-engine`):\n\n```bash\ncd docker\ndocker build -t bitorch/engine .\ndocker run -it --rm --gpus all --volume \"/path/to/your/project\":\"/workspace\" bitorch/engine:latest\n```\n\nCheck the [docker readme](docker/README.md) for options and more details.\n\n#### Conda on MacOS (with MLX)\n\n1. We recommend to create a virtual environment for and activate it. In the following example we use a conda environment for python 3.9, \nbut virtualenv should work as well.\n```bash\nconda create -y --name bitorch-engine python=3.9\nconda activate bitorch-engine\n```\n2. Install our customized torch that allows gradients on INT tensors and install it with pip (this URL is for macOS\nwith Python 3.9 - you can find other versions [here](https://packages.greenbit.ai/whl/)):\n```bash\npip install \"https://packages.greenbit.ai/whl/macosx/torch/torch-2.2.1-cp39-none-macosx_11_0_arm64.whl\"\n```\n3. For MacOS users and to use OpenMP acceleration, install OpenMP with Homebrew and configure the environment:\n```bash\nbrew install libomp\n# during libomp installation it should remind you, you need something like this:\nexport LDFLAGS=\"-L$(brew --prefix)/opt/libomp/lib\"\nexport CPPFLAGS=\"-I$(brew --prefix)/opt/libomp/include\"\n```\n4. To use the [mlx](https://github.com/ml-explore/mlx) accelerated `MPQLinearLayer`, you need to install the python library.\n```bash\n# use one of the following, to either install with pip or conda:\npip install mlx==0.4.0\nconda install conda-forge::mlx=0.4.0\n```\n Currently, we only tested version 0.4.0. However, newer versions might also work.\n To train the `MPQLinearLayer` you need to install our custom PyTorch version (see steps above).\n Without it, you need to specify `requires_grad=False` when initializing `MPQLinearLayer`.\n5. You should now be able to build with:\n```bash\ngit clone --recursive https://github.com/GreenBitAI/bitorch-engine\ncd bitorch-engine\npip install -e . -v\n```\n\n## Build options\n\n### Building Specific Extensions\n\nWhile developing, a specific cpp/cuda extension can be (re-)build, by using the environment variable `BIE_BUILD_ONLY`,\nlike so:\n```bash\nBIE_BUILD_ONLY=\"bitorch_engine/layers/qlinear/binary/cpp\" pip install -e . -v\n```\nIt needs to a relative path to one extension directory.\n\n### Building for a Specific CUDA Architecture\n\nTo build for a different CUDA Arch, use the environment variable `BIE_CUDA_ARCH` (e.g. use 'sm_75', 'sm_80', 'sm_86'):\n```bash\nBIE_CUDA_ARCH=\"sm_86\" pip install -e . -v\n```\n\n### Force Building CUDA Modules\n\nIf you have CUDA development libraries installed, but `torch.cuda.is_available()` is False, e.g. in HPC or docker environments,\nyou can still build the extensions that depend on CUDA, by setting `BIE_FORCE_CUDA=\"true\"`:\n```bash\nBIE_FORCE_CUDA=\"true\" pip install -e . -v\n```\n\n### Skip Library File Building\n\nIf you just want to avoid rebuilding any files, you can set `BIE_SKIP_BUILD`:\n```bash\nBIE_SKIP_BUILD=\"true\" python3 -m build --no-isolation --wheel\n```\nThis would create a wheel and package `.so` files without trying to rebuild them.\n\n## Development\n\nTo adjust the build options or address build failures, modify the configurations in \n[cpp_extension.py](bitorch_engine/utils/cpp_extension.py)/\n[cuda_extension.py](bitorch_engine/utils/cuda_extension.py).\n\nYou may want to clean the build output before rebuilding, which may help to avoid errors and/or install development requirements:\n```bash\npython setup.py clean\n# now build like usually, use \".[dev]\" for development requirements, e.g.\nCUDA_HOME=\"${CONDA_PREFIX}\" pip install -e \".[dev]\" -v\n```\n\nYou can run our tests with pytest:\n```bash\npytest\n```\n\n### Cuda Device Selection\n\nTo select a certain CUDA device, set the environment variable `BIE_DEVICE`, e.g.:\n```bash\nexport BIE_DEVICE=1  # This selects the second CUDA device, as indexing starts from 0.\n```\n\n## Documentation\n\nCheck out the [Documentation](https://greenbitai.github.io/bitorch-engine) for API reference.\n\n## Examples\n\n- Basic example scripts can be found directly in [examples](examples).\n- [green-bit-llm-trainer](https://github.com/GreenBitAI/green-bit-llm/tree/main/green_bit_llm/sft) showcases the fine-tuning training of LLMs with quantized parameters.\n- [green-bit-llm-inference](https://github.com/GreenBitAI/green-bit-llm/tree/main/green_bit_llm/inference) showcases the BIE's adeptness at supporting fast inference for 4 to 2-bits LLMs.\n\n## Contributors\n\nBIE is under active development and currently maintained by contributors: [Haojin Yang](https://github.com/yanghaojin), [Joseph Bethge](https://github.com/Jopyth), [Nianhui Guo](https://github.com/NicoNico6), [Maximilian Schulze](https://github.com/max-3l), Hong Guo, [Paul Mattes](https://github.com/Snagnar).\n\nCheck our [contributing guide](CONTRIBUTING.md) to learn about how to contribute to the project.\n\n## License\n\nBitorch Engine is made available under the [Apache 2.0 License](LICENSE). See the LICENSE file for details.\n\n## Citation\nIf you use our approach in your research, please cite our work as follows:\n```\n@article{bitorch_engine,\n  title={Bitorch Engine: Streamlining AI with Open-Source Low-Bit Quantization},\n  author={Yang, Haojin and Bethge, Joseph and Guo, Nianhui and Schulze, Maximilian and Guo, Hong},\n  journal={https://github.com/GreenBitAI/bitorch-engine},\n  year={2024}\n}\n```\n\n## References and Acknowledgements\n\nThis project builds upon or uses concepts from the following open-source projects:\n\n- **[PyTorch](https://github.com/pytorch/pytorch)**\n- **[CUTLASS](https://github.com/NVIDIA/cutlass)**\n- **[MLX](https://github.com/ml-explore/mlx)**\n- **[ExLlamaV2](https://github.com/turboderp/exllamav2)**\n- **[TCBNN](https://github.com/pnnl/TCBNN)**\n- **[GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa)**\n\nWe extend our heartfelt gratitude to the developers of these projects for their invaluable contributions to the open-source community. Without their exceptional work, none of this would be possible.\nThe corresponding licenses of the reference projects can be found in the [licenses](licenses) directory of the source tree.\n\n### Open Source Software Acknowledgment\n\nThis project makes use of open source software (OSS) components. The original code of these components is kept under their respective licenses and copyrights. We are grateful to the open-source community for making these resources available. For specific information about each component's license, please refer to the corresponding sections within our project documentation or the direct references provided in the \"References\" section of this document.\n\nWe endeavor to comply with all open source licenses and their requirements, including proper acknowledgment and notice. If there are any concerns or questions regarding our license acknowledgments, please reach out to us for clarification.\n",
    "readme_length": 16986
  },
  {
    "name": "GCE-GNN",
    "full_name": "CCIIPLab/GCE-GNN",
    "description": "The source code for \"Global Context Enhanced Graph Neural Network for Session-based Recommendation\".",
    "stars": 129,
    "forks": 31,
    "language": "Python",
    "url": "https://github.com/CCIIPLab/GCE-GNN",
    "topics": [],
    "created_at": "2020-11-12T06:39:07Z",
    "updated_at": "2025-10-27T08:59:06Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# GCE-GNN\r\n\r\n## Code\r\n\r\nThis is the source code for SIGIR 2020 Paper: _Global Context Enhanced Graph Neural Networks for Session-based Recommendation_.\r\n\r\n## Requirements\r\n\r\n- Python 3\r\n- PyTorch >= 1.3.0\r\n- tqdm\r\n\r\n## Usage\r\n\r\nData preprocessing:\r\n\r\nThe code for data preprocessing can refer to [SR-GNN](https://github.com/CRIPAC-DIG/SR-GNN).\r\n\r\nTrain and evaluate the model:\r\n~~~~\r\npython build_graph.py --dataset diginetica --sample_num 12\r\npython main.py --dataset diginetica\r\n~~~~\r\n\r\n## Citation\r\n\r\n~~~~\r\n@inproceedings{wang2020global,\r\n    title={Global Context Enhanced Graph Neural Networks for Session-based Recommendation},\r\n    author={Wang, Ziyang and Wei, Wei and Cong, Gao and Li, Xiao-Li and Mao, Xian-Ling and Qiu, Minghui},\r\n    booktitle={Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},\r\n    pages={169--178},\r\n    year={2020}\r\n}\r\n~~~~",
    "readme_length": 919
  },
  {
    "name": "FEQE",
    "full_name": "thangvubk/FEQE",
    "description": "Official code (Tensorflow) for paper \"Fast and Efficient Image Quality Enhancement via Desubpixel Convolutional Neural Networks\"",
    "stars": 129,
    "forks": 19,
    "language": "Python",
    "url": "https://github.com/thangvubk/FEQE",
    "topics": [
      "deep-convolutional-networks",
      "image-enhancement",
      "image-super-resolution",
      "mobile-devices"
    ],
    "created_at": "2018-07-21T04:44:41Z",
    "updated_at": "2025-09-16T11:01:56Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# FEQE\nOfficial implementation for [Fast and Efficient Image Quality Enhancement via Desubpixel Convolutional Neural Networks](http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Vu_Fast_and_Efficient_Image_Quality_Enhancement_via_Desubpixel_Convolutional_Neural_ECCVW_2018_paper.pdf), ECCV workshop 2018\n## Citation\nPlease cite our project if it is helpful for your research\n```\n@InProceedings{Vu_2018_ECCV_Workshops},\nauthor = {Vu, Thang and Van Nguyen, Cao and Pham, Trung X. and Luu, Tung M. and Yoo, Chang D.},\ntitle = {Fast and Efficient Image Quality Enhancement via Desubpixel Convolutional Neural Networks},\nbooktitle = {The European Conference on Computer Vision (ECCV) Workshops},\nmonth = {September},\nyear = {2018}\n}\n```\n\n<p align=\"center\">\n    <img src=\"https://github.com/thangvubk/FEQE/blob/master/docs/P_results.PNG\">\n</p> \n<p align=\"center\">\n    Comparison of proposed FEQE with other state-of-the-art super-resolution and enhancement methods\n</p>\n<p align=\"center\">\n    <img src=\"https://github.com/thangvubk/FEQE/blob/master/docs/net.PNG\">\n</p> \n<p align=\"center\">\n    Network architecture\n</p>\n<p align=\"center\">\n    <img src=\"https://github.com/thangvubk/FEQE/blob/master/docs/sub-des.PNG\">\n</p> \n<p align=\"center\">\n    Proposed desubpixel\n</p>\n\n\n\n## PIRM 2018 challenge results (super-resolution on mobile devices task)\n\n<p align=\"center\">\n    <img src=\"https://github.com/thangvubk/FEQE/blob/master/docs/PIRM.PNG\">\n</p> \n<p align=\"center\">\n    TEAM_ALEX placed the first in overall benchmark score. Refer to <a href=\"http://ai-benchmark.com/challenge.html\">PIRM 2018</a> for details.\n</p>\n\n## Dependencies\n- 1 Nvidia GPU (4h training on Titan Xp)\n- ``Python3``\n- ``tensorflow 1.10+``\n- ``tensorlayer 1.9+``\n- ``tensorboardX 1.4+``\n\n## Download datasets, models, and results\n### Dataset\n- Train: DIV2K (800 2K-resolution images)\n- Valid: DIV2K (9 val images)\n- Test: Set5, Set14, B100, Urban100\n- Download [train+val+test](https://drive.google.com/file/d/1dyL6KxaBI8Aq7E3AnuIK-RODkqXUAfcF/view?usp=sharing) datasets\n- Download [test only](https://drive.google.com/file/d/1bch29fFj5t7IwoNjceuK8lFM6-ivwrP5/view?usp=sharing) dataset\n    \n### Pretrained models\n- Download [pretrained models](https://drive.google.com/file/d/1ok7-Y0Ldbyi9Ii0Cm3wTzMx8vPvt6zIR/view?usp=sharing) including 1 PSNR-optimized model and 1 perception-optimized model\n- Download [pretrained VGG](https://drive.google.com/file/d/1KLZOwxW0KpQxRwwUepVYEi147UG9IRIx/view?usp=sharing) used for VGG loss\n    \n### Paper results\n- Download [paper results](https://drive.google.com/file/d/1KMpp_6Rp4XmRCQxdRIRpC1XdBLS4WrcS/view?usp=sharing) (images) of the test datasets\n\n## Project layout (recommended)\n```\nFEQE/\nâ”œâ”€â”€ checkpoint\nâ”‚Â Â  â”œâ”€â”€ FEQE\nâ”‚Â Â  â””â”€â”€ FEQE-P\nâ”œâ”€â”€ data\nâ”‚Â Â  â”œâ”€â”€ DIV2K_train_HR\nâ”‚Â Â  â”œâ”€â”€ DIV2K_valid_HR_9\nâ”‚Â Â  â””â”€â”€ test_benchmark\nâ”œâ”€â”€ docs\nâ”œâ”€â”€ model\nâ”œâ”€â”€ results\nâ””â”€â”€ vgg_pretrained\n    â””â”€â”€ imagenet-vgg-verydeep-19.mat\n```\n## Quick start\n1. Download [test only](https://drive.google.com/file/d/1bch29fFj5t7IwoNjceuK8lFM6-ivwrP5/view?usp=sharing) dataset dataset and put into ``data/`` directory\n2. Download [pretrained models](https://drive.google.com/file/d/1ok7-Y0Ldbyi9Ii0Cm3wTzMx8vPvt6zIR/view?usp=sharing) and put into ``checkpoint/`` directory\n3. Run ``python test.py --dataset <DATASET_NAME>``\n4. Results will be saved into ``results/`` directory\n\n## Training\n1. Download [train+val+test](https://drive.google.com/file/d/1dyL6KxaBI8Aq7E3AnuIK-RODkqXUAfcF/view?usp=sharing) datasets dataset and put into ``data/`` directory\n2. Download [pretrained VGG](https://drive.google.com/file/d/1KLZOwxW0KpQxRwwUepVYEi147UG9IRIx/view?usp=sharing) and put into ``vgg_pretrained/`` directory\n3. Pretrain with MSE loss on scale 2: ``python train.py --checkpoint checkpoint/mse_s2 --alpha_vgg 0 --scale 2 --phase pretrain``\n4. Finetune with MSE loss on scale 4 (FEQE-P): ``python main.py --checkpoint checkpoint/mse_s4 --alpha_vgg 0 --pretrained_model checkpoint_test/mse_s2/model.ckpt``\n5. Finetune with full loss on scale 4: ``python main.py --checkpoint checkpoint/full_s4 ---pretrained_model checkpoint_test/mse_s4/model.ckpt``\n6. All Models with be saved into ``checkpoint/`` direcory\n\n## Visualization\n1. Start tensorboard: ``tensorboard --logdir checkpoint``\n2. Enter: ``YOUR_IP:6006`` to your web browser.\n3. Result ranges should be similar to:\n\n![Tensorboard](https://github.com/thangvubk/FEQE/blob/master/docs/tensorboard.gif)\n\n## Comprehensive testing\n1. Test FEQE model (defaults): follow [Quick start](#quick-start)\n2. Test FEQE-P model: ``python test.py --dataset <DATASET> --model_path <FEQE-P path>``\n3. Test perceptual quality: refer to [PIRM validation code](https://github.com/roimehrez/PIRM2018)\n\n## Quantitative and Qualitative results\n<p align=\"center\">\n    <img src=\"https://github.com/thangvubk/FEQE/blob/master/docs/quan.PNG\">\n</p> \n<p align=\"center\">\n    PSNR/SSIM/Perceptual-Index comparison. Red indicates the best results\n</p>\n\n<p align=\"center\">\n    <img src=\"https://github.com/thangvubk/FEQE/blob/master/docs/time.PNG\">\n</p> \n<p align=\"center\">\n    Running time comparison. Red indicates the best results\n</p>\n\n<p align=\"center\">\n    <img src=\"https://github.com/thangvubk/FEQE/blob/master/docs/qual.PNG\">\n</p> \n<p align=\"center\">\n    Qualitative comparison\n</p>\n",
    "readme_length": 5285
  },
  {
    "name": "PINNs-based-MPC",
    "full_name": "Jonas-Nicodemus/PINNs-based-MPC",
    "description": "We discuss nonlinear model predictive control (NMPC) for multi-body dynamics via physics-informed machine learning methods. Physics-informed neural networks (PINNs) are a promising tool to approximate (partial) differential equations. PINNs are not suited for control tasks in their original form since they are not designed to handle variable control actions or variable initial values. We thus present the idea of enhancing PINNs by adding control actions and initial conditions as additional network inputs. The high-dimensional input space is subsequently reduced via a sampling strategy and a zero-hold assumption. This strategy enables the controller design based on a PINN as an approximation of the underlying system dynamics. The additional benefit is that the sensitivities are easily computed via automatic differentiation, thus leading to efficient gradient-based algorithms. Finally, we present our results using our PINN-based MPC to solve a tracking problem for a complex mechanical system, a multi-link manipulator.",
    "stars": 129,
    "forks": 30,
    "language": "Python",
    "url": "https://github.com/Jonas-Nicodemus/PINNs-based-MPC",
    "topics": [],
    "created_at": "2021-09-22T07:05:40Z",
    "updated_at": "2025-11-17T10:15:33Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "<!-- PROJECT SHIELDS -->\n[![arXiv][arxiv-shield]][arxiv-url]\n[![DOI][doi-shield]][doi-url]\n[![MIT License][license-shield]][license-url]\n[![LinkedIn][linkedin-shield]][linkedin-url]\n\n# [Physics-Informed Neural Networks-based Model Predictive Control for Multi-link Manipulators](https://doi.org/10.1016/j.ifacol.2022.09.117)\nProspective contribution to the <a href=\"https://www.mathmod.at/\">MATHMOD 2022 Vienna</a> conference.\n\n<!-- TABLE OF CONTENTS -->\n<details open=\"open\">\n  <summary><h2 style=\"display: inline-block\">Table of Contents</h2></summary>\n  <ol>\n    <li>\n      <a href=\"#about-the-project\">About The Project</a>\n      <ul>\n        <li><a href=\"#citing\">Citing</a></li>\n        <li><a href=\"#built-with\">Built With</a></li>\n      </ul>\n    </li>\n    <li>\n      <a href=\"#getting-started\">Getting Started</a>\n      <ul>\n        <li><a href=\"#prerequisites\">Prerequisites</a></li>\n        <li><a href=\"#installation\">Installation</a></li>\n      </ul>\n    </li>\n    <li><a href=\"#usage\">Usage</a></li>\n    <li><a href=\"#license\">License</a></li>\n    <li><a href=\"#contact\">Contact</a></li>\n  </ol>\n</details>\n\n<!-- ABOUT THE PROJECT -->\n## About The Project\n\nWe discuss nonlinear model predictive control (NMPC) for multi-body dynamics via physics-informed machine learning methods. Physics-informed neural networks (PINNs) are a promising tool to approximate (partial) differential equations. PINNs are not suited for control tasks in their original form since they are not designed to handle variable control actions or variable initial values. We thus present the idea of enhancing PINNs by adding control actions and initial conditions as additional network inputs. The high-dimensional input space is subsequently reduced via a sampling strategy and a zero-hold assumption. This strategy enables the controller design based on a PINN as an approximation of the underlying system dynamics. The additional benefit is that the sensitivities are easily computed via automatic differentiation, thus leading to efficient gradient-based algorithms. Finally, we present our results using our PINN-based MPC to solve a tracking problem for a complex mechanical system, a multi-link manipulator.\n\n<!-- For more information, please refer to the following: doi -->\n\n### Citing\nIf you use this project for academic work, please consider citing our\n[publication](https://doi.org/10.1016/j.ifacol.2022.09.117):\n\n    Jonas Nicodemus, Jonas Kneifl, JÃ¶rg Fehr, Benjamin Unger,\n    Physics-informed Neural Networks-based Model Predictive Control for Multi-link Manipulators,\n    IFAC-PapersOnLine, Volume 55, Issue 20, 2022.\n\n### Built With\n\n* [TensorFlow](https://www.tensorflow.org/)\n* [Python](https://www.python.org/)\n\n<!-- GETTING STARTED -->\n## Getting Started\n\nTo get a local copy up and running follow these simple steps.\n\n### Prerequisites\n\nA python environment is required, we recommend using a virtual environment.\n\n### Installation\n\n1. Clone the repo\n   ```sh\n   git clone git@github.com:Jonas-Nicodemus/PINNs-based-MPC.git\n   ```\n2. Go into the directory\n   ```sh\n   cd PINNs-based-MPC\n   ```\n3. Install dependencies\n   ```sh\n   pip install -r requirements.txt\n   ```\n\n<!-- USAGE EXAMPLES -->\n## Usage\n\nThere are two executable scripts located in `src`.\n* `train_pinn.py` can be executed to learn weights and overwrite the already existing ones,\n   which can be found under `resources/weights`.\n* `main.py`, then evaluates the PINN first in self-loop prediction mode and subsequently in\n  closed-loop mode connected to the real system (emulated by RK45) for the given reference trajectory.\n\n\n<!-- LICENSE -->\n## License\n\nDistributed under the MIT License. See `LICENSE` for more information.\n\n<!-- CONTACT -->\n## Contact\n\nJonas Nicodemus - jonas.nicodemus@simtech.uni-stuttgart.de\n\nBenjamin Unger - benjamin.unger@simtech.uni-stuttgart.de\n\nJonas Kneifl - jonas.kneifl@itm.uni-stuttgart.de\n\nJÃ¶rg Fehr - joerg.fehr@itm.uni-stuttgart.de\n\nProject Link: [https://github.com/Jonas-Nicodemus/PINNs-based-MPC](https://github.com/Jonas-Nicodemus/PINNs-based-MPC)\n\n[license-shield]: https://img.shields.io/github/license/Jonas-Nicodemus/PINNs-based-MPC.svg?style=for-the-badge\n[license-url]: https://github.com/Jonas-Nicodemus/PINNs-based-MPC/blob/main/LICENSE\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555\n[linkedin-url]: https://linkedin.com/in/jonas-nicodemus-a34931209/\n[doi-shield]: https://img.shields.io/badge/DOI-10.5281%20%2F%20zenodo.5520662-blue.svg?style=for-the-badge\n[doi-url]: https://zenodo.org/badge/latestdoi/409099116\n[arxiv-shield]: https://img.shields.io/badge/arXiv-2109.10793-b31b1b.svg?style=for-the-badge\n[arxiv-url]: https://arxiv.org/abs/2109.10793\n",
    "readme_length": 4744
  },
  {
    "name": "qtransformer",
    "full_name": "rdisipio/qtransformer",
    "description": "Quantum-enhanced transformer neural network",
    "stars": 127,
    "forks": 30,
    "language": "Python",
    "url": "https://github.com/rdisipio/qtransformer",
    "topics": [],
    "created_at": "2020-12-28T23:43:02Z",
    "updated_at": "2025-11-25T03:54:07Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# Example of a Quantum-enhanced transformer neural network\n\nGet the code:\n\n```\ngit clone https://github.com/rdisipio/qtransformer.git\ncd qtransformer\n```\n\nCreate a Python 3.x virtual environment and install libraries:\n```\npython3 -m venv qml\nsource qml/bin/activate\npip3 install --upgrade pip\npip3 install -r requirements.txt\n```\n\n",
    "readme_length": 331
  },
  {
    "name": "BankTextCategorizer",
    "full_name": "j-convey/BankTextCategorizer",
    "description": "Automated Categorization: Utilizing the power of neural networks, this project offers an automated solution to categorize bank descriptions, reducing manual effort and enhancing efficiency while maintaining privacy.",
    "stars": 127,
    "forks": 27,
    "language": "Python",
    "url": "https://github.com/j-convey/BankTextCategorizer",
    "topics": [
      "bart",
      "machine-learning",
      "neural-networks"
    ],
    "created_at": "2023-08-16T23:02:37Z",
    "updated_at": "2025-11-20T15:34:05Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# Bank Transaction Categorization\nA simple and powerful tool that uses a neural network to categorize bank transaction descriptions into respective categories and subcategories.\n## ðŸ’¡ Motivation\nMy primary motivation for creating this project was to provide an alternative to existing solutions by emphasizing two critical values:\n\n1. Self-hosting: By allowing users to host the solution on their infrastructure, this project offers complete control over data, operations, and customization. Self-hosting eliminates the dependency on third-party services which might shut down, change their terms, or even introduce pricing that might not be favorable to all.\n2. Privacy-friendly: In an era where every click, every view, and even every scroll is tracked, privacy has become a scarce commodity. This project is designed with privacy at its core. No unnecessary data collection, no sneaky trackers, and no third-party analytics. Your data stays yours, and that's how it should be.\n\n## ðŸ“‹ Overview\nThis program is designed to help with the categorization of bank transaction descriptions. It leverages the power of the BERT model to recognize patterns in transaction descriptions and subsequently classify them into predefined categories and subcategories.\n\n## âœ¨ Features\n- Data preprocessing to clean and prepare the transaction descriptions for the neural network.\n- Utilizes the BERT model, a state-of-the-art NLP model, for accurate categorization.\n- Simultaneous categorization into main category and subcategory.\n- Easy to use interface with simple Python functions.\n\n## ðŸš€ How to Use\n### Setup & Installation\nMake sure you have all the necessary libraries installed. Most importantly, ensure that torch, transformers, and pandas are available.\n```bash\npip install torch transformers pandas\n```\n### Categories/Subcategories\nThis program comes with a standard set of categories and subcategories to categorize your bank transactions. Below is the default categorization schema:\n```python\ncategories = {\n    'Auto': ['Gas','Maintenance', 'Upgrades', 'Other_Auto'],\n    'Baby': ['Diapers', 'Formula', 'Clothes', 'Toys', 'Other_Baby'],\n    'Clothes': ['Clothes', 'Shoes', 'Jewelry', 'Bags_Accessories'],\n    'Entertainment': ['Sports_Outdoors', 'Movies_TV', 'DateNights', 'Arts_Crafts', 'Books', 'Games', 'Guns', 'E_Other'],\n    'Electronics': ['Accessories', 'Computer', 'TV', 'Camera', 'Phone','Tablet_Watch', 'Gaming', 'Electronics_misc'],\n    'Food': ['Groceries', 'FastFood_Restaurants'],\n    'Home': ['Maintenance', 'Furniture_Appliances', 'Hygiene', 'Gym',\n        'Home_Essentials', 'Kitchen', 'Decor', 'Security', 'Yard_Garden', 'Tools'],\n    'Medical': ['Health_Wellness'],\n    'Kids': ['K_Toys'],\n    'Personal_Care': ['Hair', 'Makeup_Nails', 'Beauty', 'Massage','Vitamins_Supplements', 'PC_Other'],\n    'Pets': ['Pet_Food', 'Pet_Toys', 'Pet_Med', 'Pet_Grooming', 'Pet_Other'],\n    'Subscriptions_Memberships': ['Entertainment', 'Gym', 'Sub_Other'],\n    'Travel': ['Hotels', 'Flights', 'Car_Rental', 'Activities']\n}\n\n```\n#### Flexibility in Categorization\nHowever, we understand that every user might have specific needs, and the default categories might not fit everyone. You have the flexibility to modify, add, or remove categories and subcategories as per your requirements.\n\n#### How to Customize:\nUpdate the Dictionary: Modify the categories dictionary in the code with your desired categories and subcategories. The key should be the main category, and the values should be a list containing the subcategories.\n\nUpdate Training Data: It's crucial that once you modify the categories and subcategories, you also need to change the training data. Ensure that the data has labels corresponding to your new categories and subcategories.\n\nRe-Train the Model: With the updated categories and training data, re-run the main() function to train the model on the new data.\n\nBy following these steps, you can easily customize the categorization to suit your personal or business needs.\n\n\n## ðŸ–¥ï¸ UI\n![image](https://github.com/j-convey/BankTextCategorizer/assets/85854964/6564f384-7181-4daa-a014-17c200f72090)\n![image](https://github.com/j-convey/BankTextCategorizer/assets/85854964/f05b291a-cc4b-418f-a328-937eb771da5a)\n![image](https://github.com/j-convey/BankTextCategorizer/assets/85854964/9b4a0499-37e2-4e70-846d-ea8dd75bfa26)\n\n\n## ðŸ—ï¸ Code Structure\n- Data Preprocessing: DataPreprocessor class is responsible for reading the CSV file, cleaning the data, tokenizing the sentences, and preparing the data for training.\n- BERT Model: BertModel class initializes the BERT model with the appropriate number of categories and subcategories.\n- Training: train_category_model function handles the training loop, including batching, forward and backward passes, and early stopping.\n\n## ðŸ“ˆ Performance\nHere are my results after using main.csv dataset (62,793 lines of data) for 2 epochs. This took around 10 hours to complete based on my hardware. This is without using data augmentation to double the size due to time restaints.\n![cat_modelV1](https://github.com/j-convey/BankTextCategorizer/assets/85854964/f457198d-4de0-4ef2-b7eb-3f30d6c14d58)\n\n\n## ðŸ”® Future Improvements\n- Adding Visualizers for viewing categorized data to Summary and Details tab.\n- Refining the data preprocessing pipeline.\n- Fine-tuning the BERT model for better performance.\n\n## ðŸ¤ Contribute\n1. Consider contributing to the project.\n2. Add a Github Star to the project.\n3. Post about the project on X.\n\n",
    "readme_length": 5475
  },
  {
    "name": "Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron",
    "full_name": "fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron",
    "description": "Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks",
    "stars": 113,
    "forks": 28,
    "language": "Python",
    "url": "https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron",
    "topics": [
      "deep-learning",
      "deep-neural-networks",
      "snn",
      "spiking-neural-networks"
    ],
    "created_at": "2020-11-09T07:52:03Z",
    "updated_at": "2025-11-20T02:27:27Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron\n\n[ä¸­æ–‡README](./README_cn.md)\n\nThis  repository contains the origin codes and TensorBoard logs for the paper *[Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks](https://arxiv.org/abs/2007.05785)*. The trained models are too large that we can't upload them to this repository. But we used a identical seed during training, and we can ensure that the user can get almost the same accuracy when using our codes to train.\n\n## Accuracy\n\nThis table shows the accuracy of using PLIF neurons, tau_0=2 and max pooling:\n\n|            | MNIST  | Fashion-MNIST | CIFAR10 | N-MNIST | CIFAR10-DVS | DVS128 Gesture |\n| ---------- | ------ | ------------- | ------- | ------- | ----------- | -------------- |\n| accuracy-A | 97.72% | 94.38%        | 93.50%  | 99.61%  | 74.80%      | 97.57%         |\n| accuracy-B | 99.63% | 93.85%        | 92.58%  | 99.57%  | 69.00%      | 96.53%         |\n\nThis table shows the accuracy-A of using PLIF/LIF neurons, different tau/tau_0 and average/max pooling:\n\n|               | pooling | MNIST  | Fashion-MNIST | CIFAR-10 | N-MNIST | CIFAR10-DVS | DVS128 Gesture |\n| ------------- | ------- | ------ | ------------- | -------- | ------- | ----------- | -------------- |\n| PLIF,tau_0=2  | max     | 99.72% | 94.38%        | 93.5%    | 99.61%  | 74.8%       | 97.57%         |\n| PLIF,tau_0=16 | max     | 99.73% | 94.65%        | 93.23%   | 99.53%  | 70.5%       | 92.01%         |\n| LIF,tau=2     | max     | 99.69% | 94.17%        | 93.03%   | 99.64%  | 73.6%       | 96.88%         |\n| LIF,tau=16    | max     | 99.49% | 94.47%        | 47.5%    | 99.15%  | 62.4%       | 76.74%         |\n| PLIF,tau_0=2  | avg     | 99.71% | 94.74%        | 93.3%    | 99.66%  | 72.7%       | 97.22%         |\n\n## Directory Structure\n\n`codes` contains the origin codes:\n\n`models.py` defines the networks.\n\n`train.py` trains models on the training set, tests on the test set alternately, and records the maximum test accuracy, which is the accuracy-A in the paper.\n\n`train_val.py` splits the origin training set into a new training set and validation set, trains on the new training set, tests on the validation set alternately, and records the test accuracy on the test set only once, with the model achieving the maximum validation accuracy, which is the accuracy-B in the paper.\n\n`logs` contains `A` and `B` directories, which contains TensorBoard logs for different accuracies, respectively.\n\n## Dependency\n\nThe origin codes uses the old version SpikingJelly. To maximize reproducibility, the user can download the latest SpikingJelly and rollback to the version that we used to train:\n\n\n```bash\ngit clone https://github.com/fangwei123456/spikingjelly.git\ncd spikingjelly\ngit reset --hard 73f94ab983d0167623015537f7d4460b064cfca1\npython setup.py install\n```\n\nHere is the commit information:\n\n```bash\ncommit 73f94ab983d0167623015537f7d4460b064cfca1\nAuthor: fangwei123456 <fangwei123456@pku.edu.cn>\nDate:   Wed Sep 30 16:42:25 2020 +0800\n\n    å¢žåŠ detach resetçš„é€‰é¡¹\n```\n\n\n## Datasets\n\nThe line 64 of `train.py`, and line 84 of `train_val.py` defines the dataset path:\n\n`dataset_dir = '/userhome/datasets/' + dataset_name`\n\nwhere `/userhome/datasets/` is the root path of all datasets.\n\nThe root path of all datasets should have the following directory structure:\n\n```\n|-- CIFAR10\n|   |-- cifar-10-batches-py\n|   `-- cifar-10-python.tar.gz\n|-- CIFAR10DVS\n|   |-- airplane.zip\n|   |-- automobile.zip\n|   |-- bird.zip\n|   |-- cat.zip\n|   |-- deer.zip\n|   |-- dog.zip\n|   |-- events\n|   |-- frames_num_20_split_by_number_normalization_None\n|   |-- frog.zip\n|   |-- horse.zip\n|   |-- ship.zip\n|   `-- truck.zip\n|-- DVS128Gesture\n|   |-- DvsGesture.tar.gz\n|   |-- LICENSE.txt\n|   |-- README.txt\n|   |-- events_npy\n|   |-- extracted\n|   |-- frames_num_20_split_by_number_normalization_None\n|   `-- gesture_mapping.csv\n|-- FashionMNIST\n|   |-- FashionMNIST\n|-- MNIST\n|   `-- MNIST\n`-- NMNIST\n    |-- Test.zip\n    |-- Train.zip\n    |-- events\n    `-- frames_num_10_split_by_number_normalization_None\n```\n\nMNIST, Fashion-MNIST and CIFAR10 dataset can be available from [torchvision](https://github.com/pytorch/vision). For neuromorphic datasets' installation, see\n\nhttps://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.4/spikingjelly.datasets.html\n\n## Running codes\n\nHere are the origin running codes for accuracy-B:\n\n| Dataset        | Running codes                                                |\n| -------------- | ------------------------------------------------------------ |\n| MNIST          | python ./codes/train_val.py -init_tau 2.0 -use_max_pool -use_plif -device cuda:0 -dataset_name MNIST -log_dir_prefix /userhome/plif_test/logsd -T 8 -max_epoch 1024 -detach_reset |\n| Fashion-MNIST  | python ./codes/train_val.py -init_tau 2.0 -use_max_pool -use_plif -device cuda:0 -dataset_name FashionMNIST -log_dir_prefix /userhome/plif_test/logsd -T 8 -max_epoch 1024 -detach_reset |\n| CIFAR10        | python ./codes/train_val.py -init_tau 2.0 -use_max_pool -use_plif -device cuda:0 -dataset_name CIFAR10 -log_dir_prefix /userhome/plif_test/logsd -T 8 -max_epoch 1024 -detach_reset |\n| N-MNIST        | python ./codes/train_val.py -init_tau 2.0 -use_max_pool -device cuda:0 -dataset_name NMNIST -log_dir_prefix /userhome/plif_test/logsd -T 10 -max_epoch 1024 -detach_reset -channels 128 -number_layer 2 -split_by number -normalization None -use_plif |\n| CIFAR10-DVS    | python ./codes/train_val.py -init_tau 2.0 -use_max_pool -device cuda:0 -dataset_name CIFAR10DVS -log_dir_prefix /userhome/plif_test/logsd -T 20 -max_epoch 1024 -detach_reset -channels 128 -number_layer 4 -split_by number -normalization None -use_plif |\n| DVS128 Gesture | python ./codes/train_val.py -init_tau 2.0 -use_max_pool -device cuda:0 -dataset_name DVS128Gesture -log_dir_prefix /userhome/plif_test/logsd -T 20 -max_epoch 1024 -detach_reset -channels 128 -number_layer 5 -split_by number -normalization None -use_plif |\n\nThe code can recovery training from the interruption. It will load the exist model and continue training from the last epoch.\n\n## Arguments Definition\n\nThis table shows the definition of all arguments:\n\n| argument        | meaning                                                      | type                                                         | default |\n| --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------- |\n| init_tau        | tau of all LIF neurons, or tau_0 of PLIF neurons             | float                                                        | -       |\n| batch_size      | training batch size                                          | int                                                          | 16      |\n| learning_rate   | learning rate                                                | float                                                        | 1e-3    |\n| T_max           | period of the learning rate schedule                         | int                                                          | 64      |\n| use_plif        | use PLIF neurons                                             | action='store_true'                                          | False   |\n| alpha_learnable | if given, `alpha` in the surrogate function is learnable     | action='store_true'                                          | False   |\n| use_max_pool    | if given, the network will use max pooling, else use average pooling | action='store_true'                                          | False   |\n| device          | use which device to train                                    | str                                                          | -       |\n| dataset_name    | use which dataset                                            | str(`MNIST`,`FashionMNIST`,`CIFAR10`,`NMNIST`,`CIFAR10DVS`or`DVSGesture`) | -       |\n| log_dir_prefix  | the path for TensorBoard to save logs                        | str                                                          | -       |\n| T               | simulating time-step                                         | int                                                          | -       |\n| channels        | the out channels of Conv2d for neuromorphic datasets         | int                                                          | -       |\n| number_layer    | the number of Conv2d layers for neuromorphic datasets        | int                                                          | -       |\n| split_by        | how to split the events to integrate them to frames          | str(`time` or`number` )                                      | -       |\n| normalization   | normalization for frames during being integrated             | str(`frequency`,`max`,`norm`,`sum` or`None`)                 | -       |\n| max_epoch       | maximum training epoch                                       | int                                                          | -       |\n| detach_reset    | whether detach the voltage reset during backward             | action='store_true'                                          | False   |\n\nFor more details about `split_by`å’Œ`normalization`, see\n\nhttps://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.4/spikingjelly.datasets.html#integrate-events-to-frames-init-en\n\n## New Implement\n\nSpkingJelly (0.0.0.0.12 or the latest version) has added the network with LIF/max-pooling as an example: \n\n0.0.0.0.12: https://spikingjelly.readthedocs.io/zh_CN/0.0.0.0.12/clock_driven_en/14_classify_dvsg.html\n\nlatest: https://spikingjelly.readthedocs.io/zh_CN/latest/activation_based_en/classify_dvsg.html\n\nThe codes are written by the new version of SpikingJelly, which are faster than codes in this repository. \n\nAll networks in this paper are available at SpikingJelly: \n\n0.0.0.0.12: https://github.com/fangwei123456/spikingjelly/blob/0.0.0.0.12/spikingjelly/clock_driven/model/parametric_lif_net.py\n\nlatest: https://github.com/fangwei123456/spikingjelly/blob/master/spikingjelly/activation_based/model/sew_resnet.py\n\n## Cite\n\n```\n@InProceedings{Fang_2021_ICCV,\n    author    = {Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Masquelier, Timothee and Huang, Tiejun and Tian, Yonghong},\n    title     = {Incorporating Learnable Membrane Time Constant To Enhance Learning of Spiking Neural Networks},\n    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n    month     = {October},\n    year      = {2021},\n    pages     = {2661-2671}\n}\n```\n",
    "readme_length": 10551
  },
  {
    "name": "gpinn",
    "full_name": "lu-group/gpinn",
    "description": "Gradient-enhanced physics-informed neural networks for forward and inverse PDE problems",
    "stars": 103,
    "forks": 18,
    "language": "Python",
    "url": "https://github.com/lu-group/gpinn",
    "topics": [],
    "created_at": "2021-08-19T05:52:47Z",
    "updated_at": "2025-11-25T05:07:23Z",
    "homepage": "https://doi.org/10.1016/j.cma.2022.114823",
    "license": "Apache License 2.0",
    "readme": "# gPINN: Gradient-enhanced physics-informed neural networks\n\nThe data and code for the paper [J. Yu, L. Lu, X. Meng, & G. E. Karniadakis. Gradient-enhanced physics-informed neural networks for forward and inverse PDE problems. *Computer Methods in Applied Mechanics and Engineering*, 393, 114823, 2022](https://doi.org/10.1016/j.cma.2022.114823).\n\n## Code\n\n- [Function approximation](src/function.py)\n- Forward PDE problems\n    - [Poisson equation in 1D](src/poisson_1d.py)\n    - [Diffusion-reaction equation](src/diffusion_reaction.py)\n    - [Poisson equation in 2D](src/poisson_2d.py)\n- Inverse PDEs problems\n    - Brinkman-Forchheimer model\n        - [Case 1](src/brinkman_forchheimer_1.py)\n        - [Case 2](src/brinkman_forchheimer_2.py)\n    - [Diffusion-reaction system](src/diffusion_reaction_inverse.py)\n- gPINN enhanced by RAR\n    - [Burgers' equation](src/burgers.py)\n    - [Allen-Cahn equation](src/allen_cahn.py)\n\n## Cite this work\n\nIf you use this data or code for academic research, you are encouraged to cite the following paper:\n\n```\n@article{yu2022gradient,\n  title   = {Gradient-enhanced physics-informed neural networks for forward and inverse {PDE} problems},\n  author  = {Yu, Jeremy and Lu, Lu and Meng, Xuhui and Karniadakis, George Em},\n  journal = {Computer Methods in Applied Mechanics and Engineering},\n  volume  = {393},\n  pages   = {114823},\n  year    = {2022},\n  doi     = {https://doi.org/10.1016/j.cma.2022.114823}\n}\n```\n\n## Questions\n\nTo get help on how to use the data or code, simply open an issue in the GitHub \"Issues\" section.\n",
    "readme_length": 1565
  },
  {
    "name": "Radar-multiple-perspective-object-detection",
    "full_name": "Xiangyu-Gao/Radar-multiple-perspective-object-detection",
    "description": "Codes and template data for paper \"RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object Recognition\"",
    "stars": 79,
    "forks": 24,
    "language": "Python",
    "url": "https://github.com/Xiangyu-Gao/Radar-multiple-perspective-object-detection",
    "topics": [],
    "created_at": "2022-04-12T00:53:21Z",
    "updated_at": "2025-11-26T07:09:43Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# Radar Multiple Perspective Object Detection\n\nAutomotive Radar Object Recognition in the Bird-eye View Using Range-Velocity-Angle (RVA) Heatmap Sequences\n\n<p align=\"center\"> <img src='docs/grap_abs.png' align=\"center\" height=\"300px\"> </p>\n\n> [**RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object Recognition**](https://arxiv.org/pdf/2011.08981.pdf),            \n> Xiangyu Gao, Guanbin Xing, Sumit Roy, and Hui Liu\n> *arXiv technical report* ([arXiv 2011.08981](https://arxiv.org/abs/2011.08981))  \n\n    @ARTICLE{9249018,  author={Gao, Xiangyu and Xing, Guanbin and Roy, Sumit and Liu, Hui},  \n        journal={IEEE Sensors Journal},   \n        title={RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object Recognition},   \n        year={2021},  volume={21},  number={4},  pages={5119-5132},  doi={10.1109/JSEN.2020.3036047}}\n\n## Important Updates\n***(Sep. 12, 2023) We updated the [instruction](./docs/UseUWCR.md) for converting the radar ADC data from [UWCR dataset](https://github.com/Xiangyu-Gao/Raw_ADC_radar_dataset_for_automotive_object_detection) to the training/testing data format used in this repo.***\n\n***(June 17, 2022) The input data for slice3d.py script has been changed to the raw ADC data now [slice_sample_data](https://drive.google.com/drive/folders/1TGW6BHi5EZsSCtTsJuwYIQVaIWjl8CLY?usp=sharing).***\n\n***(June 2, 2023) We provided an [instruction](./docs/UseUWCR.md) for converting the annotations from [UWCR dataset](https://github.com/Xiangyu-Gao/Raw_ADC_radar_dataset_for_automotive_object_detection) format to our format in [convert_annotations.py](./utils/convert_annotations.py).***\n\n## Abstract\nMillimeter-wave radars are being increasingly integrated into commercial vehicles to support new advanced driver-assistance systems by enabling robust and high-performance object detection, localization, as well as recognition - a key component of new environmental perception. In this paper, we propose a novel radar multiple-perspectives convolutional neural network (RAMP-CNN) that extracts the location and class of objects based on further processing of the range-velocity-angle (RVA) heatmap sequences. To bypass the complexity of 4D convolutional neural networks, we propose to combine several lower-dimension NN models within our RAMP-CNN model that nonetheless approach the performance upper bound with lower complexity. The extensive experiments show that the proposed RAMP-CNN model achieves better average recall and average precision than prior works in all testing scenarios. Besides, the RAMP-CNN model is validated to work robustly under the nighttime, which enables low-cost radars as a potential substitute for pure optical sensing under severe conditions.\n\n## Highlights\n<p align=\"center\"> <img src='docs/res.png' align=\"center\" height=\"230px\"> </p>\n\n## Use RAMP-CNN\n\nAll radar configurations and algorithm configurations are included in [config](config.py).\n\n### Software Requirement and Installation\n\nPython 3.6, pytorch-1.5.1 (please refer to [INSTALL](requirements.txt) to set up libraries.)\n\nLinux system (Preferred). *If using Windows, please update the Linux-format paths in scripts, e.g., '/' -> '\\\\'.*\n\n### Download Sample Data and Model\n1. From the below Google Drive link\n    ```\n    https://drive.google.com/drive/folders/1TGW6BHi5EZsSCtTsJuwYIQVaIWjl8CLY?usp=sharing\n    ```\n\n2. Decompress the downloaded files and relocate them as the following directory manners:\n    ```\n    './template_files/slice_sample_data'\n    './template_files/train_test_data'\n    './results/C3D-20200904-001923'\n    ```\n\n## 3D Slicing of Range-Velocity-Angle Data\nFor convenience, in the sample codes, we use the raw ADC data of each frame as input and perform the [Range, Velocity, and Angle FFT](https://github.com/Xiangyu-Gao/mmWave-radar-signal-processing-and-microDoppler-classification) during the process of slicing. Run the following codes for 3D slicing.\n    \n    python slice3d.py\n    \n\nThe slicing results are the RA slices, RV slices, and VA slices as shown in the below figure.\n<p align=\"center\"> <img src='docs/slice_viz.png' align=\"center\" height=\"230px\"> </p>\n\n## Train and Test\n1. Prepare the input data (RA, RV, and VA slices) and ground truth confidence map for training and testing. Note that the provided training and testing data is in the post-3D slicing format, so you can skip the last step if you use the provided data here:\n    ```\n    python prepare_data.py -m train -dd './data/'\n    python prepare_data.py -m test -dd './data/'\n    ```\n2. Run training:\n    ```\n    python train_dop.py -m C3D\n    ```\n    You will get training outputs as follows:\n    ```\n    No data augmentation\n    Number of sequences to train: 1\n    Training files length: 111\n    Window size: 16\n    Number of epoches: 100\n    Batch size: 3\n    Number of iterations in each epoch: 37\n    Cyclic learning rate\n    epoch 1, iter 1: loss: 8441.85839844 | load time: 0.0571 | backward time: 3.1147\n    epoch 1, iter 2: loss: 8551.98437500 | load time: 0.0509 | backward time: 2.9038\n    epoch 1, iter 3: loss: 8019.63525391 | load time: 0.0531 | backward time: 2.9171\n    epoch 1, iter 4: loss: 8376.16015625 | load time: 0.0518 | backward time: 2.9146\n    ...\n    ```\n3. Run testing:\n    ```\n    python test.py -m C3D -md C3D-20200904-001923\n    ```\n    You will get testing outputs as follows:\n    ```\n   rodnet_21_0000087601_000001.pkl\n   ['2019_05_28_pm2s012']\n   2019_05_28_pm2s012\n   Length of testing data: 111\n   loading time: 0.02\n   finished ra normalization\n   finished v normalization\n   Testing 2019_05_28_pm2s012/000000-000016... (0)\n   2019_05_28_pm2s012/0000000000.jpg inference finished in 0.6613 seconds.\n   processing time: 3.69\n   loading time: 0.02\n   finished ra normalization\n   finished v normalization\n   Testing 2019_05_28_pm2s012/000008-000024... (0)\n   2019_05_28_pm2s012/0000000008.jpg inference finished in 0.5039 seconds.\n    ...\n    ```\n4. Run evaluation:\n    ```\n    python evaluate.py -md C3D-20200904-001923\n    ```\n    You will get evaluation outputs as follows:\n    ```\n    true seq\n   ./results/C3D-20200904-001923/2019_05_28_pm2s012/rod_res.txt\n    Average Precision  (AP) @[ OLS=0.50:0.90 ] = 0.9126\n    Average Recall     (AR) @[ OLS=0.50:0.90 ] = 0.9653\n   pedestrian: 1913 dets, 1792 gts\n    Average Precision  (AP) @[ OLS=0.50:0.90 ] = 0.9126\n    Average Precision  (AP) @[ OLS=0.50      ] = 0.9713\n    Average Precision  (AP) @[ OLS=0.60      ] = 0.9713\n    Average Precision  (AP) @[ OLS=0.70      ] = 0.9489\n    Average Precision  (AP) @[ OLS=0.80      ] = 0.9062\n    Average Precision  (AP) @[ OLS=0.90      ] = 0.7053\n    Average Recall     (AR) @[ OLS=0.50:0.90 ] = 0.9653\n    Average Recall     (AR) @[ OLS=0.50      ] = 0.9994\n    Average Recall     (AR) @[ OLS=0.75      ] = 0.9821\n    ...\n    ```\n\n## Radar Data Augmentation\nRun the below codes to check the results of 3 proposed data augmentation algorithms: flip, range-translation, and angle-translation.\n\n    python data_aug.py\n\nThe below figure shows the performance of doing 10-bin range-translation (move upword), 25-degree angle-translation (move rightword), and angle flip on original RA images. You may use these codes to develop your radar data augmentation and even generate new data. \n<p align=\"center\"> <img src='docs/aug_viz.png' align=\"center\" height=\"230px\"> </p>\n\n## Use the UWCR Dataset\nIf you want to explore more radar raw data in the [UWCR dataset](https://github.com/Xiangyu-Gao/Raw_ADC_radar_dataset_for_automotive_object_detection), it is necessary to make the data/annotation format conversion since the sample data and labels used this repository have different structures from that in the UWCR dataset.\n\nPlease refer to the instruction [UseUWCR](./docs/UseUWCR.md) for the annotation format conversion and training/testing data conversion.\n\n## License\n\nRAMP-CNN is released under an MIT license (see [LICENSE](LICENSE)).\n\n## Acknowlegement\nThis project is not possible without multiple great open-source codebases and datasets. We list some notable examples below.  \n\n* [microDoppler](https://github.com/Xiangyu-Gao/mmWave-radar-signal-processing-and-microDoppler-classification)\n* [rodnet](https://github.com/yizhou-wang/RODNet)\n* [raw_ADC_radar_dataset_for_automotive_object_detection](https://github.com/Xiangyu-Gao/Raw_ADC_radar_dataset_for_automotive_object_detection)\n* [raw_2D_MIMO_radar_dataset_for_carry_object_detection](https://github.com/Xiangyu-Gao/Raw_2D_MIMO_radar_dataset_for_carry_object_detection)\n",
    "readme_length": 8510
  },
  {
    "name": "TSTNN",
    "full_name": "key2miao/TSTNN",
    "description": "transformer based neural network for speech enhancement in time domain",
    "stars": 75,
    "forks": 13,
    "language": "Python",
    "url": "https://github.com/key2miao/TSTNN",
    "topics": [],
    "created_at": "2021-02-04T21:32:43Z",
    "updated_at": "2025-11-11T15:30:24Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# TSTNN\nThis is an official PyTorch implementation of paper \"TSTNN: Two-Stage Transformer based Neural Network for Speech Enhancement in Time Domain\", which has been accepted by ICASSP 2021. More details will be showed soon!\n",
    "readme_length": 225
  },
  {
    "name": "GraphTR",
    "full_name": "lqfarmer/GraphTR",
    "description": "Graph Neural Network for Tag Ranking in Tag-enhanced Video Recommendationï¼ˆCIKM20ï¼‰",
    "stars": 70,
    "forks": 13,
    "language": "Python",
    "url": "https://github.com/lqfarmer/GraphTR",
    "topics": [],
    "created_at": "2020-08-06T02:42:43Z",
    "updated_at": "2024-01-04T16:49:06Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# GraphTR\n\nGraph Neural Network for Tag Ranking in Tag-enhanced Video Recommendationï¼ˆCIKM-2020)\nQi Liu*, Ruobing Xie*, Lei Chen, Shukai Liu, Ke Tu, Peng Cui, Bo Zhang and Leyu Lin.\n\n# Operating environment\n\npython 2.7.15 tensorflow 1.14.0\n\n# Train DFN model\n\nsh -x train.sh\n\n# CITE\n\nIf the codes help you, please cite the following paper:\n\nQi Liu*, Ruobing Xie*, Lei Chen, Shukai Liu, Ke Tu, Peng Cui, Bo Zhang and Leyu Lin. Graph Neural Network for Tag Ranking in Tag-enhanced Video Recommendation. CIKM 2020. (* indicates equal contribution).\n",
    "readme_length": 545
  },
  {
    "name": "PhysenNet",
    "full_name": "FeiWang0824/PhysenNet",
    "description": "Code for physics-enhanced deep neural network (PhysenNet).",
    "stars": 67,
    "forks": 13,
    "language": "Python",
    "url": "https://github.com/FeiWang0824/PhysenNet",
    "topics": [],
    "created_at": "2021-02-14T12:33:03Z",
    "updated_at": "2025-12-01T08:52:53Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# PhysenNet\n\nTensorflow implementation of paper: [Phase imaging with an untrained neural network.](https://www.nature.com/articles/s41377-020-0302-3) We provide the experiment data for a quick demo.\n\n## Citation\nIf you find this project useful, we would be grateful if you cite the **PhysenNet paperï¼š**\n\nFei Wang, Yaoming Bian, Haichao Wang, Meng Lyu, Giancarlo Pedrini, Wolfgang Osten, George Barbastathis and Guohai Situ. Phase imaging with an untrained neural network. *Light Sci Appl* **9**, 77 (2020).\n\n\n## Abstract\nMost of the neural networks proposed so far for computational imaging (CI) in optics employ a supervised training strategy, and thus need a large training set to optimize their weights and biases. Setting aside the requirements of environmental and system stability during many hours of data acquisition, in many practical applications, it is unlikely to be possible to obtain sufficient numbers of ground-truth images for training. Here, we propose to overcome this limitation by incorporating into a conventional deep neural network a complete physical model that represents the process of image formation. The most significant advantage of the resulting physics-enhanced deep neural network (PhysenNet) is that it can be used without training beforehand, thus eliminating the need for tens of thousands of labeled data. We take single-beam phase imaging as an example for demonstration. We experimentally show that one needs only to feed PhysenNet a single diffraction pattern of a phase object, and it can automatically optimize the network and eventually produce the object phase through the interplay between the neural network and the physical model. This opens up a new paradigm of neural network design, in which the concept of incorporating a physical model into a neural network can be generalized to solve many other CI problems.\n\n## Pipeline\n![avatar](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41377-020-0302-3/MediaObjects/41377_2020_302_Fig1_HTML.png?as=webp)\n\n## How to use\n**Step 1: Configuring required packages**\n\npython 3.6\n\ntensorflow 1.9.0\n\nmatplotlib 3.1.3\n\nnumpy 1.18.1\n\npillow 7.1.2\n\n**Step 2: Run main.py after download and extract the ZIP file.**\n\n## Results\n![avatar](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41377-020-0302-3/MediaObjects/41377_2020_302_Fig5_HTML.png?as=webp)\n\n## License\nFor academic and non-commercial use only.\n\n",
    "readme_length": 2452
  },
  {
    "name": "FeNNol",
    "full_name": "FeNNol-tools/FeNNol",
    "description": "Force-field-enhanced Neural Networks optimized library",
    "stars": 64,
    "forks": 9,
    "language": "Python",
    "url": "https://github.com/FeNNol-tools/FeNNol",
    "topics": [],
    "created_at": "2023-09-12T11:27:54Z",
    "updated_at": "2025-11-28T16:43:16Z",
    "homepage": "https://fennol-tools.github.io/FeNNol/",
    "license": "GNU Lesser General Public License v3.0",
    "readme": "\n[![PyPI - Version](https://img.shields.io/pypi/v/FeNNol?link=https%3A%2F%2Fpypi.org%2Fproject%2FFeNNol%2F)](https://pypi.org/project/FeNNol/)\n[![DOI:10.1063/5.0217688](https://zenodo.org/badge/DOI/10.1063/5.0217688.svg)](https://doi.org/10.1063/5.0217688) \n\n\n## FeNNol: Force-field-enhanced Neural Networks optimized library\nFeNNol is a library for building, training and running neural network potentials for molecular simulations. It is based on the JAX library and is designed to be fast and flexible.\n\nFeNNol's documentation is available [here](https://fennol-tools.github.io/FeNNol/) and the article describing the library at https://doi.org/10.1063/5.0217688\n\nActive Learning tutorial in this [Colab notebook](https://colab.research.google.com/drive/1Z3G_jVSF60_nbDdJwbgyLdJBHTYuQ5nL?usp=sharing)\n\n## Installation\n### From PyPI\n```bash\n# CPU version\npip install fennol\n\n# GPU version\npip install \"fennol[cuda]\"\n```\n\n### Latest version from Github repo\nYou can start with a fresh environment, for example using venv:\n```bash\npython -m venv fennol\nsource fennol/bin/activate\n```\n\nThe first step is to install jax (see details at: https://jax.readthedocs.io/en/latest/installation.html). For example, to install the latest version using pip:\n```bash\n# CPU version\npip install -U jax\n\n# GPU version\npip install -U \"jax[cuda12]\"\n```\n\nThen, you can clone the repo and install FeNNol using pip:\n```bash\ngit clone https://github.com/FeNNol-tools/FeNNol.git\ncd FeNNol\npip install .\n```\n\n### Optional dependencies\n- Some modules require e3nn-jax (https://github.com/e3nn/e3nn-jax) which can be installed with:\n```bash\npip install --upgrade e3nn-jax\n```\n- The provided training script requires pytorch (at least the cpu version) for dataloaders:\n```bash\npip install torch --index-url https://download.pytorch.org/whl/cpu\n```\n- For the Deep-HP interface, cffi and pycuda are required:\n```bash\npip install cffi pycuda\n```\n\n## Examples\nTo learn how to train a FeNNol model, you can check the examples in the [`examples/training`](https://github.com/fennol-tools/FeNNol/tree/main/examples/training) directory. The `README.md` file in that directory contains instructions on how to train a model on the aspirin revMD17 dataset.\n\nTo learn how to run molecular dynamics simulations with FeNNol models, you can check the examples in the [`examples/md`](https://github.com/fennol-tools/FeNNol/tree/main/examples/md) directory. The `README.md` file in that directory contains instructions on how to run simulations with the provided ANI-2x model.\n\n\n## Citation\n\nPlease cite this paper if you use the library.\n```\nT. PlÃ©, O. Adjoua, L. LagardÃ¨re and J-P. Piquemal. FeNNol: an Efficient and Flexible Library for Building Force-field-enhanced Neural Network Potentials. J. Chem. Phys. 161, 042502 (2024)\n```\n\n```\n@article{ple2024fennol,\n    author = {PlÃ©, Thomas and Adjoua, Olivier and LagardÃ¨re, Louis and Piquemal, Jean-Philip},\n    title = {FeNNol: An efficient and flexible library for building force-field-enhanced neural network potentials},\n    journal = {The Journal of Chemical Physics},\n    volume = {161},\n    number = {4},\n    pages = {042502},\n    year = {2024},\n    month = {07},\n    doi = {10.1063/5.0217688},\n    url = {https://doi.org/10.1063/5.0217688},\n}\n\n```\n\n## License\n\nThis project is licensed under the terms of the GNU LGPLv3 license. See [LICENSE](https://github.com/fennol-tools/FeNNol/blob/main/LICENSE) for additional details.\n",
    "readme_length": 3440
  },
  {
    "name": "FQSE",
    "full_name": "ssi-research/FQSE",
    "description": "Fully Quantized Neural Networks For Speech Enhancement",
    "stars": 63,
    "forks": 10,
    "language": "Python",
    "url": "https://github.com/ssi-research/FQSE",
    "topics": [],
    "created_at": "2023-05-21T09:17:43Z",
    "updated_at": "2025-09-16T09:07:36Z",
    "homepage": null,
    "license": "Apache License 2.0",
    "readme": "# Fully Quantized Speech Enhancement (FQSE)\n\n## Paper (InterSpeech 2023)\nhttps://www.isca-archive.org/interspeech_2023/cohen23_interspeech.html\n\n## Abstract\nDeep learning models have shown state-of-the-art results in speech enhancement. However, deploying such models on an eight-bit integer-only device is challenging. In this work, we analyze the gaps in deploying a vanilla quantization-aware training method for speech enhancement, revealing two significant observations. First, quantization mainly affects signals with a high input Signal-to-Noise Ratio (SNR). Second, quantizing the model's input and output shows major performance degradation. Based on our analysis, we propose Fully Quantized Speech Enhancement (FQSE), a new quantization-aware training method that closes these gaps and enables eight-bit integer-only quantization. FQSE introduces data augmentation to mitigate the quantization effect on high SNR. Additionally, we add an input splitter and a residual quantization block to the model to overcome the error of the input-output quantization. We show that FQSE closes the performance gaps induced by eight-bit quantization.\n\n![VideoBlocks](figures/FQSE_scheme.png)\n\n![VideoBlocks](figures/FQSE_ConvTasNet.png)\n\n## Librimix Dataset\n\nLibriMix is an open source dataset for source separation in noisy environments. It is derived from LibriSpeech signals (clean subset) and WHAM noise. It offers a free alternative to the WHAM dataset and complements it.\nIt will also enable cross-dataset experiments. Please refer to [Librimix](https://github.com/JorisCos/LibriMix) for more information.\n\n## Installation\n ```shell script\n pip install requirements.txt\n ```\n\n## Quantization Aware Training (QAT)\n\n1. Generate Librimix dataset according to [Librimix](https://github.com/JorisCos/LibriMix). Please use Libri2Mix 16kHz and 'min' version of the dataset.\n\n2. Download the pre-trained model:\n   https://huggingface.co/JorisCos/ConvTasNet_Libri1Mix_enhsingle_16k/blob/main/pytorch_model.bin\n\n3. Edit the configuration file (YML) `configs/convtasnet_16k_fqse.yml`.\n   - Set work_dir: `/home/user-name`\n   - Set dataset csv folders generated by step 1 as follows:\n     - dataset->train_dir: `/your-librimix-path/data/wav16k/min/train-360`\n     - dataset->valid_dir: `/your-librimix-path/data/wav16k/min/dev`\n   - Set pre-trained model path in training->pretrained: `pytorch_model.bin`\n   \n   Note: The `convtasnet_16k_fqse.yml` configuration is our QAT 8bit (FQSE) as in the paper. \n   \n5. Run `train.py`:\n    ```shell script\n    python train.py -y configs/convtasnet_16k_fqse.yml\n    ```\n   \n## Validation\n1. Edit the configuration file (YML) configs/convtasnet_16k.yml:\n   - Set dataset csv folder generated by step 1 as follows:\n     - dataset->test_dir: `/your-librimix-path/data/wav16k/min/test`\n   - Set the model path in model->model_path: `trained_model.pth`\n2. Run `val.py`:\n    ```shell script\n    python val.py -y configs/convtasnet_16k.yml\n    ```\n\n### SI-SNR benchmark on Librimix\n| Network            | Float | Vanilla QAT 8bit | FQSE 8bit |\n|--------------------|------:|-----------------:|----------:|\n| ConvTasNet [1]     | 14.74 |            14.42 |     14.77 |\n\n\n## Inference\nRun `infer.py` on noisy speech:\n ```shell script\n python infer.py -y configs/convtasnet_16k_fqse.yml -a samples/speech/test_1spk_noisy_1.wav\n ```\n   \n## Export\nRun `export.py`:\n ```shell script\n python export.py -y configs/convtasnet_16k_fqse.yml --torchscript --onnx\n ```\n\n****\n\n## Citation\n\nIf you find this project useful in your research, please consider cite:\n\n```BibTeX\n@inproceedings{cohen23_interspeech,\n  author={Elad Cohen and Hai Victor Habi and Arnon Netzer},\n  title={{Towards Fully Quantized Neural Networks For Speech Enhancement}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n  pages={181--185},\n  doi={10.21437/Interspeech.2023-883}\n}\n```\n****\n\n## Thanks\n\nhttps://github.com/asteroid-team/asteroid\n\n## References\n\n[1] https://arxiv.org/pdf/1809.07454.pdf\n\n\n",
    "readme_length": 3986
  },
  {
    "name": "ESRGCNN",
    "full_name": "hellloxiaotian/ESRGCNN",
    "description": "Image Super-resolution with An Enhanced Group Convolutional Neural Network (Neural Networks, 2022) ",
    "stars": 63,
    "forks": 6,
    "language": "Python",
    "url": "https://github.com/hellloxiaotian/ESRGCNN",
    "topics": [],
    "created_at": "2021-01-24T15:27:10Z",
    "updated_at": "2025-10-10T09:26:20Z",
    "homepage": "https://github.com/hellloxiaotian/ESRGCNN",
    "license": "N/A",
    "readme": "## Image Super-resolution with An Enhanced Group Convolutional Neural Network is proposed by Chunwei Tian, Yixuan Yuan, Shichao Zhang, Chia-Wen Lin, Wangmeng Zuo, David Zhang, 2021. It is implemented by Pytorch. And this work is obtained at https://arxiv.org/pdf/2205.14548. This paper is reported by the 52CV at https://mp.weixin.qq.com/s/Y4SOhhkx9OvCBYAURAIRsw and AIWalker at https://mp.weixin.qq.com/s/uK0S6pGynfhWyhe0Vx64XA. This paper is accepted by the Neural Networks (CCF B/SCI-IF:9.657) in 2022. \n\n## It is collected by the OCSC and invited to conduct an bechmark of super-resolution. \n\n## This paper uses group convolutions and residual operations to enhance deep and wide correlations of different channels to implement an efficient SR network.\n[![OSCS Status](https://www.oscs1024.com/platform/badge/hellloxiaotian/ESRGCNN.svg?size=small)](https://www.oscs1024.com/project/hellloxiaotian/ESRGCNN?ref=badge_small)\n\nhttps://user-images.githubusercontent.com/25679314/196603835-7a53e29b-4de2-4fec-bdf2-1fe4c5ba2239.mp4\n\n## Absract\n#### CNNs with strong learning abilities are widely chosen to resolve super-resolution problem. However, CNNs depend on deeper network architectures to improve performance of image super-resolution, which may increase computational cost in general. In this paper, we present an enhanced super-resolution group CNN (ESRGCNN) with a shallow architecture by fully fusing deep and wide channel features to extract more accurate low-frequency information in terms of correlations of different channels in single image super-resolution (SISR). Also, a signal enhancement operation in the ESRGCNN is useful to inherit more long-distance contextual information for resolving long-term dependency. An adaptive up-sampling operation is gathered into a CNN to obtain an image super-resolution model with low-resolution images of different sizes. Extensive experiments report that our ESRGCNN surpasses the state-of-the-arts in terms of SISR performance, complexity, execution speed, image quality evaluation and visual effect in SISR. Code is found at https://github.com/hellloxiaotian/ESRGCNN.\n\n## Requirements (Pytorch)  \n#### Pytorch 0.41\n\n#### Python 2.7\n\n#### torchvision\n\n#### torchsummary\n\n#### openCv for Python\n\n#### HDF5 for Python\n\n#### Numpy, Scipy\n\n#### Pillow, Scikit-image\n\n#### importlib\n\n## Commands\n### Training datasets\n\n#### The training dataset is downloaded at https://pan.baidu.com/s/1uqdUsVjnwM_6chh3n46CqQ ï¼ˆsecret codeï¼šauh1ï¼‰(baiduyun) or https://drive.google.com/file/d/1TNZeV0pkdPlYOJP1TdWvu5uEroH-EmP8/view (google drive)\n\n### Test datasets\n\n#### The test dataset of Set5 is downloaded at é“¾æŽ¥ï¼šhttps://pan.baidu.com/s/1YqoDHEb-03f-AhPIpEHDPQ (secret codeï¼šatwu) (baiduyun) or https://drive.google.com/file/d/1hlwSX0KSbj-V841eESlttoe9Ew7r-Iih/view?usp=sharing (google drive)\n\n#### The test dataset of Set14 is downloaded at é“¾æŽ¥ï¼šhttps://pan.baidu.com/s/1GnGD9elL0pxakS6XJmj4tA (secret codeï¼švsks) (baiduyun) or https://drive.google.com/file/d/1us_0sLBFxFZe92wzIN-r79QZ9LINrxPf/view?usp=sharing (google drive)\n\n#### The test dataset of B100 is downloaded at é“¾æŽ¥ï¼šhttps://pan.baidu.com/s/1GV99jmj2wrEEAQFHSi8jWw ï¼ˆsecret codeï¼šfhs2) (baiduyun) or https://drive.google.com/file/d/1G8FCPxPEVzaBcZ6B-w-7Mk8re2WwUZKl/view?usp=sharing (google drive)\n\n#### The test dataset of Urban100 is downloaded at é“¾æŽ¥ï¼šhttps://pan.baidu.com/s/15k55SkO6H6A7zHofgHk9fw (secret codeï¼š2hny) (baiduyun) or https://drive.google.com/file/d/1yArL2Wh79Hy2i7_YZ8y5mcdAkFTK5HOU/view?usp=sharing (google drive)\n\n### preprocessing\n\n### cd dataset\n\n### python div2h5.py\n\n### Training a model for different scales (also regarded as blind SR)\n\n#### python esrgcnn/train.py --patch_size 83 --batch_size 32 --max_steps 600000 --decay 400000 --model esrgcnn --ckpt_name esrgcnn --ckpt_dir checkpoint/esrgcnn --scale 0 --num_gpu 1\n\n### Using a model to test different scales of 2,3 and 4 (also regarded as blind SR)\n\n#### python tcw_sample_b.py --model esrgcnn --test_data_dir dataset/Set5 --scale 2 --ckpt_path checkpoint/esrgcnn.pth --sample_dir samples_singlemodel_urban100_x2\n\n#### python tcw_sample_b.py --model esrgcnn --test_data_dir dataset/Set5 --scale 3 --ckpt_path checkpoint/esrgcnn.pth --sample_dir samples_singlemodel_urban100_x3\n\n#### python tcw_sample_b.py --model esrgcnn --test_data_dir dataset/Set5 --scale 4 --ckpt_path checkpoint/esrgcnn.pth --sample_dir samples_singlemodel_urban100_x4\n\n\n### 1. Network architecture of ESRGCNN.\n![RUNOOB å›¾æ ‡](./images/1.png)\n\n### 2. A parallel upsampling operation for training a blind SR model.\n![RUNOOB å›¾æ ‡](./images/2.png)\n\n### 3. A upsampling operation for testing a blind SR model.\n![RUNOOB å›¾æ ‡](./images/3.png)\n\n### 4. ESRGCNN for x2, x3 and x4 on Set5.\n![RUNOOB å›¾æ ‡](./images/5.png)\n\n### 5. ESRGCNN for x2, x3 and x4 on Set14.\n![RUNOOB å›¾æ ‡](./images/6.png)\n\n### 6. ESRGCNN for x2, x3 and x4 on B100.\n![RUNOOB å›¾æ ‡](./images/7.png)\n\n### 7. ESRGCNN for x2, x3 and x4 on U100.\n![RUNOOB å›¾æ ‡](./images/8.png)\n\n### 8. ESRGCNN for x2 on B100\n\n![RUNOOB å›¾æ ‡](./images/9.png)\n\n### 9. Running time of different methods on hr images of size 256x256, 512x512 and 1024x1024 for x2.\n\n![RUNOOB å›¾æ ‡](./images/10.png)\n\n### 10. Complexities of different methods for x2.\n![RUNOOB å›¾æ ‡](./images/11.png)\n\n### 11. ESRGCNN for x2, x3 and x4 on B100 about FSIM\n\n![RUNOOB å›¾æ ‡](./images/12.png)\n\n### 12. Visual results of U100 for x3.\n\n![RUNOOB å›¾æ ‡](./images/13.png)\n\n### 13. Visual results of B100 for x2.\n![RUNOOB å›¾æ ‡](./images/14.png)\n\n### If you want to cite this paper, please refer to the following formats:\n#### 1. Tian C, Yuan Y, Zhang S, et al. Image Super-resolution with An Enhanced Group Convolutional Neural Network[J]. arXiv preprint arXiv:2205.14548, 2022.\n#### 2. @article{tian2022image,\n####    title={Image Super-resolution with An Enhanced Group Convolutional Neural Network},\n####    author={Tian, Chunwei and Yuan, Yixuan and Zhang, Shichao and Lin, Chia-Wen and Zuo, Wangmeng and Zhang, David},\n####    journal={arXiv preprint arXiv:2205.14548},\n####    year={2022}\n###     }\n",
    "readme_length": 6023
  },
  {
    "name": "BEM",
    "full_name": "BinCVER/BEM",
    "description": "Bayesian Neural Networks for One-to-Many Mapping in Image Enhancement",
    "stars": 62,
    "forks": 5,
    "language": "Python",
    "url": "https://github.com/BinCVER/BEM",
    "topics": [
      "image-enhancement",
      "low-light-image-enhancement",
      "underwater-image-enhancement"
    ],
    "created_at": "2024-10-22T01:21:02Z",
    "updated_at": "2025-11-15T06:46:05Z",
    "homepage": "https://arxiv.org/pdf/2501.14265",
    "license": "MIT License",
    "readme": "# Bayesian Enhancement Model\n\nThis is the official code for the paper [Bayesian Neural Networks for One-to-Many Mapping in Image Enhancement](https://arxiv.org/pdf/2501.14265)\n\n   \n\n## Demo: BEM's No-Reference Inference with CLIP\n![clip default](./assets/clip_default.gif) \n<!-- ![clip noise](./assets/clip_noise.gif) -->\n### Input Image\n<p align=\"center\">\n    <img src=\"assets/input_demo.png\" align=\"center\" width=\"100%\">\n</p>\n\n \n## News ðŸ’¡\n- **2025.11.08** Our paper has been accepted to AAAI 2026ðŸŽ‰ðŸŽ‰ðŸŽ‰.\n- **2025.03.24** We released the pretrained models for [NTIRE 2025 Low-light Image Enhancement Challenge](https://codalab.lisn.upsaclay.fr/competitions/21636). \n- **2025.03.04** The Code for BEM version 2 has been released. \n- **2024.10.31** Masked Image Modeling (MIM) is implemented to enhance our Stage-II network. We havenâ€™t evaluated its effectiveness, and our team has no future plan to inlcude MIM into the current paper or draw new papers for it. We welcome anyone interested in continuing this research and invite discussions.\n- **2024.10.31** The model checkpoints are released.  \n- **2024.10.21** Code has been released. We train each model multiple times and report the median of the test results. Therefore, the results you obtain may be higher than those reported in the paper. If you encounter any difficulties in reproducing our work, please issue them. We look forward to seeing future developments based on the Bayesian Enhancement Model âœ¨\n\n\n## HD Visulisation\n<p align=\"center\"> <img src='assets/vis_hd.png' align=\"center\" > </p>\n\n##  Bayesian Enhancement Model\n\n<p align=\"center\"> <img src='./assets/twostagev2.png' align=\"center\" > </p>\n\n\n\n\n## Checkpoints\n#### We released the pre-trained models [Here](https://github.com/Anonymous1563/Bayesian-Enhancement-Model/releases/tag/checkpoints_%26_visual_results)\n\n\n## Results\n#### We released our enhanced images for all the datasets used in the paper [Here](https://github.com/Anonymous1563/Bayesian-Enhancement-Model/releases/tag/checkpoints_%26_visual_results)\n<details close>\n<summary><b>Performance on LOL-v1, LOL-v2-real and LOL-v2-syn:</b></summary>\n\n![results1](./assets/result_lol.png)\n</details>\n\n<details close>\n<summary><b>Performance on LIME, NPE, MEF, DICM and VV:</b></summary>\n\n![results2](./assets/result_upaired5sets.png)\n</details>\n\n<details close>\n<summary><b>Performance on UIEB, C60, U50 and UCCS:</b></summary>\n\n![results3](./assets/result_uie.png)\n</details>\n\n\n<details close>\n<summary><b>Visulisation on LIME, NPE, MEF, DICM and VV:</b></summary>\n\n![results4](./assets/vis_5sets.png)\n</details>\n\n<details close>\n<summary><b>Visulisation on UIEB, U45 and C60:</b></summary>\n\n![results5](./assets/vis_uie.png)\n</details>\n\n\n\n## Dependencies and Installation\n\n- Python 3.10.12\n- Pytorch 1.13.1\n\n#### Create Conda Environment\n\n```bash\nconda create -n BEM python=3.10.12\nconda activate BEM\n```\n\n#### Clone Repo\n\n```bash\ngit clone https://github.com/BinCVER/Bayesian-Enhancement-Model.git\n```\n\n#### Install Dependencies\n\n```bash\ncd Bayesian-Enhancement-Model\npip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118\n\npip install -r requirements.txt\n\n```\n\n#### Install BasicSR\n\n```bash\npython setup.py develop --no_cuda_ext\n```\n\n#### Install 2D Selective Scan\n```bash\ncd kernels/selective_scan && pip install .\n```\n\n\n\n\n&nbsp;\n\n## Prepare Dataset\nDownload the LOLv1 and LOLv2 datasets from [here](https://daooshee.github.io/BMVC2018website/).\n\nDownload the LIME, NPE, MEF, DICM, and VV datasets from [here](https://daooshee.github.io/BMVC2018website/).\n\nDownload UIEB datasets from [here](https://github.com/Anonymous1563/Bayesian-Enhancement-Model/releases/tag/checkpoints_%26_visual_results).\n&nbsp;\n\n\n## Full-Reference Evaluation\n\n#### Low-Light Image Enhancement\n```shell\n# LOL-v1\npython3 Enhancement/eval.py --opt experiments/CG_UNet_LOLv1/CG_UNet_LOLv1.yml --weights experiments/CG_UNet_LOLv1/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_LOLv1/IE_UNet_LOLv1.yml --cond_weights experiments/IE_UNet_LOLv1/ckpt.pth \\\n--lpips --dataset LOLv1\n\n# LOL-v2-real\npython3 Enhancement/eval.py --opt experiments/CG_UNet_LOLv2Real/CG_UNet_LOLv2Real.yml --weights experiments/CG_UNet_LOLv2Real/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_LOLv2Real/IE_UNet_LOLv2Real.yml --cond_weights experiments/IE_UNet_LOLv2Real/ckpt.pth \\\n--lpips --dataset LOLv2Real\n\n# LOL-v2-syn\npython3 Enhancement/eval.py --opt experiments/CG_UNet_LOLv2Syn/CG_UNet_LOLv2Syn.yml --weights experiments/CG_UNet_LOLv2Syn/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_LOLv2Syn/IE_UNet_LOLv2Syn.yml --cond_weights experiments/IE_UNet_LOLv2Syn/ckpt.pth \\\n--lpips --dataset LOLv2Syn\n```\n\n- Evaluate using Groundtruth Mean\n```shell\n# LOL-v1\npython3 Enhancement/eval.py --opt experiments/CG_UNet_LOLv1/CG_UNet_LOLv1.yml --weights experiments/CG_UNet_LOLv1/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_LOLv1/IE_UNet_LOLv1.yml --cond_weights experiments/IE_UNet_LOLv1/ckpt.pth \\\n--lpips --dataset LOLv1 --GT_mean\n```\n\n- BEM's Deterministic Mode (BEM-DNN)\n```shell\n# LOL-v1\npython3 Enhancement/eval.py --opt experiments/CG_UNet_LOLv1/CG_UNet_LOLv1.yml --weights experiments/CG_UNet_LOLv1/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_LOLv1/IE_UNet_LOLv1.yml --cond_weights experiments/IE_UNet_LOLv1/ckpt.pth \\\n--lpips --dataset LOLv1 --GT_mean --deterministic\n```\n\n#### Underwater Image Enhancement\n```shell\n# UIEB\npython3 Enhancement/eval.py --opt experiments/CG_UNet_UIEB/CG_UNet_UIEB.yml --weights experiments/CG_UNet_UIEB/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_UIEB/IE_UNet_UIEB.yml --cond_weights experiments/IE_UNet_UIEB/ckpt.pth \\\n--lpips -dataset UIEB\n```\n\n## No-Reference Evaluation\n\n#### Low-Light Image Enhancement\n```shell\n# DICM with NIQE\npython3 Enhancement/eval.py --opt experiments/CG_UNet_LOLv2Syn/CG_UNet_LOLv2Syn.yml --weights experiments/CG_UNet_LOLv2Syn/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_LOLv2Syn/IE_UNet_LOLv2Syn.yml --cond_weights experiments/IE_UNet_LOLv2Syn/ckpt.pth \\\n--dataset DICM --input_dir datasets/DICM --no_ref  niqe\n\n# VV with CLIP-IQA\npython3 Enhancement/eval.py --opt experiments/CG_UNet_LOLv2Syn/CG_UNet_LOLv2Syn.yml --weights experiments/CG_UNet_LOLv2Syn/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_LOLv2Syn/IE_UNet_LOLv2Syn.yml --cond_weights experiments/IE_UNet_LOLv2Syn/ckpt.pth \\\n--dataset DICM --input_dir datasets/VV --no_ref  clip\n```\n\n\n#### Underwater Image Enhancement\n```shell\n\n# C60 with UIQM\npython3 Enhancement/eval.py --opt experiments/CG_UNet_UIEB/CG_UNet_UIEB.yml --weights experiments/CG_UNet_UIEB/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_UIEB/IE_UNet_UIEB.yml --cond_weights experiments/IE_UNet_UIEB/ckpt.pth \\\n--dataset DICM --input_dir datasets/C60 --no_ref uiqm_uciqe --uiqm_weight 1.0\n\n# C60 with UCIQE\npython3 Enhancement/eval.py --opt experiments/CG_UNet_UIEB/CG_UNet_UIEB.yml --weights experiments/CG_UNet_UIEB/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_UIEB/IE_UNet_UIEB.yml --cond_weights experiments/IE_UNet_UIEB/ckpt.pth \\\n--dataset DICM --input_dir datasets/C60 --no_ref uiqm_uciqe --uiqm_weight 0.0\n\n# U45 with UIQM\npython3 Enhancement/eval.py --opt experiments/CG_UNet_UIEB/CG_UNet_UIEB.yml --weights experiments/CG_UNet_UIEB/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_UIEB/IE_UNet_UIEB.yml --cond_weights experiments/IE_UNet_UIEB/ckpt.pth \\\n--dataset DICM --input_dir datasets/U45 --no_ref uiqm_uciqe --uiqm_weight 1.0\n\n# U45 with UCIQE\npython3 Enhancement/eval.py --opt experiments/CG_UNet_UIEB/CG_UNet_UIEB.yml --weights experiments/CG_UNet_UIEB/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_UIEB/IE_UNet_UIEB.yml --cond_weights experiments/IE_UNet_UIEB/ckpt.pth \\\n--dataset DICM --input_dir datasets/U45 --no_ref uiqm_uciqe --uiqm_weight 0.0\n\n# UCCS with UIQM\npython3 Enhancement/eval.py --opt experiments/CG_UNet_UIEB/CG_UNet_UIEB.yml --weights experiments/CG_UNet_UIEB/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_UIEB/IE_UNet_UIEB.yml --cond_weights experiments/IE_UNet_UIEB/ckpt.pth \\\n--dataset DICM --input_dir datasets/U45 --no_ref uiqm_uciqe --uiqm_weight 1.0\n\n# UCCS with UCIQE\npython3 Enhancement/eval.py --opt experiments/CG_UNet_UIEB/CG_UNet_UIEB.yml --weights experiments/CG_UNet_UIEB/ckpt.pth \\\n--cond_opt /experiments/IE_UNet_UIEB/IE_UNet_UIEB.yml --cond_weights experiments/IE_UNet_UIEB/ckpt.pth \\\n--dataset DICM --input_dir datasets/UCCS --no_ref uiqm_uciqe --uiqm_weight 0.0\n```\n\n## Training\n\n```shell\n\n# Stage-I on LOL-v1\npython3 basicsr/train.py --opt Options/CG_UNet_LOLv1.yml\n# Stage-II on LOL-v1\npython3 basicsr/train.py --opt Options/IE_UNet_LOLv1.yml\n\n\n# Stage-I on LOL-v2-real\npython3 basicsr/train.py --opt Options/CG_UNet_LOLv2Real.yml\n# Stage-II on LOL-v2-real\npython3 basicsr/train.py --opt Options/IE_UNet_LOLv2Real.yml\n\n\n# Stage-I on LOL-v2-syn\npython3 basicsr/train.py --opt Options/CG_UNet_LOLv2Syn.yml\n# Stage-II on LOL-v2-syn\npython3 basicsr/train.py --opt Options/IE_UNet_LOLv2Syn.yml\n\n\n# Stage-I on UIEB\npython3 basicsr/train.py --opt Options/CG_UNet_UIEB.yml\n# Stage-II on UIEB\npython3 basicsr/train.py --opt Options/IE_UNet_UIEB.yml\n\n```\n&nbsp;\n\n\n## Citation\n\n```shell\n@article{huang2026bayesian,\n  title={Bayesian Neural Networks for One-to-Many Mapping in Image Enhancement},\n  author={Huang, Guoxi and Yang, Qirui and Qi, Zipeng and Lin, RuiRui and Bull, David and Anantrasirichai, Nantheera},\n  journal={Proceedings of the AAAI Conference on Artificial Intelligence},\n  year={2026}\n}\n```\n\n\n",
    "readme_length": 9399
  },
  {
    "name": "simple-cnaps",
    "full_name": "plai-group/simple-cnaps",
    "description": "Source codes for \"Improved Few-Shot Visual Classification\" (CVPR 2020), \"Enhancing Few-Shot Image Classification with Unlabelled Examples\" (WACV 2022), and \"Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain, Active and Continual Few-Shot Learning\" (Neural Networks 2022 - in submission)",
    "stars": 58,
    "forks": 15,
    "language": "Python",
    "url": "https://github.com/plai-group/simple-cnaps",
    "topics": [
      "conditional-neural-process",
      "deep-learning",
      "few-shot-classifcation",
      "few-shot-learning",
      "few-shot-recognition",
      "meta-dataset",
      "meta-learning",
      "metric-learning",
      "mini-imagenet",
      "tiered-imagenet"
    ],
    "created_at": "2021-09-18T19:41:35Z",
    "updated_at": "2025-03-16T09:05:43Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# Improved Few-Shot Visual Classification\n\nThis repository contains source codes for the following papers and thesis:\n\n- [Improved Few-Shot Visual Classification](https://openaccess.thecvf.com/content_CVPR_2020/html/Bateni_Improved_Few-Shot_Visual_Classification_CVPR_2020_paper.html) \n\n  @ IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020\n  \n  ([Paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Bateni_Improved_Few-Shot_Visual_Classification_CVPR_2020_paper.html), [ArXiv](https://arxiv.org/abs/2006.12245), [PapersWithCode](https://paperswithcode.com/paper/improved-few-shot-visual-classification), [Video](https://www.youtube.com/watch?v=qE2cJrOi2J0))\n  \n- [Enhancing Few-Shot Image Classification with Unlabelled Examples](https://arxiv.org/abs/2006.12245) \n\n  @ IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2022\n  \n  ([Paper](https://openaccess.thecvf.com/content/WACV2022/html/Bateni_Enhancing_Few-Shot_Image_Classification_With_Unlabelled_Examples_WACV_2022_paper.html), [ArXiv](https://arxiv.org/abs/2006.12245), [PapersWithCode](https://paperswithcode.com/paper/improving-few-shot-visual-classification-with), [Video](https://www.youtube.com/watch?v=ht8hckNHfSY))\n  \n- [Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain, Active and Continual Few-Shot Learning](https://arxiv.org/abs/2201.05151) \n  \n  @ IEEE TPAMI Special Issue on Learning with Fewer Labels in Computer Vision, 2022 (in submission)\n  \n  ([ArXiv](https://arxiv.org/abs/2006.12245), [PapersWithCode](https://paperswithcode.com/paper/beyond-simple-meta-learning-multi-purpose))\n  \n- [On Label-Efficient Computer Vision: Building Fast and Effective Few-Shot Image Classifiers](https://open.library.ubc.ca/soa/cIRcle/collections/ubctheses/24/items/1.0402554) \n  \n  @ UBC cIRcle Thesis Archives, 2021\n  \n  ([Thesis](https://open.library.ubc.ca/soa/cIRcle/collections/ubctheses/24/items/1.0402554?o=0), [PapersWithCode](https://paperswithcode.com/paper/on-label-efficient-computer-vision-building), [Video](https://youtu.be/KJ7UVKpCddg))\n\nThe code base has been authored by Peyman Bateni, Jarred Barber, Raghav Goyal, Vaden Masrani, Dr. Jan-Willemn van de Meent, Dr. Leonid Sigal and Dr. Frank Wood. The source codes build on the original code base for CNAPS authored by Dr. John Bronskill, Jonathan Gordon, James Reqeima, Dr. Sebastian Nowozin, and Dr. Richard E. Turner. We would like to thank them for their help, support and early sharing of their work. To see the original CNAPS repository, visit https://github.com/cambridge-mlg/cnaps.\n\n## Simple CNAPS\n\nSimple CNAPS proposes the use of hierarchically regularized cluster means and covariance estimates within a Mahalanobis-distance based classifer for improved few-shot classification accuracy. This method incorporates said classifier within the same neural adaptive feature extractor as CNAPS. For more details, please refer to our paper on Simple CNAPS: [Improved Few-Shot Visual Classification](https://openaccess.thecvf.com/content_CVPR_2020/html/Bateni_Improved_Few-Shot_Visual_Classification_CVPR_2020_paper.html). The source code for this paper has been provided in the [simple-cnaps-src](https://github.com/plai-group/simple-cnaps/tree/master/simple-cnaps-src) directory. To reproduce our results, please refer to the README.md file within that folder.\n\nGlobal Meta-Dataset Rank (Simple CNAPS): https://github.com/google-research/meta-dataset#training-on-all-datasets\n\nGlobal Mini-ImageNet Rank (Simple CNAPS):\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improved-few-shot-visual-classification/few-shot-image-classification-on-mini-2)](https://paperswithcode.com/sota/few-shot-image-classification-on-mini-2?p=improved-few-shot-visual-classification)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improved-few-shot-visual-classification/few-shot-image-classification-on-mini-3)](https://paperswithcode.com/sota/few-shot-image-classification-on-mini-3?p=improved-few-shot-visual-classification)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improved-few-shot-visual-classification/few-shot-image-classification-on-mini-12)](https://paperswithcode.com/sota/few-shot-image-classification-on-mini-12?p=improved-few-shot-visual-classification)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improved-few-shot-visual-classification/few-shot-image-classification-on-mini-13)](https://paperswithcode.com/sota/few-shot-image-classification-on-mini-13?p=improved-few-shot-visual-classification)\n\nGlobal Tiered-ImageNet Rank (Simple CNAPS):\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improved-few-shot-visual-classification/few-shot-image-classification-on-tiered)](https://paperswithcode.com/sota/few-shot-image-classification-on-tiered?p=improved-few-shot-visual-classification)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improved-few-shot-visual-classification/few-shot-image-classification-on-tiered-1)](https://paperswithcode.com/sota/few-shot-image-classification-on-tiered-1?p=improved-few-shot-visual-classification)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improved-few-shot-visual-classification/few-shot-image-classification-on-tiered-2)](https://paperswithcode.com/sota/few-shot-image-classification-on-tiered-2?p=improved-few-shot-visual-classification)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improved-few-shot-visual-classification/few-shot-image-classification-on-tiered-3)](https://paperswithcode.com/sota/few-shot-image-classification-on-tiered-3?p=improved-few-shot-visual-classification)\n\n## Transductive CNAPS\nTransductive CNAPS extends the Simple CNAPS framework to the transductive few-shot learning setting where all query examples are provided at once. This method uses a two-step transductive task-encoder for adapting the feature extractor as well as a soft k-means cluster refinement procedure, resulting in better test-time accuracy. For additional details, please refer to our paper on Transductive CNAPS: [Enhancing Few-Shot Image Classification with Unlabelled Examples](https://arxiv.org/abs/2006.12245). The source code for this work is provided under the [transductive-cnaps-src](https://github.com/plai-group/simple-cnaps/tree/master/transductive-cnaps-src) directory. To reproduce our results, please refer to the README.md file within this folder.\n\nGlobal Meta-Dataset Rank (Transductive CNAPS): https://github.com/google-research/meta-dataset#training-on-all-datasets\n\nGlobal Mini-ImageNet Rank (Transductive CNAPS):\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improving-few-shot-visual-classification-with/few-shot-image-classification-on-mini-2)](https://paperswithcode.com/sota/few-shot-image-classification-on-mini-2?p=improving-few-shot-visual-classification-with)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improving-few-shot-visual-classification-with/few-shot-image-classification-on-mini-3)](https://paperswithcode.com/sota/few-shot-image-classification-on-mini-3?p=improving-few-shot-visual-classification-with)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improving-few-shot-visual-classification-with/few-shot-image-classification-on-mini-12)](https://paperswithcode.com/sota/few-shot-image-classification-on-mini-12?p=improving-few-shot-visual-classification-with)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improving-few-shot-visual-classification-with/few-shot-image-classification-on-mini-13)](https://paperswithcode.com/sota/few-shot-image-classification-on-mini-13?p=improving-few-shot-visual-classification-with)\n\nGlobal Tiered-ImageNet Rank (Transductive CNAPS):\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improving-few-shot-visual-classification-with/few-shot-image-classification-on-tiered)](https://paperswithcode.com/sota/few-shot-image-classification-on-tiered?p=improving-few-shot-visual-classification-with)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improving-few-shot-visual-classification-with/few-shot-image-classification-on-tiered-1)](https://paperswithcode.com/sota/few-shot-image-classification-on-tiered-1?p=improving-few-shot-visual-classification-with)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improving-few-shot-visual-classification-with/few-shot-image-classification-on-mini-12)](https://paperswithcode.com/sota/few-shot-image-classification-on-mini-12?p=improving-few-shot-visual-classification-with)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/improving-few-shot-visual-classification-with/few-shot-image-classification-on-mini-13)](https://paperswithcode.com/sota/few-shot-image-classification-on-mini-13?p=improving-few-shot-visual-classification-with)\n\n## Active and Continual Learning\n\nWe additionally evaluate both methods within the paradigms of \"out of the box\" active and continual learning. These settings were first proposed by Requeima et al., and studies how well few-shot classifiers, trained for few-shot learning, can be deployed for active and continual learning without any problem-specific finetuning or training. For additional details on our active and continual learning experiments and algorithms, please refer to our latest paper: [Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain, Active and Continual Few-Shot Learning](https://arxiv.org/abs/2201.05151). For code and instructions to reproduce the experiments reported, please refer to the [active-learning](https://github.com/plai-group/simple-cnaps/tree/master/active-learning) and [continual-learning](https://github.com/plai-group/simple-cnaps/tree/master/continual-learning) folders.\n\n## Meta-Dataset Results\n\n| Dataset                         | Simple CNAPS | Simple CNAPS | Transductive CNAPS | Transductive CNAPS |\n| ---                             | ---          | ---          | ---                | ---                |\n| ```--shuffle_dataset```         | False        | True         | False              | True               |\n| In-Domain Datasets              | ---          | ---          | ---                | ---                |\n| ILSVRC                          | 58.6Â±1.1     | 56.5Â±1.1     | 58.8Â±1.1           | 57.9Â±1.1           |\n| Omniglot                        | 91.7Â±0.6     | 91.9Â±0.6     | 93.9Â±0.4           | 94.3Â±0.4           |\n| Aircraft                        | 82.4Â±0.7     | 83.8Â±0.6     | 84.1Â±0.6           | 84.7Â±0.5           |\n| Birds                           | 74.9Â±0.8     | 76.1Â±0.9     | 76.8Â±0.8           | 78.8Â±0.7           |\n| Textures                        | 67.8Â±0.8     | 70.0Â±0.8     | 69.0Â±0.8           | 66.2Â±0.8           |\n| Quick Draw                      | 77.7Â±0.7     | 78.3Â±0.7     | 78.6Â±0.7           | 77.9Â±0.6           |\n| Fungi                           | 46.9Â±1.0     | 49.1Â±1.2     | 48.8Â±1.1           | 48.9Â±1.2           |\n| VGG Flower                      | 90.7Â±0.5     | 91.3Â±0.6     | 91.6Â±0.4           | 92.3Â±0.4           |\n| Out-of-Domain Datasets          | ---          | ---          | ---                | ---                |\n| Traffic Signs                   | 73.5Â±0.7     | 59.2Â±1.0     | 76.1Â±0.7           | 59.7Â±1.1           |\n| MSCOCO                          | 46.2Â±1.1     | 42.4Â±1.1     | 48.7Â±1.0           | 42.5Â±1.1           |\n| MNIST                           | 93.9Â±0.4     | 94.3Â±0.4     | 95.7Â±0.3           | 94.7Â±0.3           |\n| CIFAR10                         | 74.3Â±0.7     | 72.0Â±0.8     | 75.7Â±0.7           | 73.6Â±0.7           |\n| CIFAR100                        | 60.5Â±1.0     | 60.9Â±1.1     | 62.9Â±1.0           | 61.8Â±1.0           |\n| ---                             | ---          | ---          | ---                | ---                |\n| In-Domain Average Accuracy      | 73.8Â±0.8     | 74.6Â±0.8     | 75.2Â±0.8           | 75.1Â±0.8           |\n| Out-of-Domain Average Accuracy  | 69.7Â±0.8     | 65.8Â±0.8     | 71.8Â±0.8           | 66.5Â±0.8           |\n| Overall Average Accuracy        | 72.2Â±0.8     | 71.2Â±0.8     | 73.9Â±0.8           | 71.8Â±0.8           |\n\n## Mini-ImageNet Results\n\n| Setup                           | 5-way 1-shot | 5-way 5-shot    | 10-way 1-shot    | 10-way 5-shot    |\n| ---                             | ---          | ---             | ---              | ---              |\n| Simple CNAPS                    | 53.2Â±0.9     | 70.8Â±0.7        | 37.1Â±0.5         | 56.7Â±0.5         |\n| Transductive CNAPS              | 55.6Â±0.9     | 73.1Â±0.7        | 42.8Â±0.7         | 59.6Â±0.5         |\n| ---                             | ---          | ---             | ---              | ---              |\n| Simple CNAPS + FETI             | 77.4Â±0.8     | 90.3Â±0.4        | 63.5Â±0.6         | 83.1Â±0.4         |\n| Transductive CNAPS + FETI       | 79.9Â±0.8     | 91.5Â±0.4        | 68.5Â±0.6         | 85.9Â±0.3         |\n\n## Tiered-ImageNet Results\n\n| Setup                           | 5-way 1-shot | 5-way 5-shot    | 10-way 1-shot    | 10-way 5-shot    |\n| ---                             | ---          | ---             | ---              | ---              |\n| Simple CNAPS                    | 63.0Â±1.0     | 80.0Â±0.8        | 48.1Â±0.7         | 70.2Â±0.6         |\n| Transductive CNAPS              | 65.9Â±1.0     | 81.8Â±0.7        | 54.6Â±0.8         | 72.5Â±0.6         |\n| ---                             | ---          | ---             | ---              | ---              |\n| Simple CNAPS + FETI             | 71.4Â±1.0     | 86.0Â±0.6        | 57.1Â±0.7         | 78.5Â±0.5         |\n| Transductive CNAPS + FETI       | 73.8Â±1.0     | 87.7Â±0.6        | 65.1Â±0.8         | 80.6Â±0.5         |\n\n## Citation\nWe hope you have found our code base helpful! If you use this repository, please cite our papers:\n\n```\n@InProceedings{Bateni2020_SimpleCNAPS,\n    author = {Bateni, Peyman and Goyal, Raghav and Masrani, Vaden and Wood, Frank and Sigal, Leonid},\n    title = {Improved Few-Shot Visual Classification},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month = {June},\n    year = {2020}\n}\n\n@InProceedings{Bateni2022_TransductiveCNAPS,\n    author    = {Bateni, Peyman and Barber, Jarred and van de Meent, Jan-Willem and Wood, Frank},\n    title     = {Enhancing Few-Shot Image Classification With Unlabelled Examples},\n    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},\n    month     = {January},\n    year      = {2022},\n    pages     = {2796-2805}\n}\n\n@misc{Bateni2022_BeyondSimpleMetaLearning,\n    title={Beyond Simple Meta-Learning: Multi-Purpose Models for Multi-Domain, Active and Continual Few-Shot Learning}, \n    author={Peyman Bateni and Jarred Barber and Raghav Goyal and Vaden Masrani and Jan-Willem van de Meent and Leonid Sigal and Frank Wood},\n    year={2022},\n    eprint={2201.05151},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\n@phdthesis{Bateni2021_Thesis, \n    series      = {Electronic Theses and Dissertations (ETDs) 2008+}, \n    title       = {On label-eÃ¯Â¬â‚¬icient computer visionÃ¢â‚¬Â¯: building fast and effective few-shot image classifiers}, \n    url         = {https://open.library.ubc.ca/collections/ubctheses/24/items/1.0402554}, \n    DOI         = {http://dx.doi.org/10.14288/1.0402554}, \n    school      = {University of British Columbia}, \n    author      = {Bateni, Peyman}, \n    year        = {2021}, \n    collection  = {Electronic Theses and Dissertations (ETDs) 2008+}\n}\n```\n\n**If you would like to ask any questions or reach out regarding any of the papers, please email me directly at peyman.bateni@hotmail.com (my cs.ubc.ca email may have expired by the time you are emailing as I have graduated!).\n",
    "readme_length": 16096
  },
  {
    "name": "TCNN",
    "full_name": "LXP-Never/TCNN",
    "description": "TCNN Temporal convolutional neural network for real-time speech enhancement in the time domain",
    "stars": 54,
    "forks": 9,
    "language": "Python",
    "url": "https://github.com/LXP-Never/TCNN",
    "topics": [
      "noise-suppression",
      "speech-enhancement"
    ],
    "created_at": "2022-01-18T09:33:08Z",
    "updated_at": "2025-09-16T15:01:20Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# TCNN\n\n[Pandey A, Wang D L. TCNN: Temporal convolutional neural network for real-time speech enhancement in the time domain[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019: 6875-6879.](https://ieeexplore.ieee.org/abstract/document/8683634)\n\nå¼€æºæ¨¡åž‹ä»£ç ï¼ˆéžå®˜æ–¹å¤çŽ°ï¼‰\n\n## ç½‘ç»œæ¡†æž¶å›¾\n\n![image-20220119165638711](image/image-20220119165638711.png)\n\n## æ¯ä¸€å±‚çš„è¾“å…¥è¾“å‡º\n\n![image-20220119165559904](image/image-20220119165559904.png)\n\n\n\n# How to train\n\n- [Temporal-Convolutional-Neural-Network-Single-Channel-Speech-Enhancement](https://github.com/HardeyPandya/Temporal-Convolutional-Neural-Network-Single-Channel-Speech-Enhancement)\n\n\nThanks to all those who are making their codes open source - promoting education, learning and research.\n",
    "readme_length": 779
  },
  {
    "name": "BE-CNN",
    "full_name": "TJUMMG/BE-CNN",
    "description": "Bit-Depth Enhancement via Convolutional Neural Network",
    "stars": 49,
    "forks": 2,
    "language": "Python",
    "url": "https://github.com/TJUMMG/BE-CNN",
    "topics": [],
    "created_at": "2020-08-15T02:43:20Z",
    "updated_at": "2025-03-06T14:29:02Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# BE-CNN\nBit-Depth Enhancement via Convolutional Neural Network\n\n# Instructions: \n   1) Install TensorFlow(GPU);\n   2) Run 4-16/test_416.py to recover 16-bit images from 4-bit versions.\\\n      Run 8-16/test_816.py to recover 16-bit images from 8-bit versions.\\\n      It will directly compress and reconstruct images from test/.\n   3) Results output to results_416/ or results_816/.\n   * The image size in the code needs to be changed.\n\n*********************************************************************\n\nIf you use this code, please cite the following publication:\\\n__Y.Su, W.Sun, J.Liu, G.Zhai, P.Jing, \"Photo-realistic Image Bit-depth Enhancement via Residual Transposed Convolutional Neural Network\", to appear in NEUROCOMPUTING__\n    \n*********************************************************************\n\nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\n\nFor a copy of the GNU General Public License, please see <http://www.gnu.org/licenses/>. \n\n*********************************************************************\n\nHere we thanks Christian Ledig et al. who are authors of  \"Photo-realistic single image super-resolution using a generative adversarial network\", published in IEEE Conference of Computer Vision and Pattern Recognition, for referring to their outstanding work.\n\n*********************************************************************\n",
    "readme_length": 1782
  },
  {
    "name": "NeuralRejuvenation-CVPR19",
    "full_name": "joe-siyuan-qiao/NeuralRejuvenation-CVPR19",
    "description": "Neural Rejuvenation: Improving Deep Network Training by Enhancing Computational Resource Utilization at CVPR'19",
    "stars": 48,
    "forks": 4,
    "language": "Python",
    "url": "https://github.com/joe-siyuan-qiao/NeuralRejuvenation-CVPR19",
    "topics": [],
    "created_at": "2019-04-05T03:31:32Z",
    "updated_at": "2024-06-03T10:49:25Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# Neural Rejuvenation @ CVPR19\n\n[Neural Rejuvenation: Improving Deep Network Training by\nEnhancing Computational Resource Utilization](https://arxiv.org/pdf/1812.00481.pdf)  \nSiyuan Qiao, Zhe Lin, Jianming Zhang, Alan Yuille  \nIn Conference on Computer Vision and Pattern Recognition, 2019 **Oral Presentation**\n\n```\n@inproceedings{NR,\n   title = {Neural Rejuvenation: Improving Deep Network Training by\n   Enhancing Computational Resource Utilization},\n   author = {Qiao, Siyuan and Lin, Zhe and Zhang, Jianming and Yuille, Alan},\n   booktitle = {CVPR},\n   year = {2019}\n}\n```\n\nNeural Rejuvenation is a training method for deep neural networks that focuses on improving the computation resource utilization.\nDeep neural networks are usually over-parameterized for their tasks\nin order to achieve good performances, thus are likely to\nhave underutilized computational resources.\nAs models with higher computational costs (e.g. more parameters or more computations)\nusually have better performances, we study the problem of\nimproving the resource utilization of neural networks so that\ntheir potentials can be further realized.\nTo this end, we propose a novel optimization method named Neural Rejuvenation. As its name suggests, our method detects dead neurons\nand computes resource utilization in real time, rejuvenates\ndead neurons by resource reallocation and reinitialization,\nand trains them with new training schemes.\n\n## Training\nThe code was implemented and tested with PyTorch 0.4.1.post2.\nIf you are using other versions, please be aware that there might be some incompatibility issues.\nThe code is based on [pytorch-classification](https://github.com/bearpaw/pytorch-classification) by [Wei Yang](https://github.com/bearpaw/).\n\n### CIFAR\n#### VGG19 (BN)\n```bash\npython cifar.py -d cifar10 -a vgg19_bn --epochs 300 --schedule 150 225 --gamma 0.1 --checkpoint checkpoints/cifar10/vgg19_bn --nr-target 0.25 --nr-sparsity 1.5e-4\n```\n\n#### ResNet-164\n```bash\npython cifar.py -d cifar10 -a resnet --depth 110 --epochs 300 --schedule 150 225 --gamma 0.1 --wd 1e-4 --checkpoint checkpoints/cifar10/resnet-164 --nr-target 0.25 --nr-sparsity 1.5e-4\n```\n\n#### DenseNet-BC\n```bash\npython cifar.py -d cifar10 -a densenet --depth 100 --growthRate 40 --train-batch 64 --epochs 300 --schedule 150 225 --wd 1e-4 --gamma 0.1 --checkpoint checkpoints/cifar10/densenet-bc-100-12 --nr-target 0.25 --nr-sparsity 1.0e-4\n```\n\n### ImageNet\n#### VGG16 (BN)\n```bash\npython imagenet.py -a vgg_nr --data ~/dataset/ILSVRC2012/ --epochs 100 --schedule 30 60 90 --gamma 0.1 -c checkpoints/imagenet/vgg16 --nr-target 0.5 --train-batch 128 --test-batch 100 --nr-compress-only --gpu-id 0,1,2,3 --image-size 224 -j 20\npython imagenet.py -a vgg_nr --data ~/dataset/ILSVRC2012/ --epochs 100 --schedule 30 60 90 --gamma 0.1 -c checkpoints/imagenet/vgg16 --nr-bn-target 0.5 --train-batch 128 --test-batch 100 --resume checkpoints/imagenet/vgg16/NR_vgg16_nr_0.5.pth.tar --gpu-id 0,1,2,3 --image-size 224 -j 20\n```\nNote that without --nr-compress-only, the program will automatically go to the second line.\nWriting it as two steps makes debug easier.\n\n## Experimental Results\n### CIFAR\n| Model                     | # Params          |  Dataset  | nr_sparsity | Err |\n| -------------------       | ----------------- | --------- | ------- | ------- |\n| VGG-19                    | 9.99M             | CIFAR-10  | 1.5e-4  | 4.19    |\n| VGG-19                    | 10.04M            | CIFAR-100 | 3e-4    | 21.53   |\n| ResNet-164                | 0.88M             | CIFAR-10  | 1.50e-4 | 5.13    |\n| ResNet-164                | 0.92M             | CIFAR-100 | 2.50e-4 | 23.84   |\n| DenseNet-100-40           | 4.12M             | CIFAR-10  | 1.00e-4 | 3.40    |\n| DenseNet-100-40           | 4.31M             | CIFAR-100 | 2.00e-4 | 18.59   |\n\n### ImageNet\n| Model               | # Params      | FLOPs | Top-1 | Top-5 |\n| ------------------- | ------------- | ----- | ----- | ----- |\n| DenseNet-121        | 8.22M | 3.13G | 24.50 | 7.49  |\n| VGG-16              | 36.4M | 23.5G | 23.11 | 6.69  |\n| ResNet-18           | 11.9M | 2.16G | 28.86 | 9.93  |\n| ResNet-34           | 21.9M | 3.77G | 25.77 | 8.10  |\n| ResNet-50           | 26.4M | 3.90G | 22.93 | 6.47  |\n| ResNet-101          | 46.6M | 6.96G | 21.22 | 5.76  |\n",
    "readme_length": 4288
  },
  {
    "name": "Spikingformer-CML",
    "full_name": "zhouchenlin2096/Spikingformer-CML",
    "description": "Enhancing the Performance of Transformer-based Spiking Neural Networks by SNN-optimized Downsampling with Precise Gradient Backpropagation",
    "stars": 44,
    "forks": 2,
    "language": "Python",
    "url": "https://github.com/zhouchenlin2096/Spikingformer-CML",
    "topics": [
      "3rd-generation-of-artificial-neural-networks",
      "binary",
      "brain-inspired",
      "deep-learning",
      "energy-efficiency",
      "neuromorphic-computing",
      "pytorch",
      "snn",
      "spike",
      "spiking-neural-network",
      "spiking-transformer",
      "spikingjelly",
      "transformer"
    ],
    "created_at": "2023-05-08T08:32:03Z",
    "updated_at": "2025-10-22T06:32:53Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "# Enhancing the Performance of Transformer-based Spiking Neural Networks by SNN-optimized Downsampling with Precise Gradient Backpropagation, [Arxiv 2023](https://arxiv.org/abs/2305.05954)\n# SGLFormer: Spiking Global-Local-Fusion Transformer with high performance, [This link](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1371290/full)\nOur CML models achieve the state-of-the-art performance on several datasets (eg. **77.64 %** on ImageNet, **96.04 %** on CIFAR10, **80.37 %** on CIFAR100, **81.4% on** CIFAR10-DVS) in directly trained spiking neural networks in 2023/05. Our model achieves **78.46 %** on ImageNet with 288 * 288 resolution.\n\nOur newly improved model with CML, named [SGLFormer](https://github.com/ZhangHanN1/SGLFormer), has achieved SOTA performance on several datasets (eg. **83.73 %** on ImageNet, **96.76 %** on CIFAR10, **82.26 %** on CIFAR100, **82.9%** on CIFAR10-DVS) in directly trained SNNs in 2024/03.\n\n<br>\n<br>\n<p align=\"center\">\n<img src=\"https://github.com/zhouchenlin2096/Spikingformer-CML/blob/master/imgs/SNN-optimized-downsampling.png\">\n</p>\n\n## News\n[2024.3.12] Our work with CML has been accepted in Frontiers in Neuroscience 2024.\n\n[2023.9.11] Update origin_logs.\n\n[2023.8.18] Update trained models.\n\n## Reference\nIf you find this repo useful, please consider citing:\n```\n@ARTICLE{10.3389/fnins.2024.1371290,\n  AUTHOR={Zhang, Han  and Zhou, Chenlin  and Yu, Liutao  and Huang, Liwei  and Ma, Zhengyu  and Fan, Xiaopeng  and Zhou, Huihui  and Tian, Yonghong },\n  TITLE={SGLFormer: Spiking Global-Local-Fusion Transformer with high performance},\n  JOURNAL={Frontiers in Neuroscience},\n  VOLUME={18},\n  YEAR={2024},\n  URL={https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1371290},\n  DOI={10.3389/fnins.2024.1371290},\n  ISSN={1662-453X}\n}\n\n@misc{zhou2023enhancing,\n      title={Enhancing the Performance of Transformer-based Spiking Neural Networks by Improved Downsampling with Precise Gradient Backpropagation}, \n      author={Chenlin Zhou and Han Zhang and Zhaokun Zhou and Liutao Yu and Zhengyu Ma and Huihui Zhou and Xiaopeng Fan and Yonghong Tian},\n      year={2023},\n      eprint={2305.05954},\n      archivePrefix={arXiv},\n      primaryClass={cs.NE}\n}\n```\n\n## Main results on ImageNet-1K\n\n| Model                     | Resolution| T     |  Param.     |Top-1 Acc| Download |\n| :---                     | :---:     | :---: | :---:        |:---:    | :---:    |\n| CML + Spikformer-8-384    | 224x224   | 4     |  16.81M     |72.73    |     -    |\n| CML + Spikformer-8-512    | 224x224   | 4     |  29.68M     |75.61    |     -    |\n| CML + Spikformer-8-768    | 224x224   | 4     |  66.34M     |77.34    |     -    |\n| CML + Spikingformer-8-384 | 224x224   | 4     |  16.81M     |74.35    |     -    |\n| CML + Spikingformer-8-512 | 224x224   | 4     |  29.68M     |76.54    |     -    |\n| CML + Spikingformer-8-768 | 224x224   | 4     |  66.34M     |77.64    | [here](https://pan.baidu.com/s/1uTq6aPMknwb2PjDZ3J4g5g) |\n| CML + Spikingformer-8-768 | 288x288   | 4     |  66.34M     |78.46    |     -    |\n\nDownload password: abcd\n\n## Main results on CIFAR10/CIFAR100\n\n| Model                      | T      |  Param.     | CIFAR10 Top-1 Acc | Download |CIFAR100 Top-1 Acc|\n| :---                      | :---:  | :---:       |  :---:          | :---:  |:---: |\n| CML + Spikformer-4-256     | 4      |  4.15M      | 94.82          |     -      |77.64  |\n| CML + Spikformer-2-384     | 4      |  5.76M      | 95.63          |     -      |78.75  |\n| CML + Spikformer-4-384     | 4      |  9.32M      | 95.93          |     -      |79.65  |\n| CML + Spikformer-4-384-400E  | 4         |  9.32M | 96.04          |     -      |80.02  |\n| CML + Spikingformer-4-256  | 4      |  4.15M      | 94.94          |     -      |78.19  |\n| CML + Spikingformer-2-384  | 4      |  5.76M      | 95.54          |     -      |78.87  |\n| CML + Spikingformer-4-384  | 4      |  9.32M      | 95.81          |     -      |79.98  |\n| CML + Spikingformer-4-384-400E  | 4      |  9.32M     | 95.95      |  [here](https://pan.baidu.com/s/1HRLjzLHEu5ANtmtJyinYcQ)     |80.37  |\n\nDownload password: abcd\n\n## Main results on CIFAR10-DVS/DVS128\n\n| Model                     | T      |  Param.     |  CIFAR10 DVS Top-1 Acc  | DVS 128 Top-1 Acc|\n| :---                     | :---:  | :---:       | :---:                   |:---:             |\n| CML + Spikformer-2-256    | 10     |  2.57M      | 79.2                    | 97.6             |\n| CML + Spikformer-2-256    | 16     |  2.57M      | 80.9                    | 98.6             |\n| CML + Spikingformer-2-256 | 10     |  2.57M      | 80.5                    | 97.2             |\n| CML + Spikingformer-2-256 | 16     |  2.57M      | 81.4                    | 98.6             |\n\n\n## Requirements\ntimm==0.6.12; cupy==11.4.0; torch==1.12.1; spikingjelly==0.0.0.0.12; pyyaml; \n\ndata prepare: ImageNet with the following folder structure, you can extract imagenet by this [script](https://gist.github.com/BIGBALLON/8a71d225eff18d88e469e6ea9b39cef4).\n```\nâ”‚imagenet/\nâ”œâ”€â”€train/\nâ”‚  â”œâ”€â”€ n01440764\nâ”‚  â”‚   â”œâ”€â”€ n01440764_10026.JPEG\nâ”‚  â”‚   â”œâ”€â”€ n01440764_10027.JPEG\nâ”‚  â”‚   â”œâ”€â”€ ......\nâ”‚  â”œâ”€â”€ ......\nâ”œâ”€â”€val/\nâ”‚  â”œâ”€â”€ n01440764\nâ”‚  â”‚   â”œâ”€â”€ ILSVRC2012_val_00000293.JPEG\nâ”‚  â”‚   â”œâ”€â”€ ILSVRC2012_val_00002138.JPEG\nâ”‚  â”‚   â”œâ”€â”€ ......\nâ”‚  â”œâ”€â”€ ......\n```\n\n## Train\n### Training  on ImageNet\nSetting hyper-parameters in imagenet.yml\n\n```\ncd imagenet\npython -m torch.distributed.launch --nproc_per_node=8 train.py\n```\n\n### Testing ImageNet Val data\nDownload the trained model first [here](https://pan.baidu.com/s/1uTq6aPMknwb2PjDZ3J4g5g).\n```\ncd imagenet\npython test.py\n```\n\n### Training  on CIFAR10\nSetting hyper-parameters in cifar10.yml\n```\ncd cifar10\npython train.py\n```\n\n### Training  on CIFAR100\nSetting hyper-parameters in cifar100.yml\n```\ncd cifar10\npython train.py\n```\n\n### Training  on DVS128 Gesture\n```\ncd dvs128-gesture\npython train.py\n```\n\n### Training  on CIFAR10-DVS\n```\ncd cifar10-dvs\npython train.py\n```\n\n## Acknowledgement & Contact Information\nRelated project: [Spikingformer](https://github.com/zhouchenlin2096/Spikingformer), [spikformer](https://github.com/ZK-Zhou/spikformer), [pytorch-image-models](https://github.com/huggingface/pytorch-image-models), [spikingjelly](https://github.com/fangwei123456/spikingjelly).\n\nFor help or issues using this git, please submit a GitHub issue.\n\nFor other communications related to this git, please contact zhouchl@pcl.ac.cn or zhouchenlin19@mails.ucas.ac.cn.\n",
    "readme_length": 6511
  },
  {
    "name": "fastenhancer",
    "full_name": "aask1357/fastenhancer",
    "description": "Speed-optimized streaming neural speech enhancement network",
    "stars": 43,
    "forks": 12,
    "language": "Python",
    "url": "https://github.com/aask1357/fastenhancer",
    "topics": [],
    "created_at": "2025-09-04T07:47:13Z",
    "updated_at": "2025-11-17T10:23:06Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# Introduction\nOfficial repository of \"FastEnhancer: Speed-Optimized Streaming Neural Speech Enhancement.\"  \n[Paper](https://arxiv.org/abs/2509.21867) | [Documentation](https://aask1357.github.io/fastenhancer/)\n\n# Install\nPlease refer to [document](https://aask1357.github.io/fastenhancer/installation).\n\n# Datasets\nPlease refer to [document](https://aask1357.github.io/fastenhancer/dataset).\n\n# Training\nPlease refer to [document](https://aask1357.github.io/fastenhancer/train).\n\n# Inference\n## PyTorch Inference\nPytorch checkpoints and tensorboard logs are provided in [releases](https://github.com/aask1357/fastenhancer/releases).  \nPlease refer to [document](https://aask1357.github.io/fastenhancer/metrics) for calculating objective metrics.  \nPlease refer to [document](https://aask1357.github.io/fastenhancer/pytorch) for pytorch inference.\n\n## ONNXRuntime Inference\nONNX models are provided in [releases](https://github.com/aask1357/fastenhancer/releases).  \nPlease refer to [document](https://aask1357.github.io/fastenhancer/onnx) for streaming inference using ONNXRuntime. \n\n# Results\n## Voicebank-Demand 16kHz\n\n<p align=\"center\"><b>Table 1.</b> Performance on Voicebank-Demand testset.</p>\n<table>\n  <thead>\n    <tr>\n      <th rowspan=\"2\">Model</th>\n      <th rowspan=\"2\">Para.<br>(K)</th>\n      <th rowspan=\"2\">MACs</th>\n      <th rowspan=\"2\">RTF<br>(Xeon)</th>\n      <th rowspan=\"2\">RTF<br>(M1)</th>\n      <th rowspan=\"2\">DNSMOS<br>(P.808)</th>\n      <th colspan=\"3\">DNSMOS (P.835)</th>\n      <th rowspan=\"2\">SCOREQ</th>\n      <th rowspan=\"2\">SISDR</th>\n      <th rowspan=\"2\">PESQ</th>\n      <th rowspan=\"2\">STOI</th>\n      <th rowspan=\"2\">ESTOI</th>\n      <th rowspan=\"2\">WER</th>\n    </tr>\n    <tr>\n      <th>SIG</th>\n      <th>BAK</th>\n      <th>OVL</th>\n    </tr>\n  </thead>\n  <tbody align=center>\n    <tr>\n      <td>GTCRN<sup>a</sup></td>\n      <td><strong>24</strong></td>\n      <td><strong>40M</strong></td>\n      <td>0.060</td>\n      <td>0.042</td>\n      <td>3.43</td>\n      <td>3.36</td>\n      <td>4.02</td>\n      <td>3.08</td>\n      <td>0.330</td>\n      <td>18.8</td>\n      <td>2.87</td>\n      <td>0.940</td>\n      <td>0.848</td>\n      <td>3.6</td>\n    </tr>\n    <tr>\n      <td>LiSenNet<sup>b</sup></td>\n      <td>37</td>\n      <td>56M</td>\n      <td>-</td>\n      <td>-</td>\n      <td>3.34</td>\n      <td>3.30</td>\n      <td>3.90</td>\n      <td>2.98</td>\n      <td>0.425</td>\n      <td>13.5</td>\n      <td>3.08</td>\n      <td>0.938</td>\n      <td>0.842</td>\n      <td>3.7</td>\n    </tr>\n    <tr>\n      <td>LiSenNet<sup>c</sup></td>\n      <td>37</td>\n      <td>56M</td>\n      <td>0.034</td>\n      <td>0.028</td>\n      <td>3.42</td>\n      <td>3.34</td>\n      <td><strong>4.03</strong></td>\n      <td>3.07</td>\n      <td>0.335</td>\n      <td>18.5</td>\n      <td>2.98</td>\n      <td>0.941</td>\n      <td>0.851</td>\n      <td>3.4</td>\n    </tr>\n    <tr>\n      <td>FSPEN<sup>d</sup></td>\n      <td>79</td>\n      <td>64M</td>\n      <td>0.046</td>\n      <td>0.038</td>\n      <td>3.40</td>\n      <td>3.33</td>\n      <td>4.00</td>\n      <td>3.05</td>\n      <td>0.324</td>\n      <td>18.4</td>\n      <td>3.00</td>\n      <td>0.942</td>\n      <td>0.850</td>\n      <td>3.6</td>\n    </tr>\n    <tr>\n      <td>BSRNN<sup>d</sup></td>\n      <td>334</td>\n      <td>245M</td>\n      <td>0.059</td>\n      <td>0.062</td>\n      <td>3.44</td>\n      <td>3.36</td>\n      <td>4.00</td>\n      <td>3.07</td>\n      <td>0.303</td>\n      <td>18.9</td>\n      <td>3.06</td>\n      <td>0.942</td>\n      <td>0.855</td>\n      <td>3.4</td>\n    </tr>\n    <tr>\n      <td><i>FastEnhancer</i>_B</td>\n      <td>92</td>\n      <td>262M</td>\n      <td><strong>0.022</strong></td>\n      <td><strong>0.026</strong></td>\n      <td><strong>3.47</strong></td>\n      <td><strong>3.38</strong></td>\n      <td>4.02</td>\n      <td><strong>3.10</strong></td>\n      <td><strong>0.285</strong></td>\n      <td><strong>19.0</strong></td>\n      <td><strong>3.13</strong></td>\n      <td><strong>0.945</strong></td>\n      <td><strong>0.861</strong></td>\n      <td><strong>3.2</strong></td>\n    </tr>\n    <tr><td colspan=15></td></tr>\n    <tr>\n      <td><i>FastEnhancer</i>_T</td>\n      <td><strong>22</strong></td>\n      <td><strong>55M</strong></td>\n      <td><strong>0.012</strong></td>\n      <td><strong>0.013</strong></td>\n      <td>3.42</td>\n      <td>3.34</td>\n      <td>4.01</td>\n      <td>3.06</td>\n      <td>0.334</td>\n      <td>18.6</td>\n      <td>2.99</td>\n      <td>0.940</td>\n      <td>0.850</td>\n      <td>3.6</td>\n    </tr>\n    <tr>\n      <td><i>FastEnhancer</i>_B</td>\n      <td>92</td>\n      <td>262M</td>\n      <td>0.022</td>\n      <td>0.026</td>\n      <td>3.47</td>\n      <td>3.38</td>\n      <td>4.02</td>\n      <td>3.10</td>\n      <td>0.285</td>\n      <td>19.0</td>\n      <td>3.13</td>\n      <td>0.945</td>\n      <td>0.861</td>\n      <td>3.2</td>\n    </tr>\n    <tr>\n      <td><i>FastEnhancer</i>_S</td>\n      <td>195</td>\n      <td>664M</td>\n      <td>0.034</td>\n      <td>0.048</td>\n      <td>3.49</td>\n      <td>3.40</td>\n      <td>4.03</td>\n      <td>3.12</td>\n      <td>0.265</td>\n      <td>19.2</td>\n      <td>3.19</td>\n      <td>0.947</td>\n      <td>0.866</td>\n      <td>3.2</td>\n    </tr>\n    <tr>\n      <td><i>FastEnhancer</i>_M</td>\n      <td>492</td>\n      <td>2.9G</td>\n      <td>0.101</td>\n      <td>0.173</td>\n      <td>3.48</td>\n      <td>3.39</td>\n      <td>4.02</td>\n      <td>3.11</td>\n      <td>0.243</td>\n      <td>19.4</td>\n      <td>3.24</td>\n      <td>0.950</td>\n      <td>0.873</td>\n      <td><strong>2.8</strong>\n    </tr>\n    <tr>\n      <td><i>FastEnhancer</i>_L</td>\n      <td>1105</td>\n      <td>11G</td>\n      <td>0.313</td>\n      <td>0.632</td>\n      <td><strong>3.53</strong></td>\n      <td><strong>3.44</strong></td>\n      <td><strong>4.04</strong></td>\n      <td><strong>3.16</strong></td>\n      <td><strong>0.239</strong></td>\n      <td><strong>19.6</strong></td>\n      <td><strong>3.26</strong></td>\n      <td><strong>0.952</strong></td>\n      <td><strong>0.877</strong></td>\n      <td>3.1</td>\n    </tr>\n  </tbody>\n</table>\n<p><sup>a</sup> Evaluated using the official checkpoint.<br>\n<sup>b</sup> Trained using the official training code. Not streamable because of input normalization and griffin-lim. Thus, RTFs are not reported.<br>\n<sup>c</sup> To make the model streamable, input normalization and griffin-lim are removed. Trained following the experimental setup of FastEnhancer.<br>\n<sup>d</sup> Re-implemented and trained following the experimental setup of FastEnhancer for a fair comparison.</p>\n\n## DNS-Challenge 16kHz\n* Trained using DNS-Challenge-3 wideband training dataset.\n  * Without `emotional_speech` and `singing_voice`.\n  * With VCTK-0.92 clean speech except `p232` and `p257` speakers.\n  * RIRs were not convolved to the clean speech.\n  * Unlike in Voicebank-Demand, we didn't use PESQLoss.\n* Tested using DNS-Challenge-1 dev-testset-synthetic-no-reverb dataset.  \n\n<p align=\"center\"><b>Table 2.</b> Performance on DNS-Challenge1 dev-testset-synthetic-no-reverb.</p>\n<table>\n  <thead>\n    <tr>\n      <th rowspan=\"2\">Model</th>\n      <th rowspan=\"2\">Para.<br>(K)</th>\n      <th rowspan=\"2\">MACs</th>\n      <th rowspan=\"2\">RTF<br>(Xeon)</th>\n      <th rowspan=\"2\">RTF<br>(M1)</th>\n      <th rowspan=\"2\">DNSMOS<br>(P.808)</th>\n      <th colspan=\"3\">DNSMOS (P.835)</th>\n      <th rowspan=\"2\">SCOREQ</th>\n      <th rowspan=\"2\">SISDR</th>\n      <th rowspan=\"2\">PESQ</th>\n      <th rowspan=\"2\">STOI</th>\n      <th rowspan=\"2\">ESTOI</th>\n    </tr>\n    <tr>\n      <th>SIG</th>\n      <th>BAK</th>\n      <th>OVL</th>\n    </tr>\n  </thead>\n  <tbody align=center>\n    <tr>\n      <td>GTCRN<sup>a</sup></td>\n      <td>24</td>\n      <td><strong>40M</strong></td>\n      <td>0.060</td>\n      <td>0.042</td>\n      <td>3.85</td>\n      <td>3.35</td>\n      <td>3.98</td>\n      <td>3.05</td>\n      <td>0.551</td>\n      <td>14.8</td>\n      <td>2.26</td>\n      <td>0.934</td>\n      <td>0.871</td>\n    </tr>\n    <tr>\n      <td><i>FastEnhancer</i>_T</td>\n      <td><strong>22</strong></td>\n      <td>55M</td>\n      <td><strong>0.012</strong></td>\n      <td><strong>0.013</strong></td>\n      <td>3.81</td>\n      <td>3.35</td>\n      <td>4.07</td>\n      <td>3.10</td>\n      <td>0.522</td>\n      <td>15.4</td>\n      <td>2.43</td>\n      <td>0.940</td>\n      <td>0.879</td>\n    </tr>\n    <tr>\n      <td><i>FastEnhancer</i>_B</td>\n      <td>92</td>\n      <td>262M</td>\n      <td>0.022</td>\n      <td>0.026</td>\n      <td>3.92</td>\n      <td>3.43</td>\n      <td>4.12</td>\n      <td>3.20</td>\n      <td>0.396</td>\n      <td>16.7</td>\n      <td>2.69</td>\n      <td>0.953</td>\n      <td>0.903</td>\n    </tr>\n    <tr>\n      <td><i>FastEnhancer</i>_S</td>\n      <td>195</td>\n      <td>664M</td>\n      <td>0.034</td>\n      <td>0.048</td>\n      <td>3.96</td>\n      <td>3.46</td>\n      <td>4.13</td>\n      <td>3.23</td>\n      <td>0.373</td>\n      <td>17.5</td>\n      <td>2.79</td>\n      <td>0.960</td>\n      <td>0.914</td>\n    </tr>\n    <tr>\n      <td><i>FastEnhancer</i>_M</td>\n      <td>492</td>\n      <td>2.9G</td>\n      <td>0.101</td>\n      <td>0.173</td>\n      <td>3.98</td>\n      <td>3.48</td>\n      <td>4.14</td>\n      <td>3.26</td>\n      <td>0.345</td>\n      <td>18.4</td>\n      <td>2.78</td>\n      <td>0.965</td>\n      <td>0.924</td>\n    </tr>\n    <tr>\n      <td><i>FastEnhancer</i>_L</td>\n      <td>1105</td>\n      <td>11G</td>\n      <td>0.313</td>\n      <td>0.632</td>\n      <td><strong>4.02</strong></td>\n      <td><strong>3.51</strong></td>\n      <td><strong>4.16</strong></td>\n      <td><strong>3.29</strong></td>\n      <td><strong>0.298</strong></td>\n      <td><strong>19.5</strong></td>\n      <td><strong>2.94</strong></td>\n      <td><strong>0.971</strong></td>\n      <td><strong>0.935</strong></td>\n    </tr>\n  </tbody>\n</table>\n<p><sup>a</sup> Evaluated using the official checkpoint. It should be noted that this model was trained for both noise suppression and de-reverberation, whereas FastEnhancers were trained only for noise suppression. If GTCRN is trained for noise suppression only, its performance may be higher.<br>\n",
    "readme_length": 10034
  },
  {
    "name": "SNR-Based-Progressive-Learning-of-Deep-Neural-Network-for-Speech-Enhancement",
    "full_name": "haoxiangsnr/SNR-Based-Progressive-Learning-of-Deep-Neural-Network-for-Speech-Enhancement",
    "description": "Implementation of the paper \"SNR-Based Progressive Learning of Deep Neural Network for Speech Enhancement.\"",
    "stars": 43,
    "forks": 13,
    "language": "Python",
    "url": "https://github.com/haoxiangsnr/SNR-Based-Progressive-Learning-of-Deep-Neural-Network-for-Speech-Enhancement",
    "topics": [
      "deep-neural-networks",
      "pytorch",
      "speech-enhancement"
    ],
    "created_at": "2019-04-12T13:25:42Z",
    "updated_at": "2025-04-28T08:44:39Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# SNR-Based Progressive Learning of DNN for Speech Enhancement\n\nThe implementation of the paper _[SNR-Based Progressive Learning of Deep Neural Network for Speech Enhancement](https://pdfs.semanticscholar.org/8184/c50be9a5d63aed3122962650eb19e58a7515.pdf)_.\n\nUnofficial implementation, with some differences from the original paper.\n\n## Todo\n\n- [x] Data pre-processing for training, validation and test\n- [x] PL DNN models\n- [x] Training data\n- [x] Validation data\n- [x] Training logic\n- [x] validation logic\n- [x] visualization for validation set (waveform, audio, metrics)\n- [x] Config parameters\n- [ ] Integrate pre-processing scripts in the project\n- [ ] Global Normalization\n- [ ] Test script\n\n### Model coverage\n\n- [x] Single-SNR\n- [ ] Multi-SNR\n- [ ] Baseline DNN model\n\n## Dependencies\n\n- tqdm\n- pypesq\n- pystoi\n- librosa\n- pytorch==1.0\n- matplotlib\n- tensorboardX\n\n## Usage\n\n#### Data Pre-processing\n\n:sweat_smile: waiting for integration into the project.\n\n#### Training\n\n```bash\n# specify training configuration (-C), and GPU devices (-D).\npython train.py -C config/train/<name>.json -D 0\n\n# Resume expriment (-R)\npython train.py -C config/train/<name>.json -D 0 -R\n```\n\nConfiguration for Training: \n\n```\n{\n    # å®žéªŒåï¼Œæ¯æ¬¡å®žéªŒéƒ½ä¸èƒ½é‡å¤\n    \"name\": \"basic_1000\", \n    # å®žéªŒä½¿ç”¨çš„ GPU æ•°é‡ï¼Œéœ€è¦é…åˆ train.py çš„ -D é€‰é¡¹\n    \"n_gpu\": 1, \n    # æ˜¯å¦ä½¿ç”¨ Cudnn åŠ å¿«é€Ÿåº¦ï¼Œä½¿ç”¨æ—¶æ— æ³•ä¿è¯å®žéªŒçš„å¯é‡å¤æ€§\n    \"use_cudnn\": true,\n    # æŸå¤±å‡½æ•°ï¼Œè§ models/loss.py\n    \"loss_func\": \"mse_loss\",\n    # Checkpointsï¼Œlogs ç­‰ç›®å½•æ‰€åœ¨çš„ä½ç½®ï¼Œå¯ä»¥æŒ‡å®šåœ¨å¦å¤–çš„æ•°æ®ç›˜ä¸­\n    \"save_location\": \"/media/imucs/DataDisk/haoxiang/Experiment/DNN\",\n    # æœ¬æ¬¡å®žéªŒçš„æè¿°ä¿¡æ¯ï¼ŒåŽç»­ä¼šæ‰“å°åœ¨ tensorboardX ä¸­\n    \"description\": \"ä¿®å¤ epoch æ˜¾ç¤ºé”™è¯¯çš„é—®é¢˜\",\n    # å¯è§†åŒ–è¯„ä»·æŒ‡æ ‡çš„é¢‘çŽ‡\n    \"visualize_metrics_period\": 10,\n    # ä¼˜åŒ–å™¨å‚æ•°\n    \"optimizer\": {\n        \"lr\": 0.01\n    },\n    # è®­ç»ƒè¿‡ç¨‹çš„å¯é…ç½®å‚æ•°\n    \"trainer\": {\n        \"epochs\": 1000,\n        # æ¨¡åž‹å­˜å‚¨çš„é¢‘çŽ‡\n        \"save_period\": 3\n    },\n    # æ‰€æœ‰å…³äºŽè®­ç»ƒçš„æ•°æ®é›†\n    \"train_dataset\": {\n        \"mixture_dataset\": \"/home/imucs/Center/timit_single_snr_limit_500/train/dB-5.npy\",\n        \"clean_dataset\": \"/home/imucs/Center/timit_single_snr_limit_500/train/clean.npy\",\n        \"target_1_dataset\": \"/home/imucs/Center/timit_single_snr_limit_500/train/dB5.npy\",\n        \"target_2_dataset\": \"/home/imucs/Center/timit_single_snr_limit_500/train/dB15.npy\"\n    },\n    # æ‰€æœ‰å…³äºŽéªŒè¯çš„æ•°æ®é›†\n    \"valid_dataset\": {\n        \"mixture_dataset\": \"/home/imucs/Center/timit_waveform_192/test/dB-5.npy\",\n        \"clean_dataset\": \"/home/imucs/Center/timit_waveform_192/test/clean.npy\"\n    },\n    # è®­ç»ƒæ•°æ®é›†ä¸­çš„å‚æ•°\n    \"train_data\": {\n        \"batch_size\": 20000,\n        \"shuffle\": true,\n        \"num_workers\": 100\n    },\n    # éªŒè¯æ•°æ®é›†ä¸­çš„å‚æ•°\n    \"valid_data\": {\n        \"limit\": 20,\n        \"offset\": 1,\n        \"batch_size\": 1,\n        \"num_workers\": 1\n    }\n}\n```\n\n#### Visualization\n\n```bash\ntensorborad --logdir <training_config:save_location>/logs --port <port>\n```\n\n#### Test(TODO)\n\nSimilar to the validation part in `trainer/trainer.py`.",
    "readme_length": 2911
  },
  {
    "name": "Causal-U-Net",
    "full_name": "YangangCao/Causal-U-Net",
    "description": "unofficial PyTorch implementation of ã€ŠA Causal U-net based Neural Beamforming Network for Real-Time Multi-Channel Speech Enhancementã€‹",
    "stars": 40,
    "forks": 11,
    "language": "Python",
    "url": "https://github.com/YangangCao/Causal-U-Net",
    "topics": [],
    "created_at": "2021-12-01T08:10:47Z",
    "updated_at": "2025-11-04T06:12:39Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# Causal-U-Net\nunofficial PyTorch implementation of ã€ŠA Causal U-net based Neural Beamforming Network for Real-Time Multi-Channel Speech Enhancementã€‹\n",
    "readme_length": 149
  },
  {
    "name": "NSNet",
    "full_name": "GuillaumeVW/NSNet",
    "description": "This in an implementation of NSNet in PyTorch and PyTorch Lightning. NSNet is a recurrent neural network for single channel speech enhancement.",
    "stars": 40,
    "forks": 12,
    "language": "Python",
    "url": "https://github.com/GuillaumeVW/NSNet",
    "topics": [],
    "created_at": "2020-08-20T15:58:33Z",
    "updated_at": "2025-09-18T08:47:40Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# NSNet\nThis in an implementation of NSNet [1] in PyTorch and PyTorch Lightning.\nNSNet is a recurrent neural network for single channel speech enhancement.\nThis was implemented as part of my thesis for the Master in Electrical Engineering at Ghent University.\n\n## Prerequisites\n* torch 1.4\n* pytorch_lightning 0.7.6\n* torchaudio 1.4\n* soundfile 0.10.3.post1\n\n## How to train\nA dataset containing both clean speech and corresponding noisy speech (i.e. clean speech with noise added) is required.\n\nRunning _train_nn.py_ starts the training.\n\nThe _train_dir_ variable should contain the path to a folder containing a _clean_ and a _noisy_ folder, containing the clean WAV files and the noisy WAV files respectively. The filename of a noisy WAV file must be the same as the corresponding clean WAV file, with optionally a suffix added delimited by _+_,\ne.g. clean01.wav &rarr; clean01+noise.wav\n\nThe _val_dir_ follows the same convention, but this folder is used for validation.\n\n## How to test\nRunning the _test_nn.py_ file results in the output (denoised) WAV files.\n\n_testing_dir_ should point to a folder with the same structure as _train_dir_ and _val_dir_.\n\n## Acknowledgements\n[1] Y. Xia, S. Braun, C. K. A. Reddy, H. Dubey, R. Cutler, and I. Tashev, â€œWeighted Speech Distortion Losses for Neural-network-based Real-time Speech Enhancement,â€ arXiv:2001.10601 [cs, eess], Feb. 2020.",
    "readme_length": 1384
  },
  {
    "name": "Pitfalls-of-Memorization",
    "full_name": "facebookresearch/Pitfalls-of-Memorization",
    "description": "Understanding the interplay between memorization and generalization in neural networks, featuring MAT, a learning algorithm to enhance robustness by mitigating spurious correlations.",
    "stars": 40,
    "forks": 1,
    "language": "Python",
    "url": "https://github.com/facebookresearch/Pitfalls-of-Memorization",
    "topics": [],
    "created_at": "2024-12-06T22:37:18Z",
    "updated_at": "2025-11-14T13:35:23Z",
    "homepage": null,
    "license": "Other",
    "readme": "# The Pitfalls of Memorization: When Memorization Hurts Generalization\n\n![License](https://img.shields.io/badge/license-CC--BY--NC-blue.svg)\n![Python](https://img.shields.io/badge/python-3.8+-brightgreen.svg)\n![Build](https://img.shields.io/badge/build-passing-brightgreen.svg)\n\nThis repository contains the code associated with the paper: \\\n**[The Pitfalls of Memorization: When Memorization Hurts Generalization](https://arxiv.org/abs/2412.07684)**  \n**Authors:** Reza Bayat*, Mohammad Pezeshki*, Elvis Dohmatob, David Lopez-Paz, Pascal Vincent\n\nWe explore the interplay between memorization and generalization in neural networks. Includes Memorization-Aware Training (MAT), a novel framework to mitigate the adverse effects of memorization and spurious correlations, alongside theoretical insights, algorithms, and experiments that deepen our understanding of how memorization impacts generalization under distribution shifts.\n\n## The Interpretable Experiment (Figure 1)\n\n```bash\npython interpretable_experiment.py\n```\n![](https://github.com/facebookresearch/Pitfalls-of-Memorization/blob/main/assets/interpretable_experiment.gif?raw=true)\n\n## Memorization: The Good, the Bad, and the Ugly (Figure 3)\n\n```bash\npython good_bad_ugly_memorization.py\n```\n![](https://github.com/facebookresearch/Pitfalls-of-Memorization/blob/main/assets/good_bad_ugly_memorization.gif?raw=true)\n\n## Subpopulation Shift Experiments (Table 1)\n\nInstall the required packages and download the datasets:\n\n```bash\npip install -r requirements.txt\npython download.py --download --data_path ./data waterbirds celeba civilcomments multinli\nexport PYTHONPATH=$PYTHONPATH:./XRM\n```\n\nWe first run XRM and store the held-out predictions for the training set as well as the inferred group labels for the validation set. For more details, checkout the instructions in the [XRM repo](https://github.com/facebookresearch/XRM). As an example, this is how it can be done for the Waterbirds dataset:\n\n```bash\npython main.py --phase 1 --datasets Waterbirds --group_labels no --algorithm XRM --out_dir ./phase_1_results --num_hparams_combs 10 --num_seeds 1 --slurm_partition <your_slurm_partition>\n```\n\nTo run the MAT algorithm:\n\n```bash\npython main.py --phase 2 --datasets Waterbirds --group_labels yes --algorithm MAT --out_dir ./phase_2_results --phase_1_dir ./phase_1_results --num_hparams_combs 10 --num_seeds 1 --slurm_partition <your_slurm_partition>\n```\n\nTo read the results:\n- Model selection using the best 'va_wga', i.e., validation worst group accuracy (ground-truth annotations)\n```bash\npython XRM/read_results.py --dir phase_2_results --datasets Waterbirds --algorithms MAT --group_labels yes --selection_criterion va_wga\n```\n- Model selection using the best 'va_gi_wga', i.e., validation worst group accuracy (XRM-inferred annotations)\n```bash\npython XRM/read_results.py --dir phase_2_results --datasets Waterbirds --algorithms MAT --group_labels yes --selection_criterion va_gi_wga\n```\n\n## License\n\nThis source code is released under the CC-BY-NC license, included [here](LICENSE).\n\n## Citation\n\nIf you make use of our work or code, please cite this work :)\n```\n@article{bayat2024pitfalls,\n  title={The Pitfalls of Memorization: When Memorization Hurts Generalization},\n  author={Bayat, Reza and Pezeshki, Mohammad and Dohmatob, Elvis and Lopez-Paz, David and Vincent, Pascal},\n  journal={arXiv preprint arXiv:2412.07684},\n  year={2024}\n}\n```\n",
    "readme_length": 3418
  },
  {
    "name": "SKD-BNN",
    "full_name": "Wzixin/SKD-BNN",
    "description": "The pytorch implementation of our paper : Self-Knowledge Distillation enhanced Binary Neural Networks using Underutilized Information.",
    "stars": 38,
    "forks": 0,
    "language": "Python",
    "url": "https://github.com/Wzixin/SKD-BNN",
    "topics": [],
    "created_at": "2023-04-15T14:51:48Z",
    "updated_at": "2025-09-14T20:03:06Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# SKD-BNN\n\nThis project is the PyTorch implementation of our paper : Self-knowledge distillation enhanced binary neural networks derived from underutilized information\n\n![image](Graphical-Abstract.png)\n\n**Dependencies**\n\n- Ubuntu == 18.04\n- GPU == NVIDIA V100\n- GPU Driver == 460.106.00\n- CUDA == 11.2.2\n- cuDNN == 8.2.1\n- Python == 3.8\n- Pytorch == 1.9.1\n- Torchvision == 0.10.0\n\n**Accuracy** \n\nMNIST:\n|   Model   | Bit-Width (W/A) | Top-1 Acc. (%) |\n| --------- | --------------- | ------------ |\n|   3-MLP   | 1 / 1           | 98.78        |\n\nCIFAR-10:\n|   Model   | Bit-Width (W/A) | Top-1 Acc. (%) |\n| --------- | --------------- | ------------ |\n| VGG-Small | 1 / 1           | 91.4         |\n| ResNet-20 | 1 / 1           | 87.2         |\n| ResNet-18 | 1 / 1           | 93.0         | \n\nImageNet:\n|   Model   | Bit-Width (W/A) | Top-1 Acc. (%) |\n| --------- | --------------- | ------------ |\n| ResNet-18 | 1 / 1           | 59.7         |\n\n**Citation**\n\nIf you find our code useful for your research, please consider citing:\n\nZeng, K., Wan, Z., Gu, H. et al. Self-knowledge distillation enhanced binary neural networks derived from underutilized information. Appl Intell (2024). https://doi.org/10.1007/s10489-024-05444-8\n\n## Wzixin\n\n                                                                                        \n                       ,--.                                           ,--.         ,--. \n      .--.--.      ,--/  /|    ,---,                  ,---,.        ,--.'|       ,--.'| \n     /  /    '. ,---,': / '  .'  .' `\\              ,'  .'  \\   ,--,:  : |   ,--,:  : | \n    |  :  /`. / :   : '/ / ,---.'     \\     ,---,.,---.' .' |,`--.'`|  ' :,`--.'`|  ' : \n    ;  |  |--`  |   '   ,  |   |  .`\\  |  ,'  .' ||   |  |: ||   :  :  | ||   :  :  | | \n    |  :  ;_    '   |  /   :   : |  '  |,---.'   ,:   :  :  /:   |   \\ | ::   |   \\ | : \n     \\  \\    `. |   ;  ;   |   ' '  ;  :|   |    |:   |    ; |   : '  '; ||   : '  '; | \n      `----.   \\:   '   \\  '   | ;  .  |:   :  .' |   :     \\'   ' ;.    ;'   ' ;.    ; \n      __ \\  \\  ||   |    ' |   | :  |  ':   |.'   |   |   . ||   | | \\   ||   | | \\   | \n     /  /`--'  /'   : |.  \\'   : | /  ; `---'     '   :  '; |'   : |  ; .''   : |  ; .' \n    '--'.     / |   | '_\\.'|   | '` ,/            |   |  | ; |   | '`--'  |   | '`--'   \n      `--'---'  '   : |    ;   :  .'              |   :   /  '   : |      '   : |       \n                ;   |,'    |   ,.'                |   | ,'   ;   |.'      ;   |.'       \n                '---'      '---'                  `----'     '---'        '---'         \n                                                                                        \n                                                                                       \n",
    "readme_length": 2756
  },
  {
    "name": "4d-cbct",
    "full_name": "plesqui/4d-cbct",
    "description": "A deep convolutional neural network (based on the 'U-Net') to enhance the image quality of 4-D Cone Beam CT",
    "stars": 34,
    "forks": 10,
    "language": "Python",
    "url": "https://github.com/plesqui/4d-cbct",
    "topics": [],
    "created_at": "2018-06-03T05:12:28Z",
    "updated_at": "2025-10-17T00:33:49Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# 4d-cbct\nA deep convolutional neural network model (based on the 'U-Net') to enhance the image quality of 4-D Cone Beam CT\n\n## Objective\nIn this project, inspired by the SPARE Challenge (http://sydney.edu.au/medicine/image-x/research/SPARE-Challenge.php), we are investigating the performance of deep learning models to improve the quality of 4-dimensional cone-beam CT images. In particular, we have implemented a deep-convolutional neural network based on the 'U-Net' architecture (Ronneberger et al 2015). The model presented here corresponds to our first prototype. \n\n\n## The Model\n![U-Net](https://github.com/plesqui/4d-cbct/blob/master/U-Net-architecture.png?raw=true \"U-Net\")\n\nThe figure above shows the architecture of the original 2-D U-Net that was implemented for image segmentation tasks (https://arxiv.org/abs/1505.04597). Our model contains the following modifications:\n\n- We have replaced the Maxpooling layers by 2-D Convolutional layers.\n- We have replaced the up-convolution layers by re-size (using nearest neighbours) + 2-D convolutions. This modification is intended to prevent the network from exibiting artifacts typical of deconvolutional layers. A very nice description of this problem can be found here: https://distill.pub/2016/deconv-checkerboard/. \n- Our input/output corresponds to 448 x 448 cbct axial slices. \n\n## The Data\nThe data was provided by the SPARE Challenge. The SPARE challenge is led by Dr Andy Shieh and Prof Paul Keall at the ACRF Image X Institute, The University of Sydney. Collaborators who have contributed to the datasets include A/Prof Xun Jia, Miss Yesenia Gonzalez, and Mr Bin Li from the University of Texas Southwestern Medical Center, and Dr Simon Rit from the Creatis Medical Imaging Research Center.\n\nThe data consisted of 4-Dimensional cone-beam CT images of 12 patients acquired in 1 minute (sparse input data, suffering from high levels of noise and artifacts), and the corresponding high-quality images (complete output data). These data will be released to the public by the organizers of the challenge in the future.\n\n## Preliminary Results (Prototype model)\n![U-Net](https://github.com/plesqui/4d-cbct/blob/master/preliminary.JPG?raw=true \"U-Net\")\n\nThe figure above illustrates the performance of our prototype on images from the validation set. The top-row displays three cone-beam CT slices reconstructed from 1-minute scans (input data). The middle row shows the improvements made by our model (predictions). The bottom row shows the ground-truth (high-quality images).\n\n# Quantitative assessment of the prototype performance\nIn deep learning applications to enhance image data, the mean-square-error loss function (applied on a pixel-by-pixel basis) is often used. However, different groups have shown that the selection of a loss function, more relevant to the imaging-task at hand, can greatly improve the overall performance of the model. For instance, Zhao et al 2015 proposed several alternatives to the mean-square-error loss function for de-noising, super-resolution, and JPEG artifacts removal. The authors proposed a loss function which is a combination of the mean-absolute-error and the structural similarity. Read the study here: https://arxiv.org/abs/1511.08861. \n\nAnother very recent study by Taghanaki et al 2018 showed that a simple network with the proper loss function can outperform more complex architectures (e.g. networks with skip connections) in image segmentation tasks. Read their work here: https://arxiv.org/abs/1805.02798\n\nIn light of these results, we decided to investigate the following research question:\n1) Using the U-Net architecture, what is the optimum loss-fuction for denoising and artifact removal of 4-D cone-beam CT images? \nTo this end, we evaluated the performance of our prototype model with the following loss functions:\n- Loss A: mean-squared error\n- Loss B: mean-absolute error\n- Loss C: structural similarity \n- Loss D: 0.75 * (mean-square error) + 0.25 * (structural similarity)\n\nWe assessed the performance of each trained version of our prototype model by evaluating multiple metrics (mean-square error, mean-absolute error, peak signal-to-noise ratio and structural similarity) on the test dataset (i.e., images of patients that were not shown during training). In particular, we computed these metrics on both the entire image of each patient and also within the patient body only. The patient body on each image was segmented using a region growing algorithm (available on the SimpleITK library for python. The code is available in my repository). The results are shown in the four figures below. Overall, we do observe an improvement in all the image quality metrics with respect to the initial 'un-enhanced' images (referred to as 'Original' in the figures). \n\n![per](https://github.com/plesqui/4d-cbct/blob/master/metrics_eval1.png?raw=true \"Performance assessment\")\n\n![per2](https://github.com/plesqui/4d-cbct/blob/master/metrics_eval2.png?raw=true \"Performance assessment\")\n\n# Future work\nOur prototype was built to improve the quality of the reconstructed images. One limitation of this approach is that the performance of the model will depend on the quality/artifacts present on the input images. Such quality of inputs also is sensitive to the method applied to reconstruct the measured projection data. To overcome this limitation, and to generalize our model as much as possible, we are investigating the following research question:\n\n2) Can we build a deep learning model that improves the quality of the measured projection data (i.e., the sinograms)? How does the performance of such model compares to the performance of our current prototype?\n",
    "readme_length": 5685
  },
  {
    "name": "Components-Loss",
    "full_name": "ifnspaml/Components-Loss",
    "description": "Components loss for neural networks in mask-based speech enhancement",
    "stars": 33,
    "forks": 7,
    "language": "Python",
    "url": "https://github.com/ifnspaml/Components-Loss",
    "topics": [],
    "created_at": "2019-08-06T12:34:21Z",
    "updated_at": "2024-10-23T09:15:55Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# Components Loss\n\nThese scripts are referring to the paper \"Components Loss for Neural Networks in Mask-Based Speech Enhancement\". In this repository, we provide the source code for training the mask-based speech enhancement convolutional neural networks (CNNs) using our proposed components loss (CL), which includes both 2 components loss (2CL) and 3 components loss (3CL). The corresponding test code is also offered.\n\nThe code was written by Ziyi Xu and with the help from Ziyue Zhao and Samy Elshamy.\n\n\n## Introduction\n\nWe propose a novel components loss (CL) for the training of neural networks for mask-based speech enhancement. During the training process, the proposed CL offers separate control over preservation of the speech component quality, suppression of the residual noise component power, and preservation of a naturally sounding residual noise component. We obtain a better and more balanced performance in almost all employed instrumental quality metrics over the baseline losses, the latter comprising the conventional mean squared error (MSE) loss function and also auditory-related loss functions, such as the perceptual evaluation of speech quality (PESQ) loss and the recently proposed perceptual weighting filter loss.\n\n## Prerequisites\n\n- [Matlab](https://www.mathworks.com/) 2014a or later\n- [Python](https://www.python.org/) 3.6\n- CPU or NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-toolkit) 9.0 [CuDNN](https://developer.nvidia.com/cudnn) 7.0.5\n\n\n## Getting Started\n\n### Installation\n\n- Install [TensorFlow](https://www.tensorflow.org/) 1.5.0 and [Keras](https://www.tensorflow.org/) 2.1.4\n- Some Python packages need to be installed, please see detailed information in the Python scripts (e.g. numpy, scipy.io, and sklearn)\n- Install [Matlab](https://www.mathworks.com/)\n\n### Datasets\n\nNote that in this project the clean speech signals are taken from the [Grid corpus](https://doi.org/10.1121/1.2229005) (downsampled to 16 kHz) and noise signals are taken from the [ChiMe-3](https://ieeexplore.ieee.org/abstract/document/7404837/) database.\n\n### Training and validation data preparation\n\n - We use [Matlab](https://www.mathworks.com/) to prepare the input and target magnitude spectra for both training and validation sets.\n - To run the training script, you need:\n1. ```training_input_noisy.mat``` (normalized noisy speech amplitude spectra, with zero mean and unit variance)\n2. ```validation_input_noisy.mat``` (normalized noisy speech amplitude spectra, with zero mean and unit variance)\n3. ```training_pure_noise.mat``` (amplitude spectra of noise component)\n4. ```validation_pure_noise.mat``` (amplitude spectra of noise component)\n5. ```training_clean_speech.mat``` (amplitude spectra of speech component)\n6. ```validation_clean_speech.mat``` (amplitude spectra of speech component)\n- All matrices have the dimensions of L*K (e.g. 1,000,000 *132). K represents the number of input and output frequency bins and is set to 132, and L represents the number of frames.\n- All `.mat` files must be stored in `version 7.3`, using Matlab command `save('filename.mat','variable','-v7.3')` to enable very large data matrix saving.\n- Small examples are placed under the directory: `./ training_data/`. To start your own training, replace these `.mat` files by your own data. More details are in the Python scripts. You can try the training script by using these small examples.\n\n### Train the DNN models\n\n - Run the Python script to train the CNN model with the **proposed 2CL** based on the prepared training/validation data:\n```bash\npython Mask-based_CNN_2CL_training.py\n```\n\n - Run the Python script to train the CNN model with the **proposed 3CL** based on the prepared training/validation data:\n```bash\npython Mask-based_CNN_3CL_training.py\n```\n\n### Test data preparation \n\n - We also use [Matlab](https://www.mathworks.com/) to prepare the input magnitude spectra for test data and to store the phase information for the time-domain signal recovering.\n- To run the test script, you need:\n1. ```test_input_noisy_speech.mat``` (normalized noisy speech amplitude spectra, with zero mean and unit variance using the statistics collected on the training data)\n2. ```test_pure_noise.mat``` (amplitude spectra of noise component, used to generate the _filtered_ noise component, which can be used for white-box based performance measures)\n3. ```test_clean_speech.mat``` (amplitude spectra of speech component, used to generate the _filtered_ speech component, which can be used for white-box based performance measures)\n4. ```test_noisy_speech_unmorm.mat``` (unnormalized noisy speech amplitude spectra, used for predicting enhanced speech)\n- All matrices have the dimensions of L*K (e.g. 1,000 *132) as explained before.\n- All `.mat` files are stored using Matlab command `save('filename.mat','variable')`, which allows to save maximum 2 GB `.mat` file. If you have a very large test data, you also need to store `.mat` files in `-v7.3`, and to modify the corresponding data loading part in the test script.\n- Small examples are placed under the directory: `./ test_data /`. To start your test, replace these `.mat` files by your own data. More details are in the Python scripts. You can try the test script by using these small examples.\n- The output of the scripts include:\n1. ```test_n_tilde.mat.mat``` (_filtered_ noise amplitude spectra)\n2. ```test_s_tilde.mat``` (_filtered_ speech amplitude spectra)\n3. ```test_s_hat.mat``` (_enhanced_ speech amplitude spectra)\n- The _filtered_ noise and speech amplitude spectra are then used to reconstruct the filtered noise and speech time domain signal, which can be used for white-box based performance measures.\n### Test of the trained CNN models\n\n - Run the Python script to test the trained CNN model with the **proposed 2CL** using the prepared test data:\n```bash\npython Mask-based_CNN_2CL_test.py\n```\n\n - Run the Python script to test the trained CNN model with the **proposed 3CL** using the prepared test data:\n```bash\npython Mask-based_CNN_3CL_test.py\n```\n\n### Time-domain signal reconstruction\n\n - The stored test data phase information is used to recover the time domain signal by IFFT with overlap add (OLA).\n \n ## Audio Demos\n - We provide audio demos using files from the test dataset in the presence of pedestrian (PED) noise at 10dB signal-to-noise ratio (SNR) level. The audios include speech from both female and male test speakers. \n - We put the audio demos under the directory: `./Audio_demo/`.\n - We also offer the corresponding audio demos in the format of `.wav` files, and put them under the directory: `./Audio_demo_wav_file/`.\n \n ## Citation\n\nIf you use the losses and/or scripts in your research, please cite\n\n```\n@article{xu2019Comploss,\n  author =  {Z. Xu, S. Elshamy, Z. Zhao and T. Fingscheidt},\n  title =   {{Components Loss for Neural Networks in Mask-Based Speech Enhancement}},\n  journal = {arXiv preprint arXiv: 1908.05087},\n  year =    {2019},\n  month =   {Aug.}\n}\n```\n\n## Acknowledgements\n- The author would like to thank Ziyue Zhao and Samy Elshamy for the advice concerning the construction of these source code in GitHub.\n",
    "readme_length": 7147
  },
  {
    "name": "SIAN",
    "full_name": "rootlu/SIAN",
    "description": "Code and data for ECML-PKDD paper \"Social Influence Attentive Neural Network for Friend-Enhanced Recommendation\"",
    "stars": 33,
    "forks": 11,
    "language": "Python",
    "url": "https://github.com/rootlu/SIAN",
    "topics": [
      "ecml-pkdd-paper",
      "friend-enhanced-recommendation",
      "graph-neural-networks",
      "heterogeneous-graph-neural-network",
      "social-recommendation",
      "wechat"
    ],
    "created_at": "2020-06-15T07:35:14Z",
    "updated_at": "2025-05-09T08:26:51Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# SIAN\nCode and data for ECML-PKDD paper \"[Social Influence Attentive Neural Network for Friend-Enhanced Recommendation](https://yuanfulu.github.io/publication/PKDD-SIAN.pdf)\"\n\n## Requirements\n\n- Python 2.7\n- PyTorch 0.4.1\n- numpy\n- scipy\n- My machine with two GPUs (NVIDIA GTX-1080 *2) and two CPUs (Intel Xeon E5-2690 * 2)\n\n## Description\n\n```\nâ”œâ”€â”€ baselines  # baseline code\nâ”‚Â Â  â”œâ”€â”€ Eval4Baselines.py\n\nâ”œâ”€â”€ code  # Our Model: SIAN\nâ”‚Â Â  â”œâ”€â”€ Attention.py  # attention layer\nâ”‚Â Â  â”œâ”€â”€ DataUtil.py  # data loader \nâ”‚Â Â  â”œâ”€â”€ Evaluation.py  # model evaluation\nâ”‚Â Â  â”œâ”€â”€ FeatureAgg.py  # attentive feature aggregator \nâ”‚Â Â  â”œâ”€â”€ Fusion.py  # feature fusion layer \nâ”‚Â Â  â”œâ”€â”€ HeteInf.py  # the main class for SIAN\nâ”‚Â Â  â”œâ”€â”€ InfluenceProp.py  # social influence coupler \nâ”‚Â Â  â”œâ”€â”€ Logging.py  #log\n\nâ”‚Â Â  â””â”€â”€ trainHeteInf.py  # the main function for SIAN\nâ””â”€â”€ data  # dataset\n    â”œâ”€â”€ Data4Baselines.ipynb  #\n    â”œâ”€â”€ DataProcessor.ipynb\n    â”œâ”€â”€ ItemProfileEmbed.ipynb\n    â”œâ”€â”€ WechatTencent.ipynb\n    â”œâ”€â”€ wxt  # FWD dataset \n    â”‚Â Â  â”œâ”€â”€ biz2id\n    â”‚Â Â  â”œâ”€â”€ biz_profile.npy\n    â”‚Â Â  â”œâ”€â”€ item2id\n    â”‚Â Â  â”œâ”€â”€ item_profile.npy\n    â”‚Â Â  â”œâ”€â”€ user2id\n    â”‚Â Â  â”œâ”€â”€ user_profile.npy\n    â”‚Â Â  â”œâ”€â”€ wxt.att.analysis\n    â”‚Â Â  â”œâ”€â”€ wxt.interaction.graph\n    â”‚Â Â  â”œâ”€â”€ wxt.item.biz\n    â”‚Â Â  â”œâ”€â”€ wxt.social.graph\n    â”‚Â Â  â”œâ”€â”€ wxt.test.rating.712\n    â”‚Â Â  â”œâ”€â”€ wxt.train.rating.712\n    â”‚Â Â  â”œâ”€â”€ wxt.user.biz\n    â”‚Â Â  â””â”€â”€ wxt.val.rating.712\n    â”œâ”€â”€ wxt.ipynb\n    â”œâ”€â”€ yelp  # yelp dataset\n    â”‚Â Â  â”œâ”€â”€ item_profile.npy\n    â”‚Â Â  â”œâ”€â”€ user_profile.npy\n    â”‚Â Â  â”œâ”€â”€ yelp.att.analysis\n    â”‚Â Â  â”œâ”€â”€ yelp.interaction.graph\n    â”‚Â Â  â”œâ”€â”€ yelp.social.graph\n    â”‚Â Â  â”œâ”€â”€ yelp.test.rating.712\n    â”‚Â Â  â”œâ”€â”€ yelp.train.rating.712\n    â”‚Â Â  â””â”€â”€ yelp.val.rating.712\n    â”œâ”€â”€ yelp.ipynb\n\n\nâ”œâ”€â”€ log  # saved log file\nâ”‚Â Â  â”œâ”€â”€ wxt.0.0.6023.0.35225.model\n```\n\n\n\n## Dataset \n\nFWD dataset (i.e., wxt data) can be downloaded from [Google Drive](https://drive.google.com/drive/folders/10R8_ESb4fYW0WLPIBVn3g8si3mNIeqY1?usp=sharing) and [BaiduYun](https://pan.baidu.com/s/1zNagRTvdOwsONAFBtM8IFg) ï¼ˆæå–ç ï¼ši6qyï¼‰\n\n\n\n## Reproducing results in the paper\n\nLoad the saved models in `log/` dir.\n\n\n\n## Training\n\n```\npython trainHeteInf.py  --help\n```\n\n\n\n## Reference\n\n```\n@inproceedings{Yuanfu2020SIAN,\n  title={Social Influence Attentive Neural Network for Friend-Enhanced Recommendation},\n  author={Yuanfu Lu, Ruobing Xie, Chuan Shi, Yuan Fang, Wei Zhang, Xu Zhang, Leyu Lin.}\n  booktitle={Proceedings of ECML-PKDD},\n  year={2020}\n}\n```\n",
    "readme_length": 2427
  },
  {
    "name": "BKDSNN",
    "full_name": "Intelligent-Computing-Research-Group/BKDSNN",
    "description": "This is the official project repository for BKDSNN: Enhancing the Performance of Learning-based Spiking Neural Networks Training with Blurred Knowledge Distillation, which has been accepted by ECCV2024.",
    "stars": 32,
    "forks": 6,
    "language": "Python",
    "url": "https://github.com/Intelligent-Computing-Research-Group/BKDSNN",
    "topics": [],
    "created_at": "2024-07-09T15:25:16Z",
    "updated_at": "2025-10-29T08:14:11Z",
    "homepage": null,
    "license": "N/A",
    "readme": "<div align=\"center\"><h1>&nbsp;BKDSNN: Enhancing the Performance of Learning-based Spiking Neural Networks Training with Blurred Knowledge Distillation</h1></div>\n\n\n<p align=\"center\">\n| <a href=\"http://arxiv.org/\"><b>Paper</b></a> | <a href=\"http://arxiv.org/\"><b>Blog</b></a> |\n</p>\n\n\n<p align=\"center\">\n  <a href=\"https://opensource.org/license/mulanpsl-2-0\">\n    <img src=\"https://img.shields.io/badge/License-MuLan_PSL_2.0-blue.svg\" alt=\"License\">\n  </a>\n  <a href=\"https://github.com/\">\n    <img src=\"https://img.shields.io/badge/Maintained%3F-yes-green.svg\" alt=\"Maintenance\">\n  </a>\n  <a href=\"https://github.com/\">\n    <img src=\"https://img.shields.io/badge/Contributions-welcome-brightgreen.svg?style=flat\" alt=\"Contributions welcome\">\n  </a>\n</p>\n\n\n## Contents\n- [News](#news)\n- [Introduction](#introduction)\n- [Usage](#Usage)\n  - [Dataset](#Dataset)\n  - [Train](#Train)\n  - [Evaluation](#Evaluation) \n\n## News\n\n- [2024/7] Code of BKDSNN is released!\n\n## Introduction\n\n![image](architecture.png)\n\n\nThis is the official project repository for BKDSNN: Enhancing the Performance of Learning-based Spiking Neural Networks Training with Blurred Knowledge Distillation, which has been accepted by ECCV2024.\n\n## Usage\n\n### Dataset\n\n### Train\n\n### Evaluation\n",
    "readme_length": 1260
  },
  {
    "name": "KAA",
    "full_name": "LuckyTiger123/KAA",
    "description": "The code Implementation of the paper â€œKAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks\".",
    "stars": 30,
    "forks": 4,
    "language": "Python",
    "url": "https://github.com/LuckyTiger123/KAA",
    "topics": [],
    "created_at": "2025-01-23T07:50:21Z",
    "updated_at": "2025-11-19T11:57:26Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# KAA\nThis is a Pytorch code Implementation of the paper [*KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks*](https://arxiv.org/abs/2501.13456), which is accepted by the ICLR 2025. Specifically, we apply KAN to the scoring functions of attentive GNNs and achieve better theoretical expressiveness and experimental results.\n\n![image-20250219152005843](./PIC/headline.png)\n\n### Installation\n\nWe used the following packages under `Python 3.10`.\n\n```\ntorch 2.4.0\ntorch_geometric 2.5.3\nogb 1.3.6\nnumpy 1.26.3\npandas 2.2.2\n```\n\n### Backbone Models\n\nIn our experiments, we introduced five backbone models: GAT, GLCN, CFGAT, GT, and SAN. Their original scoring functions and the KAA version of the scoring functions are shown in the table below.\n\n![image-20250220161524572](./PIC/scoring_f.png)\n\n### Usage\n\nThe training files for each dataset and task generally follow a unified format. Taking the node classification task on Cora as an example, locate the `train.py` for the corresponding model under the `node_classification/`. It has the following optional parameters:\n\n```shell\nusage: train.py [-h] [--model MODEL] [--hidden_dim HIDDEN_DIM] [--heads HEADS] [--device_num DEVICE_NUM] [--epoch_num EPOCH_NUM] [--lr LR] [--drop_rate DROP_RATE] [--kan_layers KAN_LAYERS] [--grid_size GRID_SIZE] [--spline_order SPLINE_ORDER] [--seed SEED] [--dataset DATASET] [--train_round TRAIN_ROUND] [--file_id FILE_ID]\n\nPyTorch implementation of downstream adaptation.\n\noptions:\n  -h, --help            show this help message and exit\n  --model MODEL         the used model type\n  --hidden_dim HIDDEN_DIM\n                        the hidden dimension\n  --heads HEADS         the head number\n  --device_num DEVICE_NUM\n                        the device number\n  --epoch_num EPOCH_NUM\n                        the epoch number\n  --lr LR               the learning rate\n  --drop_rate DROP_RATE\n                        the dropping rate\n  --kan_layers KAN_LAYERS\n                        the kan layer number\n  --grid_size GRID_SIZE\n                        the grid size of kan\n  --spline_order SPLINE_ORDER\n                        the spline order of kan\n  --seed SEED           the random seed\n  --dataset DATASET     the test dataset\n  --train_round TRAIN_ROUND\n                        the train round number\n  --file_id FILE_ID     the file id\n```\n\nYou can conveniently use the methods described later under **Reproducibility** to obtain the results from the paper.\n\n### Experimental Results\n\n#### Node-Level Tasks\n\nThe performance of each model on node-level tasks is shown in the table below.\n\n![image-20250219170335105](./PIC/node_level.png)\n\n#### Graph-Level Tasks\n\nThe performance of each model on graph-level tasks is shown in the table below.\n\n![image-20250219152931229](./PIC/graph_level.png)\n\n### Reproducibility\n\nTo obtain the results of the KAA version of each model in the experiment, you can run the corresponding commands in the `run.sh`.\n\n```\nbash run.sh\n```\n\n### Citation\n\nYou can cite our paper by following bibtex.\n\n```\n@inproceedings{Fang2025KAAKA,\n  title={KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks},\n  author={Taoran Fang and Tianhong Gao and Chunping Wang and Yihao Shang and Wei Chow and Lei Chen and Yang Yang},\n  booktitle={International Conference on Learning Representations},\n  year={2025}\n}\n```\n\n",
    "readme_length": 3369
  },
  {
    "name": "TFNet-for-Environmental-Sound-Classification",
    "full_name": "Hadryan/TFNet-for-Environmental-Sound-Classification",
    "description": "Learning discriminative and robust time-frequency representations for environmental sound classification: Convolutional neural networks (CNN) are one of the best-performing neural network architectures for environmental sound classification (ESC). Recently, attention mechanisms have been used in CNN to capture the useful information from the audio signal for sound classification, especially for weakly labelled data where the timing information about the acoustic events is not available in the training data, apart from the availability of sound class labels. In these methods, however, the inherent time-frequency characteristics and variations are not explicitly exploited when obtaining the deep features. In this paper, we propose a new method, called time-frequency enhancement block (TFBlock), which temporal attention and frequency attention are employed to enhance the features from relevant frames and frequency bands. Compared with other attention mechanisms, in our method, parallel branches are constructed which allow the temporal and frequency features to be attended respectively in order to mitigate interference from the sections where no sound events happened in the acoustic environments. The experiments on three benchmark ESC datasets show that our method improves the classification performance and also exhibits robustness to noise.",
    "stars": 30,
    "forks": 4,
    "language": null,
    "url": "https://github.com/Hadryan/TFNet-for-Environmental-Sound-Classification",
    "topics": [],
    "created_at": "2019-12-22T21:48:52Z",
    "updated_at": "2025-05-23T07:44:00Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# TFNet-for-Environmental-Sound-Classification\n\nby *Helin Wang*, *Yuexian Zou*, *Dading Chong*, *Wenwu Wang*\n\n## Abstract\nConvolutional neural networks (CNN) are one of the best-performing neural network architectures for environmental sound classification (ESC). Recently, attention mechanisms have been used in CNN to capture the useful information from the audio signal for sound classification, especially for weakly labelled data where the timing information about the acoustic events is not available in the training data, apart from the availability of sound class labels. In these methods, however, the inherent time-frequency characteristics and variations are not explicitly exploited when obtaining the deep features. In this paper, we propose a new method, called time-frequency enhancement block (TFBlock), which temporal attention and frequency attention are employed to enhance the features from relevant frames and frequency bands. Compared with other attention mechanisms, in our method, parallel branches are constructed which allow the temporal and frequency features to be attended respectively in order to mitigate interference from the sections where no sound events happened in the acoustic environments. The experiments on three benchmark ESC datasets show that our method improves the classification performance and also exhibits robustness to noise.\n\n## Introduction\nThis repository is for the ICME 2020 paper (submitted), '[Learning discriminative and robust time-frequency representations for environmental sound classification](https://arxiv.org/pdf/1912.06808.pdf)'.\n\n## Our network\n\n<img src=\"./fig/x1.png\" width=\"100%\" alt=\"TFNet\">\n",
    "readme_length": 1664
  },
  {
    "name": "ConvolutionaNeuralNetworksToEnhanceCodedSpeech",
    "full_name": "linksense/ConvolutionaNeuralNetworksToEnhanceCodedSpeech",
    "description": "In this work we propose two postprocessing approaches applying convolutional neural networks (CNNs) either in the time domain or the cepstral domain to enhance the coded speech without any modification of the codecs. The time domain approach follows an end-to-end fashion, while the cepstral domain approach uses analysis-synthesis with cepstral domain features. The proposed postprocessors in both domains are evaluated for various narrowband and wideband speech codecs in a wide range of conditions. The proposed postprocessor improves speech quality (PESQ) by up to 0.25 MOS-LQO points for G.711, 0.30 points for G.726, 0.82 points for G.722, and 0.26 points for adaptive multirate wideband codec (AMR-WB). In a subjective CCR listening test, the proposed postprocessor on G.711-coded speech exceeds the speech quality of an ITU-T-standardized postfilter by 0.36 CMOS points, and obtains a clear preference of 1.77 CMOS points compared to G.711, even en par with uncoded speech.",
    "stars": 28,
    "forks": 12,
    "language": "Python",
    "url": "https://github.com/linksense/ConvolutionaNeuralNetworksToEnhanceCodedSpeech",
    "topics": [
      "1d-convolution",
      "cnn",
      "gan",
      "generative-adversarial-network",
      "keras",
      "mfcc",
      "post-processing",
      "speech-enhancement",
      "speech-processing",
      "speech-reconstruction",
      "wasserstein-gan"
    ],
    "created_at": "2018-03-21T20:38:30Z",
    "updated_at": "2024-01-24T13:28:55Z",
    "homepage": "https://ansleliu.github.io/CNN.html",
    "license": "BSD 3-Clause \"New\" or \"Revised\" License",
    "readme": "# Convolutional Neural Networks to Enhance Coded Speech\n(Here Part of the project codeï¼Œ**Not for commercial use!!!**) \n  \n**Abstract**â€”Enhancing coded speech suffering from far-end acoustic background noise, quantization noise, and potentially transmission errors, is a challenging task. In this work we propose two postprocessing approaches applying convolutional neural networks (CNNs) either in the time domain or the cepstral domain to enhance the coded speech without any modification of the codecs. The time domain approach follows an end-to-end fashion, while the cepstral domain approach uses analysis-synthesis with\ncepstral domain features. The proposed postprocessors in both domains are evaluated for various narrowband and wideband speech codecs in a wide range of conditions. The proposed postprocessor improves speech quality (PESQ) by up to 0.25 MOS-LQO points for G.711, 0.30 points for G.726, 0.82 points for G.722, and 0.26 points for adaptive multirate wideband codec(AMR-WB). In a subjective CCR listening test, the proposed postprocessor on G.711-coded speech exceeds the speech quality of an ITU-T standardized postfilter by 0.36 CMOS points, and obtains a clear preference of 1.77 CMOS points compared to G.711, even en par with uncoded speech.\n\n**Index Termsâ€”convolutional neural networks, speech codecs, speech enhancement.**\n\nIf you use **Convolutional Neural Networks to Enhance Coded Speech** in your research, please cite:\n```bibtex\n@article{cnn2codedspeech,\n  title={Convolutional Neural Networks to Enhance Coded Speech},\n  author={Zhao, Ziyue and Liu, Huijun and Fingscheidt, Tim},\n  journal={Transactions on Audio, Speech and Language Processing},\n  year={2018}\n}\n```\n\n<p align=\"center\">\n<img src=\"https://github.com/ansleliu/ConvolutionaNeuralNetworksToEnhanceCodedSpeech/blob/master/CNN2EnhancedSpeech.PNG\" />\n</p>\n\n",
    "readme_length": 1852
  },
  {
    "name": "SRCNN-anime",
    "full_name": "TanakitInt/SRCNN-anime",
    "description": "A Super-Resolution Convolutional Neural Network builds for artwork, anime, and illustration. Senior Project - Artwork Enlargement and Quality Improvement using Machine Learning. ICITEE 2021 - Enhancement of Anime Imaging Enlargement Using Modified Super-Resolution CNN.",
    "stars": 28,
    "forks": 2,
    "language": "Python",
    "url": "https://github.com/TanakitInt/SRCNN-anime",
    "topics": [
      "anime",
      "cnn",
      "computer-vision",
      "convolutional-neural-networks",
      "deep-learning",
      "denoising",
      "image-processing",
      "keras",
      "machine-learning",
      "python",
      "srcnn",
      "super-resolution",
      "tensorflow",
      "upscaling",
      "waifu2x"
    ],
    "created_at": "2020-12-14T01:58:13Z",
    "updated_at": "2025-06-13T10:26:41Z",
    "homepage": "https://arxiv.org/abs/2110.02321",
    "license": "MIT License",
    "readme": "# SRCNN-anime\n\n#### A Modified Super-Resolution Convolutional Neural Network (m-SRCNN) build for artwork, anime, and illustration.  \n\n#### ICITEE 2021 Accepted in JSCI11 Special Session - Enhancement of Anime Imaging Enlargement Using Modified Super-Resolution CNN\n\n![Dev Banner](sample/contents/SRCNN-anime_Banner.png)\n\n#### For references and details please scroll down\n\nA 4th year Senior Project Github repository for  \n\"Artwork Enlargement and Quality Improvement using Machine Learning\"\n\n<img src=\"sample/contents/IPDL_Icon_2021_Black.png\" width=\"500\">  \n\n[Image Processing and Deep Learning Laboratory (IPDL Lab)](http://prip.it.kmitl.ac.th/)  \nFaculty of Information Technology, King Mongkut's Institute of Technology Ladkrabang  \n\nTanakit Intaniyom - [TanakitInt](https://github.com/TanakitInt)  \nWarinthorn Thananporn - [TIVOLI777](https://github.com/TIVOLI777)   \n\nProfessor :  \nAsst. Prof. Dr. Kuntpong Woraratpanya - [Google Scholar](https://scholar.google.com/citations?user=q13vj6UAAAAJ&hl=en)\n\nDuration : 11 February 2020 - 14 January 2021 (Senior Project) - 8 September 2021 (Paper)  \n\nPublic Release date : 14 January 2021  \n\nPaper Release date : 7 October 2021  \nhttps://arxiv.org/abs/2110.02321  \n\n<img src=\"sample/contents/ABOARD_Club_Banner_2021_Colorful-Black.png\" width=\"500\">  \n\nSpecial thanks for Sample images :  \n[Anime Cosplay and Boardgame Club](https://www.facebook.com/AniBoardIT)  \n\nVideo Presentation:  \nhttps://youtu.be/tqI4JqqG0Yk  \n\nPDF Powerpoint Presentation:  \nhttps://drive.google.com/file/d/1_JO13a_-Afj_UnDLnbtQcHtDuD8_0z3n/view?usp=sharing  \n\n## I'm interested in this project!  \n\n<img src=\"sample/contents/Giyuu-Me-No-friend-computer-Meme.png\" width=\"500\"> \n\n#### Buy me a coffee! â˜• (Thank you very much!) [Paypal](https://www.paypal.me/TanakitInt)\n\nIf you interested in this project, feel free to contact me at [Email at my GitHub Profile](https://github.com/TanakitInt/) or [Twitter](https://twitter.com/TanakitInt)  \nFor any education purposes, you can directly use my [GitHub repository name](https://github.com/TanakitInt/SRCNN-anime) as reference.  \nFor any other purposes, such as commercial product, please contact me before using any of this project.  \n\n## Issue(s), Bug(s) report, etc...\n\nWe welcome you to report any bug(s) or issue(s).  \nWe're appreciated in your finding! \nYou can directly raise the issue(s) in this GitHub repository or contact me at [Email at my GitHub Profile](https://github.com/TanakitInt/) or [Twitter](https://twitter.com/TanakitInt) \n\n## Simple Diagram  \nFor more detailed diagram, [Click here](Diagram/figures)\n\n<img src=\"Diagram/Simple.png\" width=\"600\">\n\nMore Results from experiment  \n[Click here for more experiment samples](sample/)\n\n### Sample Comparison with waifu2x  \n\n![waifu2x-compare](Diagram/figures/fig_10a_v2.png)  \n\n### Real world example (Default Settings)\n\n![miyagami-san](Diagram/figures/fig_9.png)\n\n## Train your own model  \n\n### Input Output Comparisons  \n\n[Click here for more Input comparisons](sample/input) \n\n[Click here for more Output comparisons](sample/output) \n\n![InOut-Compare](Diagram/figures/fig_8_v2.png)\n\n### Download pre-trained weights (.h5 files)  \n[Click here to Download](_model)  \n\nThere are 4 models seperated which are:  \n- **SRCNN original up bicubic** - Original SRCNN trained with **bicubic** upscaled datasets  \n- **SRCNN original up bilinear** - Original SRCNN trained with **bilinear** upscaled datasets  \n- **m-SRCNN up bicubic** - Our m-SRCNN trained with **bicubic** upscaled datasets  \n- **m-SRCNN up bilinear (Best model)** - Our m-SRCNN trained with **bilinear** upscaled datasets  \n\n### INSTALL PYTHON PACKAGE\n`` 0_PYHON_3_PACKAGE_INSTALL.bat ``\n\n### PREPARE DATA  \n**Input your own data in dataset folder ``dataset/original/`` (Training set) and ``dataset/test/`` (Validation set) first!**   \n(Split train-test as your own wish, Recommended : 80/20)  \n\n#### **Prepare data quick start**  \n`` 1_PREPARE_DATA_QUICK_START.bat ``  \n\n### TRAINING  \n\n#### **Training quick start**  \n`` 2_TRAINING_QUICK_START.bat ``  \n\n### PREDICTION\n\n**Please input your image at `` user-input/ `` folder, the final output will be at `` user-output/ ``**\n\n#### **For prediction quick start**  \n`` 3_PREDICTION_QUICK_START.bat ``\n\n#### **If you have reference for high resolution image**\nFor model testing, we need to have original high resolution for result comparison.  \n\nIf you have reference for high resolution image (Ground Truth),  \nplace it at `` input/ `` folder and rename to `` 1-ref.png ``.  \nMake sure it's same resolution as output.    \n\n### POST-PROCESSING\n\n#### **For image denoising**\n`` 4_IMG_POST_PROCESSING.bat ``\n\n### ADDITIONAL FEATURE\n\n### Feature Comparisons  \n\n[Click here for more Feature comparison](Diagram/figures/fig_8_v2.png)\n\n#### **Settings**\nSee `` Diagram/figures/fig_5_Program_Diagram_-_Framework_(Revised)_v5.png `` for usage. \n[Click here](Diagram/figures/fig_5_Program_Diagram_-_Framework_(Revised)_v5.png)\n\nPlease set the settings at `` settings/ ``\n\n`` settings_2-passes.txt ``   \nFor Double Enhancement, 0 or 1. Default 0.  \n  \n`` settings_2-passes-denoise-as-input.txt ``   \nFor Double Enhancement input, 0 or 1. Default 1.  \n  \n`` settings_bicubic.txt ``   \nFor Bicubic scale enlargement input, possitive float. Default 2.  \n  \n`` settings_bilateral_filter.txt ``   \nFor Enhancement bilateral filter, float. Default 50.  \n  \n`` settings_fastNlMeans_filter.txt ``   \nFor Enhancement denoise filter, possitive interger or zero. Default 7.  \n  \n`` settings_final_bilateral_filter.txt ``   \nFor Double Enhancement bilateral filter, float. Default 100.  \n  \n`` settings_final_fastNlMeans_filter.txt ``   \nFor Double Enhancement denoise filter, possitive interger. Default 14.  \n   \n`` settings_final_medianblur_filter.txt ``   \nFor Double Enhancement Median Blur filter, possitive odd interger. Default 1.  \n  \n`` settings_updown.txt ``  \nFor Double Enlargement (Upsampling nx and 2x and Downsampling to n/2x), 0 or 1. Default 0.  \n\n`` settings_updown-denoise-as-input.txt ``  \nFor Double Enlargement (updown) input, 0 or 1. Default 1.  \n\n#### **Enlargement and Enhancement - DEFAULT RECOMMENDED**  \n`` 10_1-PASS_SLOW.bat `` For Slow mode  \n`` 11_1-PASS_EXPRESS.bat `` For Express mode  \n\n#### **Image Enhancement Only**\n`` 12_1-PASS_ENHANCEMENT_ONLY_EXPRESS.bat ``  \n\n#### **Double Enhancement - MORE ENHANCEMENT**  \n`` 20_2-PASSES_SLOW.bat `` For Slow mode  \n`` 21_2-PASSES_EXPRESS.bat `` For Express mode  \n\n#### **Double Enlargement - BETTER RESULTS FOR LOWER RESOLUTION IMAGE**  \n`` 30_UPDOWN_SLOW.bat `` For Slow mode  \n`` 31_UPDOWN_EXPRESS.bat `` For Express mode  \n\n#### **Reference tests** \n![ref-test](Diagram/figures/fig_10_v2.png)\n\n### Hardware, Software and Limitation\nTraining Time : 2 Hours (for each single model)  \nTraining Epoch : 50\n\n - Hardware  \nCPU = Intel Core i5-11400F  \nGPU = Nvidia GeForce GTX 750Ti  \nRAM = 16 GB  \nSSD = 480 GB  \n\n - Core Software  \ntensorflow==2.2.0  \nCUDA==10.1.243  \ncuDNN==7.6.5  \npython==3.7.9  \n\n - Python 3.7.9 used Package  \nkeras==2.4.3  \nopencv-python==4.4.0.44  \nnumpy==1.19.2  \nmatplotlib==3.3.2  \nscikit-image==0.17.2  \nh5py==2.10.0  \n\n - Other  \nGPUtil==1.4.0  \npydotplus==2.0.2  \n\n### Training, Validation, and Testing Datasets we used\n\nNico-illust : https://nico-opendata.jp/en/seigadata/index.html  \n\n### Project References\n\n- Based Paper : Image Super-Resolution Using Deep Convolutional Networks  \nhttps://arxiv.org/abs/1501.00092  \nhttp://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html\n\n- Projects in Machine Learning : Beginner To Professional  \nhttps://www.udemy.com/course/machine-learning-for-absolute-beginners/  \nhttps://medium.com/datadriveninvestor/using-the-super-resolution-convolutional-neural-network-for-image-restoration-ff1e8420d846\n\n### Github Repository References\n\n- waifu2x    \nhttps://github.com/nagadomi/waifu2x  \nhttps://github.com/lltcggie/waifu2x-caffe  \n\n- Code   \nhttps://github.com/MarkPrecursor/SRCNN-keras  \nhttps://github.com/rezaeiii/SRCNN  \nhttps://github.com/Maximellerbach/Image-Processing-using-AI  \nhttps://github.com/tegg89/SRCNN-Tensorflow \n\n### Footnote\n\n**SRCNN-anime Project was made by this GitHub owner so do not use as your own project/work, copyrighted work.** \n\nThanks for the original work anime-style art images from Nikamon Saelim, Apinyarut Manakul, and Patharapan Hongtawee and everyone those who contribute and support to this project.\n\n![reveal](sample/IMG_0522.jpg)  \n\n\n\n",
    "readme_length": 8438
  },
  {
    "name": "bert-enhancer",
    "full_name": "khanhlee/bert-enhancer",
    "description": "A Transformer Architecture Based on BERT and 2D Convolutional Neural Network to Identify DNA Enhancers from Sequence Information",
    "stars": 25,
    "forks": 13,
    "language": "Python",
    "url": "https://github.com/khanhlee/bert-enhancer",
    "topics": [],
    "created_at": "2020-12-19T03:20:17Z",
    "updated_at": "2025-05-21T03:11:02Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# Bert-Enhancer\nA transformer architecture based on BERT and 2D convolutional neural network to identify DNA enhancers from sequence information\n\nRecently, language representation models have drawn a lot of attention in the natural language processing field due to their remarkable results. Among them, bidirectional encoder representations from transformers (BERT) has proven to be a simple, yet powerful language model that achieved novel state-of-the-art performance. BERT adopted the concept of contextualized word embedding to capture the semantics and context of the words in which they appeared. In this study, we present a novel technique by incorporating BERT-based multilingual model in bioinformatics to represent the information of DNA sequences. We treated DNA sequences as natural sentences and then used BERT models to transform them into fixed-length numerical matrices. As a case study, we applied our method to DNA enhancer prediction, which is a well-known and challenging problem in this field. We then observed that our BERT-based features improved more than 5â€“10% in terms of sensitivity, specificity, accuracy and Matthews correlation coefficient compared to the current state-of-the-art features in bioinformatics. Moreover, advanced experiments show that deep learning (as represented by 2D convolutional neural networks; CNN) holds potential in learning BERT features better than other traditional machine learning techniques. In conclusion, we suggest that BERT and 2D CNNs could open a new avenue in biological modeling using sequence information.\n\n![Image browser window](figures/flowchart.png)\n\n## Step by step for training model\n### Dependencies\n- Python 3\n- Tensorflow 1.x: https://www.tensorflow.org/\n- BERT: https://github.com/google-research/bert\n\n### Prediction step-by-step:\n### Step 1\nUse \"extract_seq.py\" file to generate seq files from full FASTA file\n- *python extract_seq.py*\n\n### Step 2\nUse command line in \"bert2json.txt\" to train BERT model and extract features\n\n### Step 3\nUse \"jsonl2csv.py\" to transfrom JSON to CSV files:\n- *python jsonl2csv.py json_file csv_file*\n\n### Step 4\nUse \"2D_CNN.py\" to train 2D CNN model from generated CSV files\n\n## Citation\nPlease cite our paper as:\n>Le NQK, Ho QT, Nguyen TTD, and Ou YY (2021) A Transformer Architecture Based on BERT and 2D Convolutional Neural Network to Identify DNA Enhancers from Sequence Information. *Briefings in Bioinformatics.* https://doi.org/10.1093/bib/bbab005.\n",
    "readme_length": 2470
  },
  {
    "name": "Attention-SE.pytorch",
    "full_name": "chanil1218/Attention-SE.pytorch",
    "description": "An Attention-based Neural Network Approach for Single Channel Speech Enhancement",
    "stars": 25,
    "forks": 18,
    "language": "Python",
    "url": "https://github.com/chanil1218/Attention-SE.pytorch",
    "topics": [
      "attention",
      "speech-enhancement"
    ],
    "created_at": "2019-11-18T11:06:47Z",
    "updated_at": "2025-04-28T08:47:58Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# Attention-SE.pytorch\nAN ATTENTION-BASED NEURAL NETWORK APPROACH FOR SINGLE CHANNEL SPEECH ENHANCEMENT\n\n## Requirements\n\n* PYSTOI \n  * Toolbox : [mpariente/pystoi](https://github.com/mpariente/pystoi)\n* PYPESQ\n  * ToolBox : [vBaiCai/python-pesq](https://github.com/vBaiCai/python-pesq)\n* Other\n```python3\ntorch == 1.2.0\nnumpy \nlibrosa\n```\n\n## DataSet\nNoise dataset uses 'MUSAN' dataset\nWe generate noisy mixed data from dataset\n\n|Set|Train|Valid|Test|Test2|Test3|Test4|\n|---|-----|-----|----|-----|-----|-----|\n|Noise|Musan|Musan|Musan|Musan|DEMAND|DEMAND|\n|SNR|0-20dB|0-20dB|0-20dB|-5-0dB|0-20dB|-5-0dB|\n\n#### Musan data set\nSpeech and Noise recording\n- URL : http://www.openslr.org/17/\n- Cites\n```BibTeX\n@misc{musan2015,\n  author = {David Snyder and Guoguo Chen and Daniel Povey},\n  title = {{MUSAN}: {A} {M}usic, {S}peech, and {N}oise {C}orpus},\n  year = {2015},\n  eprint = {1510.08484},\n  note = {arXiv:1510.08484v1}\n}\n```\n\n#### Data Loading Script\nReference URL : \n- https://github.com/sweetcocoa/DeepComplexUNetPyTorch\n- https://github.com/jtkim-kaist/Speech-enhancement\n- https://github.com/chanil1218/DCUnet.pytorch\n\n- load_dataset.py\n```Python3\n# load_dataset\n# Below is how to use data loader\n# data_type = ['val', 'train', 'test', 'test2', 'test3', 'test4']\n\nimport load_dataset from AudioDataset\n\ntrain_dataset = AudioDataset(data_type='train')\ntrain_data_loader = DataLoader(dataset=train_dataset, batch_size=4, collate_fn=train_dataset.collate, shuffle=True, num_workers=4)\n```\n\n- load_single_data.py\n```Python3\n# load_single_data\n# Below is how to use data loader\n# data_type = ['val', 'train', 'test', 'test2', 'test3', 'test4']\n\nimport load_single_data from AudioDataset\n\ntrain_data = AudioDataset(data_type='train', data_name=\"p287_171\")\ntrain_data_loader = DataLoader(dataset=train_data, collate_fn=train_data.collate, shuffle=True, num_workers=4)\n```\n\n\n\n## Attention-based SE Model\nReference : Xiang Hao 'AN ATTENTION-BASED NEURAL NETWORK APPROACH FOR SINGLE CHANNEL\nSPEECH ENHANCEMENT', 2019\nURL : http://lxie.nwpu-aslp.org/papers/2019ICASSP-XiangHao.pdf\n\n\n## Train\nArguments : \n- batch_size : Train batch size, default = 128\n- dropout_p : Attention model's dropout rate, default = 0\n- attn_use : Use Attention model, if it is False, Train with LSTM model.  default = False\n- stacked_encoder : Use Stacked attention model, if it is False, Train with Extanded Attention model. default = False\n- hidden_size : Size of RNN. default = 0\n- num_epochs : Train epochs number. default = 100\n- learning_rate : Training Learning rate. default = 5e-4\n- ck_name : Name with save/load check point. default = 'SEckpt.pt'\n\n```bash\nCUDA_VISIBLE_DEVICES=GPUNUMBERS \\\npython3 train.py --batch_size 128 \\\n                 --dropout_p 0.2\\\n                 --attn_use True \\\n                 --stacked_encoder True\\\n                 --attn_len 5\\\n                 --hidden_size 448\\\n                 --num_epochs 61\\\n                 --ck_name '5_448_stacked_dropout0_2.pt'\n\n# You can check other arguments from the source code.                   \n```\n\n## Test\nTest print mean loss, PESQ, and STOI.\nArguments : \n- batch_size : Train batch size, default = 128\n- dropout_p : Attention model's dropout rate, default = 0\n- attn_use : Use Attention model, if it is False, Train with LSTM model.  default = False\n- stacked_encoder : Use Stacked attention model, if it is False, Train with Extanded Attention model. default = False\n- hidden_size : Size of RNN. default = 0\n- ck_name : Name with load check point. default = 'SEckpt.pt'\n- test_set : Name of data_type\n\n```bash\nCUDA_VISIBLE_DEVICES=GPUNUMBERS \\\npython3 test.py --batch_size 128 \\\n                --dropout_p 0.2\\\n                --attn_use True \\\n                --stacked_encoder True\\\n                --attn_len 5\\\n                --hidden_size 448\\\n                --test_set 'test'\\\n                --ck_name '5_448_stacked_dropout0_2.pt'\n\n# You can check other arguments from the source code.                   \n```\n\n## Single file test\nSingle file test return sample outputs with .wav files.\n- clean.wav : select clean voice data\n- mixed.wav : noisy voice data\n- out.wav : return output from model\nArguments :\n- batch_size : Train batch size, default = 128\n- dropout_p : Attention model's dropout rate, default = 0\n- attn_use : Use Attention model, if it is False, Train with LSTM model.  default = False\n- stacked_encoder : Use Stacked attention model, if it is False, Train with Extanded Attention model. default = False\n- hidden_size : Size of RNN. default = 0\n- ck_name : Name with load check point. default = 'SEckpt.pt'\n- test_set : Name of data_type\n- wav : Name of clean data\n\n```bash\nCUDA_VISIBLE_DEVICES=GPUNUMBERS \\\npython3 test_single.py --batch_size 128 \\\n                       --dropout_p 0.2\\\n                       --attn_use True \\\n                       --stacked_encoder True\\\n                       --attn_len 5\\\n                       --hidden_size 448\\\n                       --test_set 'test'\\\n                       --wav 'p232_238'\n                       --ck_name '5_448_stacked_dropout0_2.pt'\n                      \n\n# You can check other arguments from the source code.                   \n```\n",
    "readme_length": 5187
  },
  {
    "name": "FedConv",
    "full_name": "UCSC-VLAA/FedConv",
    "description": "[TMLR'24] This repository includes the official implementation our paper \"FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning\"",
    "stars": 25,
    "forks": 0,
    "language": "Python",
    "url": "https://github.com/UCSC-VLAA/FedConv",
    "topics": [],
    "created_at": "2022-11-17T15:35:34Z",
    "updated_at": "2025-07-10T11:14:31Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# FedConv\n* **Pytorch implementation for paper:** [FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning](https://arxiv.org/abs/2310.04412)\n* Note that we simulate Federated Learning in one local machine for research usage, and do not involve real communication between different clients.\n\n<div align=\"center\">\n  <img src=\"fedconv_teaser.png\"/>\n</div>\n\n## Usage\n### 0. Installation\n\n- Run `cd FedConv`\n- Install the libraries listed in requirements.txt \n\n### 1. Prepare Dataset \n\nWe provide the data partitions for CIFAR-10, COVID-FL, and iNaturalist datasets. \n\n- CIFAR-10 dataset \n    * Download the three sets of simulated data partitions from [CIFAR-10](https://drive.google.com/file/d/17Dz0u1wRqWfN9yXptTsmTe3mL6fGgIQX/view?usp=sharing)\n    * Put the downloaded cifar10.npy at sub-folder ```data ```\n    \n- COVID-FL dataset\n    * Download the data and partitions file from [COVID-FL](https://drive.google.com/file/d/1BiG30JJ7U2BT0x92DjwfPeLb-uwTHdUV/view?usp=sharing)\n\n- iNaturalist dataset\n    * Download the partition following instructions from [FedScale](https://github.com/SymbioticLab/FedScale/tree/master/benchmark/dataset/inaturalist)\n\n### 2. Set (download) the Pretrained Models\n- We provide our models pretrained from Imagenet-1k\n    - [FedConv-Normal](https://drive.google.com/file/d/16sI242zjpM2grd_gmeeo4QkOEAfcRhDW/view?usp=sharing)\n    - [FedConv-Invert](https://drive.google.com/file/d/1mj53LsN2_a5dRW0hNEBaKt0kaGnfg0tT/view?usp=sharing)\n    - [FedConv-InvertUp](https://drive.google.com/file/d/1JIImj1r2wkgSj-a_y41ovkuh8SmkrBNf/view?usp=sharing)\n- Then put the pretrained model under the sub-folder ```checkpoint```\n\n### 3. Train Model\n- Use the commands below to train models in different datasets\n    - CIFAR-10: ```bash cifar_fedconv.sh```\n    - COVID-FL: ```bash covid_fedconv.sh```\n    - iNatualist: ```bash inat_fedconv.sh```\n\n- All the checkpoints, results, and log files will be saved to the ```--output_dir``` folder, with the final performance saved at log_file.txt \n\n### 4. Trained Models Checkpoint\n- We provide our models trained and validated in the COVID-FL dataset\n    - [FedConv-Normal](https://drive.google.com/file/d/1p8BdYK9n8UlC8Cw6oShHr5CJElvLgPhU/view?usp=sharing)\n    - [FedConv-Invert](https://drive.google.com/file/d/1AaYKJB25Bfb_-ETARR3qTZlJI9a0rOZv/view?usp=sharing)\n    - [FedConv-InvertUp](https://drive.google.com/file/d/1A85XIQSTYikIhWU-J1k1ANKWR52KmblD/view?usp=sharing)\n\n## Additional Notes\n- Some important tags for training setting:  \n    - ```--net_name```: name of models to run. In our works, you can choose models directly from resnet50, vit_small_patch16_224, swin_tiny_patch4_window7_224, convnext_tiny, fedconv_base, fedconv_invert, and fedconv_invertup. \n    - ```--dataset```: we provide implement of CIFAR-10 and COVID-FL in  ```main.py```, iNatualist in  ```main_select.py```\n    - ```--save_model_flag```: set to True if need to save the checkpoints \n    - ```--output_dir```: the output directory where checkpoints/results/logs will be written \n    - ```--E_epoch```: local training epoch E in FL train\n    - ```--max_communication_rounds```: total communication rounds, set 100 in default.\n    - ```--split_type```: type of data partitions, supports [\"split_1\", \"split_2\", \"split_3\"] for CIFAR-10, [\"real_test\"] for COVID-FL and iNatualist.\n    - ```--num_local_clients```: Num of local clients joined in each FL train. -1 (usage of all local clients) for CIFAR-10 and COVID-FL, 25 for iNaturalist.  \n\n- Also refer to the ```main.py``` and ```main_select.py``` for more tags\n\n## Acknowledgments\n- This work is supported by a gift from Open Philanthropy, TPU Research Cloud Program, and Google Cloud Research Credits program.\n- ResNet50, ViT, Swin-Transformer, and ConvNext implementations are based on https://github.com/rwightman/pytorch-image-models\n- Our code is based on https://github.com/Liangqiong/ViT-FL-main\n\n## Citation\n\n```\n@article{xu2024fedconv,\n   title   = {FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning},\n   author  = {Xu, Peiran and Wang, Zeyu and Mei, Jieru and Qu, Liangqiong and Yuille, Alan and Xie, Cihang and Zhou, Yuyin},\n   journal = {TMLR},\n   year    = {2024}\n}\n```\n\n",
    "readme_length": 4272
  },
  {
    "name": "EHNet",
    "full_name": "ododoyo/EHNet",
    "description": "A neural network consist of cnn and lstm for speech enhancement",
    "stars": 24,
    "forks": 10,
    "language": "Python",
    "url": "https://github.com/ododoyo/EHNet",
    "topics": [],
    "created_at": "2018-08-02T13:17:12Z",
    "updated_at": "2024-06-07T03:00:53Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# EHNet\nA neural network consist of cnn and lstm for speech enhancement\n",
    "readme_length": 72
  },
  {
    "name": "ESoinn",
    "full_name": "huagc/ESoinn",
    "description": "Enhanced Self-Organizing Incremental Neural Network implementation with Python 3(E SOINN)",
    "stars": 23,
    "forks": 7,
    "language": "Python",
    "url": "https://github.com/huagc/ESoinn",
    "topics": [
      "clustering",
      "python",
      "python3"
    ],
    "created_at": "2017-08-23T10:08:12Z",
    "updated_at": "2025-06-09T19:25:47Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# ESoinn\nEnhanced Self-Organizing Incremental Neural Network implementation with Python\n\nBased on https://github.com/nkmry/soinn\n\n### Execute demo.py to run the demo\nResult of 3 classes classification\n\n![](result.png)\n",
    "readme_length": 218
  },
  {
    "name": "HIDE",
    "full_name": "yf-li15/HIDE",
    "description": "The pytorch implementation of \"Enhancing Hypergraph Neural Networks with Intent Disentanglement for Session-based Recommendation\" (SIGIR'22)",
    "stars": 23,
    "forks": 2,
    "language": "Python",
    "url": "https://github.com/yf-li15/HIDE",
    "topics": [],
    "created_at": "2022-10-04T10:37:55Z",
    "updated_at": "2025-07-17T12:28:18Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# HIDE\nThis is the pytorch implementation of our SIGIR'22 paper:  \n\nYinfeng Li, Chen Gao, Hengliang Luo, Depeng Jin, Yong Li, [**Enhancing Hypergraph Neural Networks with Intent Disentanglement for Session-based Recommendation**](https://dl.acm.org/doi/10.1145/3477495.3531794), In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'22).\n\nEnvironment: Pytorch == 1.12.1 and Python >= 3.6.8.\n\nIf you use our codes in your research, please cite:\n```\n@inproceedings{li2022enhancing,\n  title={Enhancing Hypergraph Neural Networks with Intent Disentanglement for Session-based Recommendation},\n  author={Li, Yinfeng and Gao, Chen and Luo, Hengliang and Jin, Depeng and Li, Yong},\n  booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  pages={1997--2002},\n  year={2022}\n}\n```\n",
    "readme_length": 912
  },
  {
    "name": "tf-bind-transformer",
    "full_name": "lucidrains/tf-bind-transformer",
    "description": "A repository with exploration into using transformers to predict DNA â†” transcription factor binding",
    "stars": 88,
    "forks": 9,
    "language": "Python",
    "url": "https://github.com/lucidrains/tf-bind-transformer",
    "topics": [
      "artificial-intelligence",
      "attention-mechanism",
      "deep-learning",
      "gene-expression",
      "transcription-factor-binding",
      "transcription-factors",
      "transformers"
    ],
    "created_at": "2021-12-08T00:37:11Z",
    "updated_at": "2025-12-01T16:49:56Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "## Transcription Factor binding predictions with Attention and Transformers\n\nA repository with exploration into using transformers to predict DNA â†” transcription factor binding.\n\n## Install\n\nRun the following at the project root to download dependencies\n\n```bash\n$ python setup.py install --user\n```\n\nThen you must install `pybedtools`  as well as `pyBigWig`\n\n```bash\n$ conda install --channel conda-forge --channel bioconda pybedtools pyBigWig\n```\n\n## Usage\n\n```python\nimport torch\nfrom tf_bind_transformer import AdapterModel\n\n# instantiate enformer or load pretrained\n\nfrom enformer_pytorch import Enformer\nenformer = Enformer.from_hparams(\n    dim = 1536,\n    depth = 2,\n    target_length = 256\n)\n\n# instantiate model wrapper that takes in enformer\n\nmodel = AdapterModel(\n    enformer = enformer,\n    aa_embed_dim = 512,\n    contextual_embed_dim = 256\n).cuda()\n\n# mock data\n\nseq = torch.randint(0, 4, (1, 196_608 // 2)).cuda() # for ACGT\n\naa_embed = torch.randn(1, 1024, 512).cuda()\naa_mask = torch.ones(1, 1024).bool().cuda()\n\ncontextual_embed = torch.randn(1, 256).cuda() # contextual embeddings, including cell type, species, experimental parameter embeddings\n\ntarget = torch.randn(1, 256).cuda()\n\n# train\n\nloss = model(\n    seq,\n    aa_embed = aa_embed,\n    aa_mask = aa_mask,\n    contextual_embed = contextual_embed,\n    target = target\n)\n\nloss.backward()\n\n# after a lot of training\n\ncorr_coef = model(\n    seq,\n    aa_embed = aa_embed,\n    aa_mask = aa_mask,\n    contextual_embed = contextual_embed,\n    target = target,\n    return_corr_coef = True\n)\n```\n\n## Using ESM or ProtAlbert for fetching of transcription factor protein embeddings\n\n```python\nimport torch\nfrom enformer_pytorch import Enformer\nfrom tf_bind_transformer import AdapterModel\n\nenformer = Enformer.from_hparams(\n    dim = 1536,\n    depth = 2,\n    target_length = 256\n)\n\nmodel = AdapterModel(\n    enformer = enformer,\n    use_aa_embeds = True,                            # set this to True\n    aa_embed_encoder = 'esm',                        # by default, will use esm, but can be set to 'protalbert', which has a longer context length of 2048 (vs esm's 1024)\n    contextual_embed_dim = 256\n).cuda()\n\n# mock data\n\nseq = torch.randint(0, 4, (1, 196_608 // 2)).cuda()\ntf_aa = torch.randint(0, 21, (1, 4)).cuda()           # transcription factor amino acid sequence, from 0 to 20\n\ncontextual_embed = torch.randn(1, 256).cuda()\ntarget = torch.randn(1, 256).cuda()\n\n# train\n\nloss = model(\n    seq,\n    aa = tf_aa,\n    contextual_embed = contextual_embed,\n    target = target\n)\n\nloss.backward()\n```\n\n- [ ] add alphafold2\n\n## Context passed in as free text\n\nOne can also pass the context (cell type, experimental parameters) directly as free text, which will be encoded by a text transformer trained on pubmed abstracts.\n\n```python\nimport torch\nfrom tf_bind_transformer import AdapterModel\n\n# instantiate enformer or load pretrained\n\nfrom enformer_pytorch import Enformer\nenformer = Enformer.from_hparams(\n    dim = 1536,\n    depth = 2,\n    target_length = 256\n)\n\n# instantiate model wrapper that takes in enformer\n\nmodel = AdapterModel(\n    enformer = enformer,\n    use_aa_embeds = True,\n    use_free_text_context = True,        # this must be set to True\n    free_text_embed_method = 'mean_pool' # allow for mean pooling of embeddings, instead of using CLS token\n).cuda()\n\n# mock data\n\nseq = torch.randint(0, 4, (2, 196_608 // 2)).cuda() # for ACGT\ntarget = torch.randn(2, 256).cuda()\n\ntf_aa = [\n    'KVFGRCELAA',                  # single protein\n    ('AMKRHGLDNY', 'YNDLGHRKMA')   # complex, representations will be concatted together\n]\n\ncontextual_texts = [\n    'cell type: GM12878 | dual cross-linked',\n    'cell type: H1-hESC'\n]\n\n# train\n\nloss = model(\n    seq,\n    target = target,\n    aa = tf_aa,\n    contextual_free_text = contextual_texts,\n)\n\nloss.backward()\n```\n\n## Binary prediction\n\nFor predicting binary outcome (bind or not bind), just set the `binary_targets = True` when initializing either adapters\n\nex.\n\n```python\nimport torch\nfrom tf_bind_transformer import AdapterModel\nfrom enformer_pytorch import Enformer\n\n# instantiate enformer or load pretrained\n\nenformer = Enformer.from_hparams(\n    dim = 1536,\n    depth = 2,\n    target_length = 256\n)\n\n# instantiate model wrapper that takes in enformer\n\nmodel = AdapterModel(\n    enformer = enformer,\n    use_aa_embeds = True,\n    use_free_text_context = True,\n    free_text_embed_method = 'mean_pool',\n    use_squeeze_excite = True,\n    binary_target = True,                  # set this to True\n    target_mse_loss = False                # whether to use MSE loss with target value\n).cuda()\n\n# mock data\n\nseq = torch.randint(0, 4, (1, 196_608 // 2)).cuda() # for ACGT\nbinary_target = torch.randint(0, 2, (2,)).cuda()    # bind or not bind\n\ntf_aa = [\n    'KVFGRCELAA',\n    ('AMKRHGLDNY', 'YNDLGHRKMA')\n]\n\ncontextual_texts = [\n    'cell type: GM12878 | chip-seq dual cross-linked',\n    'cell type: H1-hESC | chip-seq single cross-linked'\n]\n\n# train\n\nloss = model(\n    seq,\n    target = binary_target,\n    aa = tf_aa,\n    contextual_free_text = contextual_texts,\n)\n\nloss.backward()\n```\n\n## Predicting Tracks from BigWig\n\n```python\nfrom pathlib import Path\nimport torch\nfrom enformer_pytorch import Enformer\n\nfrom tf_bind_transformer import AdapterModel\nfrom tf_bind_transformer.data_bigwig import BigWigDataset, get_bigwig_dataloader\n\n# constants\n\nROOT = Path('.')\nTFACTOR_TF = str(ROOT / 'tfactor.fastas')\nENFORMER_DATA = str(ROOT / 'chip_atlas' / 'sequences.bed')\nFASTA_FILE_PATH = str(ROOT / 'hg38.ml.fa')\nBIGWIG_PATH = str(ROOT / 'chip_atlas')\nANNOT_FILE_PATH = str(ROOT / 'chip_atlas' / 'annot.tab')\n\n# bigwig dataset and dataloader\n\nds = BigWigDataset(\n    factor_fasta_folder = TFACTOR_TF,\n    bigwig_folder = BIGWIG_PATH,\n    enformer_loci_path = ENFORMER_DATA,\n    annot_file = ANNOT_FILE_PATH,\n    fasta_file = FASTA_FILE_PATH\n)\n\ndl = get_bigwig_dataloader(ds, batch_size = 2)\n\n# enformer\n\nenformer = Enformer.from_hparams(\n    dim = 384,\n    depth = 1,\n    target_length = 896\n)\n\nmodel = AdapterModel(\n    enformer = enformer,\n    use_aa_embeds = True,\n    use_free_text_context = True\n).cuda()\n\n# mock data\n\nseq, tf_aa, context_str, target = next(dl)\nseq, target = seq.cuda(), target.cuda()\n\n# train\n\nloss = model(\n    seq,\n    aa = tf_aa,\n    contextual_free_text = context_str,\n    target = target\n)\n\nloss.backward()\n```\n## Data\n\nThe data needed for training is at <a href=\"https://remap.univ-amu.fr/download_page\">this download page</a>.\n\n### Transcription factors for Human and Mouse\n\nTo download the protein sequences for both species, you need to download the remap CRMs bed files, from which all the targets will be extracted, and fastas to be downloaded from Uniprot.\n\nDownload human remap CRMS\n\n```bash\n$ wget https://remap.univ-amu.fr/storage/remap2022/hg38/MACS2/remap2022_crm_macs2_hg38_v1_0.bed.gz\n$ gzip -d remap2022_crm_macs2_hg38_v1_0.bed.gz\n```\n\nDownload mouse remap CRMs\n\n```bash\n$ wget https://remap.univ-amu.fr/storage/remap2022/mm10/MACS2/remap2022_crm_macs2_mm10_v1_0.bed.gz\n$ gzip -d remap2022_crm_macs2_mm10_v1_0.bed.gz\n```\n\nDownloading all human transcription factors\n\n```bash\n$ python script/fetch_factor_fastas.py --species human\n```\n\nFor mouse transcription factors\n\n```bash\n$ python script/fetch_factor_fastas.py --species mouse\n````\n\n## Generating Negatives\n\n### Generating Hard Negatives\n\nFor starters, the `RemapAllPeakDataset` will allow you to load data easily from the full remap peaks bed file for training.\n\nFirstly you'll need to generate the non-peaks dataset by running the following function\n\n```python\nfrom tf_bind_transformer.data import generate_random_ranges_from_fasta\n\ngenerate_random_ranges_from_fasta(\n    './hg38.ml.fa',\n    output_filename = './path/to/generated-non-peaks.bed',    # path to output file\n    context_length = 4096,\n    num_entries_per_key = 1_000_000,                          # number of negative samples\n    filter_bed_files = [\n        './remap_all.bed',                                    # filter out by all peak ranges (todo, allow filtering namespaced to experiment and target)\n        './hg38.blacklist.rep.bed'                            # further filtering by blacklisted regions (gs://basenji_barnyard/hg38.blacklist.rep.bed)\n    ]\n)\n```\n\n### Generating Scoped Negatives - Negatives per Dataset (experiment + target + cell type)\n\nTodo\n\n## Simple Trainer class for fine-tuning\n\nworking fine-tuning training loop for bind / no-bind prediction\n\n```python\nimport torch\nfrom enformer_pytorch import Enformer\n\nfrom tf_bind_transformer import AdapterModel, Trainer\n\n# instantiate enformer or load pretrained\n\nenformer = Enformer.from_pretrained('EleutherAI/enformer-official-rough', target_length = -1)\n\n# instantiate model wrapper that takes in enformer\n\nmodel = AdapterModel(\n    enformer = enformer,\n    use_aa_embeds = True,\n    use_free_text_context = True,\n    free_text_embed_method = 'mean_pool',\n    binary_target = True,\n    target_mse_loss = True,\n    use_squeeze_excite = True,\n    aux_read_value_loss = True     # use auxiliary read value loss, can be turned off\n).cuda()\n\n# pass the model (adapter + enformer) to the Trainer\n\ntrainer = Trainer(\n    model,\n    batch_size = 2,                                   # batch size\n    context_length = 4096,                            # genetic sequence length\n    grad_accum_every = 8,                             # gradient accumulation steps\n    grad_clip_norm = 2.0,                             # gradient clipping\n    validate_every = 250,\n    remap_bed_file = './remap2022_all.bed',           # path to remap bed peaks\n    negative_bed_file = './generated-non-peaks.bed',  # path to generated non-peaks\n    factor_fasta_folder = './tfactor.fastas',         # path to factor fasta files\n    fasta_file = './hg38.ml.fa',                      # human genome sequences\n    train_chromosome_ids = [*range(1, 24, 2), 'X'],   # chromosomes to train on\n    valid_chromosome_ids = [*range(2, 24, 2)],        # chromosomes to validate on\n    held_out_targets = ['AFF4'],                      # targets to hold out for validation\n    experiments_json_path = './data/experiments.json' # path to all experiments data, at this path relative to the project root, if repository is git cloned\n)\n\nwhile True:\n    _ = trainer()\n\n```\n\nworking fine-tuning script for training on new enformer tracks, with cross-attending transcription factor protein embeddings and cell type conditioning\n\n```python\nfrom dotenv import load_dotenv\n\n# set path to cache in .env and unset the next comment\n# load_dotenv()\n\nfrom enformer_pytorch import Enformer\nfrom tf_bind_transformer import AdapterModel, BigWigTrainer\n\n# training constants\n\nBATCH_SIZE = 1\nGRAD_ACCUM_STEPS = 8\n\n# effective batch size of BATCH_SIZE * GRAD_ACCUM_STEPS = 16\n\nVALIDATE_EVERY = 250\nGRAD_CLIP_MAX_NORM = 1.5\n\nTFACTOR_FOLDER = './tfactor.fastas'\nFASTA_FILE_PATH = './hg38.ml.fa'\n\nLOCI_PATH = './sequences.bed'\nBIGWIG_PATH = './bigwig_folder'\nANNOT_FILE_PATH =  './experiments.tab'\nTARGET_LENGTH = 896\n\nTRAIN_CHROMOSOMES = [*range(1, 24, 2), 'X'] # train on odd chromosomes\nVALID_CHROMOSOMES = [*range(2, 24, 2)]      # validate on even\n\nHELD_OUT_TARGET = ['SOX2']\n\n# instantiate enformer or load pretrained\n\nenformer = Enformer.from_pretrained('EleutherAI/enformer-official-rough', target_length = TARGET_LENGTH)\n\n# instantiate model wrapper that takes in enformer\n\nmodel = AdapterModel(\n    enformer = enformer,\n    use_aa_embeds = True,\n    use_free_text_context = True,\n    free_text_embed_method = 'mean_pool',\n    aa_embed_encoder = 'protalbert'\n).cuda()\n\n\n# trainer class for fine-tuning\n\ntrainer = BigWigTrainer(\n    model,\n    loci_path = LOCI_PATH,\n    bigwig_folder_path = BIGWIG_PATH,\n    annot_file_path = ANNOT_FILE_PATH,\n    target_length = TARGET_LENGTH,\n    batch_size = BATCH_SIZE,\n    validate_every = VALIDATE_EVERY,\n    grad_clip_norm = GRAD_CLIP_MAX_NORM,\n    grad_accum_every = GRAD_ACCUM_STEPS,\n    factor_fasta_folder = TFACTOR_FOLDER,\n    fasta_file = FASTA_FILE_PATH,\n    train_chromosome_ids = TRAIN_CHROMOSOMES,\n    valid_chromosome_ids = VALID_CHROMOSOMES,\n    held_out_targets = HELD_OUT_TARGET\n)\n\n# do gradient steps in a while loop\n\nwhile True:\n    _ = trainer()\n```\n\n## Resources\n\nIf you are low on GPU memory, you can save by making sure the protein and contextual embeddings are executed on CPU\n\n```bash\nCONTEXT_EMBED_USE_CPU=1 PROTEIN_EMBED_USE_CPU=1 python train.py\n```\n\n## Data\n\nTranscription factor dataset\n\n```python\nfrom tf_bind_transformer.data import FactorProteinDataset\n\nds = FactorProteinDataset(\n    folder = 'path/to/tfactor/fastas'\n)\n\n# single factor\n\nds['ETV1'] # <seq>\n\n# multi-complexes\n\nds['PAX3-FOXO1'] # (<seq1>, <seq2>)\n\n```\n\n## Preprocessing (wip)\n\nget a copy of hg38 blacklist bed file from calico\n\n```bash\n$ gsutil cp gs://basenji_barnyard/hg38.blacklist.rep.bed ./\n```\n\nusing bedtools to filter out repetitive regions of the genome\n\n```bash\n$ bedtools intersect -v -a ./remap2022_all_macs2_hg38_v1_0.bed -b ./hg38.blacklist.rep.bed > remap2022_all_filtered.bed\n```\n\n## Caching\n\nDuring training, protein sequences and contextual strings are cached to `~/.cache.tf.bind.transformer` directory. If you would like to make sure the caching is working, you just need to run your training script with `VERBOSE=1`\n\nex.\n\n```bash\n$ VERBOSE=1 python train.py\n```\n\nYou can also force a cache clearance\n\n```bash\n$ CLEAR_CACHE=1 python train.py\n```\n\n## Todo\n\n- [x] ESM and AF2 embedding fetching integrations\n- [x] HF transformers integration for conditioning on free text\n- [x] allow for fine-tuning layernorms of Enformer easily\n- [x] add caching for external embeddings\n- [x] figure out a way for external models (ESM, transformers) to be omitted from state dictionary on saving (use singletons)\n- [x] take care of caching genetic sequences when enformer is frozen\n- [x] offer a fully transformer variant with cross-attention with shared attention matrix and FiLM conditioning with contextual embed\n- [x] also offer using pooled genetic / protein sequence concatted with context -> project -> squeeze excitation type conditioning\n- [x] use checkpointing when fine-tuning enformer\n- [x] take care of prepping dataframe with proper chromosome and training / validation split\n- [x] use basenji blacklist bed file for filtering out rows in remap\n- [x] filter remap dataframe based on tfactor fasta folder\n- [x] filter remap dataframe with hg38 blacklist\n- [x] handle targets with modifications from remap with all peaks (underscore in name)\n- [x] grad clipping\n- [x] add a safe initialization whereby rows of dataframe with targets not found in the tfactor fasta folder will be filtered out\n- [x] add accuracy metric to fine tune script\n- [x] master trainer class that handles both training / validation splitting, efficient instantiation of dataframe, filtering etc\n- [x] write a simple trainer class that takes care of the training loop\n- [x] create faster protein and context embedding derivation by optionally moving model to gpu and back to cpu when done\n- [x] use ProtTrans for longer context proteins, look into AF2\n- [x] make protalbert usable with one flag\n- [x] log auxiliary losses appropriately (read value)\n- [x] write fine-tuning script for finetuning on merged genomic track(s) from remap\n- [ ] support for custom transformers other than enformer\n- [ ] warmup in training loop\n- [ ] mixed precision\n- [ ] use wandb for experiment tracking\n- [ ] cleanup tech debt in data and protein_utils\n- [ ] explore protein model fine-tuning of layernorm\n- [ ] auto-auroc calc\n- [ ] k-fold cross validation\n- [ ] output attention intermediates (or convolution output for hypertransformer), for interpreting binding site\n- [ ] use prefect.io to manage downloading of tfactors fastas, remap scoped negative peaks, blacklist filtering etc\n\n## Appreciation\n\nThis work was generously sponsored by <a href=\"https://github.com/jeffhsu3\">Jeff Hsu</a> to be done completely open sourced.\n\n## Citations\n\n```bibtex\n@article {Avsec2021.04.07.438649,\n    author  = {Avsec, {\\v Z}iga and Agarwal, Vikram and Visentin, Daniel and Ledsam, Joseph R. and Grabska-Barwinska, Agnieszka and Taylor, Kyle R. and Assael, Yannis and Jumper, John and Kohli, Pushmeet and Kelley, David R.},\n    title   = {Effective gene expression prediction from sequence by integrating long-range interactions},\n    elocation-id = {2021.04.07.438649},\n    year    = {2021},\n    doi     = {10.1101/2021.04.07.438649},\n    publisher = {Cold Spring Harbor Laboratory},\n    URL     = {https://www.biorxiv.org/content/early/2021/04/08/2021.04.07.438649},\n    eprint  = {https://www.biorxiv.org/content/early/2021/04/08/2021.04.07.438649.full.pdf},\n    journal = {bioRxiv}\n}\n```\n\n```bibtex\n@misc{yao2021filip,\n    title   = {FILIP: Fine-grained Interactive Language-Image Pre-Training},\n    author  = {Lewei Yao and Runhui Huang and Lu Hou and Guansong Lu and Minzhe Niu and Hang Xu and Xiaodan Liang and Zhenguo Li and Xin Jiang and Chunjing Xu},\n    year    = {2021},\n    eprint  = {2111.07783},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{tay2020hypergrid,\n    title   = {HyperGrid: Efficient Multi-Task Transformers with Grid-wise Decomposable Hyper Projections},\n    author  = {Yi Tay and Zhe Zhao and Dara Bahri and Donald Metzler and Da-Cheng Juan},\n    year    = {2020},\n    eprint  = {2007.05891},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{lowe2021logavgexp,\n    title   = {LogAvgExp Provides a Principled and Performant Global Pooling Operator},\n    author  = {Scott C. Lowe and Thomas Trappenberg and Sageev Oore},\n    year    = {2021},\n    eprint  = {2111.01742},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@article{10.1093/nar/gkab996,\n    author  = {Hammal, Fayrouz and deÂ Langen, Pierre and Bergon, AurÃ©lie and Lopez, Fabrice and Ballester, Benoit},\n    title   = \"{ReMap 2022: a database of Human, Mouse, Drosophila and Arabidopsis regulatory regions from an integrative analysis of DNA-binding sequencing experiments}\",\n    journal = {Nucleic Acids Research},\n    issn    = {0305-1048},\n    doi     = {10.1093/nar/gkab996},\n    url     = {https://doi.org/10.1093/nar/gkab996},\n    eprint  = {https://academic.oup.com/nar/article-pdf/50/D1/D316/42058627/gkab996.pdf},\n}\n```\n",
    "readme_length": 18311
  },
  {
    "name": "TEPIC",
    "full_name": "SchulzLab/TEPIC",
    "description": "Annotation of genomic regions using transcription factor binding sites and epigenetic data",
    "stars": 41,
    "forks": 9,
    "language": "Python",
    "url": "https://github.com/SchulzLab/TEPIC",
    "topics": [],
    "created_at": "2016-03-15T13:37:30Z",
    "updated_at": "2025-09-10T10:46:21Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# TEPIC (version 2.2)\n-------\nTEPIC offers workflows for the prediction and analysis of Transcription Factor (TF) binding sites including:\n* TF affinity computation in user provided regions\n* Computation of continuous and discrete TF-gene scores \n* Linear regression analysis to infer potential key regulators (INVOKE)\n* Logistic regression analysis to suggest regulators related to gene expression differences between samples (DYNAMITE)\n\nA graphical overview on the workflows of TEPIC is shown below. Blue font indicates input, black italic font indicates output.\n![](docs/TEPIC_Workflow.png)\n\n## News\n06.02.2020: Our recently introduced TEPIC-extension supporting regulatory sites from chromatin capture experiments to estimate the influence of enhancers on genome-wide gene expression is now published in [BMC Epigenetics and Chromatin](https://doi.org/10.1186/s13072-020-0327-0).\n\n08.07.2019: We present a novel feature to include TFBS in regulatory sites determined by chromatin conformation capture data. Using an extended feature space representation, the INVOKE model can investigate the regulatory influence of TFs bound to promoters and enhancers separately.\n\n10.10.2018: TEPIC 2.0 is now published in [Bioinformatics](https://doi.org/10.1093/bioinformatics/bty856).\n\n13.08.2018: In addition to the gene-centric annotation, the functionality for transcript based annotation has been added.\n\n21.05.2018: A new collection of TF motifs is included. They are available in the folder [PWMs/2.1](PWMs/2.1).\n\n03.05.2018: INVOKE now supports computing a F-test to judge the importance of individual features.\n\n17.08.2017: TEPIC TF-gene scores can now be binarisied using background regions provided by the user.\n\n21.06.2017: TEPIC TF-gene scores can be binarisied using a TF and tissue specific affinity cut off. This can be combined with the dynamic networks learner [DREM](http://www.sb.cs.cmu.edu/drem/) to build gene regulatory networks that make appropriate use of time-specific epigenetics data. Further details on this new feature are available in the [description](docs/Description.pdf).\n\n13.06.2017: [DYNAMITE](https://github.com/SchulzLab/TEPIC/tree/master/MachineLearningPipelines/DYNAMITE) our workflow for learning differential TF regulation is now included in the repository.\n\n09.06.2017: Version 2.0 of TEPIC is available.\nWith version 2 of TEPIC, we introduced new features:\n* We extended the set of PSEMs.\n* TF affinities are computed using a C++ implementation of TRAP.\n* Affinities can be normalised by peak length during TF-gene score computation.\n* The length of the PSEMs can be considered in the normalisation.\n* We introduced features for peak length and peak counts.\n* Scaling can be performed in two ways: The original way as proposed in the TEPIC manuscript by directly multiplying\npeak signal and TF affinities or by generating a separate signal feature.\n\nFurther, the repository now includes the code required to learn linear models from TF gene scores to predict gene expression.\nFor further details, please see the [INVOKE](https://github.com/SchulzLab/TEPIC/tree/master/MachineLearningPipelines/INVOKE) section.\n\n\n## Introduction\n*TEPIC* offers a variety of workflows to compute and analyse TF binding site (TFBS) predictions. \nThe core functionality is the fast and efficient annotation of user provided candidate regions with TF affinities using TRAP (1). \nThese predictions are aggregated to TF-gene scores using a window-based assignment strategy.\n\nWithin this aggregation TEPIC offers exponential decay (2) and scaling of TF region scores using an epigenetic signal, e.g. the signal of an open chromatin assay.\nWhile computing TF-gene scores, TEPIC can perform normalisation for region length (optionally correcting for the length of the binding motifs as well)\nand produces separate features for peak length, peak count and/or peak signal. These features can be used in downstream applications, e.g. to determine\nthe influence of chromatin accessiblity on gene expression, without considering detailed information on TF binding. \nIn addition to the continuous TF-affinities, TEPIC offers a discrete assignment of TFs to genes using a TF-specific affinity threshold derived from random genomic sequences that\nshow similar characteristics (GC content, length) as compared to the provided regions. \nFurther details on the score computation are provided in the [description](docs/Description.pdf). \n\n\n*TEPIC* TF-gene scores can be used in several downstream applications to assess the regulatory role of TFs. Three applications are directly supported:\n* Using a linear regression analysis to highlight potential key TFs by predicting gene expression within a sample of interest [MachineLearningPipelines/INVOKE](https://github.com/SchulzLab/TEPIC/tree/master/MachineLearningPipelines/INVOKE)\n* Suggesting regulators that might be linked to gene-expression differences between samples using a logistic regression classifier [MachineLearningPipelines/DYNAMITE](https://github.com/SchulzLab/TEPIC/tree/master/MachineLearningPipelines/DYNAMITE)\n* Generating input for DREM to infer time-point specific transcriptional regulators from temporal epigenetics data [MachineLearningPipelines/EPIC-DREM](https://github.com/SchulzLab/TEPIC/tree/master/MachineLearningPipelines/EPIC-DREM)\n\nDetails on the models are provided in the respective subfolders as well as in the [description](docs/Description.pdf).\nHere, we provide a brief description on the core funtionality of TEPIC, the computation of TF-gene scores. \n\n## Installing TEPIC\nTo run *TEPIC* the following packages/software must be installed:\n* Python (minimum version 2.7)\n* [bedtools](https://github.com/arq5x/bedtools2): Installation instructions for bedtools can be found [here](http://bedtools.readthedocs.io/en/latest/content/installation.html). Please make sure to add\nthe bedtools installation to your path.\n* A C++ compiler supporting openmp to use the parallel implementation of TRAP.\n\nTo compile the C++ version of TRAP and to install possibly missing R-packages for downstream processing execute the script\n\t[Code/compile_TRAP_install_R_packages.sh](Code/compile_TRAP_install_R_packages.sh).\n\nTo use the script [findBackground](Code/findBackground.py), which is necessary to compute TF specific affinity thresholds, the following python libraries are required:\n* numpy\n* scipy\n* twobitreader\n\n## Using TEPIC\nTo start TEPIC, run the script *TEPIC.sh*, located in the folder *Code*.\n\n    ./TEPIC.sh\n\nThe following parameters are required to compute TF affinities in user defined regions:\n* -g The reference genome in plain (uncompressed) FASTA format with Ensembl-style chromosome names (i.e., without \"chr\" prefix). If a \"chr\" prefix is present, use the -j option. \n* -b Regions the user wants to be annotated; chromosome naming compatible to the reference genome file.\n* -o Prefix of the output files.\n* -p File containing position specific energy matrices (PSEM) (see next section for details).\n\nTo additionally compute TF-gene scores, the argument: \n* -a Genome annotation file (gtf). All genes contained in this file will be annotated. The file must have the original format provided by gencode, gzipped files are not supported. \n\nneeds to be specified.\n\nAdditional command arguments are:\n* -w Size of the window around the TSS of genes.\n* -d Signal of the open chromatin assay in bg format. Used to compute the average per peak signal within the regions specified in -b.\n* -e Boolean controlling exponential decay (default TRUE).\n* -n Indicates that the file in -b contains the average signal in the peaks in the specified column. In this case the -d option is not required to obtain scaled TF affinities.\n* -c Number of cores used within TRAP.\n* -f A gtf file containing genes of interest. Only regions contained in the file specified by the -b option that are within the window specified by the -w option around these genes will be annotated. The file must have the original format provided by gencode, gzipped files are not supported.\n* -y Flag indicating whether the entire gene body should be annotated with TF affinities. A window of half the size of the -w option will be additionaly considered upstream of the genes TSS.\n* -l Flag to be set if affinities should not be normalised by peak length.\n* -u Flag to be set if peak features for peak length and peak counts should not be generated.\n* -q Parameter to be set if only peak features should be generated (default FALSE).\n* -x If -d or -n is used together with this flag, the original (Decay-)Scaling formulation of TEPIC is used to compute gene-TF scores.\n* -m Path to a tab delimited file containing the length of the used PSEMs. This is incorporated in normalising peak length.\n* -z Flag indicating that the output of TEPIC should be zipped.\n* -k Path to a file containing background regions provided by the user. This option can not be used together with the -r option. \n* -r Path to a 2bit representation of the reference genome. This is required to compute a TF specific affinity threshold as well as a binary and sparse TF-gene interaction list. This can not be used together with the -k option. \n* -v p-value cut off used to determine a cut off to derive a binary score for TF binding (default 0.05).\n* -i minutes that should be spend at most per chromosome to find matching random regions (default 3).\n* -j Flag indicating that the reference genome contains a chr prefix. \n* -t Flag indicating that the annotation should be based on transcripts, not on genes.\n* -h Loop file containing chromatin contacts. Only intrachromosomal contacts are supported.\n* -s Loop window used to link a gene to a chromatin loop (default 5000).\n\n## Output \nDepending on the used arguments, TEPIC produces files containing:\n* TF affinities for all user specified regions (*Prefix_Affinity.txt*). These files are always generated.\n* Scaled TF affinities for all user specified regions (*Prefix_Scaled_Affinity.txt*). This is only generated if the -d or -n option is used together with the -x option.\n* A file containing the factors used to scale the original TF affinities (*Prefix_Peak_Coverage.txt*). This is only generated if the -d or -n option is used together with the -x option.\n* TF affinities along with features for peak length, peak counts and/or the average signal within a peak (*Prefix_Affinity_Gene_View.txt*). This is only generated if the -a option is specified.\n* Thresholded TF affinities (*Prefix_Thresholded_Affinities.txt*). These scores are computed if the -r or -k option is used.\n* Thresholded TF affinity gene-scores (*Prefix_Thresholded_Affinity_Gene_Scores.txt*). This file is generated if the -r or -k option is used together with the -a option. \n* A sparse representation that contains only those TF-gene interactions with affinities above an affinity threshold derived from random genomic sequences (*Prefix_Thresholded_Sparse_Affinity_Gene_View.txt*). This file is generated if the -r or -k option is used together with the -a option.\n\nThe *Prefix_Affinity.txt* files are tab separated files listing the genomic coordinates of candiate regions in the first column and TF affinities in the following columns:\n\n\tGenomic Position\tTF1\tTF2\t...\tTFn\n\tchr1:2321-2340\t\t0.4\t0.2\t...\t4.2\n\n\n\nThe *Prefix_Affinity_Gene_View.txt* files are tab separated files listing the Ensemble GeneID in the first column, TF gene-scores in the following columns and features on peak-length, peak-count, and peak-signal if computed.\n\n\tGENEID\t\tTF1\tTF2\t...\tTFn\tpeak length\tpeak count\tpeak signal\n\tENSG00000044612\t0.4\t0.2\t...\t4.2\t23\t\t3\t\t19.2\n\n\n\nThe *Prefix_Peak_Coverage.txt* files are tab separated files listing the genomic coordinates in the first column and the scaling value in the next column.\n\n\tGenomic Position\tFactor\n\tchr1:2321-2340\t0.4\n\n\nThe *Prefix_Thresholded_Affinities.txt* files are tab separated files listing the genomic coordinates in the first column and TF affinities in the following columns. Scores below the computed TF-specific threshold are set to zero.\n\n\tGenomic Position\t\tTF1\tTF2\t...\tTFn\n\tchr1:2321-2340\t\t0\t0\t...\t4.2\n\n\nThe *Prefix_Thresholded_Affinity_Gene_View.txt* files are tab separated files listing the Ensemble GeneID in the first column, TF gene-scores in the following columns and features on peak-length, peak-count, and peak-signal if computed.\nHere, thresholded TF affinities are used for the computation.\n\n\tGENEID\t\tTF1\tTF2\t...\tTFn\tpeak length\tpeak count\tpeak signal\n\tENSG00000044612\t0\t0\t...\t4.2\t23\t\t3\t\t19.2\n\nThe *Prefix_Conformation_Data_Affinity_Three_Peak_Based_Features_Gene_View.txt* files are based on the previous structure but extend it by including the same features, that is TF gene-scores and peak features determined for DHS residing in chromatin loops:\n\n\tGENEID\t\tTF1\tTF2\t...\tTFn\tpeak length\tpeak count\tpeak signal\tLR_TF1\t...\tLR_TFn\tLR_peak length\tLR_peak count\tLR_peak signal\n\tENSG00000044612\t0\t0\t...\t4.2\t23\t\t3\t\t19.2\t\t3.4\t...\t0.9\t14\t4\t63.3\n\nThe *Prefix_Thresholded_Sparse_Affinity_Gene_View.txt* files are tab separated files listing the Ensemble GeneID in the first column, and the name of the TF associated to this gene in the second column.\nHere, thresholded TF affinities are used for the computation. The third column of this file is required by DREM and does not carry any specific meaning.\n\n\tGENEID\t\tTF Name\tScore\n\tENSG00000044612\tCTCF\t1\n\n\n\nFurther details on the output are provided in the [description](docs/Description.pdf).\n\n\nEach run of TEPIC generates an *analysis meta datafile (amd)* containing all parameters, files, and outputs associated with the last run of TEPIC.\nTogether with the provided process xml file, the executed command lines  can be reconstructed (3). We provide *amd* files in the folder\n*MetaData*. These correspond to the gene scores of the *50kb* and *50kb-S* annotation introduced in the *TEPIC* manuscript.\n\nNote that the input files **have to** have unix file endings. Using bed graph files to compute the signal in peaks is currently only supported for homo sapiens, mus musculus, and rattus norvegicus.\n\n## Example\nTo run a trial case of *TEPIC* to compute TF-gene scores, you can use the data provided in the *Test* folder and use the command\n\n\t./TEPIC.sh -g ../Test/example_sequence.fa -b ../Test/example_regions.bed -o TEPIC-Example -p ../PWMs/1.0/pwm_vertebrates_jaspar_uniprobe_original.PSEM -a ../Test/example_annotation.gtf -w 3000 -e FALSE\n\nThis will generate TF-gene scores for the genes contained in *example_annotation.gtf*, using a window of size 3000bp, all pwms contained in *pwm_vertebrates_jaspar_uniprobe_converted.PSEM*, and without exponential decay. \n\nAdditionally, we provide a script to test several annotation versions of TEPIC. Execute the script\n\n\tbash runTestCases.sh\n\nto compute multiple trial cases. The script can be found in the folder *Test*.\n\n\n## Position specific energy matrices\nThere are three folders containing Position specific energy matrices (PSEMs):\nOur current collection of PSEMs, containing matrices from *JASPAR* (4), *HOCOMOCO* (5), and the *Kellis ENCODE Motif database* (6), is stored in the folder [PWMs/2.1](PWMs/2.1).\nThe previously used motifs are provided in the folders [PWMs/2.0](PWMs/2.0) and [PWMs/1.0](PWMs/1.0).\nThe position weight matrices used in the *TEPIC* manuscript are stored in the file\n\t[PWMs/1.0/pwm_vertebrates_jaspar_uniprobe_original.PSEM](PWMs/1.0/pwm_vertebrates_jaspar_uniprobe_original.PSEM).\n\nIn detail, the current collection contains from the JASPAR 2018 Core database:\n* 579 PSEMs for vertebrates\n* 176 PSEMs for fungi\n* 26 PSEMs for nematodes\n* 489 PSEMs for plants \n* 1 PSEM for urochordates\n* 133 PSEMs for insects\n\nAdditionally, we provide species specific collections of JASPAR matrices:\n* 3 PSEMs for Antirrhinum majus\n* 5 PSEMs for Arabidopsis lyrata\n* 440 PSEMs for Arabidopsis thaliana\n* 22 PSEMs for Caenorhabditis elegans\n* 132 PSEMs for Drosophila melanogaster\n* 1 PSEMs for Fragaria x ananassa\n* 7 PSEMs for Gallus gallus\n* 6 PSEMs for Glycine max\n* 1 PSEM for Halocynthia roretzi\n* 459 PSEMs for Homo sapiens\n* 1 PSEM for Hordeum vulgare\n* 1 PSEM for Medicago truncatula\n* 1 PSEM for Meleagris gallopavo\n* 157 PSEMs for Mus musculus\n* 1 PSEM for Neurospora crassa\n* 1 PSEM for Nicotiana\n* 4 PSEMs for Orcytolagus\n* 7 PSEMs for Oryza sativa\n* 1 PSEM for Petunia x hybrida\n* 1 PSEM for Phaeodactylum tricornutum\n* 9 PSEMs for Physcomitrella patens\n* 3 PSEMs for Pisum sativum\n* 1 PSEM for Populus trichocarpa\n* 2 PSEMs for Rattus norvegicus\n* 2 PSEMs for Rattus rattus\n* 176 PSEMs for Saccaromyces cerevisiae\n* 2 PSEMs for Solanum lycopersicum\n* 1 PSEM for Triticum aestivum\n* 4 PSEMs for Xenopus laevis\n* 8 PSEMs for Zea mays\n\nAll JASPAR matrices can be found in [PWMs/2.1/JASPAR_PSEMs](PWMs/2.1/JASPAR_PSEMs)\n\nFrom HOCOMOCO we provide 402 motifs for homo sapiens and 358 for mus musculus, available in [PWMs/2.1/HOCOMOCO_PSEMs](PWMs/2.1/HOCOMOCO_PSEMs)\n\nThe Kellis set contains 58 motifs, stored in  [PWMs/2.1/Kellis_PSEMs](PWMs/2.1/Kellis_PSEMs).\n\nAdditionally we provide non-redundant collections for homo sapiens and mus musculus considering motifs from all three sources:\n* 561 PSEMS for homo sapiens\n* 380 PSEMs for mus musculus\n\nThe matrices are stored in the folder [PWMs/2.1/Merged_PSEMs](PWMs/2.1/Merged_PSEMs).\n\n\nFurthermore, we used a motif clustering approach (7), to merge similar motifs of the files containing matrices from all three sources. This led to\n* 483 PSEMs for homo sapiens\n* 306 PSEMs for mus musculus\n\nThe matrices are stored in the folder [PWMs/2.1/Clustered_PSEMs](PWMs/2.1/Clustered_PSEMs).\n\nFiles holding the length of the PSEMs are provided too. \n\nAdditional position weight matrices can be transformed to a usable format using \n\t[Code/PSCM_to_PSEM.cpp](Code/PSCM_to_PSEM.cpp).\nThis program converts matrices in TRANSFAC format to the energy format used by TRAP. \nDetails on the parameters used for conversion can be found in the header of the provided files.\nNote that the character after the P in TRANSFAC format needs to be a zero.\n\nIn TRANSFAC format, a matrix has to have the following structure:\n\n\tXX\n\t\n\tID \\<RunningNumber\\> \\<TF-Name\\>\n\t\t\n\tXX\n\t\n\tP0        A         C         G         T\n\t\n\t1         0         93        6         1\n\t\n\t2         7         81        1         12\n\t\n\t.\n\t\n\t.\n\t\n\t.\n\t\n\t14        1         3         95        0\n\t\n\tXX\n\t\n\t//   \n\n\nIn the PSEM format, a matrix has to be in the following structure:\n\n\t\\> \\<RunningNumber\\> \\<TF-Name\\>\tlnR0: \\<value\\>\n\t\t\n\t1.56945   -0.108976 1.46047   0\n\t\n\t5.06003   4.54008   0         4.06982\n\t\n\t.\n\t\n\t.\n\t\n\t.\n\t\n\t4.59839   4.0784   4.07844   0\n\t\n\t5.11834   4.59839   0         5.11834\n\t\n\n## Acknowledgments\nWe thank Helge Roider for providing the C++ implementation of TRAP, which we slightly modified for running in parallel.\nTEPIC 2.0 has been funded by the Bundesministerium fÃ¼r Bildung und Forschung (BMBF) with project number 01DP17005 under the acronym EPIREG.\n\n## Citation\nIf you are using TEPIC and/or [INVOKE](https://github.com/SchulzLab/TEPIC/tree/master/MachineLearningPipelines/INVOKE) please cite:\n\n**Combining transcription factor binding affinities with open-chromatin data for accurate gene expression prediction**\nSchmidt et al., Nucleic Acids Research 2016; doi: 10.1093/nar/gkw1061 [full text](http://nar.oxfordjournals.org/content/early/2016/11/29/nar.gkw1061.full) \n\nIf you are using [DYNAMITE](https://github.com/SchulzLab/TEPIC/tree/master/MachineLearningPipelines/DYNAMITE) please also cite:\n\n**Epigenomic Profiling of Human CD4+ T Cells Supports a Linear Differentiation Model and Highlights Molecular Regulators of Memory Development**\nDurek et al. Cell Immunity, Volume 45, Issue 5, 15 November 2016, [full text](http://www.cell.com/immunity/fulltext/S1074-7613(16)30433-2)\n\nYou may also consider to cite our latest publication describing the TEPIC 2.0 release ... :\n\n**TEPIC 2 - An extended framework for transcription factor binding prediction and integrative epigenomic analysis**\nSchmidt et al., Bioinformatics 2018; doi: 10.1093/bioinformatics/bty856 [full text](https://doi.org/10.1093/bioinformatics/bty856)\n\n... and its extensions:\n\n**Integrative prediction of gene expression with chromatin accessibility and conformation data**\nSchmidt et al., Epigenetics & Chromatin 2020; doi: 10.1186/s13072-020-0327-0 [full text](https://doi.org/10.1186/s13072-020-0327-0)\n\nOther works that have influenced ours:\n> (1) Predicting transcription factor affinities to DNA from a biophysical model, Roider HG, et al., Bioinformatics, 2007.\n\n> (2) ChIP-Seq of transcription factors predicts absolute and differential gene expression in embryonic stem cells, Ouyang Z, et al.,  PNAS, 2009.\n\n> (3) A general concept for consistent documentation of computational analyses, Ebert P, et al.,  Database, 2015.\n\n> (4) JASPAR: an open-access database for eukaryotic transcription factor binding profiles, Sandelin A., et al., Nucleic Acids Research, 2004.\n \n> (5) HOCOMOCO: a comprehensive collection of human transcription factor binding sites models , Kulakovskiy Ivan V., et al., Nucleic Acids Research, 2013.\n\n> (6) Systematic discovery and characterization of regulatory motifs in ENCODE TF binding experiments, Kheradpour P, and Kellis M, Nucleic Acids Research, 2013.\n\n> (7) Natural similarity measures between position frequency matrices with an application to clustering, Pape U.J., et al., Bioinformatics, 2008.\n",
    "readme_length": 21362
  },
  {
    "name": "tfmodisco",
    "full_name": "kundajelab/tfmodisco",
    "description": "TF MOtif Discovery from Importance SCOres",
    "stars": 158,
    "forks": 32,
    "language": "Python",
    "url": "https://github.com/kundajelab/tfmodisco",
    "topics": [],
    "created_at": "2016-07-01T01:26:15Z",
    "updated_at": "2025-11-27T20:24:48Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# TF-MoDISco\n\nTF-MoDISco (**T**ranscription **F**actor **Mo**tif **D**iscovery from **I**mportance **Sco**res) is an algorithm for discovering sequence motifs from machine-learning-model-derived importance scores. Unlike traditional motif discovery methods that rely solely on sequence enrichment, TF-MoDISco leverages context-aware importance scores to identify patterns.\n\nThese importance scores can be generated using various attribution methods, such as DeepLIFT or SHAP, applied to models like BPNet. The algorithm identifies high-importance regions (seqlets), clusters them into motifs, and provides a report comparing discovered motifs to known databases.\n\n> [!IMPORTANT]  \n> Starting from version v2, TF-MoDISco utilizes the [tfmodisco-lite](https://github.com/jmschrei/tfmodisco-lite/) implementation and interface. This implementation is significantly more memory efficient, and in many cases faster, than the original implementation. The original implementation (v0) is still available [here](https://github.com/kundajelab/tfmodisco/tree/v0-final).\n\n## Algorithm Description\n\nThe TF-MoDISco algorithm starts with a set of importance scores on genomic sequences and performs the following tasks:\n\n1. Identify high-importance windows of the sequences, termed \"seqlets\"\n2. Divide the seqlets into positive and negative sets (metaclusters) based on the overall importance score of each seqlet\n3. Cluster recurring similar seqlets\n4. Generate motifs by aligning the clustered seqlets\n\nDuring clustering, a coarse-grained similarity is calculated as the cosine similarity between gapped k-mer representations between all pairs of seqlets. This information is used to calculate the top nearest neighbors, for which a fine-grained similarity is calculated as the maximum Jaccard index as two seqlets are aligned with all possible offsets. This sparse similarity matrix is then density adapted, similarly to t-SNE, and Leiden clustering is used to extract patterns. Finally, some heuristics are used to merge similar patterns and split apart the seqlets comprising dissimilar ones.\n\n![image](assets/overview.svg)\n\n## References\n\nTF-MoDISco is described in:\n> Wang, Tseng, Ramalingam, Schreiber, et al. \"Decoding predictive motif lexicons and syntax from deep learning models of transcription factor binding profiles.\" (manuscript in preparation)\n\nRelated tools:\n- [Fi-NeMo](https://github.com/kundajelab/Fi-NeMo): Motif instance detection using TF-MoDISco patterns\n- [BPNet](https://github.com/kundajelab/bpnet-refactor): Deep learning models for TF binding prediction\n- [ChromBPNet](https://github.com/kundajelab/chrombpnet): Deep learning models for chromatin accessibility prediction\n\n## Installation\n\nYou can install TF-MoDISco using `pip install modisco`\n\n## Running TF-MoDISco\n\nYou can run TF-MoDISco using the command line tool `modisco` which comes with the TF-MoDISco installation. This tool allows you to run TF-MoDISco on a set of sequences and corresponding attributions, and then to generate a report (like the one seen above) for the output generated from the first step.\n\n`modisco motifs -s ohe.npz -a shap.npz -n 2000 -o modisco_results.h5`\n\nThis command will run modisco on the one-hot encoded sequences in `ohe.npz`, use the attributions from `shap.npz`, use a maximum of 2000 seqlets per metacluster (this is low, but a good starting point for testing the algorithm on your own data), and will output the results to `modisco_results.h5`. The one-hot encoded sequences and attributions are assumed to be in length-last format, i.e., have the shape (# examples, 4, sequence length). Note that you can also use `npy` files if you don't want to use compressed data for some reason. \n\n> [!TIP]\n> By default, TF-MoDISco uses a window size of 400 around the center of each input region. You can override this default with `-w`.\n\nThe output saved in `modisco_results.h5` will include all of the patterns and has the following struture:\n\n```\npos_patterns/\n    pattern_0/\n        sequence: [...]\n        contrib_scores: [...]\n        hypothetical_contribs: [...]\n        seqlets/\n            n_seqlets: [...]\n            start: [...]\n            end: [...]\n            example_idx: [...]\n            is_revcomp: [...]\n            sequence: [...]\n            contrib_scores: [...]\n            hypothetical_contribs: [...]\n        subpattern_0/\n            ...\n    pattern_1/\n        ...\n    ...\nneg_patterns/\n    pattern_0/\n        ...\n    pattern_1/\n        ...\n    ...\n```\n\nwhere `[...]` denotes that data is stored at that attribute. Importantly, the seqlets are all in the correct orientation. If a seqlet has been flipped to be the reverse complement, the sequence, contribution scores, and coordinates have also been flipped. In cases where there are not enough seqlets to consider a metacluster, that attribute (`neg_patterns` or `pos_patterns`) may not appear in the file.\n\n## Generating reports\n\nThe TF-MoDISco report can be generated with the following command:\n```sh\nmodisco report -i modisco_results.h5 -o report/ -s report/ -m motifs.txt\n```\n\nEach pattern produced by TF-MoDISco is compared against the database of motifs using [TOMTOM](https://meme-suite.org/meme/tools/tomtom). A good default choice is [this collection of human motifs](https://raw.githubusercontent.com/kundajelab/MotifCompendium/refs/heads/main/pipeline/data/MotifCompendium-Database-Human.meme.txt) produced by the [MotifCompendium](https://github.com/kundajelab/MotifCompendium) package.\n\nThe report details each pattern, including seqlet importance and spatial distributions, example seqlets at different importance levels, and motif visualizations.\n\nFor users who need the legacy report format use:\n```sh\nmodisco report-simple -i modisco_results.h5 -o simple_report/ -s simple_report/ -m motifs.txt\n```\n",
    "readme_length": 5801
  },
  {
    "name": "tfmodisco-lite",
    "full_name": "jmschrei/tfmodisco-lite",
    "description": "A lite implementation of tfmodisco, a motif discovery algorithm for genomics experiments. ",
    "stars": 86,
    "forks": 25,
    "language": "Python",
    "url": "https://github.com/jmschrei/tfmodisco-lite",
    "topics": [],
    "created_at": "2022-08-03T21:58:42Z",
    "updated_at": "2025-11-28T02:53:06Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# tfmodisco-lite\n\n> [!IMPORTANT]\n> tfmodisco-lite has been merged into the [official TF-MoDISco repository](https://github.com/kundajelab/tfmodisco). Please see that for future development.\n\n> [!WARNING]\n> tfmodisco-lite v2.0.0 and above may produce slightly different results from the original TF-MoDISCo code as minor bugs are fixed and some speed improvements required swapping sorting algorithms.\n\nTF-MoDISco is a biological motif discovery algorithm that differentiates itself by using attribution scores from a machine learning model, in addition to the sequence itself, to guide motif discovery. Using the attribution scores, as opposed to the signal being predicted by the machine learning model (e.g. ChIP-seq peaks), can be beneficial because the attributions fine-map the specific sequence drivers of biology. Although in many of our examples this model is [BPNet](https://www.nature.com/articles/s41588-021-00782-6) and the attributions are from [DeepLIFT/DeepSHAP](https://captum.ai/api/deep_lift_shap.html), there is no limit on what attribution algorithm is used, or what model the attributions come from. This means that, for example, one would use [attributions from a gapped k-mer SVM](https://academic.oup.com/bioinformatics/article/35/14/i173/5529147?login=false) just as easily as [DeepSHAP on a convolutional neural network that predicts enhancer activity](https://www.nature.com/articles/s41588-022-01048-5). All that's needed to run TF-MoDISco are sequences and their corresponding per-position attribution scores.\n\n> **Note**\n> This is a rewrite of the original TF-MoDISCo code, which can be found at https://github.com/kundajelab/tfmodisco\n\nBelow, we can see just how fast and memory efficient tfmodisco-lite is on a an ATAC-seq experiment when capped at 100k seqlets. The original tfmodisco code (left) takes 5 hours and 35 Gb of RAM. tfmodisco-lite v1.0.0 (middle) takes closer to 4 hours and only requires 13 Gb of RAM. tfmodisco-lite v2.0.0 (right) takes only a little over 1.5 hours and 8 Gb of RAM.\n\n![image](https://user-images.githubusercontent.com/3916816/224192946-43434221-6da1-4875-ab00-6f782f9178ae.png)\n\n\n### Installation\n\nYou can install tfmodisco-lite using `pip install modisco-lite`\n\n### Command Line Tools\n\nYou can run tfmodisco-lite using the command line tool `modisco` which comes with the tfmodisco-lite installation. This tool allows you to run tfmodisco-lite on a set of sequences and corresponding attributions, and then to generate a report (like the one seen above) for the output generated from the first step.\n\n### Algorithm Description\n\nAt a high level, the procedure works like this: \"seqlets,\" which are short spans of sequence with high absolute attribution score, are extracted from the given examples. These seqlets are then divided into positive and negative seqlets (\"metaclusters\"). For each metacluster, a coarse-grained similarity is calculated as the cosine similarity between gapped k-mer representations between all pairs of seqlets. This information is used to calculate the top nearest neighbors, for which a fine-grained similarity is calculated as the maximum Jaccard index as two seqlets are aligned with all possible offsets. This sparse smilarity matrix is then density adapted, similarly to t-SNE, and Leiden clustering is used to extract patterns. Finally, some heuristics are used to merge similar patterns and split apart the seqlets comprising dissimilar ones. \n\nThe outputs of TF-MoDISco are motifs that summarize repeated patterns with high attribution. These patterns can be visualized using the `reports.report_motifs` function to generate an HTML file (which can be loaded into a Jupyter notebook) that, after training a BPNet model on a SPI1 data set, looks like the following:  \n\n![image](https://user-images.githubusercontent.com/3916816/189726765-47e043c5-c942-4547-9b69-bfc8b5ba3131.png)\n\ntfmodisco-lite is a lightweight version of the original [TF-MoDISco](https://github.com/kundajelab/tfmodisco) implementation, which takes significantly less memory, (sometimes) runs faster, and is significantly less code, making it easier to understand. This rewrite is an exact reimplementation (except for one minor fix) and so should be able to be used as a drop-in replacement for existing tfmodisco pipelines. \n\n#### Running tfmodisco-lite\n\n> ** Note**\n> To match the original implementation of TF-MoDISco, the default window size is 400. This means that when you pass in examples only the middle 400 are used in the procedure. It was originally implemented this way to speed up the procedure. If your window sizes are larger, or you otherwise want to consider alternate window sizes, make sure to explicitly set it with `-w`.\n\n`modisco motifs -s ohe.npz -a shap.npz -n 2000 -o modisco_results.h5`\n\nThis command will run modisco on the one-hot encoded sequences in `ohe.npz`, use the attributions from `shap.npz`, use a maximum of 2000 seqlets per metacluster (this is low, but a good starting point for testing the algorithm on your own data), and will output the results to `modisco_results.h5`. The one-hot encoded sequences and attributions are assumed to be in length-last format, i.e., have the shape (# examples, 4, sequence length). Note that you can also use `npy` files if you don't want to use compressed data for some reason. \n\nThe output saved in `modisco_results.h5` will include all of the patterns and has the following struture:\n\n```\npos_patterns/\n    pattern_0/\n        sequence: [...]\n        contrib_scores: [...]\n        hypothetical_contribs: [...]\n        seqlets/\n            n_seqlets: [...]\n            start: [...]\n            end: [...]\n            example_idx: [...]\n            is_revcomp: [...]\n            sequence: [...]\n            contrib_scores: [...]\n            hypothetical_contribs: [...]\n        subpattern_0/\n            ...\n    pattern_1/\n        ...\n    ...\nneg_patterns/\n    pattern_0/\n        ...\n    pattern_1/\n        ...\n    ...\n```\n\nwhere `[...]` denotes that data is stored at that attribute. Importantly, the seqlets are all in the correct orientation. If a seqlet has been flipped to be the reverse complement, the sequence, contribution scores, and coordinates have also been flipped. In cases where there are not enough seqlets to consider a metacluster, that attribute (`neg_patterns` or `pos_patterns`) may not appear in the file.\n\n#### Generating reports\n\nBasic reporting can be executed with the following command:  \n```sh\nmodisco report -i modisco_results.h5 -o report/ -s report/\n```\n\nYou may also generate reports compared to a given database of motifs with the following command:  \n```sh\nmodisco report -i modisco_results.h5 -o report/ -s report/ -m motifs.txt\n```\n\nThis command will take the results from the tfmodisco-lite run, as well as a reference database of motifs to compare the extracted patterns to, and generate a HTML report like the one seen above. Each pattern that is extracted by tfmodisco-lite is compared against the database of motifs using [TOMTOM](https://meme-suite.org/meme/tools/tomtom) to match them with prior knowledge.\n",
    "readme_length": 7096
  },
  {
    "name": "YAMDA",
    "full_name": "daquang/YAMDA",
    "description": "Yet Another Motif Discovery Algorithm",
    "stars": 53,
    "forks": 7,
    "language": "Python",
    "url": "https://github.com/daquang/YAMDA",
    "topics": [],
    "created_at": "2017-08-26T02:22:51Z",
    "updated_at": "2025-04-17T08:52:58Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "<p align=\"center\">\n<img src=\"https://github.com/daquang/YAMDA/raw/master/logo/logo.jpg\" width=\"300\">\n</p>\n<!---\n![YAMDA](https://github.com/daquang/YAMDA/raw/master/logo/logo.jpg \"Fast GPU-accelerated motif discovery\")\n-->\n\nA highly scalable GPU-accelerated *de novo* motif discovery software package\n\nPlease post in the Issues board or e-mail me (daquang@umich.edu) if you have any questions, suggestions, or complaints \n:)\n\n---\n\n## Table of Contents\n* [Citation](#citation)\n* [Installation](#installation)\n    * [Required dependencies](#required-dependencies)\n    * [Optional dependencies](#optional-dependencies)\n    * [Docker](#docker)\n* [Examples](#examples)\n    * [Making a masked genome FASTA](#making-a-masked-genome-fasta)\n    * [Extracting BED interval FASTA sequences](#extracting-bed-interval-fasta-sequences)\n    * [Motif discovery in ChIP-seq](#motif-discovery-in-chip-seq)\n    * [Motif discovery in DGF](#motif-discovery-in-dgf)\n* [To-Do](#to-do)\n\n---\n\n## Citation\n\n```\n@article{doi:10.1093/bioinformatics/bty396,\nauthor = {Quang, Daniel and Guan, Yuanfang and Parker, Stephen C J},\ntitle = {YAMDA: thousandfold speedup of EM-based motif discovery using deep learning libraries and GPU},\njournal = {Bioinformatics},\nvolume = {},\nnumber = {},\npages = {bty396},\nyear = {2018},\ndoi = {10.1093/bioinformatics/bty396},\nURL = {http://dx.doi.org/10.1093/bioinformatics/bty396},\neprint = {/oup/backfile/content_public/journal/bioinformatics/pap/10.1093_bioinformatics_bty396/1/bty396.pdf}\n}\n```\n\n---\n\n## Installation\nClone a copy of the YAMDA repository:\n\n```\ngit clone https://github.com/daquang/YAMDA.git\n```\n\nOr download a stable release version (v0.1 should reproduce the paper's results exactly, but uses older libraries):\n```\nwget https://github.com/daquang/YAMDA/archive/0.1.tar.gz\n```\n\nYAMDA relies on several open source software packages. Links and version numbers for the packages used to develop and\ntest YAMDA are listed below; however, typically any recent version of these packages should be fine for running YAMDA. \nThe best and easiest way to install all dependencies is with [Anaconda](https://www.anaconda.com/) (5.2, Python 3.6 \nversion). Anaconda uses pre-built binaries for specific operating systems to allow simple installation of Python and \nnon-Python software packages. macOS High Sierra or Ubuntu 18.04 is recommended.\n\n### Required dependencies\n* [Python](https://www.python.org) (3.6.5). I chose Python 3.6 instead of Python 2.7 for initial YAMDA development\nbecause the latter will  no longer be supported in 2020. YAMDA imports the following standard Python packages:\nsys, os, errno, re, argparse, pickle, and itertools.\n* [numpy](http://www.numpy.org/) (1.15.1). Python scientific computing library. Comes pre-packaged in Anaconda.\n* [scipy](https://www.scipy.org/) (1.1.0). Python scientific computing library. Comes pre-packaged in Anaconda.\n* [pyfaidx](https://github.com/mdshw5/pyfaidx) (0.5.4.1). Python wrapper module for indexing, retrieval, and in-place \nmodification of FASTA files using a samtools compatible index. Easily installed in Anaconda with the following command \nline:\n```\npip install pyfaidx\n```\n* [tqdm](https://pypi.python.org/pypi/tqdm) (4.29.0). Progress bar. Easily installed in Anaconda with the following \ncommand line:\n```\npip install tqdm\n```\n* [PyTorch](http://pytorch.org/) (1.0). Tensor computation library from Facebook AI that forms the backbone of YAMDA. \nBoth GPU and CPU versions are supported. It is recommended you check out the official \n[PyTorch website](http://pytorch.org) for foolproof methods of installation for specific operating systems and hardware \nconfigurations.\n\n**tl;dr**, the following command line should work most of the time for installing PyTorch.\n```\nconda install pytorch torchvision -c pytorch \n```\n\n### Optional dependencies\nThese are software packages and Python libraries that are not necessary to run YAMDA, but are nevertheless recommended.\nThey contain extra utilities that can extend the functionality of YAMDA or help preprocess data. Once again, I've put\nlinks and version numbers of what I used, but any recent version of these packages should be fine.\n\n* [The MEME suite](http://meme-suite.org/) (4.12.0). Appropriately enough, the MEME suite has many tools for \nprocessing FASTA and motif files. Among these are the fasta-shuffle-letters utility, which is useful for generating \nnegative controls. MEME can also be installed easily enough from its main website or through Anaconda:\n```\nconda install -c bioconda meme \n```\nHowever, for my MacBook Pro, this command line yielded some errors. I had to download a more specific set of binaries \nfor my specific operating system and version of Python, as follows:\n```\nwget https://anaconda.org/bioconda/meme/4.12.0/download/osx-64/meme-4.12.0-py36pl5.22.0_1.tar.bz2\nconda install meme-4.12.0-py36pl5.22.0_1.tar.bz2\n```\n* [biopython](http://biopython.org/) (1.7.0). Required to read bgzipped FASTA files. Convenient if you like storing \nfiles compressed.\n```\nconda install -c anaconda biopython\n```\n* [BEDTools](http://bedtools.readthedocs.io/en/latest/) (0.7.10). Standard BEDTools suite is useful for extracting FASTA \nsequences from BED files. Since I also needed the pybedtools wrapper library, I installed BEDTools with the following \nconda command:\n```\nconda install -c bioconda pybedtools\n```\n\n### Streamlined (can ignore this part if you already manually installed all dependencies)\n\n#### Anaconda Install\n```bash\ncd /tmp && wget https://repo.anaconda.com/archive/Anaconda3-5.1.0-Linux-x86_64.sh -O ./anaconda3.sh && bash ./anaconda3.sh -u -b -p /opt/anaconda3 && export PATH=\"/opt/anaconda3/bin:$PATH\" && cd -;\n```\n\n#### Install Detailed\n```bash\nconda update -yn base conda && conda update -y --prefix /opt/anaconda3 anaconda && conda create -fmy -c defaults -c anaconda -c conda-forge -c bioconda -c pytorch -n YAMDA-env python=3.6.5 numpy=1.13.3 scipy=0.19.1 pyfaidx tqdm pytorch torchvision meme anaconda biopython pybedtools && source activate YAMDA-env;\n```\n\n#### Install Easy\n```bash\nconda env create -f environment.yml && . activate YAMDA-env;\n```\n\n#### Exit Env\n`source deactivate`\n\n#### Kill Env\n`conda env remove --name YAMDA-env`\n\n### Docker (can ignore this part if you do not intend on doing a Docker installation)\n1. Install docker on whatever: https://www.docker.com/community-edition\n```bash\ncd /tmp && curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - && sudo apt-get update && apt-cache policy docker-ce && sudo apt-get install -y docker-ce && cd -;\n```\n2. Install docker-compose on same whatever: https://docs.docker.com/compose/install\n```bash\ncd /tmp && sudo curl -L https://github.com/docker/compose/releases/download/1.18.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose && sudo chmod +x /usr/local/bin/docker-compose && cd -;\n```\n3. Make docker image using the makefile `make yamda-dock`\n4. To have docker run the CMD you put into the Dockerfile `sudo docker run yamda-dock`\n5. To ssh into the image, for debugging and so on: `sudo docker run -it yamda-dock bash`\n6. When in the image don't forget to `source activate YAMDA-env`\n7. To kill the image and cleanup docker `make cleanup`\n\nDocker is screwy about importing global variables in your environment, which you'll probably want now or later. So far to do it easily and relatively conveniently you need to enter the variable 4 times in 3 different places, twice in the Dockerfile, once in the .env file, and once in the docker-compose.yml file. I made an example VAR to make it clear how to do that. \n\n---\n\n## Examples\nIn the examples folder, you will find the narrowPeak and masked FASTA files that are needed to reproduce results in the \nmanuscript. For your convenience, I have included the major preprocessing steps that typically comprise a *de novo* \nmotif discovery pipeline.\n\n### Making a masked genome FASTA\nMotif discovery for DNA usually performs better on a FASTA sequence set with all repetitive sequences masked. This is \ntypically accomplished by first generating a masked genome where all repetitive sequence residues are replaced with \ncapital N's. The following command lines will download masked hg19 chromosome FASTA files, assemble the individual \nfiles into a single FASTA file (hg19.fa.masked), and remove all intermediate files:\n```\nwget http://hgdownload.cse.ucsc.edu/goldenpath/hg19/bigZips/chromFaMasked.tar.gz\ntar zxvf chromFaMasked.tar.gz\ncat chr*.fa.masked >> hg19.fa.masked\nrm chr*.fa.masked chromFaMasked.tar.gz\n```\n### Extracting BED interval FASTA sequences\nBEDTools' fastaFromBed utility is useful for extracting letter sequences from a reference fasta file based on feature \ncoordinates. The following command lines demonstrate how to do this from an ENCODE narrowPeak file (H1 POU5F1) to \ngenerate 100 bp sequences centered on peak summits. For simplicity, we will use the same masked genome FASTA file \ngenerated in the previous example.\n```\nzcat Examples/wgEncodeAwgTfbsHaibH1hescPou5f1sc9081V0416102UniPk.narrowPeak.gz | awk -v \"OFS=\\t\" '{print $1,$2 + $10 - 50,$2 + $10 + 50}' | fastaFromBed -bed stdin -fi hg19.fa.masked -fo Examples/H1_POU5F1_ChIP_HAIB.fa.masked\n```\n### Motif discovery in ChIP-seq\nThis example demonstrates motif discovery on the H1 POU5F1 ChIP-seq data. YAMDA requires a positive FASTA file\nand a negative FASTA file. The latter is typically a dinucleotide-shuffled control version of the positive file. The \nfasta-shuffle-letters utility from the MEME-suite is useful for this purpose. \n\n```\nfasta-shuffle-letters -kmer 2 -s 0 Examples/H1_POU5F1_ChIP_HAIB.fa.masked H1_POU5F1_ChIP_HAIB_shuffled.fa.masked\n```\n\nThe run_em.py script executes the motif discovery program on the FASTA pairs. Use `python run_em.py -h` to get a \ndetailed description of the script's arguments. Note that to run this example, you do not necessarily need to run the \nprevious examples because all the necessary files have already been prepackaged with this repository. \n\n```\npython run_em.py -r -e -i Examples/H1_POU5F1_ChIP_HAIB.fa.masked -j Examples/H1_POU5F1_ChIP_HAIB_shuffled.fa.masked -oc H1_POU5F1_output \n```\n\nThe output folder H1_POU5F1_output contains the following files:\n\n* model.pkl. A saved/pickled version of the learned mixture model.\n* motifs.txt. The discovered motif(s) in Minimal MEME format. This file can be further processed with MEME utilities \nsuch as meme2images and TOMTOM.\n* positive_seqs.fa. A FASTA of the positive sequences with all instances of the discovered motif(s) erased.\n* negative_seqs.fa. A FASTA of the negative sequences with all instances of the discovered motif(s) erased.\n\n### Motif discovery in DGF\nThis example demonstrates motif discovery on the K562 Digital Genomic Footprinting dataset. This is the same example \nfrom [EXTREME](https://github.com/uci-cbcl/EXTREME).\n\nMotif discovery in DGF is similar to motif discovery in ChIP-seq; however, due to the rarity of motifs in DGF datasets, \nwe found that it helps to erase all overlapping instances of repetitive sequences such as AAAAAA/TTTTTT and \nCCCGCCC/GGGCGGG:\n\n\n```\npython erase_annoying_sequences.py -i Examples/K562_DNase.fa -o Examples/K562_DNase_eraseannoying.fa\nfasta-shuffle-letters -kmer 2 -dna -seed 0 Examples/K562_DNase_eraseannoying.fa Examples/K562_DNase_eraseannoying_shuffled.fa\n```\n\nNow we can run the YAMDA algorithm on the FASTA file:\n```\npython run_em.py -f 0.1 -r -e -maxs 20000 -i Examples/K562_DNase_eraseannoying.fa -j Examples/K562_DNase_eraseannoying_shuffled.fa -oc K562_DNase_output\n```\n\nThe -f argument is one of the most difficult, yet perhaps most important, arguments. The closest corresponding argument \nin MEME is wnsites. The closer the -f argument is to zero, the stronger the bias towards motifs with exactly the \nexpected number of sites. The default value of 0.1 works well for most ChIP-seq and some DGF datasets, but in cases of \neven rarer motifs smaller values (e.g. 0.025) is necessary.\n\n---\n\n## To-Do\nHere is a list of features I plan to add. They will be added according to demand.\n* Test YAMDA on RNA and protein sequences\n* Python 2.7 compatibility\n* Cythonize seeding step and reduce its memory overhead\n* Add more examples (e.g. SELEX data)\n* Add ZOOPS (zero or one occurrence per sequence) and OOPS (one occurrence per sequence) models. YAMDA currently only supports the TCM (two component model), whereas\nMEME supports all three. ZOOPS and OOPS may offer faster and more accurate performance for certain datasets, such as ChIP-seq.\n\nIn addition, I promise to update YAMDA as library dependencies are updated.",
    "readme_length": 12597
  },
  {
    "name": "pyScoMotif",
    "full_name": "3BioCompBio/pyScoMotif",
    "description": "Python tool for the discovery of similar 3D structural motifs across protein structures.",
    "stars": 34,
    "forks": 2,
    "language": "Python",
    "url": "https://github.com/3BioCompBio/pyScoMotif",
    "topics": [],
    "created_at": "2023-02-20T13:49:59Z",
    "updated_at": "2025-10-14T02:22:51Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# pyScoMotif: Discovery of similar 3D structural motifs across proteins\n\n## Description\n\npyScoMotif is a Python package that enables the rapid search of similar 3D structural motifs across large sets of proteins. Concretely, given a reference set of residues in a PDB file, it will find other protein structures with the same set of residues that are in a similar geometrical arragement. \n\nTypical use cases include the search of catalytic and binding sites.\n\nReference publication: \"pyScoMotif: Discovery of similar 3D structural motifs across proteins\". Gabriel Cia, Jean Kwasigroch, Basile Stamatopoulos, Marianne Rooman, Fabrizio Pucci. DOI: https://doi.org/10.1093/bioadv/vbad158\n\n**LICENSE: pyScoMotif is free to use for non-commercial purposes. To use pyScoMotif for commercial purposes, please contact us.**\n\n## Install\n\n**Note that a Python version >= 3.9 is required.**\n\nTo install pyScoMotif, simply run:\n\n```\npip install pyscomotif\n```\n\nIf the install was successful, you should be able to type and run `pyscomotif` in your terminal.\n\nDependencies are listed in the `pyproject.toml` file.\n\n## Tutorial\n\nThis tutorial shows how to use pyScoMotif, as well as some more advanced options such as [mutated motifs and position specific exchanges](#searching-for-similar-motifs-with-potential-mutations). A detailed explanation of each parameter available for the different commands can be displayed by typing `pyscomotif <command name> --help`.\n\nNote that pyScoMotif's speed is highly dependant on the computer/cluster hardware, and therefore timings could be different than those reported in our publication.\n\n### Pre-built indexes\n\nTo use pyScoMotif to search for similar motifs, we first need an index of the set of PDB files on which we want to perform the search. Given indexing large sets of PDB files such as the entire Protein Data Bank or AlphaFold2 (AF2) proteomes can take some time, pre-built indexes are available for download for the entire PDB, the AF2 global health proteomes and the AF2 human proteome at http://babylone.ulb.ac.be/pyScoMotif/data/.\n\n### Creating an index\n\nIf our pre-built indexes don't cover the PDBs you are interested in, then you will have to build an index yourself (don't panic, it's easy !). For that, we use the `create-index` command, which has 2 mandatory parameters:\n- The full path of the directory that contains our PDB files, which is given as the final argument of the command.\n- The file extension pattern of the PDB files, which can be specified with the `--pattern` option. \n\nAdditionaly, we can specify the path of the directory that will contain all the index files through the `--index_path` option, and we can also parallelize the indexing through the `--n_cores` option.\n\nSuppose we have downloaded the human proteome dataset from [AlphaFoldDB](https://alphafold.ebi.ac.uk/download), our command would be\n\n```\npyscomotif create-index --pattern=*.pdb.gz --n_cores=6 /home/user/Downloads/UP000005640_9606_HUMAN_v3\n```\n\nTo update an index with new PDB files, see the `pyscomotif update-index --help` command.\n\n### Searching for similar motifs\nOnce we have the index of our set of PDB structures, we can perform the similar motif search. \n\nWe will showcase the search of the [serine protease catalytic site](https://www.ebi.ac.uk/thornton-srv/m-csa/entry/173/), which is a 3 residue motif (His-Asp-Ser) that catalyzes the proteolyses of peptide bonds.\nFor that, we use the `motif-search` command, which has 3 mandatory parameters:\n- The full path of the index directory, which by default is created inside our database folder and is named 'pyScoMotif_index'.\n- The path of the PDB file that contains our motif of interest.\n- The list of residue identifiers of our motif.\n\nAdditionally, we can control the distance and angle tolerance values when searching for similar pairs of residues through the `--distance_delta_thr` and `--angle_delta_thr` options, and can also control the maximum RMSD of the hits returned by pyScoMotif through the `--RMSD_threshold` option. Search can also be parallelized through the `--n_cores` option. Finally, we can specify the path of the output csv file that will contain the results through the `--results_output_path` option.\n\nHere we use PDB 1pq5 and residues A56, A99 and A195 as our reference motif, so our search command would be\n```\npyscomotif motif-search --results_output_path=/home/user/Downloads/serine_protease_pyScoMotif_result.csv --n_cores=6 /home/user/Downloads/UP000005640_9606_HUMAN_v3/pyScoMotif_index /home/user/Downloads/1pq5.pdb A56 A99 A195\n```\n\nThis generates the following output table (first 5 results only)\n\n| | **matched_motif** | **similar_motif_found** | **RMSD** | **n_mutations** | **PDB_ID**            | **header_description**                                                           |\n|------|-------------------|-------------------------|----------|-----------------|-----------------------|----------------------------------------------------------------------------------|\n| 0    | A56H A99D A195S   | A225H A270D A366S       | 0.129    | 0               | AF-Q86T26-F1-model_v3 | alphafold monomer v2.0 prediction for transmembrane protease serine 11b (q86t26) |\n| 1    | A56H A99D A195S   | A74H A122D A218S        | 0.134    | 0               | AF-Q6UWY2-F1-model_v3 | alphafold monomer v2.0 prediction for serine protease 57 (q6uwy2)                |\n| 2    | A56H A99D A195S   | A70H A112D A205S        | 0.144    | 0               | AF-P49862-F1-model_v3 | alphafold monomer v2.0 prediction for kallikrein-7 (p49862)                      |\n| 3    | A56H A99D A195S   | A357H A406D A513S       | 0.144    | 0               | AF-P00750-F1-model_v3 | alphafold monomer v2.0 prediction for tissue-type plasminogen activator (p00750) |\n| 4    | A56H A99D A195S   | A70H A117D A202S        | 0.149    | 0               | AF-P08246-F1-model_v3 | alphafold monomer v2.0 prediction for neutrophil elastase (p08246)               |\n\n**matched_motif**: Reference motif residue IDs that were matched. \n**similar_motif_found**: Residue IDs of the similar motif that was found. \n**RMSD**: Root Mean Square Deviation between the reference and similar motif found.\n**n_mutations**: Number of mutated residues relative to the original reference motif.\n**PDB_ID**: PDB ID that contains the similar motif that was found.    \n**header_description**: Text description of the PDB file that contains the similar motif found.\n\n### Searching for similar motifs with potential mutations\n\nThe serine protease catalytic site example is simple in the sense that we only want to find PDB structures with motifs that exactly match the residue types of the reference motif (i.e Histidine, Aspartate and Serine). But in some cases the constraints may be more relaxed for certain residues, in which case one needs to also search for mutated versions of the motif. pyScoMotif's `motif-search` command comes with this capability built-in.\n\nIn this section we showcase the search of the [alcohol dehydrogenase](https://www.ebi.ac.uk/thornton-srv/m-csa/entry/256/) catalytic site, which can tolerate different mutations at different positions.\n\nThere are two options that control the search for mutated versions of a reference motif:\n- The `--residue_type_policy` option, for which there are 4 possibilities:\n    - \"strict\" (default): no mutations allowed, only search for motifs that have <ins>exactly</ins> the same residue types as the reference motif.\n    - \"relaxed\": residues can mutate according to their residue type group, which are: non-polar (GAVLI), polar (STPNQ), sulfur (MC), positives (KRH), negatives (DE), aromatic (FYW).\n    - \"fully_relaxed\": residues can be mutated to any of the other 19 possible residues.\n    - A custom position specific exchange in JSON format. This allows complete control over the possible mutations of each residue. The format takes residue IDs as keys and a string of possible mutations as values (e.g: '{\"A1\":\"KR\", \"A2\":\"YW\", \"A5\":\"D\"}'. Note the use of single and double quotes).\n\n- The `--max_n_mutated_residues` option, which controls the maximum number of combinations of mutations that should be allowed when generating mutated motifs. This option can be very useful if you want to search for similar motifs carrying multiple mutations. It is set to 1 by default.\n\nHere we use PDB 1hso and residues A46, A48, A51, A67 and A174 as our reference motif, and take advantage of the fully_relaxed option to search for all the possible mutated versions carrying 1 mutation, so our search command would be:\n\n```\npyscomotif motif-search --residue_type_policy=fully_relaxed --results_output_path=/home/user/Downloads/alcohol_dehydrogenase_pyScoMotif_result.csv --n_cores=6 /home/user/Downloads/UP000005640_9606_HUMAN_v3/pyScoMotif_index /home/user/Downloads/1hso.pdb A46 A48 A51 A67 A174\n```\n",
    "readme_length": 8831
  },
  {
    "name": "SLiMSuite",
    "full_name": "slimsuite/SLiMSuite",
    "description": "Open source short linear motif discovery and sequence analysis",
    "stars": 25,
    "forks": 8,
    "language": "Python",
    "url": "https://github.com/slimsuite/SLiMSuite",
    "topics": [],
    "created_at": "2015-06-01T03:59:18Z",
    "updated_at": "2025-08-20T12:04:57Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# SLiMSuite\n[<img src=\"http://www.slimsuite.unsw.edu.au/graphics/button-slimsuite.gif\" width=150 align=\"right\">][2]\nOpen source short linear motif discovery and sequence analysis.\n\nThis repository contains the core code for the SLiMSuite package of short linear motif (SLiM) discovery and sequence analysis tools from the [Edwards Lab][1].\n\nA summary of SLiMSuite and SeqSuite tools can be found in [PROGRAMS.md][6]. More information can be found at [the SLiMSuite Blog][2] and in the `docs/` folder. Release information can be found in [RELEASE.txt][3]. Please see [release_notes.txt][4] and individual modules for a summary of update history. Additional documentation can be found at the [SLiMSuite GitHub Pages][5] site.\n\nIf you have any questions, raise an issue or contact the author.\n\n[1]: http://www.slimsuite.unsw.edu.au\n[2]: http://slimsuite.blogspot.com.au\n[3]: https://github.com/slimsuite/SLiMSuite/blob/master/RELEASE.txt\n[4]: https://github.com/slimsuite/SLiMSuite/blob/master/release_notes.txt\n[5]: https://slimsuite.github.io/SLiMSuite/\n[6]: https://github.com/slimsuite/SLiMSuite/blob/master/PROGRAMS.md\n",
    "readme_length": 1121
  },
  {
    "name": "FanControl.Releases",
    "full_name": "Rem0o/FanControl.Releases",
    "description": "This is the release repository for Fan Control, a highly customizable fan controlling software for Windows.",
    "stars": 18106,
    "forks": 537,
    "language": null,
    "url": "https://github.com/Rem0o/FanControl.Releases",
    "topics": [
      "control",
      "cpu",
      "curves",
      "fan",
      "fancontrol",
      "gpu",
      "pwm",
      "speed",
      "temperature"
    ],
    "created_at": "2020-05-31T19:37:28Z",
    "updated_at": "2025-12-02T07:05:46Z",
    "homepage": "",
    "license": "Other",
    "readme": "# Fan Control\n\n\n<p align=center>\n  <a href=\"https://www.getfancontrol.com\">\n    <img src=\"Images/logo.gif\" width=60/>\n  </a>\n</p>\n\n\n<p align=center>This is the release repository for <a href=\"https://getFanControl.com\">Fan Control</a>, a focused and highly customizable fan controlling software for Windows.<br><i>Sources for this software are closed.</i></span>\n\n<br>\n<br>\n\n[![Download](https://img.shields.io/badge/Download-FanControl-green.svg?style=flat&logo=download)](/FanControl.zip?raw=true)\n[![Donate](https://img.shields.io/badge/Donate-PayPal-blue.svg?style=flat&logo=paypal)](https://www.paypal.com/donate/?business=N4JPSTUQHRJM8&no_recurring=0&item_name=Fan%20Control%20software%20creator%20and%20maintainer.%20Donations%20allow%20me%20to%20continue%20working%20on%20this%20project%20while%20keeping%20it%20free%20to%20use.%20Thank%20you%20for%20contributing%21&currency_code=USD)\n\n## Announcement\n\n[V238](https://github.com/Rem0o/FanControl.Releases/releases/tag/V238) and above now ships with a [PawnIO](https://pawnio.eu/) build of [LHM](https://github.com/LibreHardwareMonitor/LibreHardwareMonitor). This will fix the anti-virus problems encountered with WinRing0, as it is no longer shipped with FanControl.  You may still use V237 or any version before if you want to keep the WinRing0 version.\n\nThere is a known issue where FACEIT will block the new driver. See this [issue](https://github.com/namazso/PawnIO.Setup/issues/1).\n\n## WARNING ( V237 and below )\n\nAs of 09/04/2025, Winring0 (FanControl.sys) is flagged as [Trojan:Win32/Vigorf.A](https://github.com/Rem0o/FanControl.Releases/issues/3410#issuecomment-3254057373) by Windows Defender. This will cause sensors to not be detected.\nYou do not need to open an issue on the subject. Any new issues with this exact problem will be closed as duplicate.\n\nIf you experience this issue, you may:\n\n* Update to the latest version\n* (Can't officialy recommend) whitelist/exclude the detection in Windows Security/Defender. Read the [official microsoft response/article](https://support.microsoft.com/en-us/windows/microsoft-defender-antivirus-alert-vulnerabledriver-winnt-winring0-eb057830-d77b-41a2-9a34-015a5d203c42) carefully.\n* With [V235 to V237](https://github.com/Rem0o/FanControl.Releases/releases), you can easily [use an alternative LHM branch](https://github.com/Rem0o/FanControl.Releases/issues/3410#issuecomment-3258597822) that doesn't use WinRing0. Consider this a Beta, as it hasn't been widely tested yet. It may not work, be unstable, not be reliable, you get the idea.\n\n\n## New\n* New seperate \"Up\" and \"Down\" hysteresis for graph, linear and trigger fan curves\n![Fan Control](Images/Hysteresis.png)\n* The software is now open for translations. Contributions are welcomed here [FanControl.i18n](https://github.com/Rem0o/FanControl.i18n)\n* AMD GPU support through [ADLXWrapper](https://github.com/Rem0o/ADLXWrapper).\n* Fan calibration and RPM mode for fan curves. See [discussion](https://github.com/Rem0o/FanControl.Releases/discussions/2333).\n* Installer now available in addition to the portable version of the application. Both .NET 4.8 and 8.0 versions of the application are available as such [here](https://github.com/Rem0o/FanControl.Releases/releases).\n\n## Installation\n\n1. [Download the latest archive](/FanControl.zip?raw=true) <i>or</i> [an installer from the release page](https://github.com/Rem0o/FanControl.Releases/releases).\n2. Extract to the desired installation folder <i>or</i> run the installer\n3. Start FanControl.exe\n\n### Install with [Scoop](https://scoop.sh/#/apps?s=2&d=1&o=true&p=1&q=fan+control)\n\n```\nscoop bucket add extras\nscoop install fancontrol\n```\n\n### Install with [Winget](https://apps.microsoft.com/detail/9nblggh4nns1?rtc=1&hl=en-us&gl=US#activetab=pivot:overviewtab)\n\n```\nwinget install Rem0o.FanControl\n```\n\n# Documentation\n\nhttps://getfancontrol.com/docs/\n\n## Featured On\nJayzTwoCents - Everyone NEEDS this FREE piece of software... You will thank me!\n\n<a href=\"https://www.youtube.com/watch?v=uDPKVKBMQU8\"><img alt=\"JayzTwoCents - Everyone NEEDS this FREE piece of software... You will thank me!\" src=\"https://i.ytimg.com/vi/uDPKVKBMQU8/hq720.jpg?sqp=-oaymwEcCNAFEJQDSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&amp;rs=AOn4CLDpjcuKgjSlSO8bZt8bcG4eKoRB4Q\" width=\"350\" /></a>\n\n## Main features\n\n* Guided __setup__ process on first launch\n* Save, edit and load multiple __profiles__\n* Change the __theme__ and __color__ of the application.\n* Multiple temperature __sources__ ( CPU, GPU, motherboard, hard drives... )\n* Multiple fan curve __[functions](https://getfancontrol.com/docs)__, including a custom __[graph](#graph-fan-curve-editor)__\n* __Mix__ fan curves or sensor togethers (max, min, average)\n* Low resource usage\n* Advanced tuning with steps, start %, stop %, response time and hysteresis\n\n![Fan Control](Images/MainUI.png)\n\n## Uninstall\n\n### Portable\nYou can leave the files there for use further down the line, or delete them.\nNote: If you have Fan Control set to automatically start with Windows, either untick the checkbox in Fan Control, or manually delete the \"Fan Control\" task in Windows Task Scheduler.\n\n### Installer\nUninstall like any other windows program through the programs list\n\n## Plugins\n\n The plugin system let you inject any type of sensor into FanControl, see [Plugins wiki](https://github.com/Rem0o/FanControl.Releases/wiki/Plugins)\n\n![Plugin Installation](Images/PluginInstallation.png)\n\nFrom the community (notify me if I'm missing some):\n* https://github.com/AMoo-Miki/FanControlThermaltake updated fork of https://github.com/fu-raz/FanControlThermaltake\n* https://github.com/antoine-bouteiller/FanControl.LiquidCtl to interface with AIO devices (see compatible devices here https://github.com/liquidctl/liquidctl). Updated version of https://github.com/jmarucha/FanControl.Liquidctl with support for controller which returns multiples fan entries\n* https://github.com/Mourdraug/FanControl.AsusWMI to interface with ASUS motherboards through WMI methods\n* https://github.com/medevil84/FanControl.AquacomputerDevices to interface with aquacomputer HighFlowNext, Quadro and Octo devices\n* https://github.com/FoPzl/FanControl.AquacomputerQuadro to interface with aquacomputer Quadro\n* https://github.com/vision57/FanControl.GPU-Z\n* https://github.com/EvanMulawski/FanControl.CorsairLink to interface with Corsair Commander controllers and Hydro liquid coolers\n* https://github.com/EvanMulawski/FanControl.Razer to interface with Razer devices\n* https://github.com/hgross/FanControl.HomeAssistant to interface with [HomeAssistant](https://github.com/home-assistant) connected temperature sensors (i.e. ambient temperatures via Philips Hue, HomeMatic, HomeKit or many other brands & protocols)\n* https://github.com/brokenmass/Fancontrol.NzxtKraken to interface with NZXT Kraken AIO that are not yet supported by LibreHardwareMonitor for example `Kraken X2` and `Kraken X3 - new PID`. See [LHM PR](https://github.com/LibreHardwareMonitor/LibreHardwareMonitor/pull/1078)\n* https://github.com/EightB1ts/FanControl.LianLi to interface with LianLi [L-Connect 3](https://lian-li.com/l-connect3/) fan controllers\n* https://github.com/TimSirmovics/FanControl.NvThermalSensors to get GPU Hot Spot and Memory Junction temperature for Nvidia GPUs\n* https://github.com/SasaKaranovic/FanControl.OpenFan to interface with [OpenFAN](https://github.com/SasaKaranovic/OpenFanController) controller\n* https://github.com/Brian-E-Taylor/FanControl.AIDA64 to get readings out of AIDA64\n* https://github.com/Benson5650/FanControl.RazerCoolingPadPlugin to interface with Razer Laptop Cooling Pad.\n* https://github.com/chenx-dust/FanControl.GPDPlugin to interface with GPD devices\n* https://github.com/jiarandiana0307/FanControl.LenovoPlugin to control fans of Lenovo laptops with `Lenovo ACPI-Compliant Virtual Power Controller` installed\n\nFrom Rem0o\n* https://github.com/Rem0o/FanControl.IntelCtlLibrary (Intel ARC gpus)\n* https://github.com/Rem0o/FanControl.HWInfo to import HWInfo sensor data\n* https://github.com/Rem0o/FanControl.DellPlugin for dell laptops and some towers\n\n## Issues and hardware compatibility\n\n* I am not the main developer for the driver/backend portion of this software. Fan Control is basically a UI on top of existing hardware libraries. Any issue regarding hardware compatibility entirely depends on the following projects. If you can't contribute meaningfully with a branch/PR, don't pollute their issue page with \"XXXXX doesn't work\". The hardware is needed for testing. If a dev has interest in that project, is browsing it, has your specific hardware AND the knowledge to make it work, that dev won't be looking for your issue, he'll just raise a PR.\n  * https://github.com/LibreHardwareMonitor/LibreHardwareMonitor\n\n* Please only open issues on this repository for the software itself, UI, feature request and so on. If it's a \"make this work please\" hardware support request, I will link this section of the README and close your issue.\n* If you do have a special hardware compatibility request and you can provide a __working__ sample of code that can be used in .NET, like with a [Plugin](https://github.com/Rem0o/FanControl.Releases/wiki/Plugins), then feel free to submit that.\n\n## FAQ\n* __Q__: What settings should I set in my BIOS to play along nicely with FanControl?\n<br>__A__: You want to avoid any \"smart\" control from your BIOS. Setting a fixed default speed, like 50%, works great for most people. Also keep an eye if your BIOS has PWM or DC mode on. One could work better for you depending on your setup.\n* __Q__: My NVIDIA graphics card has 3 fans, but only 2 control cards show up in the UI, why?\n<br>__A__: Your card only has 2 channels, multiple fans are plugged into the same channel.\n* __Q__: My NVIDIA graphics card won't go below 30% and doesn't go to 0 RPM, why?\n<br>__A__: [See here](https://github.com/Rem0o/FanControl.Releases/wiki/Nvidia-30%25-and-0-RPM)\n* __Q__: There is no control cards / control cards are missing / control cards are not changing my fan speeds, what's the issue?\n<br>__A__: See __[ Issues and hardware compatibility](#issues-and-hardware-compatibility)__.\n* __Q__: How does __[FAN CURVE TYPE]__ works and what does its parameters do?\n<br>__A__: Click on its card's icon at the top left, a dialog will tell you.\n* __Q__: Does it run on my OS?\n<br>__A__: If your OS is Windows 10 __Or 11__, yes.\n\n## Libraries used:\n* https://github.com/LibreHardwareMonitor/LibreHardwareMonitor (main sensor source)\n* https://github.com/MaterialDesignInXAML/MaterialDesignInXamlToolkit (UI)\n* https://github.com/falahati/NvAPIWrapper (Nvidia gpu fan control and sensor reading)\n* https://github.com/Rem0o/ADLXWrapper (AMD gpu fan control and sensor reading)\n* https://github.com/punker76/gong-wpf-dragdrop (various drag and drop actions)\n\n## GitHub Sponsors\nThe GitHub Sponsor button on this page is intended for the related open-source work surrounding FanControl. This includes contributions to the open-source libraries used, mainly [LHM](https://github.com/LibreHardwareMonitor/LibreHardwareMonitor), the plugin system and the various open-source plugins I contribute to, the AMD gpu driver support through [ADLXWrapper](https://github.com/Rem0o/ADLXWrapper), and more.  Your sponsorship helps me continue to contribute to the community, maintain existing projects, and develop new ones. It does not apply to the main program (FanControl) itself. Use the [Paypal](https://www.paypal.com/donate/?cmd=_donations&business=N4JPSTUQHRJM8&currency_code=USD&source=url&item_name=Fan+Control) donation button if your intent is to support the FanControl software itself.\n",
    "readme_length": 11647
  },
  {
    "name": "johnny-five",
    "full_name": "rwaldron/johnny-five",
    "description": "JavaScript Robotics and IoT programming framework, developed at Bocoup.",
    "stars": 13404,
    "forks": 1746,
    "language": "JavaScript",
    "url": "https://github.com/rwaldron/johnny-five",
    "topics": [
      "1-wire",
      "adc",
      "arduino",
      "beaglebone-black",
      "bluetooth",
      "chip",
      "dac",
      "gpio",
      "i2c",
      "intel",
      "iot",
      "javascript",
      "pcduino",
      "photon",
      "pwm",
      "raspberry-pi",
      "robotics",
      "serial",
      "spi",
      "tessel"
    ],
    "created_at": "2012-03-30T20:09:52Z",
    "updated_at": "2025-12-02T02:55:10Z",
    "homepage": "http://johnny-five.io",
    "license": "Other",
    "readme": "![](https://github.com/rwaldron/johnny-five/raw/main/assets/sgier-johnny-five.png)\n\n# Johnny-Five\n### The JavaScript Robotics Programming Framework\n\n<!-- \n\n    Hello!\n\n    Please don't edit this file!\n\n    If you'd like to make changes to the readme contents, please make them in the tpl/.readme.md file. If you'd like to add an example: \n\n    1. Add the file in `eg/`\n    2. Add a breadboard image in `docs/breadboards`\n    3. Add an entry to `tpl/programs.json`. \n    4. Generated the markdown with: `grunt examples`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-->\n\n\n_Artwork by [Mike Sgier](http://msgierillustration.com)_\n\n[![Build, Lint, Test and Measure Coverage](https://github.com/rwaldron/johnny-five/actions/workflows/npm-grunt.yml/badge.svg)](https://github.com/rwaldron/johnny-five/actions)\n[![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/hmke71k7uemtnami/branch/main?svg=true)](https://ci.appveyor.com/project/rwaldron/johnny-five)\n[![Coverage Status](https://coveralls.io/repos/github/rwaldron/johnny-five/badge.svg?branch=main)](https://coveralls.io/github/rwaldron/johnny-five?branch=main)\n[![Install Size](https://packagephobia.now.sh/badge?p=johnny-five)](https://packagephobia.now.sh/result?p=johnny-five)\n[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/rwaldron/johnny-five)\n\n\n\n**Johnny-Five is an Open Source, Firmata Protocol based, IoT and Robotics programming framework, developed by the [Nodebots](https://twitter.com/nodebots) Community. Johnny-Five programs can be written for Arduino (all models), Electric Imp, Beagle Bone, Intel Galileo & Edison, Linino One, Pinoccio, pcDuino3, Raspberry Pi, Particle/Spark Core & Photon, Tessel 2, TI Launchpad and more!**\n\nJohnny-Five has grown from a passion project into a tool for inspiring learning and creativity for people of all ages, backgrounds, and from all across the world.\n\nJust interested in learning and building awesome things? You might want to start with the official [Johnny-Five website](http://johnny-five.io).\n\n* If you want to find the API documentation, [thatâ€™s right here](http://johnny-five.io/api/).\n* Need to figure out what platform to use for a project? We put that stuff [here](http://johnny-five.io/platform-support/).\n* Need inspiration for your next NodeBot? Check out the [examples](http://johnny-five.io/examples/).\n* Want to stay up-to-date with projects in the community? [Check this out](http://johnny-five.io/articles/).\n* Need NodeBots community or Johnny-Five project updates and announcements? [This](http://johnny-five.io/news/) is what youâ€™re looking for.\n\n\nJohnny-Five does not attempt to provide \"all the things\", but instead focuses on delivering robust, reality tested, highly composable APIs that behave consistently across all supported hardware platforms. Johnny-Five wants to be a baseline control kit for hardware projects, allowing you the freedom to build, grow and experiment with diverse JavaScript libraries of your own choice. Johnny-Five couples comfortably with:\n\n- Popular application libraries such as [Express.js](http://expressjs.com/) and [Socket.io](http://socket.io/).\n- Fellow hardware projects like [ar-drone](https://github.com/felixge/node-ar-drone), [Aerogel](https://github.com/ceejbot/aerogel) and [Spheron](https://github.com/alchemycs/spheron)\n- Bluetooth game controllers like [XBox Controller](https://github.com/andrew/node-xbox-controller) and [DualShock](https://github.com/rdepena/node-dualshock-controller)\n- IoT frameworks, such as [Octoblu](http://www.octoblu.com/)\n\n...And that's only a few of the many explorable possibilities. Check out these exciting projects: [node-pulsesensor](https://www.npmjs.org/package/node-pulsesensor), [footballbot-workshop-ui](https://www.npmjs.org/package/footballbot-workshop-ui), [nodebotui](https://www.npmjs.org/package/nodebotui), [dublin-disco](https://www.npmjs.org/package/dublin-disco), [node-slot-car-bot](https://www.npmjs.org/package/node-slot-car-bot), [servo-calibrator](https://www.npmjs.org/package/servo-calibrator), [node-ardx](https://www.npmjs.org/package/node-ardx), [nodebot-workshop](https://www.npmjs.org/package/nodebot-workshop), [phone-home](https://www.npmjs.org/package/phone-home), [purple-unicorn](https://www.npmjs.org/package/purple-unicorn), [webduino](https://www.npmjs.org/package/webduino), [leapduino](https://www.npmjs.org/package/leapduino), [lasercat-workshop](https://www.npmjs.org/package/lasercat-workshop), [simplesense](https://www.npmjs.org/package/simplesense), [five-redbot](https://www.npmjs.org/package/five-redbot), [robotnik](https://www.npmjs.org/package/robotnik), [the-blender](https://www.npmjs.org/package/the-blender)\n\n\n**Why JavaScript?**\n[NodeBots: The Rise of JavaScript Robotics](http://www.voodootikigod.com/nodebots-the-rise-of-js-robotics)\n\n## Hello Johnny\n\nThe ubiquitous \"Hello World\" program of the microcontroller and SoC world is \"blink an LED\". The following code demonstrates how this is done using the Johnny-Five framework.\n\n```javascript\nconst { Board, Led } = require(\"johnny-five\");\nconst board = new Board();\n\nboard.on(\"ready\", () => {\n  // Create an Led on pin 13\n  const led = new Led(13);\n  // Blink every half second\n  led.blink(500);\n});\n```\n\n<img src=\"https://github.com/rwaldron/johnny-five/raw/main/assets/led-blink.gif\">\n\n> Note: Node will crash if you try to run johnny-five in the node REPL, but board instances will create their own contextual REPL. Put your script in a file.\n\n\n## Supported Hardware\n\nJohnny-Five has been tested on a variety of Arduino-compatible [Boards](https://github.com/rwaldron/johnny-five/wiki/Board).\n\nFor non-Arduino based projects, a number of platform-specific [IO Plugins](https://github.com/rwaldron/johnny-five/wiki/IO-Plugins) are available. IO Plugins allow Johnny-Five code to communicate with any non-Arduino based hardware in whatever language that platforms speaks!\n\n## Documentation\n\nDocumentation for the Johnny-Five API can be found [here](http://johnny-five.io/api/) and [example programs here](http://johnny-five.io/examples/).\n\n## Guidance\n\nNeed help? Ask a question on the [NodeBots Community Forum](http://forums.nodebots.io). If you just have a quick question or are interested in ongoing design discussions, join us in the [Johnny-Five Gitter Chat](https://gitter.im/rwaldron/johnny-five).\n\nFor step-by-step examples, including an electronics primer, check out [Arduino Experimenter's Guide for NodeJS](http://node-ardx.org/) by [@AnnaGerber](https://twitter.com/AnnaGerber)\n\nHere is a list of [prerequisites](https://github.com/rwaldron/johnny-five/wiki/Getting-Started#prerequisites) for Linux, OSX or Windows.\n\nCheck out the [bluetooth guide](https://github.com/rwaldron/johnny-five/wiki/JY-MCU-Bluetooth-Serial-Port-Module-Notes) if you want to remotely control your robot.\n\n## Setup and Assemble Arduino\n\n- Recommended Starting Kit: [Sparkfun Inventor's Kit](https://www.sparkfun.com/products/12001)\n- Download [Arduino IDE](http://arduino.cc/en/main/software)\n- Plug in your Arduino or Arduino compatible microcontroller via USB\n- Open the Arduino IDE, select: File > Examples > Firmata > StandardFirmataPlus\n    + StandardFirmataPlus is available in Firmata v2.5.0 or greater\n- Click the \"Upload\" button.\n\nIf the upload was successful, the board is now prepared and you can close the Arduino IDE.\n\nFor non-Arduino projects, each IO Plugin's repo will provide its own platform specific setup instructions.\n\n\n## Hey you, here's Johnny!\n\n#### Source Code:\n\n``` bash\ngit clone git://github.com/rwaldron/johnny-five.git && cd johnny-five\n\nnpm install\n```\n\n#### npm package:\n\nInstall the module with:\n\n```bash\nnpm install johnny-five\n```\n\n\n## Example Programs\n\nTo get you up and running quickly, we provide a variety of examples for using each Johnny-Five component. One thing weâ€™re especially excited about is the extensive collection of [Fritzing](http://fritzing.org/home/) diagrams youâ€™ll find throughout the site. A huge part of doing any Johnny-Five project is handling the actual hardware, and weâ€™ve included these as part of the documentation because we realised that instructions on how to write code to control a servo are insufficient without instructions on how to connect a servo!\n\nTo interactively navigate the examples, visit the [Johnny-Five examples](http://johnny-five.io/examples/) page on the official website. If you want to link directly to the examples in this repo, you can use one of the following links.\n\n**There are presently 362 example programs with code and diagrams!**\n\n<!--extract-start:examples-->\n\n### Board\n- [Board - Basic Initialization](https://github.com/rwaldron/johnny-five/blob/main/docs/board.md)\n- [Board - Cleanup in 'exit' event](https://github.com/rwaldron/johnny-five/blob/main/docs/board-cleanup.md)\n- [Board - Multiple in one program](https://github.com/rwaldron/johnny-five/blob/main/docs/board-multi.md)\n- [Board - Specify Sampling Interval](https://github.com/rwaldron/johnny-five/blob/main/docs/board-sampling-interval.md)\n- [Board - Specify port](https://github.com/rwaldron/johnny-five/blob/main/docs/board-with-port.md)\n- [Custom Data Properties](https://github.com/rwaldron/johnny-five/blob/main/docs/custom-properties.md)\n- [Pin](https://github.com/rwaldron/johnny-five/blob/main/docs/pin.md)\n- [REPL](https://github.com/rwaldron/johnny-five/blob/main/docs/repl.md)\n\n### LED\n- [LED](https://github.com/rwaldron/johnny-five/blob/main/docs/led.md)\n- [LED - Blink](https://github.com/rwaldron/johnny-five/blob/main/docs/led-blink.md)\n- [LED - Demo sequence](https://github.com/rwaldron/johnny-five/blob/main/docs/led-demo-sequence.md)\n- [LED - Fade](https://github.com/rwaldron/johnny-five/blob/main/docs/led-fade.md)\n- [LED - Fade callback](https://github.com/rwaldron/johnny-five/blob/main/docs/led-fade-callback.md)\n- [LED - Fade with animation](https://github.com/rwaldron/johnny-five/blob/main/docs/led-fade-animation.md)\n- [LED - PCA9685](https://github.com/rwaldron/johnny-five/blob/main/docs/led-PCA9685.md)\n- [LED - Pulse](https://github.com/rwaldron/johnny-five/blob/main/docs/led-pulse.md)\n- [LED - Pulse with animation](https://github.com/rwaldron/johnny-five/blob/main/docs/led-pulse-animation.md)\n- [LED - Slider](https://github.com/rwaldron/johnny-five/blob/main/docs/led-slider.md)\n- [LED - Tessel Servo Module](https://github.com/rwaldron/johnny-five/blob/main/docs/led-tessel-servo-module.md)\n- [LEDs - An array of LEDs](https://github.com/rwaldron/johnny-five/blob/main/docs/led-array.md)\n- [LEDs - Controlling an array of LEDs](https://github.com/rwaldron/johnny-five/blob/main/docs/led-array-controller.md)\n\n### LED: RGB\n- [LED - RGB (Common Anode)](https://github.com/rwaldron/johnny-five/blob/main/docs/led-rgb-anode.md)\n- [LED - RGB (Common Anode) PCA9685](https://github.com/rwaldron/johnny-five/blob/main/docs/led-rgb-anode-PCA9685.md)\n- [LED - RGB Intensity](https://github.com/rwaldron/johnny-five/blob/main/docs/led-rgb-intensity.md)\n- [LED - Rainbow](https://github.com/rwaldron/johnny-five/blob/main/docs/led-rainbow.md)\n- [LED - Rainbow BlinkM](https://github.com/rwaldron/johnny-five/blob/main/docs/led-rgb-BLINKM.md)\n\n### LED: Digits & Matrix\n- [LED - Digital Clock](https://github.com/rwaldron/johnny-five/blob/main/docs/led-digits-clock.md)\n- [LED - Digital Clock, Dual Displays](https://github.com/rwaldron/johnny-five/blob/main/docs/led-digits-clock-dual.md)\n- [LED - Digital Clock, HT16K33](https://github.com/rwaldron/johnny-five/blob/main/docs/led-digits-clock-HT16K33.md)\n- [LED - Draw Matrix Characters Demo](https://github.com/rwaldron/johnny-five/blob/main/docs/led-chars-demo.md)\n- [LED - Enumerate Matrix Characters & Symbols](https://github.com/rwaldron/johnny-five/blob/main/docs/led-enumeratechars.md)\n- [LED - Matrix](https://github.com/rwaldron/johnny-five/blob/main/docs/led-matrix.md)\n- [LED - Matrix Demo](https://github.com/rwaldron/johnny-five/blob/main/docs/led-matrix-tutorial.md)\n- [LED - Matrix HT16K33](https://github.com/rwaldron/johnny-five/blob/main/docs/led-matrix-HT16K33.md)\n- [LED - Matrix HT16K33 16x8](https://github.com/rwaldron/johnny-five/blob/main/docs/led-matrix-HT16K33-16x8.md)\n\n### Servo\n- [Servo](https://github.com/rwaldron/johnny-five/blob/main/docs/servo.md)\n- [Servo - Continuous](https://github.com/rwaldron/johnny-five/blob/main/docs/servo-continuous.md)\n- [Servo - Drive](https://github.com/rwaldron/johnny-five/blob/main/docs/servo-drive.md)\n- [Servo - Multi-Turn](https://github.com/rwaldron/johnny-five/blob/main/docs/servo-multi-turn.md)\n- [Servo - PCA9685](https://github.com/rwaldron/johnny-five/blob/main/docs/servo-PCA9685.md)\n- [Servo - Prompt](https://github.com/rwaldron/johnny-five/blob/main/docs/servo-prompt.md)\n- [Servo - Slider control](https://github.com/rwaldron/johnny-five/blob/main/docs/servo-slider.md)\n- [Servo - Tessel Servo Module](https://github.com/rwaldron/johnny-five/blob/main/docs/servo-tessel-servo-module.md)\n- [Servos - An array of servos](https://github.com/rwaldron/johnny-five/blob/main/docs/servo-array.md)\n\n### GPS\n- [GPS - Adafruit Ultimate GPS Breakout](https://github.com/rwaldron/johnny-five/blob/main/docs/gps-adafruit.md)\n- [GPS - Default GPS](https://github.com/rwaldron/johnny-five/blob/main/docs/gps.md)\n- [GPS - Hardware Serial](https://github.com/rwaldron/johnny-five/blob/main/docs/gps-hardware-serial.md)\n- [GPS - Sparkfun GP-20U7](https://github.com/rwaldron/johnny-five/blob/main/docs/gps-GP-20U7.md)\n\n### Servo Animation\n- [Servo - Animation](https://github.com/rwaldron/johnny-five/blob/main/docs/servo-animation.md)\n- [Servo - Leg Animation](https://github.com/rwaldron/johnny-five/blob/main/docs/servo-animation-leg.md)\n\n### Color\n- [Color - EVShield EV3 (Code)](https://github.com/rwaldron/johnny-five/blob/main/docs/color-EVS_EV3.md)\n- [Color - EVShield EV3 (Raw)](https://github.com/rwaldron/johnny-five/blob/main/docs/color-raw-EVS_EV3.md)\n- [Color - EVShield NXT (Code)](https://github.com/rwaldron/johnny-five/blob/main/docs/color-EVS_NXT.md)\n- [Color - ISL29125](https://github.com/rwaldron/johnny-five/blob/main/docs/color-ISL29125.md)\n\n### Motor\n- [Motor](https://github.com/rwaldron/johnny-five/blob/main/docs/motor.md)\n- [Motor - 3 pin](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-3-pin.md)\n- [Motor - Adafruit DRV8871 DC Motor Driver Breakout](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-drv8871.md)\n- [Motor - Brake](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-brake.md)\n- [Motor - Current](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-current.md)\n- [Motor - Directional](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-directional.md)\n- [Motor - EVShield EV3](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-EVS_EV3.md)\n- [Motor - EVShield NXT](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-EVS_NXT.md)\n- [Motor - Enable Pin](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-enable.md)\n- [Motor - GROVE_I2C_MOTOR_DRIVER](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-GROVE_I2C.md)\n- [Motor - H-Bridge](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-hbridge.md)\n- [Motor - LUDUS](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-LUDUS.md)\n- [Motor - PCA9685](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-PCA9685.md)\n- [Motor - Pololu VNH5019 Dual Motor Driver Breakout](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-vnh5019.md)\n- [Motor - Sparkfun Dual H-bridge Edison Block](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-sparkfun-edison-hbridge.md)\n- [Motor - Sparkfun TB6612FNG](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-TB6612FNG.md)\n- [Motor - l298 Breakout](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-l298-breakout.md)\n- [Motors - Dual H-Bridge](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-hbridge-dual.md)\n\n### Stepper Motor\n- [Stepper - Driver](https://github.com/rwaldron/johnny-five/blob/main/docs/stepper-driver.md)\n- [Stepper - Four Wire](https://github.com/rwaldron/johnny-five/blob/main/docs/stepper-four_wire.md)\n- [Stepper - Sweep](https://github.com/rwaldron/johnny-five/blob/main/docs/stepper-sweep.md)\n\n### ESC & Brushless Motor\n- [ESC - Bidirectional](https://github.com/rwaldron/johnny-five/blob/main/docs/esc-bidirectional.md)\n- [ESC - Keypress controlled ESCs](https://github.com/rwaldron/johnny-five/blob/main/docs/esc-keypress.md)\n- [ESC - PCA9685](https://github.com/rwaldron/johnny-five/blob/main/docs/esc-PCA9685.md)\n\n### Button / Switch\n- [Button](https://github.com/rwaldron/johnny-five/blob/main/docs/button.md)\n- [Button - Bumper](https://github.com/rwaldron/johnny-five/blob/main/docs/button-bumper.md)\n- [Button - EVShield EV3](https://github.com/rwaldron/johnny-five/blob/main/docs/button-EVS_EV3.md)\n- [Button - EVShield NXT](https://github.com/rwaldron/johnny-five/blob/main/docs/button-EVS_NXT.md)\n- [Button - Options](https://github.com/rwaldron/johnny-five/blob/main/docs/button-options.md)\n- [Button - Pullup](https://github.com/rwaldron/johnny-five/blob/main/docs/button-pullup.md)\n- [Buttons - Collection w/ AT42QT1070](https://github.com/rwaldron/johnny-five/blob/main/docs/button-collection-AT42QT1070.md)\n- [Switch](https://github.com/rwaldron/johnny-five/blob/main/docs/switch.md)\n- [Switch - Magnetic Door](https://github.com/rwaldron/johnny-five/blob/main/docs/switch-magnetic-door.md)\n- [Switch - Tilt SW-200D](https://github.com/rwaldron/johnny-five/blob/main/docs/switch-tilt-SW_200D.md)\n- [Toggle Switch](https://github.com/rwaldron/johnny-five/blob/main/docs/toggle-switch.md)\n\n### Keypad\n- [Keypad - 3x4 I2C Nano Backpack](https://github.com/rwaldron/johnny-five/blob/main/docs/keypad-3X4_I2C_NANO_BACKPACK.md)\n- [Keypad - 4x4 I2C Nano Backpack](https://github.com/rwaldron/johnny-five/blob/main/docs/keypad-4X4_I2C_NANO_BACKPACK.md)\n- [Keypad - VKEY](https://github.com/rwaldron/johnny-five/blob/main/docs/keypad-analog-vkey.md)\n- [Keypad - Waveshare AD](https://github.com/rwaldron/johnny-five/blob/main/docs/keypad-analog-ad.md)\n- [Touchpad - Grove QTouch](https://github.com/rwaldron/johnny-five/blob/main/docs/keypad-QTOUCH.md)\n- [Touchpad - MPR121](https://github.com/rwaldron/johnny-five/blob/main/docs/keypad-MPR121.md)\n- [Touchpad - MPR121, Sensitivity](https://github.com/rwaldron/johnny-five/blob/main/docs/keypad-MPR121-sensitivity.md)\n- [Touchpad - MPR121QR2_SHIELD](https://github.com/rwaldron/johnny-five/blob/main/docs/keypad-MPR121QR2_SHIELD.md)\n- [Touchpad - MPR121_KEYPAD](https://github.com/rwaldron/johnny-five/blob/main/docs/keypad-MPR121_KEYPAD.md)\n- [Touchpad - MPR121_SHIELD](https://github.com/rwaldron/johnny-five/blob/main/docs/keypad-MPR121_SHIELD.md)\n\n### Relay\n- [Relay](https://github.com/rwaldron/johnny-five/blob/main/docs/relay.md)\n- [Relay - Collection](https://github.com/rwaldron/johnny-five/blob/main/docs/relay-collection.md)\n- [Relay On Analog Pin](https://github.com/rwaldron/johnny-five/blob/main/docs/relay-on-analog-pin.md)\n\n### Shift Register\n- [Shift Register](https://github.com/rwaldron/johnny-five/blob/main/docs/shift-register.md)\n- [Shift Register - Common Anode Seven Segment controller](https://github.com/rwaldron/johnny-five/blob/main/docs/shift-register-seven-segment-anode.md)\n- [Shift Register - Common Anode Seven segments, Chained](https://github.com/rwaldron/johnny-five/blob/main/docs/shift-register-daisy-chain-anode.md)\n- [Shift Register - Seven Segment controller](https://github.com/rwaldron/johnny-five/blob/main/docs/shift-register-seven-segment.md)\n- [Shift Register - Seven segments, Chained](https://github.com/rwaldron/johnny-five/blob/main/docs/shift-register-daisy-chain.md)\n\n### Infrared Reflectance\n- [IR Motion](https://github.com/rwaldron/johnny-five/blob/main/docs/ir-motion.md)\n- [IR Proximity](https://github.com/rwaldron/johnny-five/blob/main/docs/ir-proximity.md)\n- [IR Reflectance](https://github.com/rwaldron/johnny-five/blob/main/docs/ir-reflect.md)\n- [IR Reflectance Array](https://github.com/rwaldron/johnny-five/blob/main/docs/ir-reflect-array.md)\n\n### Proximity\n- [Proximity](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity.md)\n- [Proximity - EVShield EV3 (IR)](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-EVS_EV3_IR.md)\n- [Proximity - EVShield EV3 (IR)](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-EVS_EV3_IR-alert.md)\n- [Proximity - EVShield EV3 (Ultrasonic)](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-EVS_EV3_US.md)\n- [Proximity - EVShield EV3 (Ultrasonic)](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-EVS_EV3_US-alert.md)\n- [Proximity - GP2Y0A710K0F](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-GP2Y0A710K0F.md)\n- [Proximity - HC-SR04](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-hcsr04.md)\n- [Proximity - HC-SR04 (Analog)](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-hcsr04-analog.md)\n- [Proximity - HC-SR04 I2C Backpack](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-hcsr04-i2c.md)\n- [Proximity - LIDAR-Lite](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-lidarlite.md)\n- [Proximity - MB1000](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-mb1000.md)\n- [Proximity - MB1003](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-mb1003.md)\n- [Proximity - MB1010](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-mb1010.md)\n- [Proximity - MB1230](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-mb1230.md)\n- [Proximity - SRF10](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-srf10.md)\n\n### Motion\n- [Motion](https://github.com/rwaldron/johnny-five/blob/main/docs/motion.md)\n- [Motion - GP2Y0A60SZLF](https://github.com/rwaldron/johnny-five/blob/main/docs/motion-GP2Y0A60SZLF.md)\n- [Motion - GP2Y0D805Z0F](https://github.com/rwaldron/johnny-five/blob/main/docs/motion-gp2y0d805z0f.md)\n- [Motion - GP2Y0D810Z0F](https://github.com/rwaldron/johnny-five/blob/main/docs/motion-gp2y0d810z0f.md)\n- [Motion - GP2Y0D810Z0F](https://github.com/rwaldron/johnny-five/blob/main/docs/motion-gp2y0d815z0f.md)\n\n### Joystick\n- [Joystick](https://github.com/rwaldron/johnny-five/blob/main/docs/joystick.md)\n- [Joystick - Esplora](https://github.com/rwaldron/johnny-five/blob/main/docs/joystick-esplora.md)\n- [Joystick - Pan + Tilt control](https://github.com/rwaldron/johnny-five/blob/main/docs/joystick-pantilt.md)\n- [Joystick - Sparkfun Shield](https://github.com/rwaldron/johnny-five/blob/main/docs/joystick-shield.md)\n\n### LCD\n- [Grove - RGB LCD Color Previewer](https://github.com/rwaldron/johnny-five/blob/main/docs/lcd-rgb-bgcolor-previewer.md)\n- [LCD](https://github.com/rwaldron/johnny-five/blob/main/docs/lcd.md)\n- [LCD - Enumerate characters](https://github.com/rwaldron/johnny-five/blob/main/docs/lcd-enumeratechars.md)\n- [LCD - I2C](https://github.com/rwaldron/johnny-five/blob/main/docs/lcd-i2c.md)\n- [LCD - I2C PCF8574](https://github.com/rwaldron/johnny-five/blob/main/docs/lcd-i2c-PCF8574.md)\n- [LCD - I2C Runner](https://github.com/rwaldron/johnny-five/blob/main/docs/lcd-i2c-runner.md)\n- [LCD - Runner 16x2](https://github.com/rwaldron/johnny-five/blob/main/docs/lcd-runner.md)\n- [LCD - Runner 20x4](https://github.com/rwaldron/johnny-five/blob/main/docs/lcd-runner-20x4.md)\n- [LCD - Tessel 2 16x2](https://github.com/rwaldron/johnny-five/blob/main/docs/lcd-16x2-tessel.md)\n- [Tessel 2 + Grove - RGB LCD Color Previewer](https://github.com/rwaldron/johnny-five/blob/main/docs/lcd-rgb-bgcolor-previewer-tessel.md)\n- [Tessel 2 + Grove - RGB LCD Display](https://github.com/rwaldron/johnny-five/blob/main/docs/lcd-rgb-tessel-grove-JHD1313M1.md)\n\n### Compass/Magnetometer\n- [Compass - Find north](https://github.com/rwaldron/johnny-five/blob/main/docs/magnetometer-north.md)\n- [Compass - HMC5883L](https://github.com/rwaldron/johnny-five/blob/main/docs/compass-hmc5883l.md)\n- [Compass - HMC6352](https://github.com/rwaldron/johnny-five/blob/main/docs/compass-hmc6352.md)\n- [Compass - Logger](https://github.com/rwaldron/johnny-five/blob/main/docs/magnetometer-log.md)\n- [Compass - MAG3110](https://github.com/rwaldron/johnny-five/blob/main/docs/compass-MAG3110.md)\n- [Compass - MAG3110 on Tessel 2](https://github.com/rwaldron/johnny-five/blob/main/docs/compass-MAG3110-tessel.md)\n- [Compass / Magnetometer](https://github.com/rwaldron/johnny-five/blob/main/docs/magnetometer.md)\n\n### Piezo\n- [Piezo](https://github.com/rwaldron/johnny-five/blob/main/docs/piezo.md)\n\n### IMU/Multi\n- [IMU - BNO055](https://github.com/rwaldron/johnny-five/blob/main/docs/imu-bno055.md)\n- [IMU - BNO055 (Orientation)](https://github.com/rwaldron/johnny-five/blob/main/docs/imu-bno055-orientation.md)\n- [IMU - LSM303C](https://github.com/rwaldron/johnny-five/blob/main/docs/imu-lsm303c.md)\n- [IMU - MPU6050](https://github.com/rwaldron/johnny-five/blob/main/docs/imu-mpu6050.md)\n- [Multi - BME280](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-BME280.md)\n- [Multi - BMP085](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-bmp085.md)\n- [Multi - BMP180](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-bmp180.md)\n- [Multi - DHT11_I2C_NANO_BACKPACK](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-DHT11_I2C_NANO_BACKPACK.md)\n- [Multi - DHT21_I2C_NANO_BACKPACK](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-DHT21_I2C_NANO_BACKPACK.md)\n- [Multi - DHT22_I2C_NANO_BACKPACK](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-DHT22_I2C_NANO_BACKPACK.md)\n- [Multi - HIH6130](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-HIH6130.md)\n- [Multi - HTU21D](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-htu21d.md)\n- [Multi - MPL115A2](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-mpl115a2.md)\n- [Multi - MPL3115A2](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-mpl3115a2.md)\n- [Multi - MS5611](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-MS5611.md)\n- [Multi - SHT31D](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-sht31d.md)\n- [Multi - SI7020](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-SI7020.md)\n- [Multi - SI7021](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-SI7021.md)\n- [Multi - TH02](https://github.com/rwaldron/johnny-five/blob/main/docs/multi-TH02.md)\n\n### Sensors\n- [Accelerometer](https://github.com/rwaldron/johnny-five/blob/main/docs/accelerometer.md)\n- [Accelerometer - ADXL335](https://github.com/rwaldron/johnny-five/blob/main/docs/accelerometer-adxl335.md)\n- [Accelerometer - ADXL345](https://github.com/rwaldron/johnny-five/blob/main/docs/accelerometer-adxl345.md)\n- [Accelerometer - LIS3DH](https://github.com/rwaldron/johnny-five/blob/main/docs/accelerometer-LIS3DH.md)\n- [Accelerometer - MMA7361](https://github.com/rwaldron/johnny-five/blob/main/docs/accelerometer-mma7361.md)\n- [Accelerometer - MMA8452](https://github.com/rwaldron/johnny-five/blob/main/docs/accelerometer-MMA8452.md)\n- [Accelerometer - MPU6050](https://github.com/rwaldron/johnny-five/blob/main/docs/accelerometer-mpu6050.md)\n- [Accelerometer - Pan + Tilt](https://github.com/rwaldron/johnny-five/blob/main/docs/accelerometer-pan-tilt.md)\n- [Altimeter - BMP085](https://github.com/rwaldron/johnny-five/blob/main/docs/altimeter-BMP085.md)\n- [Altimeter - BMP180](https://github.com/rwaldron/johnny-five/blob/main/docs/altimeter-BMP180.md)\n- [Altimeter - MPL3115A2](https://github.com/rwaldron/johnny-five/blob/main/docs/altimeter-mpl3115a2.md)\n- [Altimeter - MS5611](https://github.com/rwaldron/johnny-five/blob/main/docs/altimeter-MS5611.md)\n- [Barometer - BMP085](https://github.com/rwaldron/johnny-five/blob/main/docs/barometer-BMP085.md)\n- [Barometer - BMP180](https://github.com/rwaldron/johnny-five/blob/main/docs/barometer-BMP180.md)\n- [Barometer - MPL115A2](https://github.com/rwaldron/johnny-five/blob/main/docs/barometer-mpl115a2.md)\n- [Barometer - MPL3115A2](https://github.com/rwaldron/johnny-five/blob/main/docs/barometer-mpl3115a2.md)\n- [Barometer - MS5611](https://github.com/rwaldron/johnny-five/blob/main/docs/barometer-MS5611.md)\n- [Gyro](https://github.com/rwaldron/johnny-five/blob/main/docs/gyro.md)\n- [Gyro - Analog LPR5150AL](https://github.com/rwaldron/johnny-five/blob/main/docs/gyro-lpr5150l.md)\n- [Gyro - I2C MPU6050](https://github.com/rwaldron/johnny-five/blob/main/docs/gyro-mpu6050.md)\n- [Hygrometer - DHT11_I2C_NANO_BACKPACK](https://github.com/rwaldron/johnny-five/blob/main/docs/hygrometer-DHT11_I2C_NANO_BACKPACK.md)\n- [Hygrometer - DHT21_I2C_NANO_BACKPACK](https://github.com/rwaldron/johnny-five/blob/main/docs/hygrometer-DHT21_I2C_NANO_BACKPACK.md)\n- [Hygrometer - DHT22_I2C_NANO_BACKPACK](https://github.com/rwaldron/johnny-five/blob/main/docs/hygrometer-DHT22_I2C_NANO_BACKPACK.md)\n- [Hygrometer - HIH6130](https://github.com/rwaldron/johnny-five/blob/main/docs/hygrometer-HIH6130.md)\n- [Hygrometer - HTU21D](https://github.com/rwaldron/johnny-five/blob/main/docs/hygrometer-htu21d.md)\n- [Hygrometer - SHT31D](https://github.com/rwaldron/johnny-five/blob/main/docs/hygrometer-sht31d.md)\n- [Hygrometer - SI7021](https://github.com/rwaldron/johnny-five/blob/main/docs/hygrometer-SI7021.md)\n- [Hygrometer - TH02](https://github.com/rwaldron/johnny-five/blob/main/docs/hygrometer-TH02.md)\n- [Sensor](https://github.com/rwaldron/johnny-five/blob/main/docs/sensor.md)\n- [Sensor - Digital Microwave](https://github.com/rwaldron/johnny-five/blob/main/docs/sensor-digital-microwave.md)\n- [Sensor - Flex sensor](https://github.com/rwaldron/johnny-five/blob/main/docs/flex.md)\n- [Sensor - Force sensitive resistor](https://github.com/rwaldron/johnny-five/blob/main/docs/sensor-fsr.md)\n- [Sensor - Microphone](https://github.com/rwaldron/johnny-five/blob/main/docs/microphone.md)\n- [Sensor - Photoresistor](https://github.com/rwaldron/johnny-five/blob/main/docs/photoresistor.md)\n- [Sensor - Potentiometer](https://github.com/rwaldron/johnny-five/blob/main/docs/potentiometer.md)\n- [Sensor - Slide potentiometer](https://github.com/rwaldron/johnny-five/blob/main/docs/sensor-slider.md)\n- [Thermometer - BMP085](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-bmp085.md)\n- [Thermometer - BMP180](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-BMP180.md)\n- [Thermometer - DHT11_I2C_NANO_BACKPACK](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-DHT11_I2C_NANO_BACKPACK.md)\n- [Thermometer - DHT21_I2C_NANO_BACKPACK](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-DHT21_I2C_NANO_BACKPACK.md)\n- [Thermometer - DHT22_I2C_NANO_BACKPACK](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-DHT22_I2C_NANO_BACKPACK.md)\n- [Thermometer - DS18B20](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-ds18b20.md)\n- [Thermometer - Dual DS18B20](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-dual-ds18b20.md)\n- [Thermometer - HIH6130](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-HIH6130.md)\n- [Thermometer - HTU21D](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-htu21d.md)\n- [Thermometer - LM335](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-lm335.md)\n- [Thermometer - LM35](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-lm35.md)\n- [Thermometer - MAX31850](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-max31850k.md)\n- [Thermometer - MCP9808](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-MCP9808.md)\n- [Thermometer - MPL115A2](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-mpl115a2.md)\n- [Thermometer - MPL3115A2](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-mpl3115a2.md)\n- [Thermometer - MPU6050](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-mpu6050.md)\n- [Thermometer - MS5611](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-MS5611.md)\n- [Thermometer - SHT31D](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-sht31d.md)\n- [Thermometer - SI7020](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-SI7020.md)\n- [Thermometer - SI7021](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-SI7021.md)\n- [Thermometer - TH02](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-TH02.md)\n- [Thermometer - TMP102](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-tmp102.md)\n- [Thermometer - TMP36](https://github.com/rwaldron/johnny-five/blob/main/docs/temperature-tmp36.md)\n\n### Expander\n- [Expander - 74HC595](https://github.com/rwaldron/johnny-five/blob/main/docs/expander-74HC595.md)\n- [Expander - CD74HC4067, 16 Channel Analog Input Breakout](https://github.com/rwaldron/johnny-five/blob/main/docs/expander-CD74HC4067_NANO_BACKPACK.md)\n- [Expander - LIS3DH](https://github.com/rwaldron/johnny-five/blob/main/docs/expander-LIS3DH.md)\n- [Expander - MCP23008](https://github.com/rwaldron/johnny-five/blob/main/docs/expander-MCP23008.md)\n- [Expander - MCP23017](https://github.com/rwaldron/johnny-five/blob/main/docs/expander-MCP23017.md)\n- [Expander - MUXSHIELD2, Analog Sensors](https://github.com/rwaldron/johnny-five/blob/main/docs/expander-MUXSHIELD2-analog-read.md)\n- [Expander - MUXSHIELD2, Digital Input and Output](https://github.com/rwaldron/johnny-five/blob/main/docs/expander-MUXSHIELD2-mixed.md)\n- [Expander - PCA9685](https://github.com/rwaldron/johnny-five/blob/main/docs/expander-PCA9685.md)\n- [Expander - PCF8574](https://github.com/rwaldron/johnny-five/blob/main/docs/expander-PCF8574.md)\n- [Expander - PCF8575](https://github.com/rwaldron/johnny-five/blob/main/docs/expander-PCF8575.md)\n- [Expander - PCF8591](https://github.com/rwaldron/johnny-five/blob/main/docs/expander-PCF8591.md)\n\n### Photon Weather Shield\n- [Photon Weather Shield: Moisture](https://github.com/rwaldron/johnny-five/blob/main/docs/sensor-photon-weather-shield-moisture.md)\n\n### Lego EVShield\n- [Button - EVShield EV3](https://github.com/rwaldron/johnny-five/blob/main/docs/button-EVS_EV3.md)\n- [Button - EVShield NXT](https://github.com/rwaldron/johnny-five/blob/main/docs/button-EVS_NXT.md)\n- [Color - EVShield EV3 (Code)](https://github.com/rwaldron/johnny-five/blob/main/docs/color-EVS_EV3.md)\n- [Color - EVShield EV3 (Raw)](https://github.com/rwaldron/johnny-five/blob/main/docs/color-raw-EVS_EV3.md)\n- [Color - EVShield NXT (Code)](https://github.com/rwaldron/johnny-five/blob/main/docs/color-EVS_NXT.md)\n- [Light - BH1750](https://github.com/rwaldron/johnny-five/blob/main/docs/light-ambient-BH1750.md)\n- [Light - EVShield EV3 (Ambient)](https://github.com/rwaldron/johnny-five/blob/main/docs/light-ambient-EVS_EV3.md)\n- [Light - EVShield EV3 (Reflected)](https://github.com/rwaldron/johnny-five/blob/main/docs/light-reflected-EVS_EV3.md)\n- [Light - EVShield NXT (Ambient)](https://github.com/rwaldron/johnny-five/blob/main/docs/light-ambient-EVS_NXT.md)\n- [Light - EVShield NXT (Reflected)](https://github.com/rwaldron/johnny-five/blob/main/docs/light-reflected-EVS_NXT.md)\n- [Light - TSL2561](https://github.com/rwaldron/johnny-five/blob/main/docs/light-ambient-TSL2561.md)\n- [Motor - EVShield EV3](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-EVS_EV3.md)\n- [Motor - EVShield NXT](https://github.com/rwaldron/johnny-five/blob/main/docs/motor-EVS_NXT.md)\n- [Proximity - EVShield EV3 (IR)](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-EVS_EV3_IR-alert.md)\n- [Proximity - EVShield EV3 (Ultrasonic)](https://github.com/rwaldron/johnny-five/blob/main/docs/proximity-EVS_EV3_US-alert.md)\n\n### Intel Edison + Grove IoT Kit\n- [Intel Edison + Grove - Accelerometer (ADXL345)](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-accelerometer-adxl345-edison.md)\n- [Intel Edison + Grove - Accelerometer (MMA7660)](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-accelerometer-mma7660-edison.md)\n- [Intel Edison + Grove - Air quality sensor](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-gas-tp401-edison.md)\n- [Intel Edison + Grove - Barometer (BMP180)](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-barometer-edison.md)\n- [Intel Edison + Grove - Button](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-button-edison.md)\n- [Intel Edison + Grove - Compass (HMC588L)](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-compass-edison.md)\n- [Intel Edison + Grove - Flame Sensor](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-flame-sensor-edison.md)\n- [Intel Edison + Grove - Gas (MQ2)](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-gas-mq2-edison.md)\n- [Intel Edison + Grove - Humidity & Temperature (TH02)](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-humidity-temperature-edison.md)\n- [Intel Edison + Grove - I2C Motor Driver](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-i2c-motor-driver-edison.md)\n- [Intel Edison + Grove - Joystick](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-joystick-edison.md)\n- [Intel Edison + Grove - LED](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-led-edison.md)\n- [Intel Edison + Grove - Light Sensor (TSL2561)](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-light-sensor-edison.md)\n- [Intel Edison + Grove - Moisture Sensor](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-moisture-edison.md)\n- [Intel Edison + Grove - Q Touch](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-q-touch.md)\n- [Intel Edison + Grove - RGB LCD](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-lcd-rgb-edison.md)\n- [Intel Edison + Grove - RGB LCD Color Previewer](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-lcd-rgb-bgcolor-previewer-edison.md)\n- [Intel Edison + Grove - RGB LCD temperature display](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-lcd-rgb-temperature-display-edison.md)\n- [Intel Edison + Grove - Relay](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-relay-edison.md)\n- [Intel Edison + Grove - Rotary Potentiometer](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-rotary-potentiometer-edison.md)\n- [Intel Edison + Grove - Servo](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-servo-edison.md)\n- [Intel Edison + Grove - Touch](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-touch-edison.md)\n\n### Grove IoT Kit (Seeed Studio)\n- [Grove - Button](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-button.md)\n- [Grove - Joystick](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-joystick.md)\n- [Grove - LED](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-led.md)\n- [Grove - Motor (I2C Driver)](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-i2c-motor-driver.md)\n- [Grove - RGB LCD](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-lcd-rgb.md)\n- [Grove - RGB LCD temperature display](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-lcd-rgb-temperature-display.md)\n- [Grove - Rotary Potentiometer](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-rotary-potentiometer.md)\n- [Grove - Servo](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-servo.md)\n- [Grove - Touch](https://github.com/rwaldron/johnny-five/blob/main/docs/grove-touch.md)\n\n### Micro Magician V2\n- [Micro Magician V2 - Accelerometer](https://github.com/rwaldron/johnny-five/blob/main/docs/micromagician-accelerometer.md)\n- [Micro Magician V2 - Motor](https://github.com/rwaldron/johnny-five/blob/main/docs/micromagician-motor.md)\n- [Micro Magician V2 - Servo](https://github.com/rwaldron/johnny-five/blob/main/docs/micromagician-servo.md)\n\n### TinkerKit\n- [TinkerKit - Accelerometer](https://github.com/rwaldron/johnny-five/blob/main/docs/tinkerkit-accelerometer.md)\n- [TinkerKit - Blink](https://github.com/rwaldron/johnny-five/blob/main/docs/tinkerkit-blink.md)\n- [TinkerKit - Button](https://github.com/rwaldron/johnny-five/blob/main/docs/tinkerkit-button.md)\n- [TinkerKit - Combo](https://github.com/rwaldron/johnny-five/blob/main/docs/tinkerkit-combo.md)\n- [TinkerKit - Continuous servo](https://github.com/rwaldron/johnny-five/blob/main/docs/tinkerkit-continuous-servo.md)\n- [TinkerKit - Gyro](https://github.com/rwaldron/johnny-five/blob/main/docs/tinkerkit-gyroscope.md)\n- [TinkerKit - Joystick](https://github.com/rwaldron/johnny-five/blob/main/docs/tinkerkit-joystick.md)\n- [TinkerKit - Linear potentiometer](https://github.com/rwaldron/johnny-five/blob/main/docs/tinkerkit-linear-pot.md)\n- [TinkerKit - Rotary potentiometer](https://github.com/rwaldron/johnny-five/blob/main/docs/tinkerkit-rotary.md)\n- [TinkerKit - Temperature](https://github.com/rwaldron/johnny-five/blob/main/docs/tinkerkit-thermistor.md)\n- [TinkerKit - Tilt](https://github.com/rwaldron/johnny-five/blob/main/docs/tinkerkit-tilt.md)\n- [TinkerKit - Touch](https://github.com/rwaldron/johnny-five/blob/main/docs/tinkerkit-touch.md)\n\n### Wii\n- [Wii Classic Controller](https://github.com/rwaldron/johnny-five/blob/main/docs/classic-controller.md)\n- [Wii Nunchuck](https://github.com/rwaldron/johnny-five/blob/main/docs/nunchuk.md)\n\n### Complete Bots / Projects\n- [Bug](https://github.com/rwaldron/johnny-five/blob/main/docs/bug.md)\n- [Kinect Robotic Arm Controller](https://github.com/rwaldron/johnny-five/blob/main/docs/kinect-arm-controller.md)\n- [Laser Trip Wire](https://github.com/rwaldron/johnny-five/blob/main/docs/laser-trip-wire.md)\n- [Line Follower](https://github.com/rwaldron/johnny-five/blob/main/docs/line-follower.md)\n- [Lynxmotion Biped BRAT](https://github.com/rwaldron/johnny-five/blob/main/docs/brat.md)\n- [Motobot](https://github.com/rwaldron/johnny-five/blob/main/docs/motobot.md)\n- [Navigator](https://github.com/rwaldron/johnny-five/blob/main/docs/navigator.md)\n- [Nodebot](https://github.com/rwaldron/johnny-five/blob/main/docs/nodebot.md)\n- [Phoenix Hexapod](https://github.com/rwaldron/johnny-five/blob/main/docs/phoenix.md)\n- [Radar](https://github.com/rwaldron/johnny-five/blob/main/docs/radar.md)\n- [Robotic Claw](https://github.com/rwaldron/johnny-five/blob/main/docs/claw.md)\n- [Whisker](https://github.com/rwaldron/johnny-five/blob/main/docs/whisker.md)\n\n### Component Plugin Template\n- [Example plugin](https://github.com/rwaldron/johnny-five/blob/main/docs/plugin.md)\n\n### IO Plugins\n- [Led Blink on Electric Imp](https://github.com/rwaldron/johnny-five/blob/main/docs/imp-io.md)\n- [Led Blink on Intel Edison Arduino Board](https://github.com/rwaldron/johnny-five/blob/main/docs/edison-io-arduino.md)\n- [Led Blink on Intel Edison Mini Board](https://github.com/rwaldron/johnny-five/blob/main/docs/edison-io-miniboard.md)\n- [Led Blink on Intel Galileo Gen 2](https://github.com/rwaldron/johnny-five/blob/main/docs/galileo-io.md)\n- [Led Blink on Raspberry Pi](https://github.com/rwaldron/johnny-five/blob/main/docs/raspi-io.md)\n- [Led Blink on Spark Core](https://github.com/rwaldron/johnny-five/blob/main/docs/spark-io.md)\n- [Led Blink on pcDuino3](https://github.com/rwaldron/johnny-five/blob/main/docs/pcduino-io.md)\n\n<!--extract-end:examples-->\n\n## Many fragments. Some large, some small.\n\n#### [Wireless Nodebot](http://jsfiddle.net/rwaldron/88M6b/show/light)\n#### [Kinect Controlled Robot Arm](http://jsfiddle.net/rwaldron/XMsGQ/show/light/)\n#### [Biped Nodebot](http://jsfiddle.net/rwaldron/WZkn5/show/light/)\n#### [LCD Running Man](http://jsfiddle.net/rwaldron/xKwaU/show/light/)\n#### [Slider Controlled Panning Servo](http://jsfiddle.net/rwaldron/kZakv/show/light/)\n#### [Joystick Controlled Laser (pan/tilt) 1](http://jsfiddle.net/rwaldron/HPqms/show/light/)\n#### [Joystick Controlled Laser (pan/tilt) 2](http://jsfiddle.net/rwaldron/YHb7A/show/light/)\n#### [Joystick Controlled Claw](http://jsfiddle.net/rwaldron/6ZXFe/show/light/)\n#### [Robot Claw](http://jsfiddle.net/rwaldron/CFSZJ/show/light/)\n#### [Joystick, Motor & Led](http://jsfiddle.net/rwaldron/gADSz/show/light/)\n#### [Build you own drone](http://github.com/darioodiaz/node-open-pilot/)\n\n\n\n## Make: JavaScript Robotics\n\n[![](http://ecx.images-amazon.com/images/I/91ae8ZZDQ2L.jpg)](http://shop.oreilly.com/product/0636920031390.do)\n\n\n\n\n## Contributing\nAll contributions must adhere to the [Idiomatic.js Style Guide](https://github.com/rwaldron/idiomatic.js),\nby maintaining the existing coding style. Add unit tests for any new or changed functionality. Lint and test your code using [grunt](https://github.com/gruntjs/grunt).\n\n\n## License\nCopyright (c) 2012, 2013, 2014 Rick Waldron <waldron.rick@gmail.com>\nLicensed under the MIT license.\nCopyright (c) 2014, 2015 The Johnny-Five Contributors\nLicensed under the MIT license.\n",
    "readme_length": 44899
  },
  {
    "name": "ESP32-Bus-Pirate",
    "full_name": "geo-tp/ESP32-Bus-Pirate",
    "description": "A Hardware Hacking Tool with Web-Based CLI That Speaks Every Protocol ",
    "stars": 2149,
    "forks": 164,
    "language": "C",
    "url": "https://github.com/geo-tp/ESP32-Bus-Pirate",
    "topics": [
      "arduino",
      "bluetooth",
      "can-bus",
      "debugging",
      "eeprom",
      "esp32",
      "gpio",
      "hardware-hacking",
      "i2c",
      "iot",
      "jtag",
      "protocol",
      "pwm",
      "rfid",
      "serial-communication",
      "spi",
      "subghz",
      "uart",
      "wifi"
    ],
    "created_at": "2025-07-05T21:59:46Z",
    "updated_at": "2025-12-02T07:00:34Z",
    "homepage": "https://geo-tp.github.io/ESP32-Bus-Pirate/webflasher/",
    "license": "MIT License",
    "readme": "# ESP32 Bus Pirate\n\n![Logo banner of the ESP32 Bus Pirate firmware](images/logo_protocols_banner_small.png)\n\n\n**ESP32 Bus Pirate** is an open-source firmware that turns your device into a multi-protocol hacker's tool, inspired by the [legendary Bus Pirate](https://buspirate.com/).\n\nIt supports sniffing, sending, scripting, and interacting with various digital protocols (I2C, UART, 1-Wire, SPI, etc.) via a serial terminal or web-based CLI. It also communicates with radio protocols like Bluetooth, Wi-Fi, Sub-GHz and RFID.\n\nUse the [ESP32 Bus Pirate Web Flasher](https://geo-tp.github.io/ESP32-Bus-Pirate/webflasher/) to install the firmware in one click. See the [Wiki](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki) for step-by-step guides on every mode and command. Check [ESP32 Bus Pirate Scripts](https://github.com/geo-tp/ESP32-Bus-Pirate-Scripts) for a collection of scripts.\n\n![Demo showing the different mode of the ESP32 Bus Pirate firmware](images/help.gif)\n![Demo showing the LittleFS file system of the ESP32 Bus Pirate firmware](images/littlefs.gif)\n\n## Features\n\n- Interactive command-line interface (CLI) via **USB Serial or WiFi Web**.\n- **Modes for:**\n   - [HiZ](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/01-HiZ) (default)\n   - [I2C](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/05-I2C) (scan, glitch, slave mode, dump, eeprom)\n   - [SPI](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/06-SPI) (eeprom, flash, sdcard, slave mode)\n   - [UART](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/03-UART) / [Half-Duplex UART](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/04-HDUART) (bridge, read, write)\n   - [1WIRE](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/02-1WIRE) (ibutton, eeprom)\n   - [2WIRE](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/07-2WIRE) (sniff, smartcard) / [3WIRE](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/08-3WIRE) (eeprom)\n   - [DIO](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/09-DIO) (Digital I/O, read, pullup, set, pwm)\n   - [Infrared](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/11-INFRARED) (device-b-gone, universal remote)\n   - [USB](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/12-USB) (HID, mouse, keyboard, gamepad, storage)\n   - [Bluetooth](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/13-BLUETOOTH) (BLE HID, scan, spoofing, sniffing)\n   - [Wi-Fi](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/14-WIFI) / [Ethernet](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/18-ETHERNET) (sniff, deauth, nmap, netcat)\n   - [JTAG](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/15-JTAG) (scan pinout, SWD)\n   - [LED](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/10-LED) (animations, set LEDs)\n   - [I2S](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/16-I2S) (test speakers, mic, play sound)\n   - [CAN](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/17-CAN) (sniff, send and receive frames)\n   - [SUBGHZ](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/19-SUBGHZ) (sniff, scan, replay)\n   - [RFID](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/20-RFID) (read, write, clone)\n   - [RF24](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/21-RF24) (scan, sniff)\n\n\n- **Protocol sniffers** for I2C, SPI, 1Wire, 2wire, CAN, Wi-Fi, Bluetooth, SubGhz.\n- Baudrate **auto-detection**, AT commands and various tools for UART.\n- Registers manipulation, **EEPROM dump tools**, identify devices for I2C.\n- Read all sort of **EEPROM, Flash** and various others tools for SPI.\n- Scripting using **Bus Pirate-style bytecode** instructions or **Python**.\n- Device-B-Gone command with more than **80 supported INFRARED protocols**.\n- Direct I/O management, **PWM, servo**, pulse.\n- Analyze radio signals and frequencies **on every bands**.\n- Near than **50 addressable LEDs protocols** supported.\n- **Ethernet and WiFi** are supported to access networks.\n- Import and export data with the **LittleFS over HTTP.**\n\n## Supported Devices\n\n\n| Device               |                                     | Description                       |\n|-----------------------|------------------------------------------|---------------------------------------------------|\n| **ESP32 S3 Dev Kit**  | ![Photo of the ESP32 S3 Dev Kit](/images/s3-devkit_s.jpg)     | More than 20 available GPIO, 1 button |\n| **M5 Cardputer**      | ![Photo of the M5 Cardputer](/images/cardputer_s.png)            | 2 GPIO (Grove), screen, keyboard, mic, speaker, IR TX, SD card, [standalone mode](#standalone-mode-for-the-cardputer)            |\n| **M5 Cardputer ADV**  | ![Photo of the M5 Cardputer ADV](/images/cardputer-adv_s.jpg)    | 12 GPIO (Grove, Header), screen, keyboard, mic, speaker, IR TX, SD card, IMU, [standalone mode](#standalone-mode-for-the-cardputer)                  |\n| **M5 Stick C Plus 2** | ![Photo of the M5 Stick C Plus 2](/images/m5stick_s.jpg)      | 5 GPIO (Grove, Header), screen, mic, buzzer, IR TX, IMU, 3 buttons                     |\n| **M5 StampS3**        | ![Photo of the M5 StampS3](/images/stamps3_s.jpg)             | 9 GPIO (exposed pins), 1 button                       |\n| **M5 AtomS3 Lite**    | ![Photo of the M5 Atom S3 Lite](/images/atom_s.jpg)            | 8 GPIO (Grove, Header), IR TX, 1 buttton                     |\n| **LILYGO T-Embed**    | ![Photo of the LILYGO T-Embed](/images/tembed_s.jpg)          | 9 GPIO (Grove, Header), screen, encoder, speaker, mic, SD card                                           |\n| **LILYGO T-Embed CC1101** | ![Photo of the LILYGO T-Embed CC1101](/images/tembedcc1101_s.jpg) | 4 GPIO (2x Qwiic), screen, encoder, speaker, mic, SD Card, CC1101, PN532, IR TX, IR RX                                   |\n| **Seeed Studio Xiao S3** | ![Photo of the Seeed Studio Xiao ESP32-S3](/images/xiaos3_s.jpg)        | 9 GPIO (exposed pins), 1 button        \n\n- **Other ESP32-S3-based Boards**\n\n  - All boards based on the **ESP32-S3 can be supported**, provided they have at least **8 MB of flash.**\n\n  - You can **flash the s3 dev-kit firmware onto any ESP32-S3 board.**\n\n  - Keep in mind that the **default pin mapping in the firmware may not match** your specific board.\n\n## Getting Started\n\n[![Banner of the ESP32 Bus Pirate web flasher](images/flasher.jpg)](https://geo-tp.github.io/ESP32-Bus-Pirate/webflasher/)\n\n1. ðŸ”§ Flash the firmware  \n   - Use the [ESP32 Bus Pirate Web Flasher](https://geo-tp.github.io/ESP32-Bus-Pirate/webflasher/) to burn the firmware directly from a web browser.\n   - You can also burn it on [M5Burner](https://docs.m5stack.com/en/download), in the M5stick, AtomS3, M5StampS3 or Cardputer category.\n\n2. ðŸ”Œ Connect via Serial or Web\n   - Serial: any terminal app (see [Connect via Serial](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Serial))\n   - Web: configure Wi-Fi and access the CLI via browser (see [Wi-Fi Connection](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/00-Terminal))\n\n3. ðŸ§ª Use commands like:\n   ```bash\n   mode\n   help\n   scan\n   sniff\n   ...\n    ```\n\n## Wiki\n\n[![Banner of the ESP32 Bus Pirate Wiki page](images/bus_pirate_wiki.png)](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/)\n\nðŸ“š Visit the **[Wiki](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki)** for detailed documentation on every mode and command.\n\nIncludes:\n- [Terminal mode](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/00-Terminal) - About serial and web terminal.\n- [Mode overviews](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki) - Browse supported modes.\n- [Instruction syntax](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Instructions) - Master the instructions.\n- [Serial setup](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Serial) - Serial access via USB.\n\nThe wiki is the best place to learn how everything works.\n\n## Scripting\n\n[![Banner of the ESP32 Bus Pirate Scripts page](images/bus_pirate_scripts.png)](https://github.com/geo-tp/ESP32-Bus-Pirate-Scripts/)\n\nðŸ› ï¸ You can [automate interactions with the ESP32 Bus Pirate](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Python) using **Python scripts over serial.**\n\n**Examples and ready-to-use scripts** are available in the repository: [ESP32 Bus Pirate Scripts](https://github.com/geo-tp/ESP32-Bus-Pirate-Scripts).\n\n**Including:** Logging data in a file, eeprom and flash dump, interracting with GPIOs, LED animation...\n   \n## ESP32 Bus Pirate on M5 Devices\n![A photo of the ESP32 Bus Pirate firmware running on M5 Stack devices](images/m5buspirate_s.jpg)\n\n## ESP32 Bus Pirate on T-Embed\n![A photo of the ESP32 Bus Pirate firmware running on Lilygo device](images/tembedbuspirate_s.jpg)\n\n## Command-Line Interfaces\n\nThe ESP32 Bus Pirate firmware provides three command-line interface (CLI) modes:\n\n| Interface         | Advantages                                                                 | Ideal for...                          |\n|------------------|-----------------------------------------------------------------------------|----------------------------------------|\n| **Web Interface** | - Accessible from any browser<br>- PC, tablets, mobiles<br>- Works over Wi-Fi<br>- No cables needed | Quick tests, demos, headless setups   |\n| **Serial Interface** | - Faster performance<br>- Instant responsiveness<br>- Handles large data smoothly | Intensive sessions, frequent interactions |\n| **Standalone** | - Only for the Cardputer<br>- On device keyboard<br>- On device screen | Portable sessions, Quick tests |\n\n\nAll interfaces share the same command structure and can be used interchangeably ([more details](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/00-Terminal)).\n\n## Mobile Web Interface over WiFi\n![An iPhone screenshot showing the Bus Pirate firmware web interface](images/presentation_mobile.png)\n\n## Standalone Mode for the Cardputer\n![A Cardputer running the ESP32 Bus pirate in standalone mode](images/standalonemode_s.png)\n\n## Using the ESP32 Bus Pirate to speak UART over WiFi\n![A demo Using the ESP32 Bus pirate firmware with UART](images/demo2.gif)\n\n## Contribute\nSee [How To Contribute](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Contribute) section, which outlines a **simple way to add a new command** to any mode.\n\n## Visuals Assets\n\n#### [![Small logo of the ESP32 Bus Pirate firmware](images/logo_square_small.png)](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Visual-Assets)\n\nSee [images, logo, presentations, photo, video, illustrations](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Visual-Assets). These visuals can be **freely used in blog posts, documentation, videos, or articles** to help explain and promote the firmware.\n\n\n## Warning\n> âš ï¸ **Voltage Warning**: Devices should only operate at **3.3V** or **5V**.  \n> - Do **not** connect peripherals using other voltage levels â€” doing so may **damage your ESP32**.\n\n> âš ï¸ **Usage Warning**: This firmware is provided for **educational, diagnostic, and interoperability testing purposes only**.\n> - Do not use it to interfere with, probe, or manipulate devices without proper authorization.\n> - Avoid any unauthorized RF transmissions (e.g., sub-GHz) that could violate local regulations or disrupt networks and communications.\n> - The authors are not responsible for any misuse of this software or hardware, including legal consequences resulting from unauthorized access or signal emission.\n> - Always stay within the bounds of your countryâ€™s laws and responsible disclosure policies.\n\n\n",
    "readme_length": 11307
  },
  {
    "name": "basic_verilog",
    "full_name": "pConst/basic_verilog",
    "description": "Must-have verilog systemverilog modules",
    "stars": 1884,
    "forks": 411,
    "language": "Verilog",
    "url": "https://github.com/pConst/basic_verilog",
    "topics": [
      "altera",
      "debounce",
      "delay",
      "encoder",
      "fifo",
      "fpga",
      "hls",
      "pwm",
      "spi-interface",
      "spi-master",
      "synchronizer",
      "tcl",
      "uart",
      "uart-controller",
      "uart-protocol",
      "uart-receiver",
      "uart-tx",
      "uart-verilog",
      "verilog",
      "xilinx"
    ],
    "created_at": "2015-12-14T18:09:40Z",
    "updated_at": "2025-12-02T06:32:21Z",
    "homepage": "",
    "license": "N/A",
    "readme": "Must-have verilog systemverilog modules\n=======================================\nOriginally published as part of https://github.com/pConst/basic_verilog   \nby Konstantin Pavlov, pavlovconst@gmail.com   \n\nHi! This is a collection of Verilog SystemVerilog synthesizable modules.   \n\nAll the code is highly reusable across typical FPGA projects and mainstream FPGA vendors.   \n\nPlease feel free to make pull requests or contact me in case you spot any code issues.   \n\nAlso, give me a pleasure, tell me if the code has got succesfully implemented in your hobby, scientific or industrial projects!   \n   \nLicensing\n---------\nThe code is licensed under CC BY-SA 4_0   \nThat means, that you can remix, transform, and build upon the material for any purpose, even commercially.   \nHowever, YOU MUST provide the name of the creator and distribute your contributions under the same license as the original.   \n   \nContents description\n--------------------\nFor your convinience I`ve tagged some sources by their \"difficulty\":   \n:green_circle: - for the most basic tasks   \n:red_circle: - for advanced or special purpose routines   \n   \nIf you are a beginner in HW design - you may want to start exploring :green_circle: code first.   \nAlmost every source file in the repository contains detailed description and instantiation template!   \n   \n|               | DIRECTORY    | DESCRIPTION |\n|---------------|--------------|-------------|\n|               | Advanced Synthesis Cookbook/ | useful code from Altera's cookbook |\n|               | KCPSM6_Release9_30Sept14/ | Xilinx's Picoblaze soft processor sources |\n| :red_circle:  | XilinxBoardStore_with_Alveo_cards_support | board definitions for Xilinx Alveo accelerator cards |\n|               | pacoblaze-2.2/ | version of Picoblaze adapted for Altera devices |\n|               | avalon_mm_master_templates/ | Avalon-MM component templates from Altera |\n|               | axi_master_slave_templates/ | AXI componet templates generated by Vivado |\n|               | benchmark_projects/ | benchmarking various IDEs to compile exact same Verilog project |\n|               | dual_port_ram_templates/ | Block RAM templates |\n|               | example_projects/ | FPGA project boilerplates and examples |\n|               | gitignores/ | gitignore files for FPGA projects |\n|               | scripts/ | useful TCL, batch and shell scripts |\n| :red_circle:  | scripts_for_intel_hls/ | useful scripts for compiling for Intel HLS |\n| :red_circle:  | scripts_for_xilinx_hls/ | useful scripts for compiling for Xilinx HLS |\n|               | xpm | Xilinx parametrizable macros sources |\n   \n|                | FILE                       | DESCRIPTION |\n|----------------|--------------------        |-------------|\n|                | adder_tree.sv              | adding multiple values together in parallel |\n|                | axi4l_logger.sv            | sniffs all AXI transactions and stores address and data to fifo |\n| :green_circle: | bin2gray.sv                | combinational Gray code to binary converter |\n|                | bin2pos.sv                 | converts binary coded value to positional (one-hot) code |\n|                | cdc_data.sv                | standard two-stage data synchronizer |\n|                | cdc_strobe.sv              | clock crossing synchronizer for one-cycle strobes |\n| :green_circle: | clk_divider.sv             | wide reference clock divider |\n|                | clogb2.svh                 | calculates counter/address width based on specified vector/RAM depth |\n| :green_circle: | debounce.v                 | two-cycle debounce for input buttons |\n| :green_circle: | delay.sv                   | useful module to make static delays or to synchronize across clock domains |\n|                | delayed_event.sv           | generates delayed pulse one clock width |\n|                | dynamic_delay.sv           | dynamic delay for arbitrary input signal |\n| :green_circle: | edge_detect.sv             | combinational edge detector, gives one-tick pulses on every signal edge |\n|                | encoder.v                  | digital encoder input logic module |\n| :red_circle:   | fast_counter.sv            | synthetic counter |\n|                | fifo_combiner.sv           | accumulates data words from multiple FIFOs to a single output FIFO |\n|                | fifo_operator.sv           | performs custom operation on data words from multiple FIFOs and stores result to a single output FIFO |\n| :red_circle:   | fifo_single_clock_ram_*.sv | single-clock FIFO buffer (queue) implementation |\n| :red_circle:   | fifo_single_clock_reg_*.sv | single-clock FIFO buffer (queue) implementation |\n| :green_circle: | gray2bin.sv                | combinational binary to Gray code converter |\n| :red_circle:   | gray_functions.vh          | Gray code parametrizable converter functions |\n| :green_circle: | hex2ascii.sv               | converts 4-bit binary nibble to 8-bit human-readable ASCII char |\n|                | leave_one_hot.sv           | combinational module that leaves only lowest hot bit |\n|                | lifo.sv                    | single-clock LIFO buffer (stack) implementation |\n|                | main_tb.sv                 | basic testbench template |\n|                | moving_average.sv          | Simple moving average implementation |\n|                | pack_unpack_array.v        | macros for packing and unpacking 2D and 3D vectors in Verilog-2001 |\n|                | pattern_detect.sv          | detects data pattern specified |\n|                | pdm_modulator.sv           | pulse density modulation generator module |\n|                | pos2bin.sv                 | converts positional (one-hot) value to binary representation |\n|                | prbs_gen_chk.sv            | PRBS pattern generator or checker |\n|                | preview_fifo.sv            | FIFO with an ability to be read 0, 1 or 2 words at once |\n|                | priority_enc.sv            | combinational priority_encoder |\n|                | pulse_gen.sv               | generates pulses with given width and delay |\n|                | pulse_stretch.sv           | configurable pulse stretcher/extender module |\n|                | pwm_modulator.sv           | pulse width modulation generator |\n| :red_circle:   | read_ahead_buf.sv          | substitutes fifo read port and performs fifo data update at the same clock cycle |\n|                | reset_set.sv               | SR trigger variant w/o metastable state, set dominates here |\n|                | reset_set_comb.sv          | synchronous SR trigger, but has a combinational output |\n|                | reverse_bytes.sv           | reverses bytes order within multi-byte array |\n|                | reverse_dimensions.sv      | reverses dimension order in SystemVerilog 2D vector |\n|                | reverse_vector.sv          | reverses signal order within multi-bit bus |\n|                | round_robin_enc.sv         | round robin combinational encoder |\n|                | round_robin_performance_enc.sv | performance improved round robin encoder |\n|                | set_reset.sv               | SR trigger variant w/o metastable state, reset dominates here |\n|                | set_reset_comb.sv          | synchronous SR trigger, but has a combinational output |\n|                | sim_clk_gen.sv             | testbench clock generator |\n| :red_circle:   | soft_latch.sv              | combinational data hold circuit |\n|                | spi_master.sv              | universal spi master module |\n| :red_circle:   | true_dual_port_write_first_2_clock_ram.sv | double port RAM/ROM module |\n| :red_circle:   | true_single_port_write_first_ram.sv | single port RAM/ROM module |\n|                | uart_debug_printer.sv      | debug data printer to UART terminal |\n| :green_circle: | uart_rx.sv                 | straightforward yet simple UART receiver |\n|                | uart_rx_shifter.sv         | UART-like receiver shifter for simple synchronous messaging inside the FPGA or between FPGAs |\n| :green_circle: | uart_tx.sv                 | straightforward yet simple UART transmitter |\n|                | uart_tx_shifter.sv         | UART-like transmitter shifter for simple synchronous messaging inside the FPGA or between FPGAs |\n   \nAlso added testbenches for selected modules.   \n   ",
    "readme_length": 8344
  },
  {
    "name": "rpi_ws281x",
    "full_name": "jgarff/rpi_ws281x",
    "description": "Userspace Raspberry Pi PWM library for WS281X LEDs",
    "stars": 1864,
    "forks": 636,
    "language": "C",
    "url": "https://github.com/jgarff/rpi_ws281x",
    "topics": [],
    "created_at": "2014-09-03T13:18:19Z",
    "updated_at": "2025-12-01T14:56:31Z",
    "homepage": null,
    "license": "BSD 2-Clause \"Simplified\" License",
    "readme": "rpi_ws281x\n==========\n\nUserspace Raspberry Pi library for controlling WS281X LEDs.\nThis includes WS2812 and SK6812RGB RGB LEDs\nPreliminary support is now included for SK6812RGBW LEDs (yes, RGB + W)\nThe LEDs can be controlled by either the PWM (2 independent channels)\nor PCM controller (1 channel) or the SPI interface (1 channel).\n\n### Bindings:\n\nLanguage-specific bindings for rpi_ws281x are available in:\n\n* Python - https://github.com/rpi-ws281x/rpi-ws281x-python\n* Rust - https://github.com/rpi-ws281x/rpi-ws281x-rust\n* Powershell - https://github.com/rpi-ws281x/rpi-ws281x-powershell\n* Java - https://github.com/rpi-ws281x/rpi-ws281x-java\n* CSharp - https://github.com/rpi-ws281x/rpi-ws281x-csharp\n* Go - https://github.com/rpi-ws281x/rpi-ws281x-go\n* Swift - https://github.com/kbongort/rpi-ws281x-swift\n\n### Background:\n\nThe BCM2835 in the Raspberry Pi has both a PWM and a PCM module that\nare well suited to driving individually controllable WS281X LEDs.\nUsing the DMA, PWM or PCM FIFO, and serial mode in the PWM, it's\npossible to control almost any number of WS281X LEDs in a chain connected\nto the appropriate output pin.\nFor SPI the Raspbian spidev driver is used (`/dev/spidev0.0`).\nThis library and test program set the clock rate to 3X the desired output\nfrequency and creates a bit pattern in RAM from an array of colors where\neach bit is represented by 3 bits as follows.\n\n    Bit 1 - 1 1 0\n    Bit 0 - 1 0 0\n\n\n### GPIO Usage:\n\nThe GPIOs that can be used are limited by the hardware of the Pi and will\nvary based on the method used to drive them (PWM, PCM or SPI).\nBeware that the GPIO numbers are not the same as the physical pin numbers\non the header.\n\nPWM:\n```\n        PWM0, which can be set to use GPIOs 12, 18, 40, and 52.\n        Only 12 (pin 32) and 18 (pin 12) are available on the B+/2B/3B\n\n        PWM1 which can be set to use GPIOs 13, 19, 41, 45 and 53.\n        Only 13 is available on the B+/2B/PiZero/3B, on pin 33\n```\n\nPCM:\n```\n        PCM_DOUT, which can be set to use GPIOs 21 and 31.\n        Only 21 is available on the B+/2B/PiZero/3B, on pin 40.\n```\n\nSPI:\n```\n        SPI0-MOSI is available on GPIOs 10 and 38.\n        Only GPIO 10 is available on all models.\n        See also note for RPi 3 below.\n```\n\n\n### Power and voltage requirements\n\nWS281X LEDs are generally driven at 5V. Depending on your actual\nLED model and data line length you might be able to successfully drive\nthe data input with 3.3V. However in the general case you probably\nwant to use a level shifter to convert from the Raspberry Pi GPIO/PWM to 5V.\n\nIt is also possible to run the LEDs from a 3.3V - 3.6V power source, and\nconnect the GPIO directly at a cost of brightness, but this isn't\nrecommended.\n\nThe test program is designed to drive a 8x8 grid of LEDs e.g.from\nAdafruit (http://www.adafruit.com/products/1487) or Pimoroni\n(https://shop.pimoroni.com/products/unicorn-hat).\nPlease see the Adafruit and Pimoroni websites for more information.\n\nKnow what you're doing with the hardware and electricity.  I take no\nreponsibility for damage, harm, or mistakes.\n\n### Build:\n\n#### Build with SCons:\n\n- Install Scons (on raspbian, `apt-get install scons`).\n- Make sure to adjust the parameters in main.c to suit your hardware.\n  - Signal rate (400kHz to 800kHz).  Default 800kHz.\n  - ledstring.invert=1 if using a inverting level shifter.\n  - Width and height of LED matrix (height=1 for LED string).\n- Type `scons` from inside the source directory.\n\n#### Build and install with CMake:\n\n- Install CMake\n- Configure your build:\n\n  For example:\n  ```\n  mkdir build\n  cd build\n  cmake -D BUILD_SHARED=OFF -D BUILD_TEST=ON ..\n  ```\n  See also for available options in `CMakeLists.txt`.\n- Type `cmake --build .` to build\n- To install built binaries and headers into your system type:\n  ```\n  sudo make install\n  ```\n\n### Running:\n\n- Type `sudo ./test` (default uses PWM channel 0).\n- That's it.  You should see a moving rainbow scroll across the\n  display.\n- More options are available, `./test -h` should show them:\n```\n./test version 1.1.0\nUsage: ./test\n-h (--help)    - this information\n-s (--strip)   - strip type - rgb, grb, gbr, rgbw\n-x (--width)   - matrix width (default 8)\n-y (--height)  - matrix height (default 8)\n-d (--dma)     - dma channel to use (default 10)\n-g (--gpio)    - GPIO to use\n                 If omitted, default is 18 (PWM0)\n-i (--invert)  - invert pin output (pulse LOW)\n-c (--clear)   - clear matrix on exit.\n-v (--version) - version information\n```\n\n### Important warning about DMA channels\n\nYou must make sure that the DMA channel you choose to use for the LEDs is not [already in use](https://www.raspberrypi.org/forums/viewtopic.php?p=609380#p609380) by the operating system.\n\nFor example, **using DMA channel 5 [will cause](https://github.com/jgarff/rpi_ws281x/issues/224) filesystem corruption** on the Raspberry Pi 3 Model B.\n\nThe default DMA channel (10) should be safe for the Raspberry Pi 3 Model B, but this may change in future software releases.\n\n### Limitations:\n\n#### PWM\n\nSince this library and the onboard Raspberry Pi audio\nboth use the PWM, they cannot be used together.  You will need to\nblacklist the Broadcom audio kernel module by creating a file\n`/etc/modprobe.d/snd-blacklist.conf` with\n\n    blacklist snd_bcm2835\n\nIf the audio device is still loading after blacklisting, you may also\nneed to comment it out in the /etc/modules file.\n\nOn headless systems you may also need to force audio through hdmi\nEdit config.txt and add:\n\n    hdmi_force_hotplug=1\n    hdmi_force_edid_audio=1\n\nA reboot is required for this change to take effect\n\nSome distributions use audio by default, even if nothing is being played.\nIf audio is needed, you can use a USB audio device instead.\n\n#### PCM\n\nWhen using PCM you cannot use digital audio devices which use I2S since I2S\nuses the PCM hardware, but you can use analog audio.\n\n#### SPI\n\nWhen using SPI the led string is the only device which can be connected to\nthe SPI bus. Both digital (I2S/PCM) and analog (PWM) audio can be used.\n\nMany distributions have a maximum SPI transfer of 4096 bytes. This can be\nchanged in `/boot/cmdline.txt` by appending\n```\n    spidev.bufsiz=32768\n```\n\nOn an RPi 3 you have to change the GPU core frequency to 250 MHz, otherwise\nthe SPI clock has the wrong frequency.\n\nDo this by adding the following line to /boot/config.txt and reboot:\n\n```\n    core_freq=250\n```\n\nOn an RPi 4 you must set a fixed frequency to avoid the idle CPU scaling changing the SPI frequency and breaking the ws281x timings:\n\nDo this by adding the following lines to /boot/config.txt and reboot:\n\n```\n    core_freq=500\n    core_freq_min=500\n```\n\nSPI requires you to be in the `gpio` group if you wish to control your LEDs\nwithout root.\n\n### Comparison PWM/PCM/SPI\n\nBoth PWM and PCM use DMA transfer to output the control signal for the LEDs.\nThe max size of a DMA transfer is 65536 bytes. Since each LED needs 12 bytes\n(4 colors, 8 symbols per color, 3 bits per symbol) this means you can\ncontrol approximately 5400 LEDs for a single strand in PCM and 2700 LEDs per string\nfor PWM (Only PWM can control 2 independent strings simultaneously)\nSPI uses the SPI device driver in the kernel. For transfers larger than\n96 bytes the kernel driver also uses DMA.\nOf course there are practical limits on power and signal quality. These will\nbe more constraining in practice than the theoretical limits above.\n\nWhen controlling a LED string of 240 LEDs the CPU load on the original Pi 2 (BCM2836) are:\n  PWM  5%\n  PCM  5%\n  SPI  1%\n\n### Usage:\n\nThe API is very simple.  Make sure to create and initialize the `ws2811_t`\nstructure as seen in [`main.c`](main.c).  From there it can be initialized\nby calling `ws2811_init()`.  LEDs are changed by modifying the color in\nthe `.led[index]` array and calling `ws2811_render()`.\nThe rest is handled by the library, which either creates the DMA memory and\nstarts the DMA for PWM and PCM or prepares the SPI transfer buffer and sends\nit out on the MISO pin.\n\nMake sure to hook a signal handler for SIGKILL to do cleanup.  From the\nhandler make sure to call `ws2811_fini()`.  It'll make sure that the DMA\nis finished before program execution stops and cleans up after itself.\n",
    "readme_length": 8203
  },
  {
    "name": "esp-idf-lib",
    "full_name": "UncleRus/esp-idf-lib",
    "description": "Component library for ESP32-xx and ESP8266",
    "stars": 1585,
    "forks": 478,
    "language": "C",
    "url": "https://github.com/UncleRus/esp-idf-lib",
    "topics": [
      "adc",
      "co2-sensor",
      "dac",
      "esp-idf",
      "esp32",
      "esp32-s2",
      "esp8266",
      "gas-sensor",
      "gpio-extender",
      "humidity-sensor",
      "i2c-device",
      "led-controller",
      "light-sensor",
      "magnetometer",
      "power-monitor",
      "pressure-sensor",
      "pwm",
      "rotary-encoder",
      "temperature-sensor",
      "wiegand"
    ],
    "created_at": "2018-03-23T11:17:22Z",
    "updated_at": "2025-11-25T21:26:11Z",
    "homepage": "https://esp-idf-lib.readthedocs.io/en/latest/",
    "license": "N/A",
    "readme": "# ESP-IDF Components library\n\n[![Main CI process](https://github.com/UncleRus/esp-idf-lib/actions/workflows/ci.yml/badge.svg)](https://github.com/UncleRus/esp-idf-lib/actions/workflows/ci.yml)\n[![Docs Status](https://readthedocs.org/projects/esp-idf-lib/badge/?version=latest&style=flat)](https://esp-idf-lib.readthedocs.io/en/latest/)\n\n> [!CAUTION]\n> New project, [esp-idf-lib](https://github.com/esp-idf-lib/core), succeeded\n> this project. We will NOT update this repository. All new issues and PRs\n> will NOT be accepted here. If you find any issues, please visit the relevant\n> component repository. For instance, `ads111x`'s component repository is\n> [esp-idf-lib/ads111x](https://github.com/esp-idf-lib/ads111x).\n>\n> Visit [esp-idf-lib/core/discussions](https://github.com/esp-idf-lib/core/discussions)\n> for discussions and questions.\n\n\nComponents for Espressif ESP32 [ESP-IDF framework](https://github.com/espressif/esp-idf)\nand [ESP8266 RTOS SDK](https://github.com/espressif/ESP8266_RTOS_SDK).\n\nPart of them ported from [esp-open-rtos](https://github.com/SuperHouse/esp-open-rtos).\n\n## Supported versions of frameworks and devices\n\n| Chip     | Framework        | Versions                                                                                                                                         |\n| -------- | ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |\n| ESP32-xx | ESP-IDF          | All officially supported versions (see [Support Period Policy](https://github.com/espressif/esp-idf/blob/master/SUPPORT_POLICY.md)) and `master` |\n| ESP8266  | ESP8266 RTOS SDK | `master`, v3.4                                                                                                                                   |\n\n_See \"Supported on\" column for each of the components._\n\n## How to use\n\n### ESP32-xx\n\nClone this repository somewhere, e.g.:\n\n```Shell\ncd ~/myprojects/esp\ngit clone https://github.com/UncleRus/esp-idf-lib.git\n```\n\nAdd path to components in your [CMakeLists.txt](https://docs.espressif.com/projects/esp-idf/en/latest/esp32/api-guides/build-system.html):\ne.g:\n\n```CMake\ncmake_minimum_required(VERSION 3.5)\nset(EXTRA_COMPONENT_DIRS /home/user/myprojects/esp/esp-idf-lib/components)\ninclude($ENV{IDF_PATH}/tools/cmake/project.cmake)\nproject(my-esp-project)\n```\n\nor with CMake [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html)\n\n```CMake\ncmake_minimum_required(VERSION 3.11)\ninclude(FetchContent)\nFetchContent_Declare(\n  espidflib\n  GIT_REPOSITORY https://github.com/UncleRus/esp-idf-lib.git\n)\nFetchContent_MakeAvailable(espidflib)\nset(EXTRA_COMPONENT_DIRS ${espidflib_SOURCE_DIR}/components)\ninclude($ENV{IDF_PATH}/tools/cmake/project.cmake)\nproject(my-esp-project)\n```\n\n### ESP8266 RTOS SDK\n\nClone this repository somewhere, e.g.:\n\n```Shell\ncd ~/myprojects/esp\ngit clone https://github.com/UncleRus/esp-idf-lib.git\n```\n\nAdd path to components in your [project makefile](https://docs.espressif.com/projects/esp8266-rtos-sdk/en/latest/api-guides/build-system.html),\ne.g:\n\n```Makefile\nPROJECT_NAME := my-esp-project\nEXTRA_COMPONENT_DIRS := /home/user/myprojects/esp/esp-idf-lib/components\nEXCLUDE_COMPONENTS := ads130e08 max7219 mcp23x17 led_strip max31865 ls7366r max31855\ninclude $(IDF_PATH)/make/project.mk\n```\n\nAs some `Kconfig.projbuild` files use `rsource`, which the `Kconfig` parser of\nESP8266 RTOS SDK does not understand, run `devtools/rewrite_kconfig.rb` to\nrewrite it before building the examples.\n\n```console\nruby devtools/rewrite_kconfig.rb path/to/Kconfig.projbuild\n```\nSee [GitHub examples](https://github.com/UncleRus/esp-idf-lib/tree/master/examples)\nor [GitLab examples](https://gitlab.com/UncleRus/esp-idf-lib/tree/master/examples).\n\n## Documentation\n\n- [Documentation](https://esp-idf-lib.readthedocs.io/en/latest/)\n- [Frequently asked questions](FAQ.md)\n\n## Components\n\n\n\n### ADC/DAC libraries\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **ads111x**              | Driver for ADS1113/ADS1114/ADS1115 and ADS1013/ADS1014/ADS1015 I2C ADC           | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **ads130e08**            | Driver for ADS130E08 ADC                                                         | MIT     | esp32, esp32s3, esp32s2, esp32c3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **hx711**                | Driver for HX711 24-bit ADC for weigh scales                                     | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | no            |\n| **mcp342x**              | Driver for 18-Bit, delta-sigma ADC MCP3426/MCP3427/MCP3428                       | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **mcp4725**              | Driver for 12-bit DAC MCP4725                                                    | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **pcf8591**              | Driver for 8-bit ADC and an 8-bit DAC PCF8591                                    | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **sgm58031**             | Driver for SGM58031 16-bit I2C ADC                                               | ISC     | esp32, esp8266, esp32s2, esp32s3, esp32c3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### Air quality sensors\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **ccs811**               | Driver for AMS CCS811 digital gas sensor                                         | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **mhz19b**               | Driver for MH-Z19B NDIR COâ‚‚ sensor                                               | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | no            |\n| **scd30**                | Driver for SCD30 COâ‚‚ sensor                                                      | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **scd4x**                | Driver for SCD40/SCD41 miniature COâ‚‚ sensor                                      | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **sfa3x**                | Driver for SFA30 formaldehyde detection module (I2C)                             | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **sgp40**                | Driver for SGP40 Indoor Air Quality Sensor for VOC Measurements                  | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### Battery controllers\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **lc709203f**            | Driver for LC709203F battery fuel gauge                                          | ISC     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **max1704x**             | Driver for MAX17043/MAX17044/MAX17048/MAX17049 battery fuel gauge                | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **mp2660**               | Driver for MP2660 5V USB, 500mA, I2C-Controlled Linear Charger with Power Path Management for Single-Cell Li-Ion Battery | BSD-3-Clause | esp32, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### Common libraries\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **calibration**          | Multi-point calibration library                                                  | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | n/a           |\n| **color**                | Common library for RGB and HSV colors                                            | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c6, esp32h2, esp32p4, esp32c5 | n/a           |\n| **esp_idf_lib_helpers**  | Common support library for esp-idf-lib                                           | ISC     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | n/a           |\n| **framebuffer**          | RGB framebuffer component                                                        | MIT     | esp32, esp32s2, esp32c3, esp32s3, esp32c6, esp32h2, esp32p4, esp32c5 | n/a           |\n| **i2cdev**               | ESP-IDF I2C master thread-safe utilities                                         | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **lib8tion**             | Math functions specifically designed for LED programming                         | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c6, esp32h2, esp32p4, esp32c5 | n/a           |\n| **noise**                | Noise generation functions                                                       | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | n/a           |\n| **onewire**              | Bit-banging 1-Wire driver                                                        | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | no            |\n\n\n\n### Current and power sensors\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **ina219**               | Driver for INA219/INA220 bidirectional current/power monitor                     | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **ina260**               | Driver for INA260 precision digital current and power monitor                    | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **ina3221**              | Driver for INA3221 shunt and bus voltage monitor                                 | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### Gas sensors\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **ccs811**               | Driver for AMS CCS811 digital gas sensor                                         | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **mhz19b**               | Driver for MH-Z19B NDIR COâ‚‚ sensor                                               | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | no            |\n| **scd30**                | Driver for SCD30 COâ‚‚ sensor                                                      | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **scd4x**                | Driver for SCD40/SCD41 miniature COâ‚‚ sensor                                      | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **sfa3x**                | Driver for SFA30 formaldehyde detection module (I2C)                             | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### GPIO expanders\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **mcp23008**             | Driver for 8-bit I2C GPIO expander MCP23008                                      | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **mcp23x17**             | Driver for I2C/SPI 16 bit GPIO expanders MCP23017/MCP23S17                       | BSD-3-Clause | esp32, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **pca9557**              | Driver for PCA9536/PCA9537/PCA9557/TCA9534 remote 4/8-bit I/O expanders for I2C-bus | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **pcf8574**              | Driver for PCF8574 remote 8-bit I/O expander for I2C-bus                         | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **pcf8575**              | Driver for PCF8575 remote 16-bit I/O expander for I2C-bus                        | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **tca6424a**             | Driver for TCA6424A low-voltage 24-bit I2C I/O expander                          | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **tca95x5**              | Driver for TCA9535/TCA9555 remote 16-bit I/O expanders for I2C-bus               | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### Humidity sensors\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **aht**                  | Driver for AHT10/AHT15/AHT20 temperature and humidity sensor                     | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **am2320**               | Driver for AM2320 temperature and humidity sensor (I2C)                          | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **bme680**               | Driver for BME680 digital environmental sensor                                   | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **dht**                  | Driver for DHT11, AM2301 (DHT21, DHT22, AM2302, AM2321), Itead Si7021            | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | no            |\n| **hdc1000**              | Driver for HDC1000 temperature and humidity sensor                               | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **hts221**               | Driver for HTS221 temperature and humidity sensor                                | ISC     | esp32, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **sfa3x**                | Driver for SFA30 formaldehyde detection module (I2C)                             | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **sht3x**                | Driver for Sensirion SHT30/SHT31/SHT35 digital temperature and humidity sensor   | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **sht4x**                | Driver for Sensirion SHT40/SHT41/SHT45 digital temperature and humidity sensor   | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **si7021**               | Driver for Si7013/Si7020/Si7021/HTU2xD/SHT2x and compatible temperature and humidity sensors | BSD-3-Clause | esp32, esp32c3, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### Inertial measurement units\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **icm42670**             | Driver for TDK ICM-42670-P 6-Axis IMU                                            | ISC     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **l3gx**                 | Driver for L3Gx(L3GD20/L3G4200D) 3-axis gyroscope sensors                        | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **lsm303**               | Driver for LSM303 3-axis accelerometer and magnetometer sensor                   | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **mpu6050**              | Driver for MPU6000/MPU6050 6-axis MotionTracking device                          | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### Input device drivers\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **button**               | HW timer-based driver for GPIO buttons                                           | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **encoder**              | HW timer-based driver for incremental rotary encoders                            | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **ls7366r**              | Driver for LS7366R Quadrature Encoder Counter                                    | MIT     | esp32, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### LED drivers\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **ht16k33**              | HT16K33 LED controller driver                                                    | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **led_strip**            | RMT-based driver for WS2812B/SK6812/APA106/SM16703 LED strips                    | MIT     | esp32, esp32s2, esp32c3, esp32s3, esp32c6, esp32h2, esp32p4, esp32c5 | yes           |\n| **led_strip_spi**        | SPI-based driver for SK9822/APA102 LED strips                                    | MIT     | esp32, esp32c3, esp8266, esp32s2, esp32c3, esp32s3, esp32c6, esp32h2, esp32p4, esp32c5 | yes           |\n| **max7219**              | Driver for 8-Digit LED display drivers, MAX7219/MAX7221                          | BSD-3-Clause | esp32, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **pca9632**              | Driver for PCA9632 4-channel PWM chip                                            | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### Light sensors\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **bh1750**               | Driver for BH1750 light sensor                                                   | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **tsl2561**              | Driver for light-to-digital converter TSL2561                                    | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **tsl2591**              | Driver for light-to-digital converter TSL2591                                    | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **tsl4531**              | Driver for digital ambient light sensor TSL4531                                  | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **veml7700**             | Driver for VEML7700 ambient light sensor                                         | ISC     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### Magnetic sensors\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **hmc5883l**             | Driver for 3-axis digital compass HMC5883L and HMC5983L                          | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **lsm303**               | Driver for LSM303 3-axis accelerometer and magnetometer sensor                   | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **qmc5883l**             | Driver for QMC5883L 3-axis magnetic sensor                                       | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **qmi8658c**             | Driver for QMI8658C 6-axis IMU sensor                                            | BSD-3-Clause | esp32, esp32c3, esp8266, esp32s2, esp32c3, esp32c6 | yes           |\n\n\n\n### Other misc libraries\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **ds3502**               | Driver for nonvolatile digital potentiometer DS3502                              | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **example**              | An example component                                                             | ISC     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | n/a           |\n| **hd44780**              | Driver for HD44780 compatible LCD text displays                                  | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | no            |\n| **impulse_sensor**       | Driver for impulse output sensors                                                | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | no            |\n| **pca9685**              | Driver for 16-channel, 12-bit PWM PCA9685                                        | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **rda5807m**             | Driver for single-chip broadcast FM radio tuner RDA5807M                         | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **tca9548**              | Driver for TCA9548A/PCA9548A low-voltage 8-channel I2C switch                    | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **tda74xx**              | Driver for TDA7439/TDA7439DS/TDA7440D audioprocessors                            | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **tps63101x**            | Driver for Texas Instruments TPS631012 and TPS631013 1.6-V to 5.5-V Input Voltage 1.5-A Buck-boost Converter with I2C | BSD-3-Clause | esp32, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **ultrasonic**           | Driver for ultrasonic range meters, e.g. HC-SR04, HY-SRF05                       | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | no            |\n| **wiegand**              | Wiegand protocol receiver                                                        | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | no            |\n\n\n\n### Pressure sensors\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **bme680**               | Driver for BME680 digital environmental sensor                                   | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **bmp180**               | Driver for BMP180 digital pressure sensor                                        | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **bmp280**               | Driver for BMP280/BME280 digital pressure sensor                                 | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **dps310**               | Driver for DPS310 barometric pressure sensor                                     | ISC     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **ms5611**               | Driver for barometic pressure sensor MS5611-01BA03                               | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **qmp6988**              | Driver for QMP6988 digital temperature and pressure sensor                       | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### Real-time clocks\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **ds1302**               | Driver for DS1302 RTC module                                                     | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | no            |\n| **ds1307**               | Driver for DS1307 RTC module                                                     | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **ds3231**               | Driver for DS1337 RTC and DS3231 high precision RTC module                       | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **pcf8563**              | Driver for PCF8563 (BM8563) real-time clock/calendar                             | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n\n\n### Temperature sensors\n\n| Component | Description | License | Supported on | Thread safety |\n| --------- | ----------- | ------- | ------------ | ------------- |\n| **aht**                  | Driver for AHT10/AHT15/AHT20 temperature and humidity sensor                     | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **am2320**               | Driver for AM2320 temperature and humidity sensor (I2C)                          | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **bh1900nux**            | Driver for BH1900NUX temperature sensor                                          | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **bme680**               | Driver for BME680 digital environmental sensor                                   | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **bmp180**               | Driver for BMP180 digital pressure sensor                                        | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **bmp280**               | Driver for BMP280/BME280 digital pressure sensor                                 | MIT     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **dht**                  | Driver for DHT11, AM2301 (DHT21, DHT22, AM2302, AM2321), Itead Si7021            | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | no            |\n| **dps310**               | Driver for DPS310 barometric pressure sensor                                     | ISC     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **ds18x20**              | Driver for DS18B20/DS18S20 families of 1-Wire temperature sensor ICs             | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | no            |\n| **hdc1000**              | Driver for HDC1000 temperature and humidity sensor                               | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **hts221**               | Driver for HTS221 temperature and humidity sensor                                | ISC     | esp32, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **lm75**                 | Driver for LM75, a digital temperature sensor and thermal watchdog               | ISC     | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **max31725**             | Driver for MAX31725/MAX31726 temperature sensors                                 | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **max31855**             | Driver for MAX31855 cold-junction compensated thermocouple-to-digital converter  | BSD-3-Clause | esp32, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **max31865**             | Driver for MAX31865 resistance converter for platinum RTDs                       | BSD-3-Clause | esp32, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **mcp960x**              | Driver for MCP9600/MCP9601, thermocouple EMF to temperature converter            | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **mcp9808**              | Driver for MCP9808 digital temperature sensor                                    | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **ms5611**               | Driver for barometic pressure sensor MS5611-01BA03                               | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **qmp6988**              | Driver for QMP6988 digital temperature and pressure sensor                       | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **sfa3x**                | Driver for SFA30 formaldehyde detection module (I2C)                             | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **sht3x**                | Driver for Sensirion SHT30/SHT31/SHT35 digital temperature and humidity sensor   | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **sht4x**                | Driver for Sensirion SHT40/SHT41/SHT45 digital temperature and humidity sensor   | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **si7021**               | Driver for Si7013/Si7020/Si7021/HTU2xD/SHT2x and compatible temperature and humidity sensors | BSD-3-Clause | esp32, esp32c3, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **sts21**                | Driver for STS21 temperature sensor                                              | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **sts3x**                | Driver for Sensirion STS30/STS31/STS35 digital temperature sensor                | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n| **tsys01**               | Driver for precision digital temperature sensor TSYS01                           | BSD-3-Clause | esp32, esp8266, esp32s2, esp32c3, esp32s3, esp32c2, esp32c6, esp32h2, esp32p4, esp32c5, esp32c61 | yes           |\n\n## Library maintainers\n\n- [Ruslan V. Uss](https://github.com/UncleRus)\n- [Tomoyuki Sakurai](https://github.com/trombik)\n\n## Credits\n\n\n\n- [Alex Stewart](https://github.com/astewart-consensus): `ds18x20` \n\n- [Alexander Bodenseher](https://github.com/saasaa): `hts221` \n\n- [Andrej Krutak](https://github.com/andree182): `bh1750` \n\n- Angelo Elias Dalzotto: `mpu6050` \n\n- [BernhardG](https://gitlab.com/mrnice): `ms5611` \n\n- [BhuvanchandraD](https://github.com/bhuvanchandra): `ds3231` \n\n- [Brian Schwind](https://github.com/bschwind): `tsl2561` `tsl4531` \n\n- [Cedric von Gunten](https://github.com/vonguced): `qmp6988` \n\n- [Christian Skjerning](https://github.com/slimcdk): `sts3x` \n\n- [David Douard](https://github.com/douardda): `mhz19b` \n\n- [Erriez](https://github.com/Erriez): `mhz19b` \n\n- [FastLED project](https://github.com/FastLED): `color` `lib8tion` `noise` \n\n- Frank Bargstedt: `bmp180` \n\n- Gabriel Boni Vicari: `mpu6050` \n\n- [Grupo de Pesquisa em Cultura Digital](http://gepid.upf.br/): `mpu6050` \n\n- GrzegorzH: `ds18x20` \n\n- [Gunar Schorcht](https://github.com/gschorcht): `bme680` `ccs811` `sht3x` `sts3x` \n\n- [Jakub Turek](https://github.com/QB4-dev): `impulse_sensor` `l3gx` `lsm303` `pca9632` \n\n- [Jan Veeh](https://github.com/janveeh): `icm42670` \n\n- [Jeff Rowberg](https://www.i2cdevlib.com/): `mpu6050` \n\n- [Jose Manuel Perez](https://github.com/jmpmscorp): `lc709203f` `sgm58031` \n\n- [Joshua Butler](https://github.com/shuki25): `max1704x` \n\n- [Joshua Kallus](https://github.com/Jkallus): `ls7366r` \n\n- [jsuiker](https://github.com/jsuiker): `dht` \n\n- [Julian Doerner](https://github.com/juliandoerner): `tsl2591` \n\n- [Lucio Tarantino](https://github.com/dianlight): `ads111x` \n\n- [Manuel Markwort](https://github.com/mmarkwort): `mp2660` `tps63101x` \n\n- [Marc Luehr](https://github.com/th3link): `veml7700` \n\n- [Nate Usher](https://github.com/nated0g): `scd30` \n\n- Pavel Merzlyakov: `ds1302` \n\n- [Raghav Jha](https://github.com/horsemann07): `mpu6050` \n\n- RichardA: `ds3231` \n\n- [Ruslan V. Uss](https://github.com/UncleRus): `ads111x` `aht` `am2320` `bh1750` `bh1900nux` `bme680` `bmp180` `bmp280` `button` `calibration` `ccs811` `dht` `ds1302` `ds1307` `ds18x20` `ds3231` `ds3502` `encoder` `framebuffer` `hd44780` `hdc1000` `hmc5883l` `hx711` `i2cdev` `ina219` `ina260` `ina3221` `led_strip` `led_strip_spi` `max31725` `max31855` `max31865` `max7219` `mcp23008` `mcp23x17` `mcp342x` `mcp4725` `mcp960x` `mcp9808` `mpu6050` `ms5611` `onewire` `pca9557` `pca9685` `pcf8563` `pcf8574` `pcf8575` `pcf8591` `qmc5883l` `qmp6988` `rda5807m` `scd30` `scd4x` `sfa3x` `sgp40` `sht3x` `sht4x` `si7021` `sts21` `sts3x` `tca6424a` `tca9548` `tca95x5` `tda74xx` `tsl2561` `tsl4531` `tsys01` `ultrasonic` `wiegand` \n\n- [Sensirion AG](https://github.com/Sensirion): `scd30` `scd4x` `sfa3x` \n\n- [sheinz](https://github.com/sheinz): `bmp280` \n\n- [Thanh Pham](https://github.com/panoti): `pcf8591` \n\n- [Timofei Korostelev](https://github.com/chudsaviet): `ht16k33` \n\n- [Tomoyuki Sakurai](https://github.com/trombik): `dps310` `esp_idf_lib_helpers` `example` `led_strip_spi` `lm75` \n\n- [Weslley Duarte](https://github.com/weslleymfd): `ads130e08` \n\n- [xyzroe](https://github.com/xyzroe): `qmi8658c` \n\n- [Zaltora](https://github.com/Zaltora): `ina3221` \n\n- zeroday: `onewire` \n",
    "readme_length": 38067
  },
  {
    "name": "PiFmRds",
    "full_name": "ChristopheJacquet/PiFmRds",
    "description": "FM-RDS transmitter using the Raspberry Pi's PWM",
    "stars": 1537,
    "forks": 344,
    "language": "C",
    "url": "https://github.com/ChristopheJacquet/PiFmRds",
    "topics": [],
    "created_at": "2014-03-30T22:19:48Z",
    "updated_at": "2025-12-01T19:52:16Z",
    "homepage": null,
    "license": "GNU General Public License v3.0",
    "readme": "Pi-FM-RDS\n=========\n\n\n## FM-RDS transmitter using the Raspberry Pi\n\nThis program generates an FM modulation, with RDS (Radio Data System) data generated in real time. It can include monophonic or stereophonic audio.\n\nIt is based on the FM transmitter created by Oliver Mattos and Oskar Weigl, and later adapted to using DMA by [Richard Hirst](https://github.com/richardghirst). Christophe Jacquet adapted it and added the RDS data generator and modulator. The transmitter uses the Raspberry Pi's PWM generator to produce VHF signals.\n\nIt is compatible with both the Raspberry Pi 1 (the original one) and the Raspberry Pi 2, 3, 4 and Zero.\n\n![](doc/vfd_display.jpg)\n\nPiFmRds has been developed for experimentation only. It is not a media center, it is not intended to broadcast music to your stereo system. See the [legal warning](#warning-and-disclaimer).\n\n## How to use it?\n\nPi-FM-RDS is tested under Raspberry Pi OS Lite (the version without desktop environment). You should be able to run it under other distributions, but the author provides no guarantees nor support. Latest Raspberry Pi OS version successfully tested: 13.1 (based on Debian Trixie).\n\nDependencies:\n\n* `sndfile` library, provided by package `libsndfile1-dev` under Raspberry Pi OS and other Debian-like distributions.\n* Linux `rpi-mailbox` driver, so you need a Linux kernel built after approximately August 2015.\n\n**Important.** The binaries compiled for one Raspberry Pi model and ARM architecture are not compatible with other models and architectures. Always re-compile when switching models, so do not skip the `make clean` step in the instructions below!\n\nThese should be the complete instructions assuming a clean Raspberry Pi OS Lite install:\n\n```bash\nsudo apt install git libsndfile1-dev\ngit clone https://github.com/ChristopheJacquet/PiFmRds.git\ncd PiFmRds/src\nmake clean\nmake\n```\n\n**Important.** If `make` reports any error, then no `pi_fm_rds` executable file is generated (and vice versa). So any error must be fixed before you can proceed to the next steps. `make` may fail if any required library is missing (see above), or it could be a bug on a specific/newer distribution. In this case, please file a bug.\n\nIf `make` reports no error (i.e. the `pi_fm_rds` executable gets generated), you can then simply run:\n\n```\nsudo ./pi_fm_rds\n```\n\nThis will generate an FM transmission on 107.9 MHz, with default station name (PS), radiotext (RT) and PI-code, without audio. The radiofrequency signal is emitted on GPIO 4 (pin 7 on header P1).\n\n\nYou can add monophonic or stereophonic audio by referencing an audio file as follows:\n\n```\nsudo ./pi_fm_rds -audio sound.wav\n```\n\nTo test stereophonic audio, you can try the file `stereo_44100.wav` provided.\n\nThe more general syntax for running Pi-FM-RDS is as follows:\n\n```\npi_fm_rds [-freq freq] [-audio file] [-ppm ppm_error] [-pi pi_code] [-ps ps_text] [-rt rt_text]\n```\n\nAll arguments are optional:\n\n* `-freq` specifies the carrier frequency (in MHz). Example: `-freq 107.9`.\n* `-audio` specifies an audio file to play as audio. The sample rate does not matter: Pi-FM-RDS will resample and filter it. If a stereo file is provided, Pi-FM-RDS will produce an FM-Stereo signal. Example: `-audio sound.wav`. The supported formats depend on `libsndfile`. This includes WAV and Ogg/Vorbis (among others) but not MP3. Specify `-` as the file name to read audio data on standard input (useful for piping audio into Pi-FM-RDS, see below).\n* `-pi` specifies the PI-code of the RDS broadcast. 4 hexadecimal digits. Example: `-pi FFFF`.\n* `-ps` specifies the station name (Program Service name, PS) of the RDS broadcast. Limit: 8 characters. Example: `-ps RASP-PI`.\n* `-rt` specifies the radiotext (RT) to be transmitted. Limit: 64 characters. Example: `-rt 'Hello, world!'`.\n* `-ctl` specifies a named pipe (FIFO) to use as a control channel to change PS and RT at run-time (see below).\n* `-ppm` specifies your Raspberry Pi's oscillator error in parts per million (ppm), see below.\n\nBy default the PS changes back and forth between `Pi-FmRds` and a sequence number, starting at `00000000`. The PS changes around one time per second.\n\n\n### Clock calibration (only if experiencing difficulties)\n\nThe RDS standards states that the error for the 57 kHz subcarrier must be less than Â± 6 Hz, i.e. less than 105 ppm (parts per million). The Raspberry Pi's oscillator error may be above this figure. That is where the `-ppm` parameter comes into play: you specify your Pi's error and Pi-FM-RDS adjusts the clock dividers accordingly.\n\nIn practice, I found that Pi-FM-RDS works okay even without using the `-ppm` parameter. I suppose the receivers are more tolerant than stated in the RDS spec.\n\nOne way to measure the ppm error is to play the `pulses.wav` file: it will play a pulse for precisely 1 second, then play a 1-second silence, and so on. Record the audio output from a radio with a good audio card. Say you sample at 44.1 kHz. Measure 10 intervals. Using [Audacity](https://www.audacityteam.org/) for example determine the number of samples of these 10 intervals: in the absence of clock error, it should be 441,000 samples. With my Pi, I found 441,132 samples. Therefore, my ppm error is (441132-441000)/441000 * 1e6 = 299 ppm, **assuming that my sampling device (audio card) has no clock error...**\n\n\n### Piping audio into Pi-FM-RDS\n\nIf you use the argument `-audio -`, Pi-FM-RDS reads audio data on standard input. This allows you to pipe the output of a program into Pi-FM-RDS. For instance, this can be used to read MP3 files using Sox:\n\n```\nsox -t mp3 http://www.linuxvoice.com/episodes/lv_s02e01.mp3 -t wav -  | sudo ./pi_fm_rds -audio -\n```\n\nOr to pipe the AUX input of a sound card into Pi-FM-RDS:\n\n```\nsudo arecord -fS16_LE -r 44100 -Dplughw:1,0 -c 2 -  | sudo ./pi_fm_rds -audio -\n```\n\n\n### Changing PS, RT and TA at run-time\n\nYou can control PS, RT and TA (Traffic Announcement flag) at run-time using a named pipe (FIFO). For this run Pi-FM-RDS with the `-ctl` argument.\n\nExample:\n\n```\nmkfifo rds_ctl\nsudo ./pi_fm_rds -ctl rds_ctl\n```\n\nAt this point, Pi-FM-RDS waits until another program opens the named pipe in write mode\n(for example `cat >rds_ctl` in the example below) before it starts transmitting.\n\nYou can use the named pipe to send â€œcommandsâ€ to change PS, RT and TA. For instance, in\nanother terminal:\n\n```\ncat >rds_ctl\nPS MyText\nRT A text to be sent as radiotext\nTA ON\nPS OtherTxt\nTA OFF\n...\n```\n\n> [!TIP]\n> The program that opens the named pipe in write mode can be started after Pi-FM-RDS\n> (like above) or before (in which case Pi-FM-RDS does not have to wait at startup).\n\nEvery line must start with either `PS`, `RT` or `TA`, followed by one space character, and the desired value. Any other line format is silently ignored. `TA ON` switches the Traffic Announcement flag to *on*, any other value switches it to *off*.\n\n\n### Non-ASCII characters\n\nYou can use the full range of characters supported by the RDS protocol. Pi-FM-RDS decodes\nthe input strings based on the system's locale variables. As of early 2024, Raspberry Pi\nOS uses by default UTF-8 and the `LANG` variable is set to `en_GB.UTF-8`. With this setup,\nit should work out of the box.\n\nIf it does not work, look at the first message that Pi-FM-RDS prints out. It should be\nsomething sensible, like:\n\n```\nLocale set to en_GB.UTF-8.\n```\n\nIf it is not consistent with your setup, or if the locale appears to be set to `(null)`,\nthen your locale variables are not set correctly and Pi-FM-RDS is incapable of working\nwith non-ASCII characters.\n\n\n### Compiling on distributions with different float ABIs\n\nThe makefile uses `-mfloat-abi=hard`, which is suited for Raspberry Pi OS. Different distributions might require different values, namely `soft` or `softfp`.\n\n\n## Warning and Disclaimer\n\nPiFmRds is an **experimental** program, designed **only for experimentation**. It is in no way intended to become a personal *media center* or a tool to operate a *radio station*, or even broadcast sound to one's own stereo system.\n\nIn most countries, transmitting radio waves without a state-issued licence specific to the transmission modalities (frequency, power, bandwidth, etc.) is **illegal**.\n\nTherefore, always connect a shielded transmission line from the RaspberryPi directly\nto a radio receiver, so as **not** to emit radio waves. Never use an antenna.\n\nEven if you are a licensed amateur radio operator, using PiFmRds to transmit radio waves on ham frequencies without any filtering between the RaspberryPi and an antenna is most probably illegal because the square-wave carrier is very rich in harmonics, so the bandwidth requirements are likely not met.\n\nI could not be held liable for any misuse of your own Raspberry Pi. Any experiment is made under your own responsibility.\n\n\n## Tests\n\nPi-FM-RDS was successfully tested with all my RDS-able devices, namely:\n\n* a Sony ICF-C20RDS alarm clock from 1995,\n* a Sangean PR-D1 portable receiver from 1998, and an ATS-305 from 1999,\n* a Samsung Galaxy S2 mobile phone from 2011,\n* a Philips MBD7020 hifi system from 2012,\n* a Silicon Labs [USBFMRADIO-RD](http://www.silabs.com/products/mcu/Pages/USBFMRadioRD.aspx) USB stick, employing an Si4701 chip, and using my [RDS Surveyor](http://rds-surveyor.sourceforge.net/) program,\n* a â€œPCear Fm Radioâ€, a Chinese clone of the above, again using RDS Surveyor.\n\nReception works perfectly with all the devices above. RDS Surveyor reports no group errors.\n\n![](doc/galaxy_s2.jpg)\n\n\n### CPU Usage\n\nCPU usage is as follows:\n\n* without audio: 9%\n* with mono audio: 33%\n* with stereo audio: 40%\n\nCPU usage increases dramatically when adding audio because the program has to upsample the (unspecified) sample rate of the input audio file to 228 kHz, its internal operating sample rate. Doing so, it has to apply an FIR filter, which is costly.\n\n## Design\n\nThe RDS data generator lies in the `rds.c` file.\n\nThe RDS data generator generates cyclically four 0A groups (for transmitting PS), and one 2A group (for transmitting RT). In addition, every minute, it inserts a 4A group (for transmitting CT, clock time). `get_rds_group` generates one group, and uses `crc` for computing the CRC.\n\nTo get samples of RDS data, call `get_rds_samples`. It calls `get_rds_group`, differentially encodes the signal and generates a shaped biphase symbol. Successive biphase symbols overlap: the samples are added so that the result is equivalent to applying the shaping filter (a [root-raised-cosine (RRC) filter ](http://en.wikipedia.org/wiki/Root-raised-cosine_filter) specified in the RDS standard) to a sequence of Manchester-encoded pulses.\n\nThe shaped biphase symbol is generated once and for all by a Python program called `generate_waveforms.py` that uses [Pydemod](https://github.com/ChristopheJacquet/Pydemod), one of my other software radio projects. This Python program generates an array called `waveform_biphase` that results from the application of the RRC filter to a positive-negative impulse pair. *Note that the output of `generate_waveforms.py`, two files named `waveforms.c` and `waveforms.h`, are included in the Git repository, so you don't need to run the Python script yourself to compile Pi-FM-RDS.*\n\nInternally, the program samples all signals at 228 kHz, four times the RDS subcarrier's 57 kHz.\n\nThe FM multiplex signal (baseband signal) is generated by `fm_mpx.c`. This file handles the upsampling of the input audio file to 228 kHz, and the generation of the multiplex: unmodulated left+right signal (limited to 15 kHz), possibly the stereo pilot at 19 kHz, possibly the left-right signal, amplitude-modulated on 38Â kHz (suppressed carrier) and RDS signal from `rds.c`. Upsampling is performed using a zero-order hold followed by an FIR low-pass filter of order 60. The filter is a sampled sinc windowed by a Hamming window. The filter coefficients are generated at startup so that the filter cuts frequencies above the minimum of:\n* the Nyquist frequency of the input audio file (half the sample rate) to avoid aliasing,\n* 15 kHz, the bandpass of the left+right and left-right channels, as per the FM broadcasting standards.\n\nThe samples are played by `pi_fm_rds.c` that is adapted from Richard Hirst's [PiFmDma](https://github.com/richardghirst/PiBits/tree/master/PiFmDma). The program was changed to support a sample rate of precisely 228 kHz.\n\n\n### References\n\n* EN 50067, Specification of the radio data system (RDS) for VHF/FM sound broadcasting in the frequency range 87.5 to 108.0 MHz.\n\n\n## History\n\n* 2024-02-21: properly handle non-ASCII characters.\n* 2015-09-05: support for the Raspberry Pi 2 and later models\n* 2014-11-01: support for toggling the Traffic Announcement (TA) flag at run-time\n* 2014-10-19: bugfix (cleanly stop the DMA engine when the specified file does not exist, or it's not possible to read from stdin)\n* 2014-08-04: bugfix (ppm now uses floats)\n* 2014-06-22: generate CT (clock time) signals, bugfixes\n* 2014-05-04: possibility to change PS and RT at run-time\n* 2014-04-28: support piping audio file data to Pi-FM-RDS' standard input\n* 2014-04-14: new release that supports any sample rate for the audio input, and that can generate a proper FM-Stereo signal if a stereophonic input file is provided\n* 2014-04-06: initial release, which only supported 228 kHz monophonic audio input files\n\n--------\n\nÂ© [Christophe Jacquet](https://jacquet.xyz/en/) (F8FTK), 2014-2025. Released under the GNU GPL v3.\n",
    "readme_length": 13440
  },
  {
    "name": "rppal",
    "full_name": "golemparts/rppal",
    "description": "A Rust library that provides access to the Raspberry Pi's GPIO, I2C, PWM, SPI and UART peripherals.",
    "stars": 1434,
    "forks": 115,
    "language": "Rust",
    "url": "https://github.com/golemparts/rppal",
    "topics": [
      "gpio",
      "i2c",
      "pwm",
      "raspberry-pi",
      "raspberrypi",
      "rust",
      "spi",
      "uart"
    ],
    "created_at": "2017-03-01T15:16:36Z",
    "updated_at": "2025-11-28T23:54:08Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# RPPAL - Raspberry Pi Peripheral Access Library\n\n[![Build status](https://github.com/golemparts/rppal/actions/workflows/ci.yml/badge.svg)](https://github.com/golemparts/rppal/actions/workflows/ci.yml)\n[![Latest release](https://img.shields.io/crates/v/rppal)](https://crates.io/crates/rppal)\n[![Minimum rustc version](https://img.shields.io/badge/rustc-v1.60.0-lightgray.svg)](https://blog.rust-lang.org/2022/04/07/Rust-1.60.0.html)\n[![Documentation](https://docs.rs/rppal/badge.svg)](https://docs.rs/rppal)\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\n## This project is no longer maintained\n\nAs of July 1, 2025, I have decided to retire RPPAL. This means:\n* No new features will be added.\n* Bug fixes will no longer be provided.\n* Support for new hardware is not planned.\n* Pull requests and issues will no longer be reviewed or addressed.\n\nI want to express my sincere gratitude to everyone who contributed to, used, and supported RPPAL over the years. Your contributions and feedback were invaluable.\n\n### Why have I retired RPPAL?\n\nRPPAL began as a passion project in 2016, nearly nine years ago, when I first started working with electronics and needed a project to work on my Rust skills. Initially, it started out as the GPIO module of my Blinkt library. As the scope expanded to include support for other peripherals, RPPAL was spun off into its own distinct project, evolving into the comprehensive library it is today.\n\nHowever, over the past several years, my personal interests and professional focus have shifted away from electronics. As a result, I haven't actively used RPPAL myself for quite some time. I no longer have a dedicated hardware test setup, nor do I plan on adding new Raspberry Pi models to my collection. This makes it impractical to thoroughly test changes or ensure compatibility with new hardware releases.\n\nMaintaining a project requires significant dedication, and without active personal use or the necessary testing environment, it's become challenging to provide the level of attention this project deserves.\n\n### What does this mean for you?\n\nYou are welcome to continue using RPPAL. However, please be aware you will not receive any further updates or support. RPPAL works with all Raspberry Pi models released before July 1, 2025, up to and including the Raspberry Pi 5.\n\n#### Forking the project\n\nIf you wish to continue its development, you may fork this project under the terms and conditions of the MIT License.\n\n## RPPAL\n\nRPPAL provides access to the Raspberry Pi's GPIO, I2C, PWM, SPI and UART peripherals through a user-friendly interface. In addition to peripheral access, RPPAL also offers support for USB to serial adapters.\n\nThe library can be used in conjunction with a variety of platform-agnostic drivers through its `embedded-hal` trait implementations. Both `embedded-hal` v0.2.7 and v1 are supported.\n\nRPPAL requires a recent release of Raspberry Pi OS. Similar Linux distributions may work, but are unsupported. Both GNU and musl `libc` targets are supported. RPPAL is compatible with the Raspberry Pi A, A+, B, B+, 2B, 3A+, 3B, 3B+, 4B, 5, CM, CM 3, CM 3+, CM 4, CM 5, CM 5 Lite, 400, 500, Zero, Zero W and Zero 2 W.\n\n## Table of contents\n\n- [Usage](#usage)\n- [Examples](#examples)\n- [Optional features](#optional-features)\n- [Supported peripherals](#supported-peripherals)\n  - [GPIO](#gpio)\n  - [I2C](#i2c)\n  - [PWM](#pwm)\n  - [SPI](#spi)\n  - [UART](#uart)\n- [Cross compilation](#cross-compilation)\n  - [Cargo](#cargo)\n  - [Visual Studio Code](#visual-studio-code)\n- [Caution](#caution)\n- [Copyright and license](#copyright-and-license)\n\n## Usage\n\nAdd a dependency for `rppal` to your `Cargo.toml` using `cargo add rppal`, or by adding the following line to your dependencies section.\n\n```toml\n[dependencies]\nrppal = \"0.22.1\"\n```\n\nIf your project requires `embedded-hal` trait implementations, specify either the `hal` or `hal-unproven` feature flag in the dependency declaration.\n\n```toml\n[dependencies]\nrppal = { version = \"0.22.1\", features = [\"hal\"] }\n```\n\nCall `new()` on any of the peripherals to construct a new instance.\n\n```rust\nuse rppal::gpio::Gpio;\nuse rppal::i2c::I2c;\nuse rppal::pwm::{Channel, Pwm};\nuse rppal::spi::{Bus, Mode, SlaveSelect, Spi};\nuse rppal::uart::{Parity, Uart};\n\nlet gpio = Gpio::new()?;\nlet i2c = I2c::new()?;\nlet pwm = Pwm::new(Channel::Pwm0)?;\nlet spi = Spi::new(Bus::Spi0, SlaveSelect::Ss0, 16_000_000, Mode::Mode0)?;\nlet uart = Uart::new(115_200, Parity::None, 8, 1)?;\n```\n\nAccess to some peripherals may need to be enabled first through `sudo raspi-config` or by editing `/boot/firmware/config.txt`. Refer to the relevant module's documentation for any required steps.\n\n## Examples\n\nThis example demonstrates how to blink an LED connected to a GPIO pin. Remember to add a resistor of an appropriate value in series, to prevent exceeding the maximum current rating of the GPIO pin and the LED.\n\n```rust\nuse std::error::Error;\nuse std::thread;\nuse std::time::Duration;\n\nuse rppal::gpio::Gpio;\nuse rppal::system::DeviceInfo;\n\n// Gpio uses BCM pin numbering. BCM GPIO 23 is tied to physical pin 16.\nconst GPIO_LED: u8 = 23;\n\nfn main() -> Result<(), Box<dyn Error>> {\n    println!(\"Blinking an LED on a {}.\", DeviceInfo::new()?.model());\n\n    let mut pin = Gpio::new()?.get(GPIO_LED)?.into_output();\n\n    // Blink the LED by setting the pin's logic level high for 500 ms.\n    pin.set_high();\n    thread::sleep(Duration::from_millis(500));\n    pin.set_low();\n\n    Ok(())\n}\n```\n\nAdditional examples can be found in the `examples` directory.\n\n## Optional features\n\nBy default, all optional features are disabled. You can enable a feature by specifying the relevant feature flag(s) in the dependency declaration for `rppal` in your `Cargo.toml`.\n\n* `hal` - Enables `embedded-hal` trait implementations for all supported peripherals. This doesn't include `unproven` traits.\n* `hal-unproven` - Enables `embedded-hal` trait implementations for all supported peripherals, including traits marked as `unproven`. Note that `embedded-hal`'s `unproven` traits don't follow semver rules. Patch releases may introduce breaking changes.\n\n## Supported peripherals\n\n### [GPIO](https://docs.rs/rppal/latest/rppal/gpio)\n\nTo ensure fast performance, RPPAL controls the GPIO peripheral by directly accessing the registers through either `/dev/gpiomem` or `/dev/mem`. GPIO interrupts are configured using the `gpiochip` character device.\n\n#### Features\n\n* Get/set pin mode and logic level\n* Configure built-in pull-up/pull-down resistors\n* Synchronous and asynchronous interrupt handlers\n* Software-based PWM implementation\n* Optional `embedded-hal` trait implementations\n\n### [I2C](https://docs.rs/rppal/latest/rppal/i2c)\n\nThe Broadcom Serial Controller (BSC) peripheral controls a proprietary bus compliant with the I2C bus/interface. RPPAL communicates with the BSC using the `i2cdev` character device.\n\n#### Features\n\n* Single master, 7-bit slave addresses, transfer rates up to 400 kbit/s (Fast-mode)\n* I2C basic read/write, block read/write, combined write+read\n* SMBus protocols: Quick Command, Send/Receive Byte, Read/Write Byte/Word, Process Call, Block Write, PEC\n* Optional `embedded-hal` trait implementations\n\n### [PWM](https://docs.rs/rppal/latest/rppal/pwm)\n\nRPPAL controls the Raspberry Pi's PWM peripheral through the `pwm` sysfs interface.\n\n#### Features\n\n* Up to four hardware PWM channels depending on the Raspberry Pi model\n* Configurable frequency, duty cycle and polarity\n* Optional `embedded-hal` trait implementations\n\n### [SPI](https://docs.rs/rppal/latest/rppal/spi)\n\nRPPAL controls the Raspberry Pi's main and auxiliary SPI peripherals through the `spidev` character device.\n\n#### Features\n\n* SPI master, mode 0-3, Slave Select active-low/active-high, 8 bits per word, configurable clock speed\n* Half-duplex reads, writes, and multi-segment transfers\n* Full-duplex transfers and multi-segment transfers\n* Customizable options for each segment in a multi-segment transfer (clock speed, delay, SS change)\n* Reverse bit order helper function\n* Optional `embedded-hal` trait implementations\n\n### [UART](https://docs.rs/rppal/latest/rppal/uart)\n\nRPPAL controls the Raspberry Pi's UART peripherals through the `ttyAMA0` (PL011) and `ttyS0` (mini UART) character devices. USB to serial adapters are controlled using the `ttyUSBx` and `ttyACMx` character devices.\n\n#### Features\n\n* Support for UART peripherals (PL011, mini UART) and USB to serial adapters\n* None/Even/Odd/Mark/Space parity, 5-8 data bits, 1-2 stop bits\n* Transfer rates up to 4 Mbit/s (device-dependent)\n* XON/XOFF software flow control\n* RTS/CTS hardware flow control with automatic pin configuration\n* Optional `embedded-hal` trait implementations\n\n## Cross compilation\n\nIf you're not working directly on a Raspberry Pi, you'll have to cross-compile your code for the appropriate ARM architecture. Check out [this guide](https://github.com/japaric/rust-cross) for more information, or try the [cross](https://github.com/japaric/cross) project for \"zero setup\" cross compilation.\n\n### Cargo\n\nFor manual cross-compilation without the use of `cross`, you will need to install the appropriate target. Most Raspberry Pi models either need the `armv7-unknown-linux-gnueabihf` target for 32-bit Linux distributions, or `aarch64-unknown-linux-gnu` for 64-bit. For some models, like the Raspberry Pi Zero, a different target triple is required.\n\nInstall the relevant target using `rustup`.\n\n```bash\nrustup target install armv7-unknown-linux-gnueabihf\n```\n\nIn the root directory of your project, create a `.cargo` subdirectory, and save the following snippet to `.cargo/config.toml`.\n\n```toml\n[build]\ntarget = \"armv7-unknown-linux-gnueabihf\"\n```\n\n### Visual Studio Code\n\nThe rust-analyzer extension for Visual Studio Code needs to be made aware of the target platform by setting the `rust-analyzer.cargo.target` configuration option. In the root directory of your project, create a `.vscode` subdirectory, and then save the following snippet to `.vscode/settings.json`.\n\n```json\n{\n    \"rust-analyzer.cargo.target\": \"armv7-unknown-linux-gnueabihf\"\n}\n```\n\n## Caution\n\nAlways be careful when working with the Raspberry Pi's peripherals, especially if you attach any external components to the GPIO pins. Improper use can lead to permanent damage.\n\n## Copyright and license\n\nCopyright (c) 2017-2025 Rene van der Meer. Released under the [MIT license](LICENSE).\n",
    "readme_length": 10441
  },
  {
    "name": "SwiftyGPIO",
    "full_name": "uraimo/SwiftyGPIO",
    "description": "A Swift library for hardware projects on Linux/ARM boards with support for GPIOs/SPI/I2C/PWM/UART/1Wire.",
    "stars": 1368,
    "forks": 141,
    "language": "Swift",
    "url": "https://github.com/uraimo/SwiftyGPIO",
    "topics": [
      "1-wire",
      "gpio",
      "i2c",
      "iot",
      "lcd-display",
      "led",
      "led-strips",
      "neopixel",
      "pwm",
      "raspberry-pi",
      "serialport",
      "spi",
      "spi-interface",
      "swift",
      "uart"
    ],
    "created_at": "2016-01-10T17:05:48Z",
    "updated_at": "2025-11-29T00:06:49Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "<p align=\"center\" style=\"padding-bottom:50px;\">\n    <img width=\"500\" height=\"200\" src=\"logo.svg\" alt=\"SwiftyGPIO\"/>\n    <br/>\n\t<a href=\"https://raw.githubusercontent.com/uraimo/SwiftyGPIO/master/LICENSE\"><img src=\"http://img.shields.io/badge/License-MIT-blue.svg?style=flat\"/></a>\n\t<a href=\"https://developer.apple.com/swift\"><img src=\"https://img.shields.io/badge/Swift-5.x-orange.svg?style=flat\"/></a> \n\t<a href=\"https://slackpass.io/swift-arm\"><img src=\"https://img.shields.io/badge/Slack-swift/arm-red.svg?style=flat\"/></a>\n\t<a href=\"https://travis-ci.org/uraimo/SwiftyGPIO\"><img src=\"https://travis-ci.org/uraimo/SwiftyGPIO.svg?branch=master\" /></a>\n</p>\n\n<p align=\"center\">\n<i>A Swift library for hardware projects on Linux/ARM boards with support for GPIOs/SPI/I2C/PWM/UART/1Wire.</i>\n</p>\n\n![](images/banner.jpg)\n\n## Summary\n\nThis library provides an easy way to interact with external sensors and devices using the digital GPIOs, SPI/I2C interfaces, 1-Wire buses, PWM signals and serial ports that boards like the Raspberry Pi provide, on Linux using Swift.\n\nLike Android Things or similar libraries in Python, SwiftyGPIO provides the basic functionalities you'll need to control different devices: sensors, displays, input devices like joypads, RGB led strips and matrices.\n\nYou'll be able to configure port attributes and read/write the current GPIOs value, use the [SPI](https://en.wikipedia.org/wiki/Serial_Peripheral_Interface_Bus) interfaces (via hardware if your board provides them or using software big-banging SPI), comunicate over a bus with [I2C](https://learn.sparkfun.com/tutorials/i2c), generate a [PWM](https://en.wikipedia.org/wiki/Pulse-width_modulation) to drive external displays, servos, leds and more complex sensors, interact with devices that expose [UART](https://learn.sparkfun.com/tutorials/serial-communication) serial connections using AT commands or custom protocols, and finally connect to [1-Wire](https://en.wikipedia.org/wiki/1-Wire) devices. \n\nWhile you'll still be able to develop your project with Xcode or another IDE, the library is built to run exclusively on Linux ARM Boards (RaspberryPis, BeagleBones, ODROIDs, OrangePis, etc...).\n\nExamples of  *[device libraries](#libraries)* and *[complete projects](#awesome-projects)* built using SwiftyGPIO that you can use as inspiration for your own DIY hardware projects are listed below, **have fun!**\n\n\n##### Content:\n- [Supported Boards](#supported-boards)\n- [Installation](#installation)\n- [Your First Project: Blinking Leds And Sensors](#your-first-project-blinking-leds-and-sensors)\n- [Usage](#usage)\n    - [GPIO](#gpio)\n    - [SPI](#spi)\n    - [I2C](#i2c)\n    - [PWM](#pwm)\n    - [Pattern-based signal generator via PWM](#pattern-based-signal-generator-via-pwm)\n    - [UART](#uart)\n    - [1-Wire](#1-wire)\n- [Examples](#examples)\n- [Built with SwiftyGPIO](#built-with-swiftygpio)\n    - [Device Libraries](#libraries)\n    - [Awesome Projects](#awesome-projects)\n    - [Support Libraries](#support-libraries)\n- [Additional documentation](#additional-documentation)\n\n\n## Supported Boards\n\nThe following boards are supported and have been tested with recent releases of Swift:\n\n* Raspberry Pi 4, 4B\n* Raspberry Pi 3, 3A+, 3B+\n* Raspberry Pi 2 (Thanks to [@iachievedit](https://twitter.com/iachievedit))\n* Raspberry Pi Zero 2 W\n* Raspberry Pi Zero W\n* Raspberry Pi Zero (Thanks to [@MacmeDan](https://twitter.com/MacmeDan))\n* Raspberry Pi Classic A,B,A+,B+ Rev1/Rev2\n* BeagleBones (Thanks to [@hpux735](https://twitter.com/hpux735))\n* OrangePi (Thanks to [@colemancda](https://github.com/colemancda))\n* OrangePi Zero (Thanks to [@eugeniobaglieri](https://github.com/eugeniobaglieri)) \n* Asus Tinkerboard (Thanks to Ernesto Lo Valvo)\n* C.H.I.P.\n\nBut basically everything that has an ARMv7/8+Ubuntu/Debian/Raspbian or an ARMv6+Raspbian/Debian should work if you can run Swift on it.\n\nPlease keep in mind that Swift on ARM is a completely community-driven effort, and that there are a multitude of possible board+OS configurations, don't expect that everything will work right away on every configuration even if most of the times it does, especially if you are the first to try a new configuration or board.\n\n## Installation\n\nTo use this library, you'll need a Linux ARM(ARMv7/8 or ARMv6) board with Swift 3.x/4.x/5.x.\n\nIf you have a RaspberryPi (A,B,A+,B+,Zero,ZeroW,2,3,4) with Ubuntu or Raspbian, get Swift 5.x from [here](https://github.com/uraimo/buildSwiftOnARM/releases/latest) or follow the instruction from [buildSwiftOnARM](https://github.com/uraimo/buildSwiftOnARM) to build it yourself in a few hours.\n\nI always recommend to try one of the latest binaries available (either Ubuntu or Raspbian) before putting in the time to compile it yourself, those binaries could(and do most of the times) also work on other Debian-bases distibutions and on different boards.\n\nAn alternative way to get these Swift binaries on your Raspberry Pi is through the [Swift on Balena](https://github.com/wlisac/swift-on-balena) project that provides well organized IoT focused Docker images.\n\nYou can also setup a cross-compiling toolchain and build ARM binaries (Ubuntu/Raspbian) from a Mac, thanks again to the work of Helge HeÃŸ (and Johannes WeiÃŸ for implementing it in SPM), read more about that [here](https://github.com/AlwaysRightInstitute/swift-mac2arm-x-compile-toolchain).\n\nTo start your project add SwiftyGPIO as a dependency in your `Package.swift`:\n\n```swift\n// swift-tools-version:4.0\nimport PackageDescription\n\nlet package = Package(\n    name: \"light\",\n    dependencies: [\n         .package(url: \"https://github.com/uraimo/SwiftyGPIO.git\", from: \"1.0.0\")\n    ]\n)\n```\n\nAnd then build with `swift build`.\n\nThe compiler will create an executable under `.build/debug/MyProject`.\n\n**IMPORTANT:** Like every library using GPIOs/SPI/I2C/etc..., if your OS does not come with a predefined user group to access these functionalities, you'll need to run your application with root privileges using `sudo`. If you are using a RaspberryPi with a Raspbian or a recent Ubuntu (from 16.04 Xenial onward) implementing /dev/gpiomem, sudo will be not required to use basic GPIOs, just launch your application calling the executable built by the compiler.\n\nOn misconfigured systems, features like the listeners may require root privileges too and advanced features like PWM sadly always require root privileges.\n\nAlternatively, a specific user group for gpio access can be configured manually as shown [here](https://arcanesciencelab.wordpress.com/2016/03/31/running-rpi3-applications-that-use-gpio-without-being-root/) or in this [answer on stackoverflow](https://stackoverflow.com/questions/30938991/access-gpio-sys-class-gpio-as-non-root/30940526#30940526).\nAfter following those instruction, remember to add your user (e.g. pi) to the gpio group with `sudo usermod -aG gpio pi` and to reboot so that the changes you made are applied.\n\n<a href=\"#first\"></a>\n## Your First Project: Blinking leds and sensors\n\nIf you prefer starting with a real project instead of just reading documentation, you'll find some ready to run examples under `Examples/` and more than a few tutorials, [full projects](#awesome-projects) and videos available online:\n\n* [Video: Swift Without Screens - Powering Connected Devices](https://www.youtube.com/watch?v=VILUaec-sCs) - Talk by Marc Aupont at *try! Swift NYC 2019 Conference*\n* [Video: Swift Hardware Hacking](https://www.youtube.com/watch?v=b9EVb0jEt8E), [#2](https://www.slideshare.net/mostgood/swift-hardware-hacking-try-swift) - Talk by Sally Shepard at *try! Swift Tokyo 2019 Conference*.\n* [Video: SwiftNIO on the Raspberry PI](https://www.youtube.com/watch?v=FPGf652O90Y),[#2](http://www.alwaysrightinstitute.com/linkerkit/) -  Talk by Helge HeÃŸ at *Serverside.swift 2018 Conference*.\n* [Video: SwiftyPi](https://www.youtube.com/watch?v=xnGOLSI45Mw) -  Talk by Kate Castellano at *try! Swift Tokyo 2018*.\n* [Video: Swift for IoT](https://www.youtube.com/watch?v=YuPM_I9bQMI) -  Talk by Subhransu Behera and Kheng Meng Yeo at *iOS Conf SG 2016*.\n* [Swift on Raspberry projects at Woolsey Workshop](https://www.woolseyworkshop.com/2018/06/20/blink-making-an-led-blink-on-a-raspberry-pi/), [#2](https://github.com/WoolseyWorkshop) - A series of tutorials and some example projects from John Woolsey.\n* [Swift Development with Raspberry Pi](https://hackernoon.com/setting-up-a-swift-development-environment-on-raspberry-pi-c7af7fceac1e), [#2](https://medium.com/@piotr.gorzelany/experimental-swift-8c9131b62a9d) - A series of popular posts by Piotr Gorzelany.\n* [Accessing RaspberryPi GPIO pins with Swift](http://mistercameron.com/2016/06/accessing-raspberry-pi-gpio-pins-with-swift/) - A great step by step guide by Cameron Perry. \n\n\n## Usage\n\nCurrently, SwiftyGPIO expose GPIOs, SPIs(if not available a bit-banging VirtualSPI can be created), I2Cs, PWMs, 1-Wire and UART ports, let's see how to use them.\n\n### GPIO\n\nLet's suppose we are using a Raspberry 3 board and have a led connected between the GPIO pin P2 (possibly with a resistance of 1K Ohm or so in between) and GND and we want to turn it on.\n\nNote that SwiftyGPIO uses the *raw Broadcom numbering scheme* ([described here](https://github.com/uraimo/SwiftyGPIO/wiki/GPIO-Pinout)) to assign a number to each pin.\n\nFirst, we need to retrieve the list of GPIOs available on the board and get a reference to the one we want to modify:\n\n```swift\nimport SwiftyGPIO\n\nlet gpios = SwiftyGPIO.GPIOs(for:.RaspberryPi3)\nvar gp = gpios[.P2]!\n```\n\nThe following are the possible values for the predefined boards:\n    \n* .RaspberryPiRev1 (Pi A,B Revision 1, pre-2012, 26 pin header)\n* .RaspberryPiRev2 (Pi A,B Revision 2, post-2012, 26 pin header) \n* .RaspberryPiPlusZero (Raspberry Pi A+ and B+, Raspberry Zero/W, all with a 40 pin header)\n* .RaspberryPi2 (Raspberry Pi 2 with a 40 pin header)\n* .RaspberryPi3 (Raspberry Pi 3 with a 40 pin header)\n* .RaspberryPi4 (Raspberry Pi 4 with a 40 pin header)\n* .BeagleBoneBlack (BeagleBone Black)\n* .CHIP (the $9 C.H.I.P. computer).\n* .OrangePi\n* .OrangePiZero\n\nThe map returned by `GPIOs(for:)` contains all the GPIOs of a specific board as described by [these diagrams](https://github.com/uraimo/SwiftyGPIO/wiki/GPIO-Pinout). \n\nAlternatively, if our board is not supported, each single GPIO object can be instantiated manually, using its SysFS GPIO Id:\n\n```swift\nvar gp = GPIO(name: \"P2\",id: 2)  // User defined name and GPIO Id\n```\n    \nThe next step is configuring the port direction, that can be either `GPIODirection.IN` or `GPIODirection.OUT`, in this case we'll choose .OUT:\n\n```swift\ngp.direction = .OUT\n```\n\nThen we'll change the pin value to the HIGH value \"1\":\n\n```swift\ngp.value = 1\n```\n\nThat's it, the led will turn on.\n\nNow, suppose we have a switch or a button connected to P2 instead, to read the value coming in the P2 port, the direction must be configured as `.IN` and the value can be read from the `value` property:\n\n```swift\ngp.direction = .IN\nlet current = gp.value\n```\n\nSome boards like the RaspberryPi allow to enable a pull up/down resistance on some of the GPIO pins to connect a pin to 3.3V (.up), 0V (.down) or leave it floating (.neither) by default when external devices are disconnected, to enable it just set the `pull` property:\n\n```swift\ngp.direction = .IN\ngp.pull = .up\n```\n\nThe pull state can only be set and not read back.\n\nThe other properties available on the GPIO object (edge,active low) refer to the additional attributes of the GPIO that can be configured but you will not need them most of the times. For a detailed description refer to the [kernel sysfs documentation](https://www.kernel.org/doc/Documentation/gpio/sysfs.txt).\n\nThe GPIO object also supports the execution of closures when the value of the pin changes. Closures can be added with the methods `onRaising` (the pin value changed from 0 to 1), `onFalling` (the value changed from 1 to 0) and `onChange` (the value simply changed from the previous one):\n\n```swift\nlet gpios = SwiftyGPIO.GPIOs(for:.RaspberryPi3)\nvar gp = gpios[.P2]!\n\n\ngp.onRaising{\n    gpio in\n    print(\"Transition to 1, current value:\" + String(gpio.value))\n}\ngp.onFalling{\n    gpio in\n    print(\"Transition to 0, current value:\" + String(gpio.value))\n}\ngp.onChange{\n    gpio in\n    gpio.clearListeners()\n    print(\"The value changed, current value:\" + String(gpio.value))\n}  \n```\n\nThe closure receives as its only parameter a reference to the GPIO object that has been updated so that you don't need to use the external variable.\nCalling `clearListeners()` removes all the closures listening for changes and disables the changes handler.\nWhile GPIOs are checked for updates, the `direction` of the pin cannot be changed (and configured as `.IN`), but once the listeners have been cleared, either inside the closure or somewhere else, you are free to modify it.\n\nSetting the `bounceTime` property will enable software debounce, that will limit the number of transitions notified to the closure allowing only one event in the specified time interval in seconds.\n\nThe following example allows only one transition every 500ms:\n\n```swift\nlet gpios = SwiftyGPIO.GPIOs(for:.RaspberryPi3)\nvar gp = gpios[.P2]!\n\ngp.bounceTime = 0.5\ngp.onRaising{\n    gpio in\n    print(\"Transition to 1, current value:\" + String(gpio.value))\n} \n```\n \nThis functionality is extremely useful when using switches, that tend to generate multiple value spikes when the switch is pressed due to the mechanical characteristics of the compoment.\n\n### SPI\n\nIf your board has a SPI connection and SwiftyGPIO has it among its presets, a list of the available SPI channels can be obtained by calling `hardwareSPIs(for:)` with one of the predefined boards.\n\nOn RaspberryPi and other boards the hardware SPI SysFS interface is not enabled by default, check out the setup guide on [wiki](https://github.com/uraimo/SwiftyGPIO/wiki/Enabling-SPI-on-RaspberryPi-and-others) to enable it if needed using `raspi-config`.\n\nLet's see some examples using a RaspberryPi 3 that has two bidirectional SPIs, managed by SwiftyGPIO as two SPIObjects:\n \n```swift\nlet spis = SwiftyGPIO.hardwareSPIs(for:.RaspberryPi3)!\nvar spi = spis[0]\n```\n\nThe interface is composed by 3 wire: a clock line (SCLK), an input line (MISO) and an output line (MOSI). One or more CS pins (with inverse logic) are available to enable or disable slave devices.\n\nAlternatively, we can create a software SPI using four GPIOs, one that will serve as clock pin (SCLK), one as chip-select (CS or CE) and the other two will be used to send and receive the actual data (MOSI and MISO). This kind of bit-banging SPI is slower than the hardware one, so, the recommended approach is to use hardware SPIs when available.\n\nTo create a software SPI, just retrieve two pins and create a `VirtualSPI` object:\n```swift\nlet gpios = SwiftyGPIO.GPIOs(for:.RaspberryPi3)\nvar cs = gpios[.P27]!\nvar mosi = gpios[.P22]!\nvar miso = gpios[.P4]!\nvar clk = gpios[.P17]!\n\nvar spi = VirtualSPI(mosiGPIO: mosi, misoGPIO: miso, clockGPIO: clk, csGPIO: cs)\n```\n\nBoth objects implement the same `SPIObject` protocol and so provide the same methods.\nTo distinguish between hardware and software SPIObjects, use the `isHardware` property.\n\nTo send one or more byte over a SPI, use the `sendData` method.\nIn its simplest form it just needs an array of UInt8 as parameter:\n\n```swift\nspi?.sendData([UInt(42)], frequencyHz: 500_000)\n```\n\nThe frequency at which the data will be sent can be specified if needed (alternatively the default will be used, that is 500khz for hardware SPIs and the best available speed for virtual SPIs).\n\nSince the interface performs only full duplex transmissions, to read some data from the SPI you'll need to write the same amount of bits. For most devices you'll use this means that you'll need to send some dummy data depending on the protocol used by your device. Check the device reference for more information.\n\nLet's see a simple example, that reads 32 bytes from a device sending just 32 empty bytes:\n\n```swift\nlet data = [ UInt8 ](repeating: 0, count: 32)\nlet res  = spi?.sendDataAndRead(data)\n```\nThe `res` array will contain the raw data received from the device. Again, what to send and how the received data should be interpreted depends from the device or IC you are using, always read the reference manual.\n\n### I2C\n\nThe I2C interface can be used to communicate using the SMBus protocol on a I2C bus, reading or writing registers on devices identified by a numerical address. This interface needs just two wires (clock and data) and unlike SPI, it does not need a dedicated chip select/enable wire to select which device will receive the signal being sent, since the address of the destination of the protocol's messages is contained in the message itself, quite an improvement.\n\nTo obtain a reference to the `I2CInterface` object, call the `hardwareI2Cs(for:)` utility method of the SwiftyGPIO class:\n\n```swift\nlet i2cs = SwiftyGPIO.hardwareI2Cs(for:.RaspberryPi3)!\nlet i2c = i2cs[1]\n```\n\nOn Raspberry Pi and other boards this interface could not enabled by default, always verify its state checking the setup guide on the [wiki](https://github.com/uraimo/SwiftyGPIO/wiki/Enabling-I2C-on-the-Raspberry-Pi) to enable it if needed using `raspi-config`.\n\nThis object provide methods to read and write registers of different sizes and to verify that a device at a certain address is reachable or to enable a CRC on the protocol's messages:\n\n```swift\nfunc isReachable(_ address: Int) -> Bool\nfunc setPEC(_ address: Int, enabled: Bool)\n```\n\nYou should choose the read method to use depending on whatever of not your device supports multiple registers (`command` in SMBus parlance) and depending of the size of the register you are going to read from:\n\n```swift\nfunc readByte(_ address: Int) -> UInt8\nfunc readByte(_ address: Int, command: UInt8) -> UInt8\nfunc readWord(_ address: Int, command: UInt8) -> UInt16\nfunc readData(_ address: Int, command: UInt8) -> [UInt8]\nfunc readI2CData(_ address: Int, command: UInt8) -> [UInt8]\n```\n\nReading and writing data blocks supports two modes, a standard SMBus mode (`readData` and `writeData`) that prepends the length of the block before the actual data, and an old style I2C mode (`readI2CData` and `writeI2CData`) that just send the data without additional metadata. Depending on the device, only one of the two modes will be supported.\n\nLet's suppose that we want to read the seconds register (id 0) from a DS1307 RTC clock, that has an I2C address of 0x68:\n\n```swift\nprint(i2c.readByte(0x68, command: 0)) //Prints the value of the 8bit register\n```\n\nYou should choose the same way one of the write functions available, just note that `writeQuick` is used to perform quick commands and does not perform a normal write. SMBus's quick commands are usually used to turn on/off devices or perform similar tasks that don't require additional parameters.\n\n```swift\nfunc writeQuick(_ address: Int)\n\nfunc writeByte(_ address: Int, value: UInt8)\nfunc writeByte(_ address: Int, command: UInt8, value: UInt8)\nfunc writeWord(_ address: Int, command: UInt8, value: UInt16)\nfunc writeData(_ address: Int, command: UInt8, values: [UInt8])\nfunc writeI2CData(_ address: Int, command: UInt8, values: [UInt8])\n```\n\nWhile using the I2C functionality doesn't require additional software to function, the tools contained in `i2c-tools` are useful to perform I2C transactions manually to verify that everything is working correctly.\n\nFor example, I recommend to always check if your device has been connected correctly running `i2cdetect -y 1`. More information on I2C, and configuration instruction for the Raspberry Pi, are available [on Sparkfun](https://learn.sparkfun.com/tutorials/raspberry-pi-spi-and-i2c-tutorial).  \n\nThe `Example/` directory contains a Swift implementation of *i2cdetect* and could be a good place to start experimenting.\n\nThe `docs/` directory contains instead a simple guide to [debug communication issues with I2C devices](https://github.com/uraimo/SwiftyGPIO/blob/master/docs/i2c-debugging.md).\n\n### PWM\n\nPWM output signals can be used to drive servo motors, RGB leds and other devices, or more in general, to approximate analog output values (e.g. generate values as if they where *between* 0V and 3.3V) when you only have digital GPIO ports.\n\nIf your board has PWM ports and is supported (at the moment only RaspberryPi boards), retrieve the available `PWMOutput` objects with the `hardwarePWMs` factory method:\n\n```swift\nlet pwms = SwiftyGPIO.hardwarePWMs(for:.RaspberryPi3)!\nlet pwm = (pwms[0]?[.P18])!\n```\n\nThis method returns all the ports that support the PWM function, grouped by the PWM channel that controls them. \n\nYou'll be able to use only one port per channel and considering that the Raspberries have two channels, you'll be able to use two PWM outputs at the same time, for example GPIO12 and GPIO13 or GPIO18 and GPIO19.\n\nOnce you've retrieved the `PWMOutput` for the port you plan to use you need to initialize it to select the PWM function. On this kind of boards, each port can have more than one function (simple GPIO, SPI, PWM, etc...) and you can choose the function you want configuring dedicated registers.\n\n```swift\npwm.initPWM()\n```\n\nTo start the PWM signal call `startPWM` providing the period in nanoseconds (if you have the frequency convert it with 1/frequency) and the duty cycle as a percentage:\n\n```swift\nprint(\"PWM from GPIO18 with 500ns period and 50% duty cycle\")\npwm.startPWM(period: 500, duty: 50)\n```\n\nOnce you call this method, the PWM subsystem of the ARM SoC will start generating the signal, you don't need to do anything else and your program will continue to execute, you could insert a `sleep(seconds)` here if you just want to wait.\n\nAnd when you want to stop the PWM signal call the `stopPWM()` method:\n\n```swift\npwm.stopPWM()\n```\n\nIf you want to change the signal being generated, you don't need to stop the previous one, just call `startPWM` with different parameters.\n\nThis feature uses the M/S algorithm and has been tested with signals with a period in a range from 300ns to 200ms, generating a signal outside of this range could lead to excessive jitter that could not be acceptable for some applications. If you need to generate a signal near to the extremes of that range and have an oscilloscope at hand, always verify if the resulting signal is good enough for what you need.\n\n### Pattern-based signal generator via PWM\n\n<p>\n<img src=\"https://github.com/uraimo/SwiftyGPIO/raw/master/images/led1.gif\" />\n<img src=\"https://github.com/uraimo/SwiftyGPIO/raw/master/images/led2.gif\" />\n<img src=\"https://github.com/uraimo/SwiftyGPIO/raw/master/images/led3.gif\" />\n</p>\n  \nThis functionality leverages the PWM to generate digital signals based on two patterns representing a 0 or a 1 value through a variation of the duty cycle. Let's look at a practical example to better understand the use case and how to use this signal generator:\n\nLet's consider for example the WS2812/NeoPixel (see the dedicated [library](https://github.com/uraimo/WS281x.swift)), a led with integrated driver used in many led strips.\n\nThis led is activated with a signal between 400Khz and 800Khz containing a series of encoded 3 byte values representing respectively the *Green*,*Blue* and *Red* color components, one for each led. Each bit of the color component byte will have  to be encoded this way:\n\n* Bit value 0: _A 1250ns signal that stays, at least, high for 350ns(T0H) and then low for 900ns(T0L), with a tollerance of 150ns._\n* Bit value 1: _A 1250ns signal that stays,at least, high for 650ns(T1H) and then low for 600ns(T0L), with a tollerance of 150ns._\n\nAnd once the whole sequence of colors for your strip of leds has been sent, you'll need to keep the voltage at 0 for 50us, before you'll be able to transmit a new sequence. The bytes sent will configure the leds of the strip starting from the last one, going backwards to the first one.\n\nThis diagram from the official documentation gives you a better idea of what those signals look like, based on the T0H,T0L,T1H,T1L defined earlier:\n\n![ws2812 timings](https://github.com/uraimo/SwiftyGPIO/raw/master/images/ws2812.png)\n  \nYou could think to just send this signal based on those 0 and 1 pattern changing the values of a GPIO, but it's actually impossible for an ARM board to keep up with the rate required by devices like the WS2812 leds and trying to generate these signals in software introduces significant jitter too. \n\nOnce the period of the pattern is lower than 100us or so you need another way to send these signals. \n\nAnd this is the problem that the pattern-based signal generator solves, leveraging PWM-capable output pins.\n\nYou'll find a complete example under `Examples/PWMPattern`, but let's describe each one of the steps needed to use this feature. \n\nIn this brief guide I'm using an 8x8 led matrix with 64 WS2812 leds (these matrices are usually marketed as NeoPixel matrix, Nulsom Rainbow matrix, etc... and you can find one of these in some Pimoroni products like the UnicornHat).\n\nFirst of all let's retrieve a `PWMOutput` object and then initialize it:\n\n```swift\nlet pwms = SwiftyGPIO.hardwarePWMs(for:.RaspberryPi3)!\nlet pwm = (pwms[0]?[.P18])!\n\n// Initialize PWM\npwm.initPWM()\n```\n\nWe'll then configure the signal generator specifying the frequency we need (800KHz for a 1250ns pattern period), the number of leds in the sequence (I'm using and 8x8 led matrix here), and the duration of the reset time (55us). We'll call the `initPWMPattern` to configure these parameters. We specify the duty cycle (percentage of the period at which the pattern should have a high value) for the 0 and 1 values.\n\n```swift\nlet NUM_ELEMENTS = 64\nlet WS2812_FREQ = 800000 // 800Khz\nlet WS2812_RESETDELAY = 55  // 55us reset\n\npwm.initPWMPattern(bytes: NUM_ELEMENTS*3, \n                   at: WS2812_FREQ, \n                   with: WS2812_RESETDELAY, \n                   dutyzero: 33, dutyone: 66) \n```\n\nOnce this is done, we can start sending data, this time we are using a function that sets the colors and another function that turn them in a series of `UInt8` in the `GBR` format:\n\n```swift\nfunc toByteStream(_ values: [UInt32]) -> [UInt8]{\n    var byteStream = [UInt8]()\n    for led in values {\n        // Add as GRB, converted from RGB+0x00\n        byteStream.append(UInt8((led >> UInt32(16))  & 0xff))\n        byteStream.append(UInt8((led >> UInt32(24)) & 0xff))\n        byteStream.append(UInt8((led >> UInt32(8))  & 0xff))\n    }\n    return byteStream\n}\n\nvar initial = [UInt32](repeating:0x50000000, count:NUM_ELEMENTS)\nvar byteStream: [UInt8] = toByteStream(initial)\n\npwm.sendDataWithPattern(values: byteStream)\n```\n\nThe method `sendDataWithPatter` will use the sequence of `UInt8` to produce a signal composed by the patterns described above.\n\nWe can then wait until the signal is completely sent and then perform the necessary final cleanup:\n\n```swift\n// Wait for the transmission to end\npwm.waitOnSendData()\n\n// Clean up once you are done with the generator\npwm.cleanupPattern()\n```\n\nAt this point you could configure a different signal calling again `initPWMPattern` if you want to.\n\n### UART\n\nIf your board support the UART serial ports feature (disable the login on serial with `raspi-config` for RaspberryPi boards), you can retrieve the list of available `UARTInterface` with `SwiftyGPIO.UARTs(for:)`:\n\n```swift\nlet uarts = SwiftyGPIO.UARTs(for:.RaspberryPi3)!\nvar uart = uarts[0]\n```\n\nOn Raspberry Pi and other boards this interface could not enabled by default, always verify its state checking the setup guide on the [wiki](https://github.com/uraimo/SwiftyGPIO/wiki/UART-Setup) to enable it if needed using `raspi-config`.\n\nBefore we can start trasmitting data, you need to configure the serial port, specifying: the speed (from 9600bps to 115200bps), the character size (6,7 or 8 bits per character), the number of stop bits (1 or 2) and the parity of your signal (no parity, odd or even). Software and hardware flow control are both disabled when using this library.\n\n```swift\nuart.configureInterface(speed: .S9600, bitsPerChar: .Eight, stopBits: .One, parity: .None)\n```\n\nOnce the port is configured you can start reading or writing strings of sequence of `UInt8` with one of the specific methods of `UARTInterface`:\n\n```swift\nfunc readString() -> String\nfunc readData() -> [CChar]\nfunc writeString(_ value: String)\nfunc writeData(_ values: [CChar])\n\nfunc hasAvailableData() throws -> Bool\nfunc readLine() -> String\n```\n\nA method to know if there is available data on the UART serial port and a specific method that reads lines of text (`\\n` is used as line terminator, the serial read is still non-canonical) are also provided.\n\n### 1-Wire\n\nIf your board provides a 1-Wire port (right now only RaspberryPi boards), you can retrieve the list of available `OneWireInterface` with `SwiftyGPIO.hardware1Wires(for:)`:\n\n```swift\nlet onewires = SwiftyGPIO.hardware1Wires(for:.RaspberryPi3)!\nvar onewire = onewires[0]\n```\n\nTo retrieve the string identifiers associated to the devices connected to the 1-Wire bus, call `getSlaves()`.\n\nThe data provided by these devices can then be retrieved via `readData(slaveId:)` using one of the identifiers obtained at the previous step.\n\nThe data coming from the device is returned by the Linux driver as a series of lines, most of which will be just protocol data you should just ignore. Check out the reference of your sensor to know how to interpret the formatted information. \n \n\n## Examples\n\nExamples for different boards and functionalities are available in the *Examples* directory, you can just start from there modifying one of those.\n\nThe following example, built to run on the C.H.I.P. board, shows the current value of all the attributes of a single GPIO port, changes direction and value and then shows again a recap of the attributes:\n\n```Swift\nlet gpios = SwiftyGPIO.GPIOs(for:.CHIP)\nvar gp0 = gpios[.P0]!\nprint(\"Current Status\")\nprint(\"Direction: \"+gp0.direction.rawValue)\nprint(\"Edge: \"+gp0.edge.rawValue)\nprint(\"Active Low: \"+String(gp0.activeLow))\nprint(\"Value: \"+String(gp0.value))\n\ngp0.direction = .OUT\ngp0.value = 1\n\nprint(\"New Status\")\nprint(\"Direction: \"+gp0.direction.rawValue)\nprint(\"Edge: \"+gp0.edge.rawValue)\nprint(\"Active Low: \"+String(gp0.activeLow))\nprint(\"Value: \"+String(gp0.value))\n```\n\nThis second example makes a led blink with a frequency of 150ms:\n\n```Swift\nimport Glibc\n\nlet gpios = SwiftyGPIO.GPIOs(for:.CHIP)\nvar gp0 = gpios[.P0]!\ngp0.direction = .OUT\n\nrepeat{\n\tgp0.value = (gp0.value == 0) ? 1 : 0\n\tusleep(150*1000)\n}while(true) \n```\n\nWe can't test the hardware SPI with the CHIP but SwiftyGPIO also provide a bit banging software implementation of a SPI interface, you just need two GPIOs to initialize it:\n\n```Swift\nlet gpios = SwiftyGPIO.GPIOs(for:.CHIP)\nvar sclk = gpios[.P0]!\nvar dnmosi = gpios[.P1]!\n\nvar spi = VirtualSPI(dataGPIO:dnmosi,clockGPIO:sclk) \n\npi.sendData([UInt8(truncatingBitPattern:0x9F)]) \n```\n\nNotice that we are converting the 0x9F `Int` using the constructor `UInt8(truncatingBitPattern:)`, that in this case it's not actually needed, but it's recommended for every user-provided or calculated integer because Swift does not support implicit truncation for conversion to smaller integer types, it will just crash if the `Int` you are trying to convert does not fit in a `UInt8`.\n\n## Built with SwiftyGPIO\n\nA few projects and libraries built using SwiftyGPIO. Have you built something that you want to share? Let me know!\n\n### Libraries\n*Libraries for specific devices.*\n\n* [SwiftFlowMeter](https://github.com/samco182/SwiftFlowMeter) - A Swift library for using Hall effect based water flow sensors.\n* [SwiftyXBee](https://github.com/samco182/SwiftyXBee) - Library for the XBee module to communicate with Zigbee devices in API mode.\n* [SwiftyOLED](https://github.com/3Qax/SwiftyOLED) - Library for OLED displays based on SSD1306 and SSD1305.\n* [SHT20](https://github.com/samco182/SwiftySHT20) - Library for the I2C SHT20 Humidity and Temperature Sensor.\n* [LSM303](https://github.com/flx/LSM303) - Triple-axis Accelerometer+Magnetometer (Compass) I2C board library.\n* [PCA9685](https://github.com/Kaiede/PCA9685/tree/swiftyGpio) - 16-Channel 12-bit PWM/Servo Driver PCA9685 I2C board library.\n* [TM1637](https://github.com/AlwaysRightInstitute/SwiftyTM1637) - Library for the TM1637 7-segment driver chip.\n* [HC-SR04 Ultrasonic sensors](https://github.com/konifer44/HCSR04.swift) - Library for the HC-SR04 ultrasonic ranging sensor.\n* [HT16K33 Leds](https://github.com/jrahaim/swift-raspberry-pi-adafruit-led) - Project that uses the HT16K33 to drive led matrices and segment displays via I2C.\n* [WS281x Leds](https://github.com/uraimo/WS281x.swift) - Library for WS2812x (WS2811,WS2812,WS2812B) RGB led strips, rings, sticks, matrices, etc...\n* [Nokia5110(PCD8544) 128x64 LCD](http://github.com/uraimo/5110lcd_pcd8544.swift) - Show text and graphics on a Nokia 3110/5110 LCD display.\n* [HD44780U Character LCD](https://github.com/uraimo/HD44780CharacterLCD.swift) - Show text on character LCDs controlled by the HD44780 or one of its clones.\n* DHTxx Temperature Sensor [#1](https://github.com/micheltlutz/DHT-SwiftyGPIO), [#2](https://github.com/pj4533/dhtxx) - Read temperature and humidity values from sensors of the DHT family (DHT11, DHT22, AM2303).\n* [SG90 Servo Motor](https://github.com/uraimo/SG90Servo.swift) - Drives a SG90 servo motor via PWM but can be easily modified to use other kind of servos.\n* [MCP3008 10 bits ADC](https://github.com/uraimo/MCP3008.swift) - Convert analog values to integers with this SPI-driven ADC.\n* [u-Blox GPS Receivers](https://github.com/uraimo/UBloxGPS.swift) - Get location data from boards with the u-Blox 6/7/8 family of A-GPS receivers with an UART serial connection (e.g. NEO6M).\n* [MPU-6050 Accelerometer/Gyro](https://github.com/uraimo/MPU-6050.swift) - Library for the MPU-6050 (and MPU-6000 family) Accelerometer and Gyroscope.\n* [DS1307 RTC](https://github.com/uraimo/DS1307.swift) - Library for the DS1307 (DS1302, DS3231) I2C Real-Time Clock.\n* [Wii Nunchuck](https://github.com/uraimo/Nunchuck.swift) - Library for the Wii Nunchuck controller.\n* [RCWL-0516](https://github.com/uraimo/RCWL-0516-Radar.swift) - Library for the RCWL-0516 Microwave Radar.\n* [DS18B20](https://github.com/uraimo/DS18B20.swift) - Library for the DS18B20 temperature sensor.\n* [AMG88xx](https://github.com/emorydunn/AMG88xx.swift) - Library for AMG88 thermopile sensors.\n\n### Awesome Projects\n*Complete IoT projects.*\n\n* [Hey Siri, Open my garage door](https://pfandrade.me/blog/hey-siri-open-my-garage-door/) - A complete project to open a garage door via HomeKit.\n* [SwiftyLinkerKit](https://github.com/SwiftyLinkerKit) - Swift Modules to build LinkerKit Projects.\n* [Swifty Adafruit Servo HAT](https://github.com/ezrover/SwiftyServo) - Control the Adafruit Servo HAT with PCA9685 and I2C with Swift.\n* [Experimental Swift on the Raspberry Pi](https://medium.com/@piotr.gorzelany/experimental-swift-8c9131b62a9d) [(GH)](https://github.com/pgorzelany/experimental-swift-server) - Experimenting with Swift and a few different devices.\n* [Portable Wifi Monitor in Swift](http://saygoodnight.com/2016/04/05/portable-wifimon-raspberrypi.html) - A battery powered wifi signal monitor to map your wifi coverage.\n* [Temperature & Humidity Monitor in Swift](http://saygoodnight.com/2016/04/13/swift-temperature-raspberrypi.html) - A temperature monitor with a Raspberry Pi and an AM2302.\n* [Motion Detector with Swift and a Beaglebone Black](http://myroboticadventure.blogspot.it/2016/04/beaglebone-black-motion-detector-with.html) - A motion detector built with a BBB using a HC-SR502 sensor.\n* [DS18B20 Temperature Sensor with Swift](http://mistercameron.com/2016/06/accessing-raspberry-pi-gpio-pins-with-swift/) - Step by step project to read temperature values from a DS18B20 sensor.\n* [Swifty Buzz](https://github.com/DigitalTools/SwiftyBuzz) - Swifty tunes with a buzzer connected to a GPIO.\n* [Swift... Swift Everywhere](https://medium.com/@darthpelo/swift-swift-everywhere-eba445ef2bcd) - A tutorial that builds a complete platform, an iOS app controlling leds through a Vapor-based REST service.\n* [Bluetooth Smart Lock](https://github.com/colemancda/Lock) - A smart lock controller with companion iOS app that unlocks 12v solenoid locks via Bluetooth.\n \n### Support libraries\n*Additional libraries that could be useful for your IoT projects.*\n\n* [SwiftyGFX](https://github.com/3Qax/SwiftyGFX) - A library with generic graphical functions useful when working with dot matrix displays.\n* [PureSwift's Bluetooth](https://github.com/PureSwift/Bluetooth) - A suite of projects to add Bluetooth functionality to your  Linux projects.\n\n## Additional documentation\n\nAdditional documentation, mostly implementation details, can be found in the `docs` directory.\n",
    "readme_length": 36874
  },
  {
    "name": "pwmetrics",
    "full_name": "paulirish/pwmetrics",
    "description": "Progressive web metrics at your fingertipz",
    "stars": 1241,
    "forks": 71,
    "language": "TypeScript",
    "url": "https://github.com/paulirish/pwmetrics",
    "topics": [
      "lighthouse",
      "metrics",
      "performance",
      "webperf"
    ],
    "created_at": "2016-08-24T04:22:43Z",
    "updated_at": "2025-11-01T03:50:16Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "# Deprecated\n\nIn favour of better support and many cool features of:\n- [Lighthouse CI](https://github.com/GoogleChrome/lighthouse-ci) - is a suite of tools that make continuously running, saving, retrieving, and asserting against Lighthouse results as easy as possible.\n- [Lighthouse CI Action](https://github.com/treosh/lighthouse-ci-action) - action integrates Lighthouse CI with Github Actions environment. Making it simple to see failed tests, upload results, run jobs in parallel, store secrets, and interpolate env variables.\n- [Treo.sh](https://treo.sh) - Page speed monitoring made simple.\n\n<h1 align=\"center\">PWMetrics</h1>\n<p align=\"center\">\n  <img title=\"Progressive web metrics\" src='https://cloud.githubusercontent.com/assets/6231516/22849188/6f84c038-f003-11e6-8990-b14f3b916e54.png' />\n</p>\n<p align=\"center\">Progressive web metrics at your fingertipz. ðŸ’…</p>\n<p align=\"center\">CLI tool and lib to gather performance metrics via <a href=\"https://github.com/GoogleChrome/lighthouse/\">Lighthouse</a>.\n\n![image](https://cloud.githubusercontent.com/assets/39191/19417867/7aead922-93af-11e6-88ec-917dad6e89d2.png)\n\n Documentation on these metrics in the works. If you hit bugs in the metrics collection, report at [Lighthouse issues](https://github.com/GoogleChrome/lighthouse/issues).\n [How to use article](https://medium.com/@denar90/easy-progressive-web-metrics-9afa5ed857c2)\n\n### Install [![NPM pwmetrics package](https://img.shields.io/npm/v/pwmetrics.svg)](https://npmjs.org/package/pwmetrics)\n```sh\n$ yarn global add pwmetrics\n# or\n$ yarn add --dev pwmetrics\n```\n\n\n### CLI Usage\n\n```sh\n$ pwmetrics <url> <flags>\n\npwmetrics http://example.com/\n\n# --runs=n     Does n runs (eg. 3, 5), and reports the median run's numbers.\n#              Median run selected by run with the median TTI.\npwmetrics http://example.com/ --runs=3\n\n# --json       Reports json details to stdout.\npwmetrics http://example.com/ --json\n\n# returns...\n# {runs: [{\n#   \"timings\": [\n#     {\n#       \"name\": \"First Contentful Paint\",\n#       \"value\": 289.642\n#     },\n#     {\n#       \"name\": \"Largest Contentful Paint\",\n#       \"value\": 292\n#     },\n#     ...\n\n# --output-path       File path to save results.\npwmetrics http://example.com/ --output-path='pathToFile/file.json'\n\n# --config        Provide configuration (defaults to `package.json`). See _Defining config_ below.\npwmetrics --config=pwmetrics-config.js\n\n# --submit       Submit results to Google Sheets. See _Defining submit_ below.\npwmetrics --submit\n\n# --upload       Upload Lighthouse traces to Google Drive. See _Defining upload_ below.\npwmetrics --upload\n\n# --view       View Lighthouse traces, which were uploaded to Google Drive, in DevTools. See _Defining view_ below.\npwmetrics --view\n\n##\n## CLI options useful for CI\n##\n\n# --expectations  Assert metrics results against provides values. See _Defining expectations_ below.\npwmetrics --expectations\n\n# --fail-on-error  Exit PWMetrics with an error status code after the first unfilled expectation.\npwmetrics --fail-on-error\n\n\n```\n\n### Defining config\n\n```sh\n# run pwmetrics with config in package.json\npwmetrics --config\n```\n\n`package.json`\n```\n...\n  \"pwmetrics\": {\n    \"url\": \"http://example.com/\",\n    // other configuration options\n  }\n...\n```\n\n```sh\n# run pwmetrics with config in pwmetrics-config.js\npwmetrics --config=pwmetrics-config.js\n```\n\n`pwmetrics-config.js`\n\n```js\nmodule.exports = {\n  url: 'http://example.com/',\n   // other configuration options. Read _All available configuration options_\n}\n```\n\n### All available configuration options\n\n`pwmetrics-config.js`\n\n```js\nconst METRICS = require('pwmetrics/lib/metrics');\n\nmodule.exports = {\n  url: 'http://example.com/',\n  flags: { // AKA feature flags\n    runs: 3, // number or runs\n    submit: true, // turn on submitting to Google Sheets\n    upload: true, // turn on uploading to Google Drive\n    view: true, // open uploaded traces to Google Drive in DevTools\n    expectations: true, // turn on assertion metrics results against provides values\n    json: true, // not required, set to true if you want json output\n    outputPath: 'stdout', // not required, only needed if you have specified json output, can be \"stdout\" or a path\n    chromePath: '/Applications/Google\\ Chrome\\ Canary.app/Contents/MacOS/Google\\ Chrome\\ Canary', //optional path to specific Chrome location\n    chromeFlags: '', // custom flags to pass to Chrome. For a full list of flags, see http://peter.sh/experiments/chromium-command-line-switches/.\n    // Note: pwmetrics supports all flags from Lighthouse\n    showOutput: true, // not required, set to false for pwmetrics not output any console.log messages\n    failOnError: false // not required, set to true if you want to fail the process on expectations errors\n  },\n  expectations: {\n    // these expectations values are examples, for your cases set your own\n    // it's not required to use all metrics, you can use just a few of them\n    // Read _Available metrics_ where all keys are defined\n    [METRICS.TTFCP]: {\n      warn: '>=1500',\n      error: '>=2000'\n    },\n    [METRICS.TTLCP]: {\n      warn: '>=2000',\n      error: '>=3000'\n    },\n    [METRICS.TTI]: {\n      ...\n    },\n    [METRICS.TBT]: {\n      ...\n    },\n    [METRICS.SI]: {\n      ...\n    },\n  },\n  sheets: {\n    type: 'GOOGLE_SHEETS', // sheets service type. Available types: GOOGLE_SHEETS\n    options: {\n      spreadsheetId: 'sheet_id',\n      tableName: 'data',\n      uploadMedian: false // not required, set to true if you want to upload only the median run\n    }\n  },\n  clientSecret: {\n    // Data object. Can be get\n    // either\n    // by (using everything in step 1 here)[https://developers.google.com/sheets/api/quickstart/nodejs#step_1_turn_on_the_api_name]\n    //\n    // example format:\n    //\n    // installed: {\n    //   client_id: \"sample_client_id\",\n    //   project_id: \"sample_project_id\",\n    //   auth_uri: \"https://accounts.google.com/o/oauth2/auth\",\n    //   token_uri: \"https://accounts.google.com/o/oauth2/token\",\n    //   auth_provider_x509_cert_url: \"https://www.googleapis.com/oauth2/v1/certs\",\n    //   client_secret: \"sample_client_secret\",\n    //   redirect_uris: [\n    //     \"url\",\n    //     \"http://localhost\"\n    //   ]\n    // }\n    //\n    // or\n    // by (using everything in step 1 here)[https://developers.google.com/drive/v3/web/quickstart/nodejs]\n  }\n}\n```\n\n### Defining expectations\n\n> [Recipes](/recipes) for using with CI\n\n```sh\n# run pwmetrics with config in package.json\npwmetrics --expectations\n```\n\n`package.json`\n```\n...\n  \"pwmetrics\": {\n    \"url\": \"http://example.com/\",\n    \"expectations\": {\n      ...\n    }\n  }\n...\n```\n\n\n```sh\n# run pwmetrics with config in pwmetrics-config.js\npwmetrics --expectations --config=pwmetrics-config.js\n```\n\n### Defining submit\n\nSubmit results to Google Sheets\n\n*Instructions:*\n\n- Copy [this spreadsheet](https://docs.google.com/spreadsheets/d/17jgt_uKxm4WvROmKMfSDzhdCAstNvyaiDP_k2XqzgD0).\n- Copy the ID of the spreadsheet into the config as value of `sheets.options.spreadsheetId` property.\n- Setup Google Developer project and get credentials. ([everything in step 1 here](https://developers.google.com/sheets/api/quickstart/nodejs#step_1_turn_on_the_api_name))\n- Take a `client_secret` and put it into the config as value of `clientSecret` property.\n\n\n```sh\n# run pwmetrics with config in package.json\npwmetrics --submit\n```\n\n```sh\n# run pwmetrics with config in pwmetrics-config.js\npwmetrics --submit --config=pwmetrics-config.js\n```\n\n`pwmetrics-config.js`\n```js\nmodule.exports = {\n  'url': 'http://example.com/',\n  'sheets': {\n    ...\n  },\n  'clientSecret': {\n    ...\n  }\n}\n```\n\n\n### Defining upload\n\nUpload Lighthouse traces to Google Drive\n\n*Instructions:*\n\n- Setup Google Developer project and get credentials. ([everything in step 1 here](https://developers.google.com/drive/v3/web/quickstart/nodejs))\n- Take a `client_secret` and put it into the config as value of `clientSecret` property.\n\n\n```sh\n# run pwmetrics with config in package.json\npwmetrics --upload\n```\n\n```sh\n# run pwmetrics with config in pwmetrics-config.js\npwmetrics --upload --config=pwmetrics-config.js\n```\n\n`pwmetrics-config.js`\n```js\nmodule.exports = {\n  'url': 'http://example.com/',\n  'clientSecret': {\n    ...\n  }\n}\n```\n\n### View Lighthouse traces in timeline-viewer\n\nShow Lighthouse traces in timeline-viewer.\n\n> Required to use `upload` flag\n\n[timeline-viewer](https://chromedevtools.github.io/timeline-viewer/) - Shareable URLs for your Chrome DevTools Timeline traces.\n\n\n```sh\n# run pwmetrics with config in package.json\npwmetrics --upload --view\n```\n\n\n```sh\n# run pwmetrics with config in your-own-file.js\npwmetrics --upload --view --config=your-own-file.js\n```\n\n`pwmetrics-config.js`\n```js\nmodule.exports = {\n  'url': 'http://example.com/',\n  'clientSecret': {\n    ...\n  }\n}\n```\n\n#### Available metrics:\n\nAll metrics now are stored in separate constant object located in `pwmetrics/lib/metrics/metrics`;\n\n```js\n// lib/metrics/metrics.ts\n{\n  METRICS: {\n    TTFCP: 'firstContentfulPaint',\n    TTLCP: 'largestContentfulPaint',\n    TBT: 'totalBlockingTime',\n    TTI: 'interactive',\n    SI: 'speedIndex'\n  }\n}\n```\n\nRead article [Performance metrics. Whatâ€™s this all about?](https://medium.com/@denar90/performance-metrics-whats-this-all-about-1128461ad6b) which is decoding this metrics.\n\n### API\n\n```js\nconst PWMetrics = require('pwmetrics');\n\nconst options = {\n  flags: {\n    runs: 3, // number or runs\n    submit: true, // turn on submitting to Google Sheets\n    upload: true, // turn on uploading to Google Drive\n    view: true, // open uploaded traces to Google Drive in DevTools\n    expectations: true, // turn on assertation metrics results against provides values\n    chromeFlags: '--headless' // run in headless Chrome\n  }\n};\n\nconst pwMetrics = new PWMetrics('http://example.com/', options); // _All available configuration options_ can be used as `options`\npwMetrics.start(); // returns Promise\n\n```\n\n#### Options\n\n<table class=\"table\" width=\"100%\">\n  <thead>\n    <tr>\n      <th width=\"10%\">Option</th>\n      <th width=\"15%\">Type</th>\n      <th width=\"40%\">Default</th>\n      <th width=\"25%\">Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n    <td style=\"text-align: center;\">flags<sup><b>*</b></sup></td>\n      <td style=\"text-align: center;\">Object</td>\n      <td>\n        <pre>\n{\n  runs: 1,\n  submit: false,\n  upload: false,\n  view: false,\n  expectations: false,\n  disableCpuThrottling: false,\n  chromeFlags: ''\n}\n        </pre>\n      </td>\n      <td>Feature flags</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center;\">expectations</td>\n      <td style=\"text-align: center;\">Object</td>\n      <td style=\"text-align: center;\">{}</td>\n      <td>See <a href=\"#defining-expectations\">Defining expectations</a> above.</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center;\">sheets</td>\n      <td style=\"text-align: center;\">Object</td>\n      <td style=\"text-align: center;\">{}</td>\n      <td>See <a href=\"#defining-submit\">Defining submit</a> above.</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center;\">clientSecret</td>\n      <td style=\"text-align: center;\">Object</td>\n      <td style=\"text-align: center;\">{}</td>\n      <td>\n        Client secrete data generated by Google API console.\n        To setup Google Developer project and get credentials apply <a href=\"https://developers.google.com/drive/v3/web/quickstart/nodejs\">everything in step 1 here</a>.\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n<sup>*</sup>pwmetrics supports all flags from Lighthouse. See [here](https://github.com/GoogleChrome/lighthouse/#cli-options) for the complete list.\n\n\n### Recipes\n\n- [gulp](/recipes/gulp/gulpfile.js)\n\n\n### License\n\nApache 2.0. Google Inc.\n",
    "readme_length": 11701
  },
  {
    "name": "c-periphery",
    "full_name": "vsergeev/c-periphery",
    "description": "A C library for peripheral I/O (GPIO, LED, PWM, SPI, I2C, MMIO, Serial) in Linux.",
    "stars": 983,
    "forks": 285,
    "language": "C",
    "url": "https://github.com/vsergeev/c-periphery",
    "topics": [],
    "created_at": "2014-05-14T17:11:58Z",
    "updated_at": "2025-11-27T01:58:25Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# c-periphery [![Build Status](https://github.com/vsergeev/c-periphery/actions/workflows/build.yml/badge.svg)](https://github.com/vsergeev/c-periphery/actions/workflows/build.yml) [![GitHub release](https://img.shields.io/github/release/vsergeev/c-periphery.svg?maxAge=7200)](https://github.com/vsergeev/c-periphery) [![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/vsergeev/c-periphery/blob/master/LICENSE)\n\n## C Library for Linux Peripheral I/O (GPIO, LED, PWM, SPI, I2C, MMIO, Serial)\n\nc-periphery is a small C library for GPIO, LED, PWM, SPI, I2C, MMIO, and Serial peripheral I/O interface access in userspace Linux. c-periphery simplifies and consolidates the native Linux APIs to these interfaces. c-periphery is useful in embedded Linux environments (including Raspberry Pi, BeagleBone, etc. platforms) for interfacing with external peripherals. c-periphery is re-entrant, has no dependencies outside the standard C library and Linux, compiles into a static library for easy integration with other projects, and is MIT licensed.\n\nUsing Python or Lua? Check out the [python-periphery](https://github.com/vsergeev/python-periphery) and [lua-periphery](https://github.com/vsergeev/lua-periphery) projects.\n\nContributed libraries: [java-periphery](https://github.com/sgjava/java-periphery), [dart_periphery](https://github.com/pezi/dart_periphery)\n\n## Examples\n\n### GPIO\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n#include <stdbool.h>\n\n#include \"gpio.h\"\n\nint main(void) {\n    gpio_t *gpio_in, *gpio_out;\n    bool value;\n\n    gpio_in = gpio_new();\n    gpio_out = gpio_new();\n\n    /* Open GPIO /dev/gpiochip0 line 10 with input direction */\n    if (gpio_open(gpio_in, \"/dev/gpiochip0\", 10, GPIO_DIR_IN) < 0) {\n        fprintf(stderr, \"gpio_open(): %s\\n\", gpio_errmsg(gpio_in));\n        exit(1);\n    }\n\n    /* Open GPIO /dev/gpiochip0 line 12 with output direction */\n    if (gpio_open(gpio_out, \"/dev/gpiochip0\", 12, GPIO_DIR_OUT) < 0) {\n        fprintf(stderr, \"gpio_open(): %s\\n\", gpio_errmsg(gpio_out));\n        exit(1);\n    }\n\n    /* Read input GPIO into value */\n    if (gpio_read(gpio_in, &value) < 0) {\n        fprintf(stderr, \"gpio_read(): %s\\n\", gpio_errmsg(gpio_in));\n        exit(1);\n    }\n\n    /* Write output GPIO with !value */\n    if (gpio_write(gpio_out, !value) < 0) {\n        fprintf(stderr, \"gpio_write(): %s\\n\", gpio_errmsg(gpio_out));\n        exit(1);\n    }\n\n    gpio_close(gpio_in);\n    gpio_close(gpio_out);\n\n    gpio_free(gpio_in);\n    gpio_free(gpio_out);\n\n    return 0;\n}\n```\n\n[Go to GPIO documentation.](docs/gpio.md)\n\n### LED\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n#include <stdbool.h>\n\n#include \"led.h\"\n\nint main(void) {\n    led_t *led;\n    unsigned int max_brightness;\n\n    led = led_new();\n\n    /* Open LED led0 */\n    if (led_open(led, \"led0\") < 0) {\n        fprintf(stderr, \"led_open(): %s\\n\", led_errmsg(led));\n        exit(1);\n    }\n\n    /* Turn on LED (set max brightness) */\n    if (led_write(led, true) < 0) {\n        fprintf(stderr, \"led_write(): %s\\n\", led_errmsg(led));\n        exit(1);\n    }\n\n    /* Get max brightness */\n    if (led_get_max_brightness(led, &max_brightness) < 0) {\n        fprintf(stderr, \"led_get_max_brightness(): %s\\n\", led_errmsg(led));\n        exit(1);\n    }\n\n    /* Set half brightness */\n    if (led_set_brightness(led, max_brightness / 2) < 0) {\n        fprintf(stderr, \"led_set_brightness(): %s\\n\", led_errmsg(led));\n        exit(1);\n    }\n\n    led_close(led);\n\n    led_free(led);\n\n    return 0;\n}\n```\n\n[Go to LED documentation.](docs/led.md)\n\n### PWM\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n\n#include \"pwm.h\"\n\nint main(void) {\n    pwm_t *pwm;\n\n    pwm = pwm_new();\n\n    /* Open PWM chip 0, channel 10 */\n    if (pwm_open(pwm, 0, 10) < 0) {\n        fprintf(stderr, \"pwm_open(): %s\\n\", pwm_errmsg(pwm));\n        exit(1);\n    }\n\n    /* Set frequency to 1 kHz */\n    if (pwm_set_frequency(pwm, 1e3) < 0) {\n        fprintf(stderr, \"pwm_set_frequency(): %s\\n\", pwm_errmsg(pwm));\n        exit(1);\n    }\n\n    /* Set duty cycle to 75% */\n    if (pwm_set_duty_cycle(pwm, 0.75) < 0) {\n        fprintf(stderr, \"pwm_set_duty_cycle(): %s\\n\", pwm_errmsg(pwm));\n        exit(1);\n    }\n\n    /* Enable PWM */\n    if (pwm_enable(pwm) < 0) {\n        fprintf(stderr, \"pwm_enable(): %s\\n\", pwm_errmsg(pwm));\n        exit(1);\n    }\n\n    /* Change duty cycle to 50% */\n    if (pwm_set_duty_cycle(pwm, 0.50) < 0) {\n        fprintf(stderr, \"pwm_set_duty_cycle(): %s\\n\", pwm_errmsg(pwm));\n        exit(1);\n    }\n\n    pwm_close(pwm);\n\n    pwm_free(pwm);\n\n    return 0;\n}\n```\n\n[Go to PWM documentation.](docs/pwm.md)\n\n### SPI\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n#include <stdint.h>\n\n#include \"spi.h\"\n\nint main(void) {\n    spi_t *spi;\n    uint8_t buf[4] = { 0xaa, 0xbb, 0xcc, 0xdd };\n\n    spi = spi_new();\n\n    /* Open spidev1.0 with mode 0 and max speed 1MHz */\n    if (spi_open(spi, \"/dev/spidev1.0\", 0, 1000000) < 0) {\n        fprintf(stderr, \"spi_open(): %s\\n\", spi_errmsg(spi));\n        exit(1);\n    }\n\n    /* Shift out and in 4 bytes */\n    if (spi_transfer(spi, buf, buf, sizeof(buf)) < 0) {\n        fprintf(stderr, \"spi_transfer(): %s\\n\", spi_errmsg(spi));\n        exit(1);\n    }\n\n    printf(\"shifted in: 0x%02x 0x%02x 0x%02x 0x%02x\\n\", buf[0], buf[1], buf[2], buf[3]);\n\n    spi_close(spi);\n\n    spi_free(spi);\n\n    return 0;\n}\n```\n\n[Go to SPI documentation.](docs/spi.md)\n\n### I2C\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n#include <stdint.h>\n\n#include \"i2c.h\"\n\n#define EEPROM_I2C_ADDR 0x50\n\nint main(void) {\n    i2c_t *i2c;\n\n    i2c = i2c_new();\n\n    /* Open the i2c-0 bus */\n    if (i2c_open(i2c, \"/dev/i2c-0\") < 0) {\n        fprintf(stderr, \"i2c_open(): %s\\n\", i2c_errmsg(i2c));\n        exit(1);\n    }\n\n    /* Read byte at address 0x100 of EEPROM */\n    uint8_t msg_addr[2] = { 0x01, 0x00 };\n    uint8_t msg_data[1] = { 0xff, };\n    struct i2c_msg msgs[2] =\n        {\n            /* Write 16-bit address */\n            { .addr = EEPROM_I2C_ADDR, .flags = 0, .len = 2, .buf = msg_addr },\n            /* Read 8-bit data */\n            { .addr = EEPROM_I2C_ADDR, .flags = I2C_M_RD, .len = 1, .buf = msg_data},\n        };\n\n    /* Transfer a transaction with two I2C messages */\n    if (i2c_transfer(i2c, msgs, 2) < 0) {\n        fprintf(stderr, \"i2c_transfer(): %s\\n\", i2c_errmsg(i2c));\n        exit(1);\n    }\n\n    printf(\"0x%02x%02x: %02x\\n\", msg_addr[0], msg_addr[1], msg_data[0]);\n\n    i2c_close(i2c);\n\n    i2c_free(i2c);\n\n    return 0;\n}\n```\n\n[Go to I2C documentation.](docs/i2c.md)\n\n### MMIO\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n#include <stdint.h>\n#include <byteswap.h>\n\n#include \"mmio.h\"\n\nstruct am335x_rtcss_registers {\n    uint32_t seconds;       /* 0x00 */\n    uint32_t minutes;       /* 0x04 */\n    uint32_t hours;         /* 0x08 */\n    /* ... */\n};\n\nint main(void) {\n    mmio_t *mmio;\n    uint32_t mac_id0_lo, mac_id0_hi;\n    volatile struct am335x_rtcss_registers *regs;\n\n    mmio = mmio_new();\n\n    /* Open Control Module */\n    if (mmio_open(mmio, 0x44E10000, 0x1000) < 0) {\n        fprintf(stderr, \"mmio_open(): %s\\n\", mmio_errmsg(mmio));\n        exit(1);\n    }\n\n    /* Read lower 2 bytes of MAC address */\n    if (mmio_read32(mmio, 0x630, &mac_id0_lo) < 0) {\n        fprintf(stderr, \"mmio_read32(): %s\\n\", mmio_errmsg(mmio));\n        exit(1);\n    }\n\n    /* Read upper 4 bytes of MAC address */\n    if (mmio_read32(mmio, 0x634, &mac_id0_hi) < 0) {\n        fprintf(stderr, \"mmio_read32(): %s\\n\", mmio_errmsg(mmio));\n        exit(1);\n    }\n\n    printf(\"MAC address: %08X%04X\\n\", __bswap_32(mac_id0_hi), __bswap_16(mac_id0_lo));\n\n    mmio_close(mmio);\n\n    /* Open RTC subsystem */\n    if (mmio_open(mmio, 0x44E3E000, 0x1000) < 0) {\n        fprintf(stderr, \"mmio_open(): %s\\n\", mmio_errmsg(mmio));\n        exit(1);\n    }\n\n    regs = mmio_ptr(mmio);\n\n    /* Read current RTC time */\n    printf(\"hours: %02x minutes: %02x seconds %02x\\n\", regs->hours, regs->minutes, regs->seconds);\n\n    mmio_close(mmio);\n\n    mmio_free(mmio);\n\n    return 0;\n}\n```\n\n[Go to MMIO documentation.](docs/mmio.md)\n\n### Serial\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n#include \"serial.h\"\n\nint main(void) {\n    serial_t *serial;\n    uint8_t s[] = \"Hello World!\";\n    uint8_t buf[128];\n    int ret;\n\n    serial = serial_new();\n\n    /* Open /dev/ttyUSB0 with baudrate 115200, and defaults of 8N1, no flow control */\n    if (serial_open(serial, \"/dev/ttyUSB0\", 115200) < 0) {\n        fprintf(stderr, \"serial_open(): %s\\n\", serial_errmsg(serial));\n        exit(1);\n    }\n\n    /* Write to the serial port */\n    if (serial_write(serial, s, sizeof(s)) < 0) {\n        fprintf(stderr, \"serial_write(): %s\\n\", serial_errmsg(serial));\n        exit(1);\n    }\n\n    /* Read up to buf size or 2000ms timeout */\n    if ((ret = serial_read(serial, buf, sizeof(buf), 2000)) < 0) {\n        fprintf(stderr, \"serial_read(): %s\\n\", serial_errmsg(serial));\n        exit(1);\n    }\n\n    printf(\"read %d bytes: _%s_\\n\", ret, buf);\n\n    serial_close(serial);\n\n    serial_free(serial);\n\n    return 0;\n}\n```\n\n[Go to Serial documentation.](docs/serial.md)\n\n## Building c-periphery with CMake\n\n### Static library\n\nBuild c-periphery into a static library:\n\n``` console\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make\n```\n\n### Shared Library\n\nBuild c-periphery into a shared library:\n\n``` console\n$ mkdir build\n$ cd build\n$ cmake -DBUILD_SHARED_LIBS=ON ..\n$ make\n```\n\nInstall the shared library and headers:\n\n``` console\n$ sudo make install\n```\n\n### Tests\n\nBuild c-periphery tests from the build directory:\n\n``` console\n$ make tests\n```\n\n### Cross-compilation\n\nSet the `CC` environment variable with the cross-compiler prior to build:\n\n``` console\n$ export CC=arm-linux-gnueabihf-gcc\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make\n```\n\nIf additional cross-compiler tools are needed, use a `CMAKE_TOOLCHAIN_FILE` to fully specify the toolchain parameters:\n\n``` console\n$ mkdir build\n$ cd build\n$ cmake -DCMAKE_TOOLCHAIN_FILE=/path/to/arm-linux-gnueabihf.cmake ..\n$ make\n```\n\n## Building c-periphery with vanilla Make\n\n### Static library\n\nBuild c-periphery into a static library:\n\n``` console\n$ make\n```\n\n### Tests\n\nBuild c-periphery tests:\n\n``` console\n$ make tests\n```\n\n### Cross-compilation\n\nSet the `CROSS_COMPILE` environment variable with the cross-compiler prefix when building:\n\n``` console\n$ CROSS_COMPILE=arm-linux-gnueabihf- make\n```\n\n## Building c-periphery into another project statically\n\nInclude the header files from `src/` and link in the `periphery.a` static library:\n\n``` console\n$ gcc -I/path/to/periphery/src myprog.c /path/to/periphery/periphery.a -o myprog\n```\n\n## Building c-periphery into another project dynamically\n\nIf the header files and shared library are installed on the system, simply link with `-lperiphery`:\n\n``` console\n$ gcc myprog.c -lperiphery -o myprog\n```\n\nOtherwise, additional include (`-I`) and library (`-L`) paths may be required.\n\n## Building c-periphery into another project with CMake\n\nAdd to project's `CMakeLists.txt`:\n\n```cmake\nfind_package(periphery REQUIRED)\n# If package is installed locally, specify search path explicitly:\n# find_package(periphery REQUIRED PATHS <path to install/dir/lib/cmake>)\n\n...\n\nadd_executable(YOUR_TARGET src/myprog.c)\ntarget_link_libraries(YOUR_TARGET PRIVATE periphery::periphery)\n```\n\n## Documentation\n\n`man` page style documentation for each interface wrapper is available in [docs](docs/) folder.\n\n## Testing\n\nThe tests located in the [tests](tests/) folder may be run to test the correctness and functionality of c-periphery. Some tests require interactive probing (e.g. with an oscilloscope), the installation of a physical loopback, or the existence of a particular device on a bus. See the usage of each test for more details on the required test setup.\n\n## License\n\nc-periphery is MIT licensed. See the included [LICENSE](LICENSE) file.\n\n",
    "readme_length": 11820
  },
  {
    "name": "pigpio",
    "full_name": "fivdi/pigpio",
    "description": "Fast GPIO, PWM, servo control, state change notification and interrupt handling with Node.js on the Raspberry Pi",
    "stars": 964,
    "forks": 88,
    "language": "JavaScript",
    "url": "https://github.com/fivdi/pigpio",
    "topics": [
      "gpio",
      "hc-sr04",
      "interrupt",
      "iot",
      "javascript",
      "motor",
      "nodejs",
      "pwm",
      "raspberry-pi",
      "servo"
    ],
    "created_at": "2015-10-13T21:52:06Z",
    "updated_at": "2025-11-03T11:04:02Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "[![Build Status](https://app.travis-ci.com/fivdi/pigpio.svg?branch=master)](https://app.travis-ci.com/github/fivdi/pigpio)\n[![npm Version](http://img.shields.io/npm/v/pigpio.svg)](https://www.npmjs.com/package/pigpio)\n[![Downloads Per Month](http://img.shields.io/npm/dm/pigpio.svg)](https://www.npmjs.com/package/pigpio)\n[![Mentioned in Awesome Node.js](https://awesome.re/mentioned-badge.svg)](https://github.com/sindresorhus/awesome-nodejs)\n\n# pigpio\n\nA wrapper for the [pigpio C library](https://github.com/joan2937/pigpio) to\nenable fast GPIO, PWM, servo control, state change notification and interrupt\nhandling with **Node.js** on the Raspberry Pi Zero, 1, 2, 3 or 4.\n\npigpio supports Node.js versions 10, 12, 14, 15 and 16.\n\n## Contents\n\n * [Features](#features)\n * [Installation](#installation)\n * [Usage](#usage)\n   * [Pulse an LED with PWM](#pulse-an-led-with-pwm)\n   * [Buttons and Interrupt Handling](#buttons-and-interrupt-handling)\n   * [Servo Control](#servo-control)\n   * [Measure Distance with a HC-SR04 Ultrasonic Sensor](#measure-distance-with-a-hc-sr04-ultrasonic-sensor)\n   * [Determine the Width of a Pulse with Alerts](#determine-the-width-of-a-pulse-with-alerts)\n   * [Debounce a Button](#debounce-a-button)\n   * [Generate a waveform](#generate-a-waveform)\n   * [Sending a wavechain](#sending-a-wavechain)\n * [API Documentation](#api-documentation)\n * [Limitations](#limitations)\n * [Troubleshooting](#troubleshooting)\n * [Related Packages](#related-packages)\n\n## Features\n\n * Digital IO\n   * Up to 3.5 million digital reads per second <sup>*)</sup>\n   * Up to 2.5 million digital writes per second <sup>*)</sup>\n * PWM on any of GPIOs 0 through 31\n   * Multiple frequencies and duty cycle ranges supported\n * Servo control on any of GPIOs 0 through 31\n   * Jitter free\n * Alerts when any of GPIOs 0 through 31 change state\n   * The time of the state change is available accurate to a few microseconds\n * Notification streams for monitoring state changes on any of GPIOs 0 through 31 concurrently\n   * The time of the state changes are available accurate to a few microseconds\n * Low latency interrupt handlers\n   * Handle up to 20000 interrupts per second <sup>*)</sup>\n * Read or write up to 32 GPIOs as one operation with banked GPIO\n * Trigger pulse generation\n * Pull up/down resistor configuration\n * Waveforms to generate GPIO level changes (time accurate to a few Âµs)\n\n*) On a Raspberry Pi 4 Model B running Raspberry Pi OS 2021-03-04 (Buster\n10.8) with pigpio v3.3.1, Node.js v16.0.0 and V79 of the pigpio C library.\n\n## Installation\n\n#### Step 1 - Install the pigpio C library\n\nThe [pigpio C library](https://github.com/joan2937/pigpio) is a prerequisite\nfor the pigpio Node.js module.\n\nRun the following command to determine which version of the pigpio C library\nis installed:\n\n```\npigpiod -v\n```\n\nFor the Raspberry Pi Zero, 1, 2 and 3 V41 or higher of the pigpio C library is\nrequired. For the Raspberry Pi 4 V69 or higher is required.\n\nIf the pigpio C library is not installed or if the installed version is too\nold, the latest version can be installed with the following commands:\n\n```\nsudo apt-get update\nsudo apt-get install pigpio\n```\n\nAlternative installation instructions for the pigpio C library can be found\n[here](http://abyz.me.uk/rpi/pigpio/download.html).\n\n**Warning:** The pigpio C library contains a number of utilities. One of these\nutilities is pigpiod which launches the pigpio C library as a daemon. This\nutility should not be used as the pigpio Node.js package uses the C library\ndirectly.\n\n#### Step 2 - Install the pigpio Node.js package\n\n```\nnpm install pigpio\n```\n\n## Usage\n\nAssume there's an LED connected to GPIO17 (pin 11) and a momentary push button\nconnected to GPIO4 (pin 7).\n\n<img src=\"https://raw.githubusercontent.com/fivdi/pigpio/master/example/led-button.png\">\n\n#### Pulse an LED with PWM\n\nUse PWM to pulse the LED connected to GPIO17 from fully off to fully on\ncontinuously.\n\n```js\nconst Gpio = require('pigpio').Gpio;\n\nconst led = new Gpio(17, {mode: Gpio.OUTPUT});\n\nlet dutyCycle = 0;\n\nsetInterval(() => {\n  led.pwmWrite(dutyCycle);\n\n  dutyCycle += 5;\n  if (dutyCycle > 255) {\n    dutyCycle = 0;\n  }\n}, 20);\n```\n\n#### Buttons and Interrupt Handling\n\nTurn the LED connected to GPIO17 on when the momentary push button connected to\nGPIO4 is pressed. Turn the LED off when the button is released.\n\n```js\nconst Gpio = require('pigpio').Gpio;\n\nconst led = new Gpio(17, {mode: Gpio.OUTPUT});\nconst button = new Gpio(4, {\n  mode: Gpio.INPUT,\n  pullUpDown: Gpio.PUD_DOWN,\n  edge: Gpio.EITHER_EDGE\n});\n\nbutton.on('interrupt', (level) => {\n  led.digitalWrite(level);\n});\n```\n\n#### Servo Control\n\nContinuously move a servo connected to GPIO10 clockwise and anti-clockwise.\n\n<img src=\"https://raw.githubusercontent.com/fivdi/pigpio/master/example/servo.png\">\n\n```js\nconst Gpio = require('pigpio').Gpio;\n\nconst motor = new Gpio(10, {mode: Gpio.OUTPUT});\n\nlet pulseWidth = 1000;\nlet increment = 100;\n\nsetInterval(() => {\n  motor.servoWrite(pulseWidth);\n\n  pulseWidth += increment;\n  if (pulseWidth >= 2000) {\n    increment = -100;\n  } else if (pulseWidth <= 1000) {\n    increment = 100;\n  }\n}, 1000);\n```\n\n#### Measure Distance with a HC-SR04 Ultrasonic Sensor\n\nThe `trigger` function can be used to generate a pulse on a GPIO and alerts can\nbe used to determine the time of a GPIO state change accurate to a few\nmicroseconds. These two features can be combined to measure distance using a\nHC-SR04 ultrasonic sensor.\n\n<img src=\"https://raw.githubusercontent.com/fivdi/pigpio/master/example/distance-hc-sr04.png\">\n\n```js\nconst Gpio = require('pigpio').Gpio;\n\n// The number of microseconds it takes sound to travel 1cm at 20 degrees celcius\nconst MICROSECDONDS_PER_CM = 1e6/34321;\n\nconst trigger = new Gpio(23, {mode: Gpio.OUTPUT});\nconst echo = new Gpio(24, {mode: Gpio.INPUT, alert: true});\n\ntrigger.digitalWrite(0); // Make sure trigger is low\n\nconst watchHCSR04 = () => {\n  let startTick;\n\n  echo.on('alert', (level, tick) => {\n    if (level == 1) {\n      startTick = tick;\n    } else {\n      const endTick = tick;\n      const diff = (endTick >> 0) - (startTick >> 0); // Unsigned 32 bit arithmetic\n      console.log(diff / 2 / MICROSECDONDS_PER_CM);\n    }\n  });\n};\n\nwatchHCSR04();\n\n// Trigger a distance measurement once per second\nsetInterval(() => {\n  trigger.trigger(10, 1); // Set trigger high for 10 microseconds\n}, 1000);\n```\n\n#### Determine the Width of a Pulse with Alerts\n\nAlerts can be used to determine the time of a GPIO state change accurate to a\nfew microseconds. Typically, alerts will be used for GPIO inputs but they can\nalso be used for outputs. In this example, the `trigger` method is used to\npulse the LED connected to GPIO17 on for 15 microseconds once per second.\nAlerts are used to measure the length of the pulse.\n\n```js\n// Assumption: the LED is off when the program is started\n\nconst Gpio = require('pigpio').Gpio;\n\nconst led = new Gpio(17, {\n  mode: Gpio.OUTPUT,\n  alert: true\n});\n\nconst watchLed = () => {\n  let startTick;\n\n  // Use alerts to determine how long the LED was turned on\n  led.on('alert', (level, tick) => {\n    if (level == 1) {\n      startTick = tick;\n    } else {\n      const endTick = tick;\n      const diff = (endTick >> 0) - (startTick >> 0); // Unsigned 32 bit arithmetic\n      console.log(diff);\n    }\n  });\n};\n\nwatchLed();\n\n// Turn the LED on for 15 microseconds once per second\nsetInterval(() => {\n  led.trigger(15, 1);\n}, 1000);\n```\n\nHere's an example of the typical output to the console:\n\n```\n15\n15\n15\n15\n15\n15\n20\n15\n15\n15\n15\n```\n\n#### Debounce a Button\nThe GPIO glitch filter will prevent alert events from being emitted if the\ncorresponding level change is not stable for at least a specified number of\nmicroseconds. This can be used to filter out unwanted noise from an input\nsignal. In this example, a glitch filter is applied to filter out the contact\nbounce of a push button.\n\n![Button debounce circuit](example/button-debounce.png)\n\n```js\nconst Gpio = require('pigpio').Gpio;\n\nconst button = new Gpio(23, {\n  mode: Gpio.INPUT,\n  pullUpDown: Gpio.PUD_UP,\n  alert: true\n});\n\nlet count = 0;\n\n// Level must be stable for 10 ms before an alert event is emitted.\nbutton.glitchFilter(10000);\n\nbutton.on('alert', (level, tick) => {\n  if (level === 0) {\n    console.log(++count);\n  }\n});\n```\n\n#### Generate a waveform\n\nWaveforms can be used to time and execute Gpio level changes with an accuracy up to 1 microsecond. The following example generates a waveform that starts with a 1Âµs pulse, then has a 2Âµs pause, followed by a 3Âµs pulse and so on.\nThe waveform definition is a simple Array where each entry is an object with the properties gpioOn, gpioOff and usDelay.\n\nThe basic workflow to generate and execute waveforms is as follows:\n\nFirst, we usually clear previous wave entries with the `waveClear` method.\nThen we can add pulses with the `waveAddGeneric` method to the cleared waveform.\nWe then create a waveId by calling the `waveCreate` method.\nTo execute the waveform, we call the `waveTxSend` method.\nOnce the wave is sent, we can delete the wave by calling the `waveDelete` method.\n\n```js\nconst pigpio = require('pigpio');\nconst Gpio = pigpio.Gpio;\n\nconst outPin = 17;\n\nconst output = new Gpio(outPin, {mode: Gpio.OUTPUT});\n\noutput.digitalWrite(0);\npigpio.waveClear();\n\nlet waveform = [];\n\nfor (let x = 0; x < 20; x++) {\n  if (x % 2 === 1) {\n    waveform.push({ gpioOn: outPin, gpioOff: 0, usDelay: x + 1 });\n  } else {\n    waveform.push({ gpioOn: 0, gpioOff: outPin, usDelay: x + 1 });\n  }\n}\n\npigpio.waveAddGeneric(waveform);\n\nlet waveId = pigpio.waveCreate();\n\nif (waveId >= 0) {\n  pigpio.waveTxSend(waveId, pigpio.WAVE_MODE_ONE_SHOT);\n}\n\nwhile (pigpio.waveTxBusy()) {}\n\npigpio.waveDelete(waveId);\n```\n#### Sending a wavechain\n\nThe `waveChain` method allows you to chain multiple waveforms together.\nA chain is basically just an array with several waveId's. However you can insert different modifiers as described [here](https://github.com/fivdi/pigpio/blob/master/doc/global.md#wavechainchain).\n\nIn the example the `chain` consists of two waves. The first waveform is transmitted normally, then the second waveform is repeated 3 times.\n```js\nconst pigpio = require('pigpio');\nconst Gpio = pigpio.Gpio;\n\nconst outPin = 17;\nconst output = new Gpio(outPin, {mode: Gpio.OUTPUT});\n\noutput.digitalWrite(0);\npigpio.waveClear();\n\nlet firstWaveForm = [];\nlet secondWaveForm = [];\n\nfor (let x = 0; x < 10; x++) {\n  if (x % 2 === 0) {\n    firstWaveForm.push({ gpioOn: outPin, gpioOff: 0, usDelay: 10 });\n  } else {\n    firstWaveForm.push({ gpioOn: 0, gpioOff: outPin, usDelay: 10 });\n  }\n}\n\npigpio.waveAddGeneric(firstWaveForm);\nlet firstWaveId = pigpio.waveCreate();\n\nfor (let x = 0; x < 10; x++) {\n  if (x % 2 === 0) {\n    secondWaveForm.push({ gpioOn: outPin, gpioOff: 0, usDelay: 20 });\n  } else {\n    secondWaveForm.push({ gpioOn: 0, gpioOff: outPin, usDelay: 20 });\n  }\n}\n\npigpio.waveAddGeneric(secondWaveForm);\nlet secondWaveId = pigpio.waveCreate();\n\nif (firstWaveId >= 0 && secondWaveId >= 0) {\n  let chain = [firstWaveId, 255, 0, secondWaveId, 255, 1, 3, 0];\n  pigpio.waveChain(chain);\n}\n\nwhile (pigpio.waveTxBusy()) {}\n\npigpio.waveDelete(firstWaveId);\npigpio.waveDelete(secondWaveId);\n```\n\n## API Documentation\n\n### Classes\n\n- [Gpio](https://github.com/fivdi/pigpio/blob/master/doc/gpio.md) - General Purpose Input Output\n- [GpioBank](https://github.com/fivdi/pigpio/blob/master/doc/gpiobank.md) - Banked General Purpose Input Output\n- [Notifier](https://github.com/fivdi/pigpio/blob/master/doc/notifier.md) - Notification Stream\n\n### pigpio Module\n\n- [Global](https://github.com/fivdi/pigpio/blob/master/doc/global.md) - Module Globals\n\n### Configuring pigpio\n\n- [Configuration](https://github.com/fivdi/pigpio/blob/master/doc/configuration.md) - pigpio configuration\n\n## Limitations\n\n * The pigpio Node.js package is a wrapper for the\n   [pigpio C library](https://github.com/joan2937/pigpio). A limitation of the\n   pigpio C library is that it can only be used by a single running process.\n * The pigpio C library and therefore the pigpio Node.js package requires\n   root/sudo privileges to access hardware peripherals.\n   \n## Troubleshooting\nIf you have a problem with the library, before you remove it from your code and start trying something else, please check the [troubleshooting page](https://github.com/fivdi/pigpio/blob/master/doc/troubleshooting.md) first. Some problems are solvable and documented.\n\n## Related Packages\n\nHere are a few links to other hardware specific Node.js packages that may be of interest.\n\n- [onoff](https://github.com/fivdi/onoff) - GPIO access and interrupt detection\n- [i2c-bus](https://github.com/fivdi/i2c-bus) - I2C serial bus access\n- [spi-device](https://github.com/fivdi/spi-device) - SPI serial bus access\n- [mcp-spi-adc](https://github.com/fivdi/mcp-spi-adc) - Analog to digital conversion with the MCP3002/4/8, MCP3202/4/8 and MCP3304\n- [pigpio-dht](https://github.com/depuits/pigpio-dht) - Implements logic to read DHT11 or DHT22/AM2302 temperature and relative humidity sensor\n- [pigpio-mock](https://github.com/deepsyx/pigpio-mock) - A pigpio mock library for development on your local machine\n\n",
    "readme_length": 13179
  },
  {
    "name": "pwm",
    "full_name": "pwm-project/pwm",
    "description": "pwm",
    "stars": 957,
    "forks": 253,
    "language": "Java",
    "url": "https://github.com/pwm-project/pwm",
    "topics": [
      "ldap",
      "password",
      "self-service",
      "user-registration"
    ],
    "created_at": "2015-10-19T21:38:14Z",
    "updated_at": "2025-11-28T14:03:05Z",
    "homepage": null,
    "license": "Other",
    "readme": "# PWM\n\nPWM is an open source password self-service application for LDAP directories.\n\nOfficial project page is at [https://github.com/pwm-project/pwm/](https://github.com/pwm-project/pwm/).\n\nPWM is a Java Servlet based application, and is packaged as a Java executable single JAR file, traditional Servlet \"WAR\" file, and docker image. \n\n# Links\n* [PWM-General Google Group](https://groups.google.com/group/pwm-general) - please ask for assistance here first.\n* [PWM Documentation Wiki](https://github.com/pwm-project/pwm/wiki) - Home for PWM documentation\n* [PWM Reference](https://www.pwm-project.org/pwm/public/reference/) - Reference documentation built into PWM.\n* [Downloads](https://github.com/pwm-project/pwm/releases)\n\n# Features\n* Web based configuration manager with over 500 configurable settings\n  * All configuration contained in a single importable/exportable file\n  * Configurable display values for every user-facing text string\n* Included localizations (not all are complete or current):\n  * English - English\n  * Catalan - catalÃ \n  * Chinese (China) - ä¸­æ–‡ (ä¸­å›½)\n  * Chinese (Taiwan) - ä¸­æ–‡ (å°ç£)\n  * Czech - ÄeÅ¡tina\n  * Danish - dansk\n  * Dutch - Nederlands\n  * English (Canada) - English (Canada)\n  * Finnish - suomi\n  * French - franÃ§ais\n  * French (Canada) - franÃ§ais (Canada)\n  * German - Deutsch\n  * Greek - Î•Î»Î»Î·Î½Î¹ÎºÎ¬\n  * Hebrew - ×¢×‘×¨×™×ª\n  * Hungarian - magyar\n  * Italian - italiano\n  * Japanese - æ—¥æœ¬èªž\n  * Korean - í•œêµ­ì–´\n  * Norwegian - norsk\n  * Norwegian BokmÃ¥l - norsk bokmÃ¥l\n  * Norwegian Nynorsk - nynorsk\n  * Polish - polski\n  * Portuguese - portuguÃªs\n  * Portuguese (Brazil) - portuguÃªs (Brasil)\n  * Russian - Ñ€ÑƒÑÑÐºÐ¸Ð¹\n  * Slovak - slovenÄina\n  * Spanish - espaÃ±ol\n  * Swedish - svenska\n  * Thai - à¹„à¸—à¸¢\n  * Turkish - TÃ¼rkÃ§e\n* LDAP Directory Support:\n  * Multiple LDAP vendor support:\n    * Generic LDAP (best-effort, LDAP password behavior and error handling is not standardized in LDAP)\n    * Directory 389\n      * Reading of configured user password policies\n    * NetIQ eDirectory\n      * Read Password Policies & Challenge Sets\n      * NMAS Operations and Error handling\n      * Support for NMAS user challenge/responses\n    * Microsoft Active Directory\n      * Reading of Fine-Grained Password Policy (FGPP) Password Setting Objects (PSO) (does not read domain policies)\n    * OpenLDAP\n  * Native LDAP retry/failover support of multiple redundant LDAP servers\n* Large set of locally configurable password polices\n  * Standard syntax rules\n  * Regex rules\n  * Password dictionary enforcement\n  * Remote REST server checking\n  * AD-style syntax groups\n  * Shared password history to prevent passwords from being reused organizationally\n* Modules\n  * Change Password\n    * as-you-type password rule enforcement\n    * password strength feedback display\n  * Account Activation / First time password assignment\n  * Forgotten Password\n    * Store Responses in local server, standard RDBMS database, LDAP server or eDirectory NMAS repositories\n    * User verification options:\n      * Email/SMS Token/PIN\n      * TOTP\n      * Remote REST service\n      * OAuth service\n      * User LDAP attribute values\n  * New User Registration / Account Creation\n  * Guest User Registration / Updating\n  * PeopleSearch (white pages)\n    * Configurable detail pages\n    * OrgChart view\n  * Helpdesk password reset and intruder lockout clearing\n  * Administration modules including intruder-lockout manager\n    * online log viewer \n    * daily stats viewer and user information debugging\n    * statistics\n    * audit records\n* Multiple Deployment Options\n  * Java WAR file (bring your own application server, tested with Apache Tomcat)\n  * Java single JAR file (bring your own Java VM)\n  * Docker container\n* Theme-able interface with several example CSS themes\n  * Mobile devices specific CSS themes\n  * Configuration support for additional web assets (css, js, images, etc)\n  * Force display of organizational \n* Captcha support using Google reCaptcha\n* Multiple SSO options\n  * Basic Authentication \n  * HTTP header username injection\n  * Central Authentication Service (CAS)\n  * OAuth client\n* REST Server APIs for most functionality\n  * Password set\n  * Forgotten password\n  * Password policy reading\n  * User attribute updates\n  * Password policy verification\n* Outbound REST API for custom integrations during user activities such as change password, new user registration, etc.    \n\n## Requirements\n\nMinimum requirements for PWM application.\n\n| PWM Version | Java [^1] | Servlet | Tomcat [^2] |\n| --- | --- | --- | --- |\n| v2.1.x | 17+ | 3.0 | 9 |\n| v2.0.x | 11+ | 3.0 | 8-9 |\n| v1.9.x (EOL) | 8-11 | 3.0 | 7-9 |\n\n[^1] There is no requirement for a specific Java implementation, PWM builds use [Adoptium](https://adoptium.net/). \n\n[^2] Tomcat isn't an explicit requirement, but it is the most common container used with PWM, and\n the one that is used for the docker and onejar builds.\n\n\n\n## Deploy / Install\nPWM is distributed in the following artifacts, you can use whichever one is most convenient.\n\n| Artifact | Description |\n| --- | --- |\n| Java Executable | Command line executable Java JAR application, includes tomcat. |\n| WAR | Standard Java WAR (Web Archive) application deployment model, you need to have a working java & tomcat configuration on your server. |\n| Docker | Docker image includes Java and Tomcat. |\n\nFor all deployment types, each PWM instance will need an _applicationPath_ directory defined on your local server for PWM's configuration,\nlog, and runtime files.  Once PWM is configured, the initial web UI will prompt the administrator for LDAP and other configuration settings.  \n\n### Java Executable\nThe 'onejar' artifact released with PWM has an embedded tomcat instance, so you don't need to install tomcat to use this\nversion.  It's ideal for testing and evaluating PWM.  You will be responsible for getting it to run as a service (if desired).  \n\nRequirements:\n* Java 11 JDK or better\n\nHelp:\n* `java -version` to ensure you have java 11 or better available\n* `java -jar pwm-onejar-2.0.0.jar` for command line help\n\nExample for running onejar executable (with /pwm-applicationPath being the location to your _applicationPath_ directory):\n```\njava -jar pwm-onejar-2.0.0.jar -applicationPath /pwm-applicationPath \n```\nBy default, the executable will remain attached to the console and listen for HTTPS connections on port 8443.\n\n\n### WAR\n\nSteps:\n1) Get Apache tomcat working to the point you can access the tomcat landing page with your browser.  See tomcat documentation/help sites for\n   assistance with installing and configuring tomcat.\n2) Set the _PWM_APPLICATIONPATH_ environment variable in your tomcat instance to a local location of your _applicationPath_ directory. See tomcat and/or your\n   operating system documentation/help sites for assistance with configuring environment variables as the method for doing this depends on OS and deployment type.\n2) Place the pwm.war file in tomcat 'webapps' directory (rename from pwm-x.x.x.war with version naming)\n3) Access with /pwm url and configure\n\n\n### Docker\nThe PWM docker image includes Java and Tomcat.  It listens using https on port 8443, and has a volume exposed\nas `/config`.  You will need to map the `/config` volume to some type of persistent docker\nvolume for PWM to retain configuration.\n\nRequirements:\n* Server running docker\n\nSteps:\n\n1. Load your docker image with image nae of default _pwm/pwm-webapp_:\n```\ndocker load --input=pwm-docker-image-v2.0.0.tar\n```\n   \n1. Create docker image named _mypwm_, map to the server's 8443 port, and set the config volume to use the server's\nlocal file system _/home/user/pwm-config_ folder (this will be the PWM application path for the container): \n```\ndocker create --name mypwm -p '8443:8443' --mount 'type=bind,source=/home/user/pwm-config,destination=/config' pwm/pwm-webapp\n```\n\n1. Start the _mypwm_ container:\n```\ndocker start mypwm\n```\n\n## Configuration\n\nBefore configuring PWM you should use an LDAP browser/editor to ensure expected functionality of your LDAP environment. \nMost difficulties encountered configuring PWM are due to LDAP setup issues or unfamiliarity with LDAP. \nThere are many LDAP browsers available, a common one is [Apache Directrory Studio](https://directory.apache.org/studio/). \nUse the browser to navigate your LDAP environment, familiarize yourself with the directory structure, and verify expected behavior.\n\nIn particular, Active Directory LDAP can be problematic because it is often mis-configured and behaves in unusual ways compared to other LDAP directories.\nSpecifically, AD LDAP uses referrals to redirect the LDAP client (PWM in this case) to servers of its choosing, thus PWM must be able to contact all domain controller server instances in the AD environment using the AD-configured DNS name. \nAD LDAP must also be configured to use SSL certificates for password modifications to work.  However, if the AD environment is well configured, PWM will work fine with it.\n\nPWM includes a web-based configuration editor.\nWhen PWM starts with no configuration, a web-based configuration guide will prompt the administrator for basic configuration information.\nAll configuration information is stored in the _PwmConfiguration.xml_ file, which will be created in the application path directory.\nThe application path is also used for other files, including a local database (_LocalDB_) (used primarily as a cache or for test environments), log files, and temporary files.\nIf multiple PWM servers are used in parallel, each server must have identical _PwmConfiguration.xml_ files.\n\nPWM uses a configuration password to protect any modifications to the configuration.\nAuthentication to PWM requires an LDAP-backed login to a configured administrative account. \nIn early setup or in cases of problems with the LDAP directory, it may be necessary to access the configuration when LDAP functionally is not available.\nFor this purpose, PWM has a \"configuration-mode\" which allows editing the config with the configuration password, but disables all other end-user functionality.\nConfiguration mode can be enabled/disabled by editing the _PwmConfiguration.xml_ file and change the`configIsEditable`property near the top of the file, and can also be changed in the web UI.\n\n### Database Usage\n\nPWM can optionally be configured with an RDBMS (also known as a SQL database server).\nWhen configured to use a database, PWM user meta-data such as challenge/response answers, TOTP tokens, usage records, and other data will be stored in the database.\nWhen not configured to use a database, PWM user meta-data will be stored to the LDAP directory.  Neither is better or worse, which one you use depends on your enviornment.\n\nAny SQL server that has a Java supported JDBC driver should work, PWM will create its own schema on the first connection.\n\n## Build\n\nBuild pre-requisites:\n* Java ( check requirements above for version )\n* Git\n* The build uses maven, but you do not need to install it; the maven wrapper in the source tree will download a local version.\n\nBuild steps:\n1. Set _JAVA_HOME_ environment variable to JDK home.\n1. Clone the git project \n1. Change to pwm directory\n1. Run the maven build \n   \nLinux example: \n```\nexport JAVA_HOME=\"/home/vm/JavaJDKDirectory\"\ngit clone https://github.com/pwm-project/pwm\ncd pwm\n./mvnw clean verify\n```  \nWindows example:\n```\nset JAVA_HOME=\"c:\\JavaJDKDirectory\" \ngit clone https://github.com/pwm-project/pwm\ncd pwm\nmvnw.cmd clean verify\n```\nOn Windows we recommend using paths without spaces for both PWM and JDK directory.\n\nArtifacts created:\n\n| Format | Directory |\n| --- | --- |\n| WAR | webapp/target |\n| Executable | onejar/target |\n| Docker | docker/target |\n\n",
    "readme_length": 11679
  },
  {
    "name": "esphome-fan-controller",
    "full_name": "patrickcollins12/esphome-fan-controller",
    "description": "ESPHome Fan Controller",
    "stars": 619,
    "forks": 80,
    "language": null,
    "url": "https://github.com/patrickcollins12/esphome-fan-controller",
    "topics": [
      "esp32",
      "esphome",
      "home-assistant",
      "pid-control",
      "pid-controller",
      "pwm",
      "temperature"
    ],
    "created_at": "2022-01-30T11:33:53Z",
    "updated_at": "2025-11-28T18:17:33Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# ESPHome Fan Controller\r\n\r\nThis project describes how to build a whisper quiet thermostat-controlled fan for cooling your media console, gaming cupboard or that dying star in your networking cabinet.\r\n\r\nThe software is ESPHome and Home Assistant. The hardware is an ESP32 with a regular 12v 120mm Computer Fan (PWM) and a Temperature Sensor (DHT11).\r\n\r\n## Cost\r\nThe electronic parts are $29 USD including the ESP32. You will also need a multimeter.\r\n\r\n## Motivation\r\nMy sons's Playstation 5 sits in our TV Console which runs hotter than Sol. Also in that Media Console is a Macmini, a Raspberry Pi and a few other devices. My wife likes to keep the door neat and closed, so it needs some cooling!\r\n\r\nI used to have a thermostat that mindlessly flipped the fan on and off whenever the temperature crossed a threshold. Not great, the fan cycling was a mood killer on movie nights and dismally failed wife approval. Enter this smart thermostat: instead of brute-force toggling, it smoothly adjusts the 12V fan speed to maintain the perfect temperature. Itâ€™ll settle on just the right power level (say, 22% power) to keep things cool without the unnecessary drama.\r\n\r\n![\"closed cabinet\"](images/fortnite.jpg)\r\n\r\n## Features\r\nThe main features are:\r\n\r\n- the **fan dynamically adjusts** it's speed based on the temperature sensor using a Process Control mechanism called PID\r\n- **adjustable target temperature**. I currently target 30degC but maybe in winter I'll reduce it to 27.\r\n- uses ESP32's Wifi to connect to Home Assistant for control and reporting\r\n- the ESP32 is standalone and so the cooling function will continue to operate without Wifi. Doesn't need HomeAssistant or Wifi to operate. Wifi is only needed for setup, manual control and reporting.\r\n- **no screen** is needed on the device itself, all management is done via Home Assistant\r\n- my system uses two fans for extra cooling. Depending on how much air you need to draw through your enclosed space you could use 1, 2, 4, 10 .. n fans.\r\n- one esp32 can control up to 10 independent enclosures each with separate temperature sensors and fans. You're only limited by the Amps of your 12v Power Bricks and the 10 pwm pins on your ESP32.\r\n- **manual speed control** over ride if you don't want to use PID Control\r\n- **no coding is needed**. Just some configuration in YAML files. In fact this repo only contains 1 file ``config-fan.yaml``.\r\n- **No resistors, capacitors or difficult soldering needed**. The fan and the temperature sensor plug straight onto the pins of the ESP32. Although I did solder mount mine on a perfboard for cleanliness and put it in a case.\r\n\r\n![\"graphs\"](images/demo.jpg)\r\n\r\nThis is a screenshot from Home Assistant. I'll show you how to setup this dashboard.\r\n\r\n## Visuals\r\n![\"inside cabinet\"](images/inside.jpg)\r\n![\"controller\"](images/real1.jpg)\r\n![\"fans\"](images/real2.jpg)\r\n\r\n## Examples from other users\r\nhttps://github.com/patrickcollins12/esphome-fan-controller/issues/34\r\n\r\n\r\n## Parts (~$29 USD)\r\n\r\n- **DHT11** - temperature and humidity sensor. I'm using the one on a board with 3-pins. Cost $0.20 USD<br><img src=\"images/dht-11.png\" width=\"100\">\r\n\r\n- **12v PWM 4-pin Computer Fan** - I'm using 2 x [120mm Corsair fans](https://www.corsair.com/us/en/Categories/Products/Fans/Magnetic-Levitation-Fans/ml-config/p/CO-9050039-WW). Any 12v PWM-controllable fan should work. Cost $8-$15 USD. I recommend getting high quality fans if you care about noise and need to move a lot of air<br><img src=\"images/corsair-fan.png\" width=\"100\">. \r\n\r\n- **12v Power Adapter** - 1A or 2A should be fine depending on your fan's current draw. Cost $7 <br><img src=\"images/12v%20power%20supply.jpeg\" width=\"100\"> \r\n\r\n- **12v DC Female Jack** - with wire outlets. You can normally buy these with the Power Adapter<br><img src=\"images/12v%20jack.jpg\" width=\"100\"> or <img src=\"images/12v%20jack%202.png\" width=\"100\"> \r\n\r\n- **LM2596 Buck Converter** - to convert 12v down to 3.3v. Cost $0.21 each (normally in packs of 6-10)<br><img src=\"images/LM2596.png\" width=\"100\"> \r\n\r\n- **ESP32**. You can use any ESP32. I'm using a NodeMCU compatible board. Mine cost $1.50 USD from Aliexpress. Note it [will also work on a Raspberry Pi Pico W](https://github.com/patrickcollins12/esphome-fan-controller/discussions/50).<br><img src=\"images/nodemcu-esp32.png\" width=\"100\"> \r\n\r\n- **Jumper wires**. Some jumper wires to connect the ESP32 to the various parts here. $1. <br><img src=\"images/jumperwires.png\" width=\"100\">\r\n\r\nYou will also need a multimeter to adjust the output of the buck converter. Optionally, if you want to move this beyond a prototype you will need some soldering equipment and a 3d printer for a case or to buy a housing.\r\n\r\n## Choosing a Good Fan\r\nYou need a 4-pin fan which has PWM. 3-pin fans aren't acceptable, they are just on/off with tachometer sensor.\r\n\r\nAs you'll see below, our fans are being powered by the PWM pin. Our expectation is that the fans stop spinning at 0% power. Some people have reported that some fans don't stop running at 0% power (or worse that they stop completely at 100% power which is weird). \r\n\r\nIt appears that Corsair and Noctua fans behave as expected so you might want to stick with them.\r\n\r\nHowever, if your fan does behave this way, you can use a MOSFET to turn it off. There are ([instructions here for how to do this](https://github.com/patrickcollins12/esphome-fan-controller/issues/17#issuecomment-1557136383)). Please post your progress to that issue.\r\n\r\n## Wiring Diagram\r\n<img src=\"images/12v%20fan%20controller%20w%20tach.png\">\r\n\r\nSome important notes:\r\n- connect the fan PWM pin to a PWM GPIO\r\n- turn the knob on the buck converter with a screwdriver to make it output exactly 3.3v. You'll need a multimeter to measure that output.\r\n- ensure the 12v and 3.3v grounds are connected together.\r\n- the Blue line is the tachometer (\"Tach\") input pin. It is optional to connect this. You can connect this to a PWM input pin of your choice (GPIO25 in the example config) and it will send 1-2 pulses per full turn (depending on the fan). You can use this to monitor actual RPM of the fan and detect a fan defect, blocked rotor, etc. You will need one PWM input for each tach on each fan.\r\n- you could easily skip the Buck converter and use two separate power sources 3.3v and 12v. \r\n- the fritzing diagram shows a 4-pin DHT-11, when in fact I have the simpler 3-pin version as shown in the parts list. The 4-pin version might need a pullup resistor, haven't tried it.\r\n\r\n## Common Wiring Error - not joining grounds \r\n\r\nNOTE: if you don't join your 3.3v and 12v ground wires together your fan will keep spinning. At least 5 different builds have reported this issue. \r\n\r\n## Installing the software onto the ESP32\r\n\r\n### Get this repo\r\nClone this github repository.\r\nFrom the command line and then cd into the directory\r\n\r\n```\r\ngit clone https://github.com/patrickcollins12/esphome-fan-controller.git\r\ncd esphome-fan-controller\r\n```\r\n\r\n### Review the YAML and read the ESPHome docs.\r\n\r\nReview the YAML file.\r\n\r\nEnsure the pins are set correctly for the PWM Fan (ledc) and the DHT-11.\r\n\r\nReview the instructions for [the ESPHome Climate Thermostat](https://esphome.io/components/climate/index.html), [ESPHome PID Climate Thermostat](https://esphome.io/components/climate/pid.html) and the [DHT-11 sensor](https://esphome.io/components/sensor/dht.html).\r\n\r\nChange the device name from ``console-fan`` to whatever seems appropriate. You might want to change the yaml filename as well.\r\n\r\nIf you want to use a Raspberry Pi Pico instead of an ESP32, change the board and platform as well [according to this discussion](https://github.com/patrickcollins12/esphome-fan-controller/discussions/50).\r\n\r\n### Setup your temperature sensor\r\n\r\nSet the correct pin for your temp sensor. Note that the DHT11 sensor is setup to use an exponential moving average. Without this filter the PID controller reacts to every minor sensor movement. If you have a faster sensor like the BME260 you might need to tweak this filter.\r\n\r\n```yaml\r\n  # GET TEMP/HUMIDITY FROM DHT11\r\n  - platform: dht\r\n    pin: GPIO33\r\n    temperature:\r\n      name: \"Temperature\"\r\n      id: console_fan_temperature\r\n      accuracy_decimals: 3\r\n\r\n      # If you don't smooth the temperature readings \r\n      # the PID controller over reacts to small changes.\r\n      filters:\r\n         - exponential_moving_average:  \r\n             alpha: 0.1\r\n             send_every: 1\r\n\r\n```\r\n\r\n(Some people [take an average of two temperature sensors](https://github.com/patrickcollins12/esphome-fan-controller/issues/5).)\r\n\r\n### Setup your PWM fan\r\n\r\nMake sure you connect your fan to a PWM capable GPIO. All ESP32 pins that can act as outputs can be used as PWM pins but GPIOs 34-39 canâ€™t generate PWM.\r\n\r\nAlso note that my fans stop spinning below 13% power, so I set that as the minimum. I have a max power of 80% applied to the fans to make them wife-friendly. You might want to remove this minimum or maximum. \r\n\r\n```yaml\r\n  - platform: ledc\r\n    id: console_heat_speed\r\n    pin: GPIO27\r\n\r\n    # 25KHz is standard PWM PC fan frequency, minimises buzzing\r\n    frequency: \"25000 Hz\"\r\n\r\n    min_power: 13%\r\n    max_power: 80%\r\n\r\n```\r\n\r\n### Setup your wifi details\r\n\r\n``mv secrets-sample.yaml secrets.yaml``\r\n\r\nEdit your wifi credentials in secrets.yaml. The .gitignore will prevent you accidentally uploading your wifi credentials to github.\r\n\r\n### Install ESPHome\r\n\r\n[Install ESPHome](https://esphome.io/guides/installing_esphome.html) according to the instructions on the ESPHome website.\r\n\r\nI prefer command-line on Mac:\r\n``pip3 install esphome``\r\n\r\nMost people use the ESPHome that runs inside Home Assistant. You can use that too.\r\n\r\n### Install to ESP32\r\n\r\nConnect your ESP32 via USB to your computer, then upload the Firmware to the ESP32.\r\n\r\n`` esphome run console-fan.yaml``\r\n\r\nAt this time if you've set the pins right, the sensor should be spitting out values and the PID can control the fan.\r\n\r\nSuccess! \r\n\r\n```\r\n% esphome logs console-fan.yaml\r\nINFO Reading configuration console-fan.yaml...\r\nINFO Starting log output from console-fan.local using esphome API\r\nINFO Successfully connected to console-fan.local\r\n...\r\n[22:54:09][C][mdns:085]:   Hostname: console-fan\r\n[22:54:09][C][homeassistant.text_sensor:023]: Homeassistant Text Sensor 'ha_kp'\r\n[22:54:09][C][homeassistant.text_sensor:024]:   Entity ID: 'input_text.kp'\r\n[22:54:10][C][homeassistant.text_sensor:023]: Homeassistant Text Sensor 'ha_ki'\r\n[22:54:10][C][homeassistant.text_sensor:024]:   Entity ID: 'input_text.ki'\r\n[22:54:10][C][homeassistant.text_sensor:023]: Homeassistant Text Sensor 'ha_kd'\r\n[22:54:10][C][homeassistant.text_sensor:024]:   Entity ID: 'input_text.kd'\r\n[22:54:10][D][dht:048]: Got Temperature=30.0Â°C Humidity=38.0%\r\n[22:54:10][D][sensor:113]: 'Humidity': Sending state 38.00000 % with 0 decimals of accuracy\r\n[22:54:11][D][dht:048]: Got Temperature=30.0Â°C Humidity=38.0%\r\n[22:54:11][D][sensor:113]: 'Humidity': Sending state 38.00000 % with 0 decimals of accuracy\r\n[22:54:12][D][dht:048]: Got Temperature=30.0Â°C Humidity=38.0%\r\n[22:54:12][D][sensor:113]: 'Humidity': Sending state 38.00000 % with 0 decimals of accuracy\r\n```\r\n\r\n## Setup Home Assistant\r\n\r\nIf the above steps worked correctly, the device will be auto-discovered by Home Assistant. You will need to add the device.\r\n\r\nMultiple sensors and switches are exposed by the ESPHome software.\r\n\r\nYou also need to setup the dashboard. I'll explain those two steps below.\r\n\r\n## Setting up the Home Assistant Dashboard\r\n\r\n![dashboard](images/ha.jpg)\r\n\r\nHere is my full dashboard in Home Assistant.\r\n\r\nFor this full dashboard configuration, checkout ```lovelace-dashboard.yaml```\r\n\r\nLet's go through this page section-by-section.\r\n\r\n### The Primary Controls\r\n\r\n<img src=\"images/primary-controls.jpg\" width=400>\r\n \r\n```yaml\r\ntype: entities\r\nentities:\r\n  - entity: fan.manual_fan_speed\r\n  - entity: sensor.fan_speed_pwm_voltage\r\n```\r\n\r\n- You can turn on  `manual fan speed` and use this fan control to adjust the speed manually. \r\n\r\n- The ``Fan Speed (PWM Voltage)`` is the % of voltage being sent out via PWM to the fan controller. At 100% it will be sending 12v, at 50% it will be sending 6v.\r\n\r\nIf `manual fan speed` is off, this next card stack will conditionally display.\r\n\r\n```yaml\r\ntype: conditional\r\nconditions:\r\n  - condition: state\r\n    entity: fan.manual_fan_speed\r\n    state_not: 'on'\r\ncard:\r\n  type: vertical-stack\r\n  cards:\r\n    - type: entities\r\n      title: Thermostat Fan (PID)\r\n      entities:\r\n        - entity: climate.console_fan_thermostat\r\n        - entity: sensor.openweathermap_temperature\r\n          name: open weather\r\n        - entity: sensor.contact_sensor_1_device_temperature\r\n          name: room temperature\r\n    - type: vertical-stack\r\n      cards:\r\n        - type: glance\r\n          entities:\r\n            - entity: sensor.console_fan_is_in_deadband\r\n              name: in_deadband?\r\n            - entity: sensor.console_fan_error_value\r\n              name: error\r\n              icon: mdi:equal\r\n        - type: glance\r\n          show_icon: false\r\n          entities:\r\n            - entity: sensor.console_fan_output_value\r\n              name: output\r\n            - entity: sensor.console_fan_p_term\r\n              name: p_term\r\n            - entity: sensor.console_fan_i_term\r\n              name: i_term\r\n            - entity: sensor.console_fan_d_term\r\n              name: d_term\r\n```\r\n\r\n- The `Console Fan Thermostat` is a controllable thermostat, by clicking it you can alter the target temperature and turn the fan on/off. These changes will be persisted to flash on the ESP32.\r\n\r\n- The `Open Weather` and `Room` temperatures are from other sensors in my house for reference.\r\n\r\n- The ``kp, ki and kd`` inputs are exposed from the device. Your ESP32 will be automatically receiving changes to these values to control the behavior of the PID controller. While you could tune these from the config.yaml it requires a compile, upload and reboot cycle each time. This is inconvenient and best to tweak in real-time. We want to expose these 3 parameters to a Home Assistant dashboard.\r\n\r\n### The Graphs\r\n\r\nAdd the fan speed and the thermostat to two separate graphs. I've also added my room temperature from a separate device for comparison.\r\n\r\n<img src=\"images/graphs.jpg\" width=400>\r\n\r\n```yaml\r\ntype: vertical-stack\r\ntitle: 3 hr\r\ncards:\r\n  - type: history-graph\r\n    entities:\r\n      - entity: sensor.fan_speed_pwm_voltage\r\n    hours_to_show: 3\r\n    refresh_interval: 0\r\n  - type: history-graph\r\n    entities:\r\n      - entity: climate.console_fan_thermostat\r\n        name: ' '\r\n      - entity: sensor.contact_sensor_1_temperature\r\n        name: room\r\n    hours_to_show: 3\r\n    refresh_interval: 0\r\n```\r\n\r\n### Helpful Details - More Sensors and Switches\r\n\r\n<img src=\"images/details.jpg\" width=400>\r\n\r\n\r\nThis dashboard YAML exposes various sensors and switches from the ESP32.\r\n\r\n```yaml\r\ntype: entities\r\nentities:\r\n  - entity: sensor.console_fan_ip_address\r\n  - entity: sensor.console_fan_wifi_strength\r\n  - entity: sensor.console_fan_uptime\r\n  - entity: sensor.humidity\r\n    name: console fan humidity\r\n  - entity: switch.console_fan_autotune\r\n  - entity: switch.console_fan_esp32_restart\r\n```\r\n\r\n- ``console_fan_autotune`` is a button which starts the [PID tuning](https://esphome.io/components/climate/pid.html#autotuning) process. I ended up abandoning this approach and manually tuning the PID.\r\n\r\n- ``console_fan_esp32_restart`` restarts the ESP32 remotely.\r\n\r\n### PID Parameters - setting the PID parameters from the frontend\r\n\r\nThis dashboard allows you to configure the PID parameters. See the next section for how to set these parameters.\r\nThis dashboard will conditionally disappear if `manual fan speed` control is `on`.\r\n\r\n<img src=\"images/pid-controls.jpg\" width=400>\r\n\r\n```yaml\r\ntype: conditional\r\nconditions:\r\n  - condition: state\r\n    entity: fan.manual_fan_speed\r\n    state_not: 'on'\r\ncard:\r\n  type: vertical-stack\r\n  cards:\r\n    - type: entities\r\n      entities:\r\n        - entity: number.kp\r\n        - entity: number.ki\r\n        - entity: number.kd\r\n        - entity: button.pid_climate_autotune\r\n      title: PID Controls Setup\r\n    - type: entities\r\n      entities:\r\n        - entity: number.deadband_threshold_low\r\n          name: Threshold Low\r\n        - entity: number.deadband_threshold_high\r\n          name: Threshold High\r\n        - entity: number.deadband_ki_multiplier\r\n          name: ki multiplier\r\n      title: Deadband Parameters\r\n```\r\n\r\n## Tuning your fan - by configuring the PID Parameters\r\n\r\nThe thermostat is controlled using a standard Process Control system called a PID. \r\n\r\nIn our system the goal of the PID control is to set the fan voltage (speed) to bring the temperature measured by the sensor to a target temperature (30degC).\r\n\r\nSearch the internet and you'll find many resources for setting up a PID Controller. There are a lot of resources for fast response systems like cruise control systems but not many for dealing with slow response cooling systems like this one.\r\n\r\nThe [ESPHome PID Climate](https://esphome.io/components/climate/pid.html) system that we're using here has some resources to explain how to tune the parameters. However, its autotune system didn't spit out useful results for me and I will explain how to manually get the right parameters. Your system will be different to mine and so your parameters will need to be slightly different. For instance, your cabinet will be a different size, I have two fans, you may only have one, etc.\r\n\r\nThere are only two parameters you will need to adjust: the kp and ki parameters. The kd parameter is not useful for us.\r\n\r\nIn my system the goal of tuning was to minimise aggressive changes to the fan (my wife complained she could hear the fan turning on and off). I don't mind the system drifting up to 2degC away from the target temporarily while it slowly reacts. A 7% drift (2degC/30degC) on some control systems could blow up a Nuclear plant or cause a Cruise Control system to crash into another car. But in our system a 7% short temperature drift is a fine tradeoff for quiet fans.\r\n\r\n### Setting the kp (gain) parameter - how aggressively to cool?\r\n\r\nKP is the main Gain. How aggressively will the fan respond to a small change in temperarature? Do you want the fan to go to 100% power in response to a 0.1degC change in temperature? In general my goal was to have the fan at 50% in response to a 1degC change in temperature, thus could normally stave off any further temperature rises, but if it the temperature did keep rising the fan will then climb to 100% power.\r\n\r\nUsing lower gain (0.1) (my preferred setting):\r\n - takes longer to cool down\r\n - is more likely to under react to fast temperature changes.\r\n - 2deg sharp spikes can occur before the system reacts\r\n - It can takes up to 10 minutes to fully close a 0.5degC gap.\r\n - but, it is much less likely to oscillate around the target temperature and cause the fan to turn on and off constantly.\r\n\r\nUsing higher gain (1.0):\r\n - responds quickly to changes in temperature\r\n - but, is more likely to oscillate and make the fan swing from 0% to 100% and back again as it tries to control the temperature. This means you can hear the fan trying to adjust.\r\n\r\n### Setting the ki parameter - how long to adjust an offset?\r\n\r\n``ki: 0.0009``\r\n\r\nThe ki parameter adjusts for temperature offset. Try setting ki to 0. Set your system initially with kp=0.1, ki=0 and kd=0. You'll find that the system operates with a constant delta/offset to the target temperature. The ki parameter adjusts for this.\r\n\r\n1/ki is the seconds it should attempt to correct an offset. So 0.03 will adjust in 30seconds. 0.0009 will close a small temperature delta in 20 minutes. See a good description here https://blog.opticontrols.com/archives/344\r\n\r\nHigher numbers like 0.03 will respond much quicker, but it also will cause a lot of noise and oscillation in the fan speed.\r\n\r\n### Setting the kd parameter - predicting a change\r\n\r\nThe kd (D in PID) is meant to pre-react and backoff early. Small parameters can help overshoot but does create some fan noise and oscillation. The interwebs says that most (70%) of process controllers don't use the D and just a PI controller.\r\n\r\n### Setting the deadband parameters - minimising changes once inside the zone\r\n\r\nIn my first contribution to ESPHome I added [Deadband to PID Climate](https://esphome.io/components/climate/pid.html#deadband-setup). Follow the instructions there to ensure that your fans stop oscillating once it reachs the correct target temperature.\r\n\r\n### Tell me\r\nI'm keen to hear what PID parameters works for your fan.\r\n",
    "readme_length": 20673
  },
  {
    "name": "python-periphery",
    "full_name": "vsergeev/python-periphery",
    "description": "A pure Python 2/3 library for peripheral I/O (GPIO, LED, PWM, SPI, I2C, MMIO, Serial) in Linux.",
    "stars": 577,
    "forks": 145,
    "language": "Python",
    "url": "https://github.com/vsergeev/python-periphery",
    "topics": [],
    "created_at": "2015-06-15T09:41:35Z",
    "updated_at": "2025-11-28T22:55:55Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# python-periphery [![Tests Status](https://github.com/vsergeev/python-periphery/actions/workflows/tests.yml/badge.svg)](https://github.com/vsergeev/python-periphery/actions/workflows/tests.yml) [![Docs Status](https://readthedocs.org/projects/python-periphery/badge/)](https://python-periphery.readthedocs.io/en/latest/) [![GitHub release](https://img.shields.io/github/release/vsergeev/python-periphery.svg?maxAge=7200)](https://github.com/vsergeev/python-periphery) [![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/vsergeev/python-periphery/blob/master/LICENSE)\n\n## Linux Peripheral I/O (GPIO, LED, PWM, SPI, I2C, MMIO, Serial) with Python 2 & 3\n\npython-periphery is a pure Python library for GPIO, LED, PWM, SPI, I2C, MMIO, and Serial peripheral I/O interface access in userspace Linux. It is useful in embedded Linux environments (including Raspberry Pi, BeagleBone, etc. platforms) for interfacing with external peripherals. python-periphery is compatible with Python 2 and Python 3, is written in pure Python, and is MIT licensed.\n\nUsing Lua or C? Check out the [lua-periphery](https://github.com/vsergeev/lua-periphery) and [c-periphery](https://github.com/vsergeev/c-periphery) projects.\n\nContributed libraries: [java-periphery](https://github.com/sgjava/java-periphery), [dart_periphery](https://github.com/pezi/dart_periphery)\n\n## Installation\n\nWith pip:\n``` text\npip install python-periphery\n```\n\nWith easy_install:\n``` text\neasy_install python-periphery\n```\n\nWith setup.py:\n``` text\ngit clone https://github.com/vsergeev/python-periphery.git\ncd python-periphery\npython setup.py install\n```\n\n## Examples\n\n### GPIO\n\n``` python\nfrom periphery import GPIO\n\n# Open GPIO /dev/gpiochip0 line 10 with input direction\ngpio_in = GPIO(\"/dev/gpiochip0\", 10, \"in\")\n# Open GPIO /dev/gpiochip0 line 12 with output direction\ngpio_out = GPIO(\"/dev/gpiochip0\", 12, \"out\")\n\nvalue = gpio_in.read()\ngpio_out.write(not value)\n\ngpio_in.close()\ngpio_out.close()\n```\n\n[Go to GPIO documentation.](https://python-periphery.readthedocs.io/en/latest/gpio.html)\n\n### LED\n\n``` python\nfrom periphery import LED\n\n# Open LED \"led0\" with initial state off\nled0 = LED(\"led0\", False)\n# Open LED \"led1\" with initial state on\nled1 = LED(\"led1\", True)\n\nvalue = led0.read()\nled1.write(value)\n\n# Set custom brightness level\nled1.write(led1.max_brightness / 2)\n\nled0.close()\nled1.close()\n```\n\n[Go to LED documentation.](https://python-periphery.readthedocs.io/en/latest/led.html)\n\n### PWM\n\n``` python\nfrom periphery import PWM\n\n# Open PWM chip 0, channel 10\npwm = PWM(0, 10)\n\n# Set frequency to 1 kHz\npwm.frequency = 1e3\n# Set duty cycle to 75%\npwm.duty_cycle = 0.75\n\npwm.enable()\n\n# Change duty cycle to 50%\npwm.duty_cycle = 0.50\n\npwm.close()\n```\n\n[Go to PWM documentation.](https://python-periphery.readthedocs.io/en/latest/pwm.html)\n\n### SPI\n\n``` python\nfrom periphery import SPI\n\n# Open spidev1.0 with mode 0 and max speed 1MHz\nspi = SPI(\"/dev/spidev1.0\", 0, 1000000)\n\ndata_out = [0xaa, 0xbb, 0xcc, 0xdd]\ndata_in = spi.transfer(data_out)\n\nprint(\"shifted out [0x{:02x}, 0x{:02x}, 0x{:02x}, 0x{:02x}]\".format(*data_out))\nprint(\"shifted in  [0x{:02x}, 0x{:02x}, 0x{:02x}, 0x{:02x}]\".format(*data_in))\n\nspi.close()\n```\n\n[Go to SPI documentation.](https://python-periphery.readthedocs.io/en/latest/spi.html)\n\n### I2C\n\n``` python\nfrom periphery import I2C\n\n# Open i2c-0 controller\ni2c = I2C(\"/dev/i2c-0\")\n\n# Read byte at address 0x100 of EEPROM at 0x50\nmsgs = [I2C.Message([0x01, 0x00]), I2C.Message([0x00], read=True)]\ni2c.transfer(0x50, msgs)\nprint(\"0x100: 0x{:02x}\".format(msgs[1].data[0]))\n\ni2c.close()\n```\n\n[Go to I2C documentation.](https://python-periphery.readthedocs.io/en/latest/i2c.html)\n\n### MMIO\n\n``` python\nfrom periphery import MMIO\n\n# Open am335x real-time clock subsystem page\nrtc_mmio = MMIO(0x44E3E000, 0x1000)\n\n# Read current time\nrtc_secs = rtc_mmio.read32(0x00)\nrtc_mins = rtc_mmio.read32(0x04)\nrtc_hrs = rtc_mmio.read32(0x08)\n\nprint(\"hours: {:02x} minutes: {:02x} seconds: {:02x}\".format(rtc_hrs, rtc_mins, rtc_secs))\n\nrtc_mmio.close()\n\n# Open am335x control module page\nctrl_mmio = MMIO(0x44E10000, 0x1000)\n\n# Read MAC address\nmac_id0_lo = ctrl_mmio.read32(0x630)\nmac_id0_hi = ctrl_mmio.read32(0x634)\n\nprint(\"MAC address: {:04x}{:08x}\".format(mac_id0_lo, mac_id0_hi))\n\nctrl_mmio.close()\n```\n\n[Go to MMIO documentation.](https://python-periphery.readthedocs.io/en/latest/mmio.html)\n\n### Serial\n\n``` python\nfrom periphery import Serial\n\n# Open /dev/ttyUSB0 with baudrate 115200, and defaults of 8N1, no flow control\nserial = Serial(\"/dev/ttyUSB0\", 115200)\n\nserial.write(b\"Hello World!\")\n\n# Read up to 128 bytes with 500ms timeout\nbuf = serial.read(128, 0.5)\nprint(\"read {:d} bytes: _{:s}_\".format(len(buf), buf))\n\nserial.close()\n```\n\n[Go to Serial documentation.](https://python-periphery.readthedocs.io/en/latest/serial.html)\n\n## Documentation\n\nDocumentation is hosted at [https://python-periphery.readthedocs.io](https://python-periphery.readthedocs.io).\n\nTo build documentation locally with Sphinx, run:\n\n```\ncd docs\nmake html\n```\n\nSphinx will produce the HTML documentation in `docs/_build/html/`.\n\nRun `make help` to see other output targets (LaTeX, man, text, etc.).\n\n## Testing\n\nThe tests located in the [tests](tests/) folder may be run under Python to test the correctness and functionality of python-periphery. Some tests require interactive probing (e.g. with an oscilloscope), the installation of a physical loopback, or the existence of a particular device on a bus. See the usage of each test for more details on the required setup.\n\n## License\n\npython-periphery is MIT licensed. See the included [LICENSE](LICENSE) file.\n\n",
    "readme_length": 5661
  },
  {
    "name": "Adafruit-PWM-Servo-Driver-Library",
    "full_name": "adafruit/Adafruit-PWM-Servo-Driver-Library",
    "description": "Adafruit PWM Servo Driver Library",
    "stars": 516,
    "forks": 322,
    "language": "C++",
    "url": "https://github.com/adafruit/Adafruit-PWM-Servo-Driver-Library",
    "topics": [
      "arduino",
      "arduino-library",
      "library",
      "pca9685",
      "pwm",
      "pwm-driver",
      "servo",
      "servo-controller"
    ],
    "created_at": "2012-05-02T16:23:54Z",
    "updated_at": "2025-11-30T21:46:01Z",
    "homepage": null,
    "license": "Other",
    "readme": "# Adafruit PCA9685 PWM Servo Driver Library ![Build Status](https://github.com/adafruit/Adafruit-PWM-Servo-Driver-Library/workflows/Arduino%20Library%20CI/badge.svg)\n\n\nThis is a library for our Adafruit 16-channel PWM & Servo driver, shield or FeatherWing\n\n<a href=\"https://www.adafruit.com/products/815\"><img src=\"https://cdn-shop.adafruit.com/970x728/815-04.jpg\" height=\"300\"/></a>\n\nPick one up today in the adafruit shop!\n  * https://www.adafruit.com/products/815\n  * https://www.adafruit.com/product/1411\n  * https://www.adafruit.com/product/2928\n\nThese drivers use I2C to communicate, 2 pins are required to interface.\n\nAdafruit invests time and resources providing this open source code, please support Adafruit and open-source hardware by purchasing products from Adafruit!\n\nWritten by Limor Fried/Ladyada  for Adafruit Industries. BSD license, check license.txt for more information. \n\nAll text above must be included in any redistribution\n",
    "readme_length": 948
  },
  {
    "name": "rpidmx512",
    "full_name": "vanvught/rpidmx512",
    "description": "Orange Pi DMX512 / RDM / MIDI / OSC / Art-Net / WS28xx / L6470 / Stepper / TLC59711 / PCA9685 / Servo / PWM / TCNet / SMPTE / RDMNet / LLRP / GD32 / GigaDevice / Raspberry Pi",
    "stars": 431,
    "forks": 110,
    "language": "C++",
    "url": "https://github.com/vanvught/rpidmx512",
    "topics": [
      "allwinner",
      "artnet",
      "bare-metal",
      "dmx512",
      "e131",
      "esp8266",
      "gd32",
      "l6470",
      "mdns",
      "midi",
      "neopixel",
      "orangepi",
      "pixel-controller",
      "rdm",
      "rdm-controller",
      "sacn",
      "smpte",
      "tcnet",
      "tlc59711",
      "ws2812b"
    ],
    "created_at": "2013-12-28T18:27:20Z",
    "updated_at": "2025-11-28T06:29:00Z",
    "homepage": "http://www.orangepi-dmx.org/",
    "license": "MIT License",
    "readme": "![GitHub](https://img.shields.io/github/license/vanvught/rpidmx512)\n[![C++ Standard](https://img.shields.io/badge/C%2B%2B-20-blue.svg)](https://img.shields.io/badge/C%2B%2B-11%-blue.svg)\n![GitHub issues](https://img.shields.io/github/issues-raw/vanvught/rpidmx512)\n![GitHub contributors](https://img.shields.io/github/contributors/vanvught/rpidmx512)\n![GitHub Sponsors](https://img.shields.io/github/sponsors/vanvught)\n\n| Main  | Development |\n| ------------- | ------------- |\n|![Master](https://github.com/vanvught/rpidmx512/actions/workflows/c-cpp.yml/badge.svg?branch=master)![Master](https://github.com/vanvught/rpidmx512/actions/workflows/linux.yml/badge.svg?branch=master)|![Development](https://github.com/vanvught/rpidmx512/actions/workflows/c-cpp.yml/badge.svg?branch=development)![Development](https://github.com/vanvught/rpidmx512/actions/workflows/linux.yml/badge.svg?branch=development)|\n\n\n\n# Orange Pi baremetal Open Source\r\n## DMX512 / RDM / Art-Net 4 / sACN E1.31 / OSC / SMPTE / Pixel controller / RDMNet LLRP Only\r\n\r\n* **Ethernet**\n  * **Art-Net 4**\n      * Pixel controller **WS28xx/SK6812/APA102/UCSx903/P9813** with DMX [Orange Pi Zero]\n         * 1x 4 Universes [1x DMX] {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_artnet_pixel_dmx.zip?raw=true)}\n         * 8x 4 Universes [2x DMX] {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_artnet_pixel_dmx_multi.zip?raw=true)}\n      * DMX Input/Output Node / **RDM** Controller\n         *  1 Port {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_artnet_dmx.zip?raw=true)} {*Orange Pi Zero*}\n         *  2 Ports {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_artnet_dmx_multi.zip?raw=true)} {*Orange Pi Zero*}\n         *  4 Ports {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_artnet_dmx_multi.zip?raw=true)} {*Orange Pi One*}\n      * **Real-time Monitor** 1 Universe {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_artnet_monitor.zip?raw=true)} {*Orange Pi One - HDMI output*}\n     * Stepper controller **L6470 RDM**\n         * Sparkfun AutoDriver chaining {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_artnet_rdm_l6470.zip?raw=true)} {*Orange Pi Zero*}\n         * Roboteurs SlushEngine Model X LT {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_artnet_rdm_l6470.zip?raw=true)} {*Orange Pi One*}\n  * **sACN E1.31** \n      * Pixel Controller **WS28xx/SK6812/APA102/UCSx903/P9813** with DMX [Orange Pi Zero]\n         *  1x 4 Universes [1x DMX] {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_e131_pixel_dmx.zip?raw=true)}\n         *  8x 4 Universes [2x DMX] {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_e131_pixel_dxm_multi.zip?raw=true)}\n      * DMX Input / Output Bridge\n         *  2 Ports {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_e131_dmx_multi.zip?raw=true)} {*Orange Pi Zero*}\n         *  4 Ports {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_e131_dmx_multi.zip?raw=true)} {*Orange Pi One*}\n      * **Real-time Monitor** 1 Universe {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_e131_monitor.zip?raw=true)} {*Orange Pi One - HDMI output*}\n      * **Art-Net** converter 4/32 Universes {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_e131_artnet.zip?raw=true)} {*Orange Pi Zero*}\n  * **Distributed Display Protocol (DDP)**\n      *  Pixel Controller **WS28xx/SK6812/APA102/UCSx903/P9813** with DMX [Orange Pi Zero]\n         * 8x 680 RGB or 8x 512 RGBW [2x DMX Out] {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_ddp_pixel_dmx_multi.zip?raw=true)}\n  * **OSC** \n      * DMX Bridge / **Pixel Controller (WS28xx/SK6812/APA102/UCSx903/P9813)** {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_osc_dmx.zip?raw=true)}\n      * **Client** with support for buttons {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_osc_client.zip?raw=true)}\n      * **Real-time Monitor** 1 Universe {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_osc_monitor.zip?raw=true)} {*Orange Pi One - HDMI output*}\n  * **PixelPusher (PP)**\n      *  Pixel Controller **WS28xx/SK6812/APA102/UCSx903/P9813** [Orange Pi Zero]\n         * 8x 680 RGB or 8x 512 RGBW {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_pp_pixel_multi.zip?raw=true)}\n\n  \n  * **Showfile**\n      *  **Player** {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_showfile.zip?raw=true)}\r\n* **RDM** \r\n  * Controller with USB [Compatible with **Enttec USB Pro protocol**] {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_dmx_usb_pro.zip?raw=true)} {*Orange Pi Zero*}\r\n  * Responder / **DMX Pixel Controller (WS28xx/SK6812/APA102/UCSx903/P9813)** {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_rdm_responder.zip?raw=true)} {*Orange Pi Zero*}\r\n  * Stepper controller L6470\r\n     * Sparkfun AutoDriver chaining {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_rdm_responder_l6470.zip?raw=true)} {*Orange Pi Zero*}\r\n     * Roboteurs SlushEngine Model X LT {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_rdm_responder_l6470.zip?raw=true)} {*Orange Pi One*}\r\n* **DMX**\r\n  * **Real-time Monitor** {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_dmx_monitor.zip?raw=true)} {*Orange Pi One - HDMI output*}\r\n* **SMPTE LTC**\r\n  * **LTC SMPTE Timecode** Reader / Writer / Generator {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_emac_ltc_smpte.zip?raw=true)}  {*Orange Pi Zero*}\r\n* **MIDI**\r\n  *  **Real-time Monitor** {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_midi_monitor.zip?raw=true)}  {*Orange Pi One - HDMI output*}\r\n\r\nAll implementations are fully according to the standards.\r\n<br>\r\n\r\n* **Wifi**\r\n  * **Art-Net 3** DMX Node / RDM Controller / Pixel Controller (WS28xx/SK6812/APA102/UCSx903) {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_wifi_artnet_dmx.zip?raw=true)} {*Orange Pi Zero*}\r\n  * **sACN E1.31** DMX Bridge  / Pixel Controller (WS28xx/SK6812/APA102/UCSx903) {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_wifi_e131_dmx.zip?raw=true)} {*Orange Pi Zero*}\r\n  * **OSC** DMX Bridge / Pixel Controller (WS28xx/SK6812/APA102/UCSx903) {[zip](https://github.com/vanvught/h3dmx512-zip/blob/master/opi_wifi_osc_dmx.zip?raw=true)} {*Orange Pi Zero*}\r\n \r\n\r\nDetailed information can be found here : [http://www.orangepi-dmx.org](http://www.orangepi-dmx.org)\r\n\r\nImage's download [https://github.com/vanvught/h3dmx512-zip](https://github.com/vanvught/h3dmx512-zip \"https://github.com/vanvught/h3dmx512-zip\")\r\n\r\nU-Boot Orange Pi Zero: [uboot-orangpi_zero.img.zip](https://github.com/vanvught/h3dmx512-zip/blob/master/uboot-orangpi_zero.img.zip?raw=true)\r\n\r\nU-Boot Orange Pi One: [uboot-orangpi_one.img.zip](https://github.com/vanvught/h3dmx512-zip/blob/master/uboot-orangpi_one.img.zip?raw=true)\r\n\r\n> Special thanks to [@trebisky](https://github.com/trebisky/orangepi) (Thomas J. Trebisky), who helped me in understanding the H3 SoC.\r\n\r\n#  Mac OS X / Linux [debugging purpose only]\r\n## Art-Net 4 / sACN E1.31 / OSC / DDP / PP\r\n- **Art-Net 4** Real-time DMX Monitor\n- **DDP** Real-time DMX Monitor {Distributed Display Protocol}\r\n- sACN **E.131** Real-time DMX Monitor\r\n- **OSC** Real-time DMX Monitor\n- **PP** Real-time DMX Monitor {PixelPusher}\n\r\n# Raspberry Pi Baremetal Open Source\r\n## DMX512 / RDM / Art-Net 3 / sACN E1.31 / Pixel controller\r\n\r\nRaspberry Pi **Open Source** solutions:\r\n\r\n* **RDM**\r\n  * RDM Controller with USB [Compatible with **Enttec USB Pro protocol**] {[SDCard](https://github.com/vanvught/rpidmx512-zip/blob/master/rpi_dmx_usb_pro.zip?raw=true)}\r\n* **Wifi**\r\n  * **Art-Net 3** DMX Node / RDM Controller / DMX Real-time Monitor / Pixel Controller (WS28xx/SK6812/AP102/UCSx903) {[SDCard](https://github.com/vanvught/rpidmx512-zip/blob/master/rpi_wifi_artnet_dmx.zip?raw=true)}\r\n  * sACN **E1.31** DMX Bridge / DMX Real-time monitor / Pixel Controller (WS28xx/SK6812/AP102/UCSx903) {[SDCard](https://github.com/vanvught/rpidmx512-zip/blob/master/rpi_wifi_e131_dmx.zip?raw=true)}\r\n  * **OSC** DMX Bridge / DMX Real-time monitor / Pixel Controller (WS28xx/SK6812/AP102/UCSx903) {[SDCard](https://github.com/vanvught/rpidmx512-zip/blob/master/rpi_wifi_osc_dmx.zip?raw=true)}\r\n\r\n\r\nAll implementations are fully according to the standards. And successfully used in live lighting shows.\r\n\r\nThe kernel7.img is running on both Raspberry Pi 2 and Raspberry Pi 3(B+).\r\n\r\n> Special thanks to [@rsta2](https://github.com/rsta2/circle) (Rene Stange), who helped me get the multi-core support working. \r\n\r\n<br>\r\n\r\n[PayPal.Me Donate](https://paypal.me/AvanVught?locale.x=nl_NL)\r\n",
    "readme_length": 8858
  },
  {
    "name": "Rc_Engine_Sound_ESP32",
    "full_name": "TheDIYGuy999/Rc_Engine_Sound_ESP32",
    "description": "Allows to play vehicle engine sounds on an ESP32. Additional sounds can play in parallel with the engine sound! Controls your lights as well. compatible with SBUS, IBUS, PWM, PPM and SUMD signals.",
    "stars": 373,
    "forks": 143,
    "language": "C",
    "url": "https://github.com/TheDIYGuy999/Rc_Engine_Sound_ESP32",
    "topics": [
      "arduino",
      "engine-sound",
      "fire-truck",
      "i-bus",
      "ibus",
      "kenworth",
      "lights",
      "peterbilt",
      "ppm",
      "pwm",
      "rc",
      "sbus",
      "siren",
      "sumd"
    ],
    "created_at": "2019-12-03T15:51:13Z",
    "updated_at": "2025-11-21T21:44:15Z",
    "homepage": "https://www.youtube.com/watch?v=s93yAAmEtbM&t=3s",
    "license": "N/A",
    "readme": "# This is an Arduino RC engine sound & light controller for ESP32\n\n[![GitHub latest release version](https://img.shields.io/github/v/release/TheDIYGuy999/Rc_Engine_Sound_ESP32.svg?style=flat)](https://github.com/TheDIYGuy999/Rc_Engine_Sound_ESP32/releases/latest)\n[![Github All Releases download count](https://img.shields.io/github/downloads/TheDIYGuy999/Rc_Engine_Sound_ESP32/total.svg?style=flat)](https://github.com/TheDIYGuy999/Rc_Engine_Sound_ESP32/releases/latest)\n[![GitHub contributors](https://img.shields.io/github/contributors/TheDIYGuy999/Rc_Engine_Sound_ESP32.svg?style=flat)](https://github.com/TheDIYGuy999/Rc_Engine_Sound_ESP32/graphs/contributors)\n\nIt's based on the ATmega 328 version: https://github.com/TheDIYGuy999/Rc_Engine_Sound\nand on bitlunis Halloween example: https://github.com/bitluni/MotionPumpkin\n\nWiring and software installation instructions see further down.\nArduino IDE or Visual Studio Code (with Platform IO extension) are supported.\n\nVideo series: https://www.youtube.com/playlist?list=PLGO5EJJClJBCjIvu8frS7LrEU3H2Yz_so\n\n************************************************************************\nYou can find the changes in the [Changelog](documentation/Changelog.md).\n************************************************************************\n\nDiscussion and support thread (in German & English): https://www.rc-modellbau-portal.de/index.php?threads/esp32-arduino-rc-sound-und-licht-controller.7183/\n\nFully assembled, tested and working 30 pin SMD version with switch mode PSU & display connector.\nOrder it here: https://www.pcbway.com/project/shareproject/Arduino_RC_engine_sound_light_controller_for_ESP32_a9334731.html\n![](documentation/pictures/30pinSmdSwitchModeVersion.png)\n\nNew: STL data for nice 3D printed housing available:\nhttps://thediyguy999.github.io/TheDIYGuy999_ESP32_Web_Flasher/products.html\n![](documentation/pictures/ESP32SoundAndLightController.png)\n\nNew: Compact version for trailer or tractor:\nhttps://thediyguy999.github.io/TheDIYGuy999_ESP32_Web_Flasher/products.html\n![](documentation/pictures/ESP32TrailerController.png)\n\nNew: RZ7886 7A ESC:\nhttps://thediyguy999.github.io/TheDIYGuy999_ESP32_Web_Flasher/products.html\n![](documentation/pictures/RZ7886top.png)\n\nNew: wireless built-in configuration website 192.168.4.1\n![](documentation/pictures/Configuration.png)\n\nNew: LCD dashboard (original by Frevic)\n![](documentation/pictures/dashboard2.JPG)\nhttps://www.facebook.com/profile.php?id=100066616574355\n\nLCD dashboard (original by Gamadril)\n![](documentation/pictures/dashboard.JPG)\n\nFully assembled, tested and working 30 pin SMD version, manufactured and pre-assembled by https://www.pcbway.com\n![](documentation/pictures/30pinSmdVersion.jpg)\n\nFully assembled, tested and working 30 pin thru hole version\n![](documentation/pictures/30PinAssembled.jpg)\n\nCompact version for excavator (IBUS & sound only, supplied by 6V BEC)\n![](documentation/pictures/compact1.JPG)\n![](documentation/pictures/compact2.JPG)\n![](documentation/pictures/compact3.JPG)\n\n## Features:\n- Unique vehicle mass inertia simulation (connect your crawler type ESC to pin 33). Throttle output is altered during shifting of a mechanical 3 speed transmission for smooth shifting, gear protection and realistic sound. Works just fine with TAMIYA 3 speed transmissions. Should work as well with crawler 2 speed transmissions. The ESC is controlled by a state machine with the following states: driving forward & reverse (varible acceleration, depending on throttle position), neutral, braking forward & reverse (variable deceleration with fine granularity, according to \"reverse throttle\" position). It also allows to control the brake lights, the brake sound, the reversing light & the reversing beep sound properly. Acceleration & deceleration (coasting & braking) are adjustable separately for each gear to ensure maximum realism.\n- Unique \"virtual clutch\" allows to rev the engine below an adjustable ESC output speed. Above, the clutch engages and ensures, that the engine sound is in synch with the wheel RPM. Sounds and behaves just great in combination with a shifting transmission!\n- Simulated automatic transmission with torque converter (if your vehicle does not have a real shifting transmission)\n- Simulated double clutch transmission\n- simulated, manually shifted 3 speed transmission (new in v5.5)\n- Virtual, switchable neutral allows to rev the engine while standing still\n- Jake brake (simulated pneumatic engine brake, mainly used in US trucks)\n- Tracked mode (dual throttle input on CH2 & CH3, for tanks, diggers etc. No ESC control support in this mode. (New in v4.5)\n- Tank cannon sound & flash (New in v4.6)\n- Triggering multiple latching and non latching actions (sounds, lights) per analog channel, using the rcTrigger library (New in v4.7, still experimental)\n- Many selectable sounds: engine cranking, engine idling, engine revving, turbo whining, diesel ignition \"knock\", wastegate valve, horns, sirens, reversing beep, air brake, parking brake, gear shifting etc.\n- Realistic engine sound is mixed together on the fly from up to 4 sounds: engine idling, turbo, wastegate (all with variable sampling rate), Diesel ignition knock (fixed sampling rate, so it does not vary in pitch)\n- Load (throttle position) dependent volume sounds: idle, rev, Diesel knock\n- Engine RPM dependent volume sounds: turbo, wastegate\n- Dozens of engine & other sounds included, you can also compose your own, using Audacity and bitlunis conversion tool (link above)\n- Engine RPM range and inertia adjustable, volume of all sounds adjustable, engine sounds separatly for load and idling.\n- Many other paramerets can be adjusted. All adjustments are easily accessible in \"adjustmentsXyz.h\"\n- Sound files up to 22'050Hz, 8bit, mono can be used\n- Compatible input signals: PWM, PPM, SBUS (inverted & non inverted signals), IBUS\n- Works best with a PAM8403 amplifier module, connected to pin 25 & 26, via 10kOhm resistors & a 10kOhm potentiometer (see schematic below)\n- The engine RPM is calculated according to RC signal input on pin 13 *** CAUTION, 3.3V max. on all pins! *** 330 Ohm resistors on all I/O pins recommended!\n- Non linear throttle curves can be generated in \"curves.h\"\n- Light effects: headlights (high & low beam), tail lights, brake lights, fog lights, roof lights, cab lights, reversing light, indicators (turn signals), hazard lights, blue light etc. (max. 12 outputs)\n- Engine vibration simulation, using a shaker motor with excentric weight: Strong vibration while cranking, medium wlile idling, slight while revving\n- Adjustable volume (via remote)\n- Use an ESP32, CPU frequency must be set to 240MHz\n- Eagle schematic & board file included. Pre made Gerber files allow you to order your board easily.\n- included, easy to use .wav to .h sound file converter\n- Channels can easily be assigned, using \"remoteSetup.h\"\n- Pre made configuration profiles for Flysky FS-i6X and Arduino Mirco RC remote (new in v.5.5)\n- Variable length for horn & siren, using loop area in sound files (new in v5.6)\n- BUS decoder for steering servo and shifting servo (connect servos to CH1 & CH2) pins\n- Trailer coupler (5th wheel) servo can be connected to the CH4 pins (not in PWM communication mode)\n- TAMIYA trailer presence switch can be connected to pin 32 (depending on \"#define THIRD_BRAKELIGHT\" setting in \"6_adjustmentsLights.h\" tab)\n- Support for non linear throttle and steering curves (for more accurate control around center position). Use \"EXPONENTIAL_THROTTLE\" & \"EXPONENTIAL_STEERING\" in \"2_adjustmentsRemote.h\"\n- Support for HOBBYWING Quicrun Fusion Motor / ESC combo. Use \"#define QUICRUN_FUSION\" in \"3_adjustmentsESC.h\"\n- Support for winch, connected to CH3 (BUS communication mode only). Use \"#define MODE2_WINCH\" in \"7_adjustmentsServo.h\" The mode 2 button is then used to switch between horn / siren sontrol and winch control via CH4. The winch is controlled by an old RC servo driver board. The speed and neutral settings are done using \"CH3L\", CH3C\" and CH3R\" positions.\n- Support for LCD dashboard\n- Support for 2812 Neopixel LED (GPIO0)\n- Support for hydlaulic excavators (hydraulic pump, hydraulic flow, track rattling sounds). Use #define FLYSKY_FS_I6S_EXCAVATOR profile for remote\n- ESP-NOW based 2.4 GHz wireless trailer control support\n- An RZ7886 motor driver IC can be used instead of a standard crawler type RC ESC\n- Battery low discharge protection options\n- Switchable crawler mode (with just minimal virtual inertia)\n- Wireless configuration website 192.168.4.1\n\n## On the todo list:\n- cornering lights (on the beacon outputs)\n- Hazards switching on, if engine off in AUTO_LIGHTS mode\n\n## Known issues:\n- Arduino IDE 1.8.7 or older is not supported and will cause compiler errors!\n- The ESP32 does not work on macOS Big Sur 11.x, but this issue can be fixed easily as described here: [Big Sur Fix](BigSurFix.md) (for v1.04)\n- macOS Monterey 10.3 does not include Python 2 anymore. For Arduino IDE, you have to install Python 3 and to change the path in pPlatform.txt according to:\nhttps://forum.arduino.cc/t/mac-os-update-killed-esp32-sketch/969580/24\n\n## How to create new .h sound files:\n\n### Audacity:\n- import the WAV sound file you want in Audacity\n- convert it to mono, if needed\n- on the bottom left, select project frequency 22'050Hz\n- search for a cyclic pattern in the idle sound (the amount of ignition pulses is usually the same as the cylinder number), cut the \"idle\" sample to exactly this length, have a close look at the zero crossings to avoid clicking noises. The loudest peak should always be at the end of the sample.\n- do the same with the \"rev\" sound. It will be 2 - 4 times shorter than the \"idle\" sample, depending on the engine and rpm of the \"rev\" sample\n- change the \"Rate\" (dropdown on the left of the sample) of the \"rev\" sample, until the length is the same as in the \"idle\" sample. This is very important!\n- duplicate a part of the \"rev\" sample (the one with the original, unchanged \"Rate\" speed). This is the \"knock\" sample. Cut it to this max length: \"Idle\" length / number of cylinders / rpm range \"MAX_RPM_PERCENTAGE\" (usually 2 - 4 or 200 - 400%)\n- adjust the volume of all samples, so that the entire dynamic range is used\n- you may also want to apply high pass or low pass filters to fine tune the sound\n- select > export audio > selected audio > WAV > 8-bit-PCM\n\n### Convert the .wav file with the modified converting tool (new in v5.2):\n![](pictures/converter.png)\n- open the included \"Audio2Header.html\" converter in your browser. It is located in the \"tools\" folder\n- adjust the export file format (no changes required)\n- select the export file type, depending on the sound you are converting (idle, rev, horn etc.)\n- open the wav file you want to convert\n- a .h file is generated and downloaded\n- move it to your \"sketch/vehicles/sounds\" directory\n\n### Include the new header file with your sound in your vehicle preset, adjust settings until you are happy:\n- include this .h file in \"Adjustments.h\" > \"yourVehiclePreset.h\"\n- knock sound settings:\n  - \"dieselKnockInterval\" = number of cylinders\n  - uncomment \"V8\" for V8 engines, R6 for inline 6 engines or \"V2\" for V2 (Harley) engines\n  - adjust \"dieselKnockAdaptiveVolumePercentage\" (how loud the \"silent\" knock pulses are compared with the loud ones), only active, if defined \"V8\", \"R6\"  or \"V2\"\n- play with the other volumes, start-, end- and switch-points until you are happy\n- the \"rev\" sound is optional and only active, if \"REV_SOUND\" is defined (// removed)\n- adjust the transition from the \"idle\" to the \"rev\" sound, using \"revSwitchPoint\", \"idleEndPoint\", \"idleVolumeProportionPercentage\". This step is very important and can make a huge difference!\n\n### Compile the new sketch:\n- compile and upload the sketch in Arduino IDE\n- the new sound should now be ready\n\n## Schematic example (use PDF for current version!):\n![](documentation/pictures/schematic.png)\n\n## PCB\n### Included PCB files:\n- See hardware folder\n\n### Recommended manufacturer:\nhttps://www.pcbway.com (including SMD assembling service, use Gerbers.zip for board, it also includes BOM. xlsx and CPL.xlsx, if you want to use the SMT assembling service)\nHow to order pre assembled boards see /Eagle_PCB/How To Order Your PCB.pdf\n\n### The easiest way to order the 30pin SMD version with display connector:\nhttps://www.pcbway.com/project/shareproject/Arduino_RC_engine_sound_light_controller_for_ESP32_a9334731.html\n\n### Assembling tutorial (for the 30 pin SMD version without display connector):\nhttps://www.youtube.com/watch?v=csQgTfxRd8Y&t=2s\n\n### Assembling tutorial (for the 36 pin version):\nhttps://www.youtube.com/watch?v=Vfaz3CzecG4&list=PLGO5EJJClJBCjIvu8frS7LrEU3H2Yz_so&index=13\n\n## Wiring:\n### Before you begin:\n- This device is not protected against wrong polarity!\n- Always use series resistors for LED headers (except TAMIYA trailer header)\n- Maximum input voltage on \"Sig\" pins = 3.3V (be careful with very old receivers, which may deliver 5V)\n- It is recommended to use a fuse between your battery and the sound controller / ESC\n\n### Supply for audio amplifier, shaker motor and LED:\n- Use an Y-cable between your battery, your ESC and The \"X1\" terminal. Battery voltage range is 7 - 12.6V\n\n### Supply for ESP32:\n- The ESP32 is not supplied through the \"X1\" terminal\n- It can be supplied through the Micro USB header\n- or through the +V and GND pin row on the top of the board (the voltage is usually coming from the BEC in your ESC, which needs to be connected to the \"ESC\" header)\n\n### ESC wiring:\n- Connect a Hobbywing 1080 ESC to the ESC header (GND, V+ and Sig)\n- Adjust the ESC parameters, using the programming card as described on the top of \"Adjustments.h\"\n- I do not recommend any other ESC\n- The ESC is controlled by the cound controller, rather than directly by the receiver. This allows to use the unique \"virtual inertia\" feature. NOTE: Use this feature at your own risk! I'm not responsible, if any damage is caused. It's running very stable and I never had an issue, but you never know.\n- \"escPulseSpan\" can be used to limit the top speed of your vehicle. 500 = not limited, anything above 500 will limit the top speed\n\n### Receiver wiring for PWM servo signals (the most common wiring):\n- Channel assignment according to \"remoteSetup.h\" and remoteSetup.xlsx\", easily adjustable (new in v5.5). It is important to plug in the wires according to the channel assignment\n- CH5 & 6 are coming in via the \"35\" & PPM\" headers\n- At least one CH needs to be connected, using a 3 pin wire, so that GND and V+ are connected as well (receiver supply)\n- CH1 - 4 headers are pairs, wired in parallel. This allows to feed servo signals through, eliminating the need for Y-cables\n- Note that you need to change the configuration as described below, if you want to use this wiring method\n\n### Receiver wiring for PPM signals:\n- Internal channel assignment as above\n- Connect a 3 pin wire fom your receiver PPM header to the RX (changed in v5.5, was PPM) header on the sound controller (Sig, V+, GND)\n- Note that you need to change the configuration as described below, if you want to use this wiring method\n- 8 channels can be read in this mode\n\n### Receiver wiring for SBUS signals (recommended):\n- Internal channel assignment as above\n- Connect a 3 pin wire fom your receiver SBUS header to the SBUS header on the sound controller (Sig, V+, GND)\n- The \"Sig\" pin on the SBUS header is 5V tolerant\n- 13 channels can be read in this mode\n\n### Receiver wiring for IBUS signals:\n- Internal channel assignment as above\n- Connect a 3 pin wire fom your receiver IBUS header to the RX header on the sound controller (Sig, V+, GND)\n- 13 channels can be read in this mode\n\n### Speakers\n- 4 or 8 ohms speakers are compatible\n- You can use one or two speakers\n- never use two speakers in parallel on a single header\n- never use two headers in parallel to drive one speaker\n- never connect capacitors to the speaker headers\n\n### LED\n- The LED need to be wired \"common positive\". This means, the long LED legs are all connected together and connect to the 5V rail, coming from the on board regulator\n- All LED (except the ones, which are connected to the TAMIYA trailer connector) need a series resistor\n- Calculate the reqired resistor according to: http://ledcalc.com (supply voltage = 5V)\n- It is not recommended to wire LED in parallel, sharing the series resistor\n- Support for WS2812 Neopixel LED (details and wiring see \"6_adjustmentsLights.h\")\n\n### LCD dashboard\n- See \"9_adjustmentsDashboard.h\"\n\n### Shaker\n- The shaker is used for engine vibration simulation. The speed can be adjusted in the vehicle configuration and will vary depending on the engine state and rpm\n- It needs to be connected to the \"shaker\" header and is supplied by the on board 5V regulator. The negative side is switched by the on board mosfet\n- Please note, that the used mosfet needs to be a logic level type. Otherwise the shaker will not work!\n- The motor should not draw more than about 300mA @ 5V. I'm using a shaker motor from GT Power.\n\n### RZ7886 7A DC motor driver instead of an ESC:\nIt is intended for smaller scale vehicles like WPL\n\n#### Prototype:\n![](documentation/pictures/RZ7886.png)\n![](documentation/pictures/RZ7886.JPG)\n![](documentation/pictures/RZ7886_1.JPG)\n\n#### Order your motor driver PCB here:\nhttps://www.pcbway.com/project/shareproject/RZ7886_based_ESC_for_ESP32_Sound_and_Light_Controller_f8f4a805.html\n\n## Software:\n### Required software for code uploading and editing:\n- Arduino IDE (not recommnded): https://www.arduino.cc/en/Main/Software\n- or Visual Studio Code with PlatformIO extension (recommended): https://code.visualstudio.com\n- Visual Studio also requires this software (restart VS Code afterwards) in order to be able to synchronise libraries: https://git-scm.com/download\n\n### Downloading and preparing the code with Arduino IDE:\n- Download the code from here (hit \"Code > Download zip\"): https://github.com/TheDIYGuy999/Rc_Engine_Sound_ESP32\n- Unzip the folder, if required\n- Remove the \"-master\" part of the folder name\n- Install libraries and board definitions below first, restart Arduino IDE\n- Open \"scr.ino\" with a double click (Arduino IDE should start)\n\n### Required ESP32 board definition (not required, if Visual Studio Code is used as IDE):\n- Install it according to: https://randomnerdtutorials.com/installing-the-esp32-board-in-arduino-ide-windows-instructions/\n- Use v1.04, v1.05 or newer are causing reboot issues!\n- Adjust settings according to:\n![](documentation/pictures/settings.png)\n\n### Required libraries. You need to install ALL of them, if using Arduino IDE. Not required, if Visual Studio Code is used as IDE:\n- statusLED: https://github.com/TheDIYGuy999/statusLED\n- SBUS: https://github.com/TheDIYGuy999/SBUS\n- rcTrigger: https://github.com/TheDIYGuy999/rcTrigger\n- IBUS: https://github.com/bmellink/IBusBM\n- TFT: https://github.com/Bodmer/TFT_eSPI\n- FastLED: https://github.com/FastLED/FastLED    \n\nDownload them in the same manner as the main code above. Store the folders in your \"Arduino/libraries\" path.\nInstall them according to: https://www.arduino.cc/en/Guide/Libraries\n\n### Uploading the code to the board:\n- IMPORTANT: Depending on your board, you may have to press and hold the \"BOOT\" button, if the IDE just shows \"Connecting........_____....\" Release it, as soon as the upload starts.\n- On macOS Big Sur, you need to apply the following fix (Arduino IDE only):  [Big Sur Fix](BigSurFix.md)\n\n### Visual Studio Code (instead of Arduino IDE) overiew:\n![](documentation/pictures/VScode.png)\n\n## Adjusting things in \"X_Xyz.h\":\n### Vehicle selection:\n\nNote, that in v5.5 the former \"Adjustments.h\" configuration file was divided into multiple files.\n\nUncomment (remove //) the vehicle you want in \"Adjustments.h\". Never uncomment more than one vehicle!\nNote, that you need to re-upload the code after you changed the settings. Otherwise it will not take effect.\nSelect >Sketch > Upload to upload the code. Important! Always lift your vehicle off the ground while uploading.\n\nIf you want to make a new vehicle, copy vehicles/00_Master.h, store it with your vehicle name. Then edit the settings as you like, add links to the sound files you want.\nAfterwards add a link to your vehicle.h (see examples below) and uncomment it\n\n```\n#include <Arduino.h>\n\n// VEHICLE SETTINGS ****************************************************************************************************\n// Select the vehicle preset you want (uncomment the one you want, remove //, never more than one)\n\n// Master --------\n//#include \"vehicles/00_Master.h\" // This master preset file contains all available sound files, which are not used in existing vehicle presets\n\n// US trucks --------\n//#include \"vehicles/CaboverCAT3408.h\" // Cabover truck with Caterpillar 3408 V8 Diesel\n//#include \"vehicles/PeterbiltDetroit8v92.h\" // Peterbilt 359 with Detroit 8V92 V8 2 stroke Diesel\n//#include \"vehicles/KenworthW900ADetroit8V71.h\" // Kenworth W900A with Detroit 8V71 V8 2 stroke Diesel\n//#include \"vehicles/KenworthW900ACAT3408.h\" // Kenworth W900A with Caterpillar 3408 V8 Diesel (King Hauler)\n//#include \"vehicles/CAT3408OpenPipes.h\" // Kenworth W900A with Caterpillar 3408 V8 Diesel and open pipes\n//#include \"vehicles/KenworthW900ACAT3408new.h\" // Kenworth W900A with Caterpillar 3408 V8 Diesel (good bass speaker required)\n//#include \"vehicles/KenworthCummins335.h\" // 1950ies Kenworth with Cummins 335 R6 Diesel\n//#include \"vehicles/MackSuperLiner.h\" // MACK Super Liner\n//#include \"vehicles/M35.h\"// AM General M35 \"deuce and a half\" military truck, turbocharged R6 universal fuel engine\n//#include \"vehicles/US_Firetruck.h\"// US firetruck with CAT3408 V8 and Allison 6 speed automatic (horn & siren loop capable)\n//#include \"vehicles/FreightlinerCummins350.h\" // Freightliner Cabover with Cummins 350 R6 Diesel\n\n// EU trucks --------\n//#include \"vehicles/Tatra813.h\" // Tatra 813 8x8 V12 Diesel military truck (old version for comparison, don't use it)\n//#include \"vehicles/Tatra813new.h\" // Tatra 813 8x8 V12 Diesel military truck\n//#include \"vehicles/UnimogU1000.h\" // Umimog U 1000 with turbocharged R6 Diesel incl. Feuerwehr \"Martinshorn\" siren\n//#include \"vehicles/MercedesActros1836.h\" // Mercedes Actros 1863 or 3363 truck with R6 Diesel\n//#include \"vehicles/MercedesActrosV6.h\" // Mercedes Actros V6 Race Truck incl. tire squealing\n//#include \"vehicles/ScaniaV8_50ton.h\" // SCANIA V8 50 ton truck. Unknown model. Bad quality!\n//#include \"vehicles/ScaniaV8.h\" // SCANIA V8 truck, unknown model\n//#include \"vehicles/1000HpScaniaV8.h\" // 1000 HP SCANIA V8 truck with open pipes. Insane sound! Good bass speakers reqired\n//#include \"vehicles/Scania143.h\" // SCANIA 143 V8 - the legend! The best sounding in my opinion\n//#include \"vehicles/ScaniaV8Firetruck.h\" // SCANIA V8 firetruck, automatic Allison 6 speed transmission with torque converter, \"Martinshorn\" siren\n//#include \"vehicles/VolvoFH16_750.h\" // Volvo FH16 750 truck. Inline 6, 750 horses, open pipes!\n//#include \"vehicles/VolvoFH16_OpenPipe.h\" // Volvo FH truck. Inline 6, open pipes, alternative version\n//#include \"vehicles/ManTgx.h\" // MAN TGX 680 V8 truck\n//#include \"vehicles/ManKat.h\" // MAN KAT V8 Diesel German Bundeswehr military truck\n//#include \"vehicles/MagirusDeutz256.h\" // Magirus Deutz 256 air coolded V8 Diesel truck\n//#include \"vehicles/MagirusMercur125.h\" // Magirus Mercur air coolded V6 Diesel truck\n//#include \"vehicles/Saurer2DM.h\" // Swiss Saurer 2DM R6 Diesel truck\n\n// Russian trucks --------\n//#include \"vehicles/Ural4320.h\" // URAL 4320 6x6 V8 Diesel military truck\n#include \"vehicles/Ural375D.h\" // URAL 375D 6x6 V8 petrol military truck\n//#include \"vehicles/URAL375.h\" // URAL 375D 6x6 V8 petrol military truck (new version with better V8 sound, but good bass speaker required)\n//#include \"vehicles/GAZ66.h\" // GAZ-66 V8 petrol military truck\n\n// Russian tanks -------\n//#include \"vehicles/IS3.h\" // IS-3 WW2 battle tank, V12 Diesel (dual ESC mode, good bass speaker required)\n\n// Tractors -------\n//#include \"vehicles/KirovetsK700.h\" // Russian Kirovets K700 monster tractor. Extreme turbo sound!\n\n// Excavators -------\n//#include \"vehicles/Caterpillar323Excavator.h\" // Caterpillar 323 excavator (use \"FLYSKY_FS_I6S_EXCAVATOR\" remote profile)\n\n// Dumpers -------\n//#include \"vehicles/Benford3TonDumper.h\" // Benford 3 ton dumper\n\n// US motorcycles --------\n//#include \"vehicles/HarleyDavidsonFXSB.h\" // Harley Davidson FXSB V2 motorcycle\n\n// US cars --------\n//#include \"vehicles/ChevyNovaCoupeV8.h\" // 1975 Chevy Nova Coupe V8\n//#include \"vehicles/1965FordMustangV8.h\"// 1965 Ford Mustang V8\n//#include \"vehicles/Chevy468.h\" // Chevy 468 big block V8\n\n// EU cars --------\n//#include \"vehicles/VwBeetle.h\" // VW KÃ¤fer / Beetle\n//#include \"vehicles/JaguarXJS.h\" // Jaguar XJS V12, manual transmission\n//#include \"vehicles/JaguarXJSautomatic.h\" // Jaguar XJS V12, automatic transmission\n//#include \"vehicles/MGBGtV8.h\" // MGB GT V8, manual transmission\n//#include \"vehicles/LaFerrari.h\" // Ferrari LaFerrari, V12, 6 speed double clutch transmission\n\n// US SUV & Pickups --------\n//#include \"vehicles/JeepGrandCherokeeTrackhawk.h\" // Jeep Grand Cherokee Trackhawk V8 monster SUV with supercharger, 6 speed automatic\n//#include \"vehicles/FordPowerstroke.h\" // Ford Powerstroke 7.3l V8 Diesel, 6 speed automatic (good bass speaker required)\n//#include \"vehicles/RAM2500_Cummins12V.h\" // Dodge RAM 2500 with inline 6, 12V Cummins 5.9l Diesel, manual transmission\n//#include \"vehicles/RAM2500_Cummins12Vautomatic.h\" // Dodge RAM 2500 with inline 6, 12V Cummins 5.9l Diesel, automatic transmission\n//#include \"vehicles/GMCsierra.h\" // GMC Sierra V8 pickup, 3 speed automatic transmission\n//#include \"vehicles/ChevyNovaCoupeV8_P407.h\" // 1975 Chevy Nova Coupe V8, special version for HG-P407, 3 speed automatic transmission\n//#include \"vehicles/JeepWranglerRubicon392V8.h\" // 2021 Jeep Wrangler Rubicon HEMI 392 V8 (starter needs rework)\n//#include \"vehicles/JeepWranglerRubicon392V8_2.h\" // 2021 Jeep Wrangler Rubicon HEMI 392 V8 (insane bass!)\n\n// EU SUV --------\n//#include \"vehicles/DefenderV8Automatic.h\" // Land Rover Defender 90 V8 automatic (very nice V8 with lots of bass)\n//#include \"vehicles/DefenderV8OpenPipeAutomatic.h\" // Land Rover Defender 90 V8 automatic, open pipes (optimised for smaller speakers)\n//#include \"vehicles/DefenderV8CrawlerAutomatic.h\" // Land Rover Defender 90 V8 automatic crawler\n//#include \"vehicles/DefenderTd5.h\" // Land Rover Defender 90 Td5 R5 Diesel\n\n// Asian SUV --------\n//#include \"vehicles/LandcruiserFJ40.h\" // Landcruiser Fj40 with inline 6 petrol engine\n//#include \"vehicles/LandcruiserFJ40Diesel.h\" // Landcruiser Fj40 with inline 6 12H Turbo Diesel engine\n//#include \"vehicles/LandcruiserFJ40Diesel2.h\" // Landcruiser Fj40 with inline 6 12H Turbo Diesel engine\n//#include \"vehicles/HiluxDiesel.h\" // Hilux Diesel with inline 6 12H Turbo Diesel engine (for HG-P407)\n\n// US locomotives --------\n//#include \"vehicles/UnionPacific2002.h\" // Union Pacific 2002 SD70M locomotive with enormous, low revving 16 cylinder Diesel\n\n// Planes --------\n//#include \"vehicles/MesserschmittBf109.h\" // Messerschmitt BF 109 WW2 German V12 plane\n\n// Generic Diesels --------\n//#include \"vehicles/generic6zylDiesel.h\" // Generic inline 6 Diesel, no turbo, manual transmission (optimised for smaller speakers)\n```\n\n### Interface type (communication mode) selection:\n\nNote, that the default communication mode is SBUS. You need to change it as follows, if you want to use classic RC servo signals.\n\n#### PWM (classic RC signals on \"CH1\" - \"CH4\", \"35\" & \"PPM\" headers, the most common interface)\n```\n// COMMUNICATION SETTINGS **********************************************************************************************\n// Choose the receiver communication mode (never uncomment more than one!) !!! ADJUST THEM BEFORE CONNECTING YOUR RECEIVER AND ESC !!!\n\n// PWM servo signal communication (CH1 - CH4, 35, PPM headers, 6 channelschannelSetup.h) --------\n// PWM mode active, if SBUS, IBUS and PPM are disabled (// in front of #define)\n\n// SBUS communication (SBUS header, 13 channels. This my preferred communication protocol)--------\n//#define SBUS_COMMUNICATION // control signals are coming in via the SBUS interface (comment it out for classic PWM RC signals)\nboolean sbusInverted = true; // false = wired to non standard (inverted) SBUS signal (for example from \"Micro RC\" receiver)\n\n// IBUS communication (RX header, 13 channels not recommended, NO FAILSAFE, if bad contact in iBUS wiring!) --------\n//#define IBUS_COMMUNICATION // control signals are coming in via the IBUS interface (comment it out for classic PWM RC signals)\n\n// SUMD communication (RX header, 12 channels, For Graupner remotes) --------\n//#define SUMD_COMMUNICATION // control signals are coming in via the SUMD interface (comment it out for classic PWM RC signals)\n\n// PPM communication (RX header, 8 channels, working fine, but channel signals are a bit jittery) --------\n//#define PPM_COMMUNICATION // control signals are coming in via the PPM interface (comment it out for classic PWM RC signals)\n```\n\n#### PPM (multiple channels pulse pause modulation, wired to \"RX\" header, 8 channels)\n```\n// COMMUNICATION SETTINGS **********************************************************************************************\n// Choose the receiver communication mode (never uncomment more than one!) !!! ADJUST THEM BEFORE CONNECTING YOUR RECEIVER AND ESC !!!\n\n// PWM servo signal communication (CH1 - CH4, 35, PPM headers, 6 channelschannelSetup.h) --------\n// PWM mode active, if SBUS, IBUS, and PPM are disabled (// in front of #define)\n\n// SBUS communication (SBUS header, 13 channels. This my preferred communication protocol)--------\n//#define SBUS_COMMUNICATION // control signals are coming in via the SBUS interface (comment it out for classic PWM RC signals)\nboolean sbusInverted = true; // false = wired to non standard (inverted) SBUS signal (for example from \"Micro RC\" receiver)\n\n// IBUS communication (RX header, 13 channels not recommended, NO FAILSAFE, if bad contact in iBUS wiring!) --------\n//#define IBUS_COMMUNICATION // control signals are coming in via the IBUS interface (comment it out for classic PWM RC signals)\n\n// SUMD communication (RX header, 12 channels, For Graupner remotes) --------\n//#define SUMD_COMMUNICATION // control signals are coming in via the SUMD interface (comment it out for classic PWM RC signals)\n\n// PPM communication (RX header, 8 channels, working fine, but channel signals are a bit jittery) --------\n#define PPM_COMMUNICATION // control signals are coming in via the PPM interface (comment it out for classic PWM RC signals)\n```\n\n#### SBUS (recommended, default setting, wired to \"SBUS\" header, 13 channels)\n```\n// COMMUNICATION SETTINGS  ********************************************************************************************************************\n// Choose the receiver communication mode (never uncomment more than one!) !!! ADJUST THEM BEFORE CONNECTING YOUR RECEIVER AND ESC !!!\n\n// PWM servo signal communication (CH1 - CH4, 35, PPM headers, 6 channels) --------\n// PWM mode active, if SBUS, IBUS, and PPM are disabled (// in front of #define)\n\n// SBUS communication (RX header, 13 channels. This is my preferred communication protocol)--------\n#define SBUS_COMMUNICATION // control signals are coming in via the SBUS interface (comment it out for classic PWM RC signals)\nboolean sbusInverted = false; // false = wired to non standard (inverted) SBUS signal (for example from my \"Micro RC\" receiver)\nuint32_t sbusBaud = 100000; // Standard is 100000. Try to lower it, if your channels are coming in unstable. Working range is about 96000 - 104000.\n\n// IBUS communication (RX header, 13 channels not recommended, NO FAILSAFE, if bad contact in iBUS wiring!) --------\n//#define IBUS_COMMUNICATION // control signals are coming in via the IBUS interface (comment it out for classic PWM RC signals)\n\n// SUMD communication (RX header, 12 channels, For Graupner remotes) --------\n//#define SUMD_COMMUNICATION // control signals are coming in via the SUMD interface (comment it out for classic PWM RC signals)\n\n// PPM communication (RX header, 8 channels, working fine, but channel signals are a bit jittery) --------\n//#define PPM_COMMUNICATION // control signals are coming in via the PPM interface (comment it out for classic PWM RC signals)\n```\n\nSBUS non standard signal (if your receiver sends a non-standard SBUS signal):\n```\nboolean sbusInverted = false; // false = wired to non standard (inverted) SBUS signal (for example from \"Micro RC\" receiver)\n```\n\nSBUS standard signal (default, used in most cases)\n```\nboolean sbusInverted = true; // false = wired to non standard (inverted) SBUS signal (for example from \"Micro RC\" receiver)\n```\n\nSBUS baudrate fine adjustment. 100000 ist standard. With some receivers, the indicators and other functions may be randomly triggered, because there are bit errors.\nThis variable allows to fine adjust the baudrate to solve this problem.\n```\nuint32_t sbusBaud = 100000; // Standard is 100000. Try to lower it, if your channels are coming in unstable. Working range is about 96000 - 104000.\n```\n\n#### IBUS (not recommended, NO FAILSAFE, if bad contact in iBUS wiring! \"RX\" header, 13 channels)\n```\n// COMMUNICATION SETTINGS **********************************************************************************************\n// Choose the receiver communication mode (never uncomment more than one!) !!! ADJUST THEM BEFORE CONNECTING YOUR RECEIVER AND ESC !!!\n\n// PWM servo signal communication (CH1 - CH4, 35, PPM headers, 6 channelschannelSetup.h) --------\n// PWM mode active, if SBUS, IBUS, SERIAL and PPM are disabled (// in front of #define)\n\n// SBUS communication (SBUS header, 13 channels. This my preferred communication protocol)--------\n//#define SBUS_COMMUNICATION // control signals are coming in via the SBUS interface (comment it out for classic PWM RC signals)\nboolean sbusInverted = true; // false = wired to non standard (inverted) SBUS signal (for example from \"Micro RC\" receiver)\n\n// IBUS communication (RX header, 13 channels not recommended, NO FAILSAFE, if bad contact in iBUS wiring!) --------\n#define IBUS_COMMUNICATION // control signals are coming in via the IBUS interface (comment it out for classic PWM RC signals)\n\n// SUMD communication (RX header, 12 channels, For Graupner remotes) --------\n//#define SUMD_COMMUNICATION // control signals are coming in via the SUMD interface (comment it out for classic PWM RC signals)\n\n// PPM communication (RX header, 8 channels, working fine, but channel signals are a bit jittery) --------\n//#define PPM_COMMUNICATION // control signals are coming in via the PPM interface (comment it out for classic PWM RC signals)\n```\n\n#### SUMD (For Graupner remotes \"RX\" header, 12 channels)\n```\n// COMMUNICATION SETTINGS  ********************************************************************************************************************\n// Choose the receiver communication mode (never uncomment more than one!) !!! ADJUST THEM BEFORE CONNECTING YOUR RECEIVER AND ESC !!!\n\n// PWM servo signal communication (CH1 - CH4, 35, PPM headers, 6 channels) --------\n// PWM mode active, if SBUS, IBUS, and PPM are disabled (// in front of #define)\n\n// SBUS communication (SBUS header, 13 channels. This is my preferred communication protocol)--------\n//#define SBUS_COMMUNICATION // control signals are coming in via the SBUS interface (comment it out for classic PWM RC signals)\nboolean sbusInverted = false; // false = wired to non standard (inverted) SBUS signal (for example from my \"Micro RC\" receiver)\n\n// IBUS communication (RX header, 13 channels not recommended, NO FAILSAFE, if bad contact in iBUS wiring!) --------\n//#define IBUS_COMMUNICATION // control signals are coming in via the IBUS interface (comment it out for classic PWM RC signals)\n\n// SUMD communication (RX header, 12 channels, For Graupner remotes) --------\n#define SUMD_COMMUNICATION // control signals are coming in via the SUMD interface (comment it out for classic PWM RC signals)\n\n// PPM communication (RX header, 8 channels, working fine, but channel signals are a bit jittery) --------\n//#define PPM_COMMUNICATION // control signals are coming in via the PPM interface (comment it out for classic PWM RC signals)\n```\n\n## Adjusting things in \"vehicles/yourVehiclePreset.h\":\n### Shaker\nadjust the shaker power for the different engine states to fit your needs:\n```\n// Shaker parameters (simulating engine vibrations) ---------------------------------------------------------------------\nconst uint8_t shakerStart = 100; // Shaker power while engine start (max. 255, about 100)\nconst uint8_t shakerIdle = 49; // Shaker power while idling (max. 255, about 49)\nconst uint8_t shakerFullThrottle = 40; // Shaker power while full throttle (max. 255, about 40)\nconst uint8_t shakerStop = 60; // Shaker power while engine stop (max. 255, about 60)\n```\n### More to come...\n\n## Prototypes:\n![](documentation/pictures/top.jpg)\n\n![](documentation/pictures/receiver_wiring.jpg)\n\n![](documentation/pictures/Bestueckt_oben.jpg)\n\n![](documentation/pictures/oben.jpg)\n\n![](documentation/pictures/unten.jpg)\n\n\n2019 - 2023, TheDIYGuy999\n",
    "readme_length": 37302
  },
  {
    "name": "nf-interpreter",
    "full_name": "nanoframework/nf-interpreter",
    "description": ":gear: nanoFramework Interpreter, CLR, HAL, PAL and reference target boards",
    "stars": 330,
    "forks": 187,
    "language": "C",
    "url": "https://github.com/nanoframework/nf-interpreter",
    "topics": [
      "adc",
      "clr",
      "csharp",
      "dotnet",
      "esp32",
      "firmware",
      "gpio",
      "hacktoberfest",
      "i2c",
      "interpreter",
      "mcu",
      "microcontroller",
      "nanoframework",
      "nanoframework-interpreter",
      "pal",
      "pwm",
      "serial-communication",
      "spi",
      "stm32"
    ],
    "created_at": "2016-11-26T15:45:06Z",
    "updated_at": "2025-12-01T22:25:45Z",
    "homepage": "https://www.nanoframework.net",
    "license": "MIT License",
    "readme": "[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE) [![#yourfirstpr](https://img.shields.io/badge/first--timers--only-friendly-blue.svg)](https://github.com/nanoframework/Home/blob/main/CONTRIBUTING.md) [![Hosted By: Cloudsmith](https://img.shields.io/badge/OSS%20hosting%20by-cloudsmith-blue?logo=cloudsmith&style=flat-square)](https://cloudsmith.com)\n [![Discord](https://img.shields.io/discord/478725473862549535.svg)](https://discord.gg/gCyBu8T)\n\n![nanoFramework logo](https://github.com/nanoframework/Home/blob/main/resources/logo/nanoFramework-repo-logo.png)\n\n-----\nDocument Language: [English](README.md) | [ä¸­æ–‡ç®€ä½“](README.zh-cn.md)\n\n### Welcome to the .NET **nanoFramework** interpreter repository!\n\n## Build status\n\n| Component | Build Status |\n|:-|---|\n| nanoBooter + nanoCLR | [![Build Status](https://dev.azure.com/nanoframework/nf-interpreter/_apis/build/status/nf-interpreter?repoName=nanoframework%2Fnf-interpreter&branchName=main)](https://dev.azure.com/nanoframework/nf-interpreter/_build/latest?definitionId=34&repoName=nanoframework%2Fnf-interpreter&branchName=main) |\n| Win32 test project | [![Build Status](https://dev.azure.com/nanoframework/nf-interpreter/_apis/build/status/nf-interpreter?repoName=nanoframework%2Fnf-interpreter&branchName=main)](https://dev.azure.com/nanoframework/nf-interpreter/_build/latest?definitionId=34&repoName=nanoframework%2Fnf-interpreter&branchName=main) |\n\n## .NET nanoFramework interpreter for reference boards\n\nEach of the linked files contain the firmware binaries for nanoBooter (if applicable) and nanoCLR in various formats (`.HEX`, `.BIN` and `.DFU`). They should be flashed to the target board using [nanoff](https://github.com/nanoframework/nanoFirmwareFlasher) or the appropriate software utility depending on the target MCU.\n\nFirmware versions contained below are released via the `main` branch, and are treated as `RTM` builds as per our stable release mandate. These releases are compiled with optimizations and the smallest possible size. For these builds, the debugging features are also disabled and only contain no (or minimal) detailed error messages.\n\nWe also have a [Community Targets](https://github.com/nanoframework/nf-Community-Targets) repository where you can find firmware versions for several other popular boards that have been contibuted by the community, although limited support is provided.\n\n| Available pre-built targets | | | |\n|---|---|---|---|\n| [ESP32 boards](#user-content-esp32-modules-and-boards) | [ESP32 boards with ethernet](#user-content-esp32-boards-with-inbuilt-Ethernet) | [ESP32_S2 boards](#user-content-esp32_s2-boards) | [ESP32_S3 boards](#user-content-esp32_s3-boards) |\n| [ESP32 riscv (C3, C6 & H2)](#user-content-esp32-risc-v-boards) | [ESP32 M5STACK](#user-content-m5stack) | |\n| [STM32 boards](#user-content-stm32-boards-and-chip-based) | | | |\n| [Silicon Labs Giant Gecko](#user-content-silicon-labs-giant-gecko-boards) | | | |\n| [NXP boards](#user-content-nxp-boards) | | | |\n| [TI boards](#user-content-ti-boards) | | | |\n\n\n***\n### ESP32 modules and boards\n\n| Target | Note | Version |\n|:---|:---|---|\n| ESP32_REV0 | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_REV0/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_REV0/latest/) |\n| ESP32_PSRAM_REV0 | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_PSRAM_REV0/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_PSRAM_REV0/latest/) |\n| ESP32_BLE_REV0 | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_BLE_REV0/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_BLE_REV0/latest/) |\n| ESP32_PSRAM_XTAL26_REV0 | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_PSRAM_XTAL26_REV0/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_PSRAM_XTAL26_REV0/latest/) |\n| ESP32_GenericDisplay_REV0 | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_GenericDisplay_REV0/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_GenericDisplay_REV0/latest/) |\n| ESP32_REV3 | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_REV3/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_REV3/latest/) |\n| ESP32_PSRAM_REV3 | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_PSRAM_REV3/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_PSRAM_REV3/latest/) |\n| ESP32_BLE_REV3 | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_BLE_REV3/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_BLE_REV3/latest/) |\n| ESP32_PSRAM_BLE_GenericGraphic_REV3 | |[![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_PSRAM_BLE_GenericGraphic_REV3/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_PSRAM_BLE_GenericGraphic_REV3/latest/) |\n| ESP32_PSRAM_REV3_IPV6 | IPV6 & Thread via NCP |[![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_PSRAM_REV3_IPV6/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_PSRAM_REV3_IPV6/latest/) |\n| ESP_WROVER_KIT | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP_WROVER_KIT/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP_WROVER_KIT/latest/) |\n| ESP32_PICO | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_PICO/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_PICO/latest/) |\n\n### ESP32 boards with inbuilt Ethernet \n| Target | Note | Version | \n|:---|---|---|\n| ESP32_ETHERNET_KIT_1.2  | Poe, Wrover + 8mb psram| [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_ETHERNET_KIT_1.2/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_ETHERNET_KIT_1.2/latest/) |\n| ESP32_OLIMEX  | Poe | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_OLIMEX/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_OLIMEX/latest/) |\n| ESP32_OLIMEX_WROVER  | Poe, Wrover + 8mb psram | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_OLIMEX_WROVER/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_OLIMEX_WROVER/latest/) |\n| ESP32_WT32_ETH01  | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_WT32_ETH01/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_WT32_ETH01/latest/) |\n| ESP32_WESP32  | Poe | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_WESP32/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_WESP32/latest/) |\n| ESP32_LILYGO | Poe | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_LILYGO/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_LILYGO/latest/) |\n\n### ESP32_S2 boards\n| Target | Note | Version | \n|:---|---|---|\n| ESP32_S2_USB | S2 boards with direct USB connection to chip. Including Adafruit Feather_S2 | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_S2_UART/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_S2_USB/latest/) |\n| ESP32_S2_UART | S2 boards with onboard USB->UART. This includes the Kaluga dev. board from Espressif | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_S2_UART/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_S2_UART/latest/) |\n\n### ESP32_S3 boards\n| Target | Note | Version | \n|:---|---|---|\n| ESP32_S3 | Display & Quad spiram support | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_S3/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_S3/latest/) |\n| ESP32_S3_BLE | Display, BLE, Quad spiram support | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_S3_BLE/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_S3_BLE/latest/) |\n| ESP32_S3_BLE_UART | Display, BLE, Quad spiram support, connection via UART| [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_S3_BLE_UART/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_S3_BLE_UART/latest/) |\n| ESP32_S3_ALL |  Display, BLE, Octal spiram support | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_S3_ALL/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_S3_ALL/latest/) |\n| ESP32_S3_ALL_UART |  Display, BLE, Octal spiram support, connection via UART | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_S3_ALL_UART/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_S3_ALL_UART/latest/) |\n\n### ESP32 risc-v boards\n| Target |  Note | Version |\n|:---|---|---|\n| ESP32_C3 | Uart -> VS (Beta C3 version) | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_C3/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_C3/latest/) |\n| ESP32_C3_REV3 | Uart -> VS | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_C3_REV3/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_C3_REV3/latest/) |\n| XIAO_ESP32C3 | USB jtag -> VS| [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/XIAO_ESP32C3/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/XIAO_ESP32C3/latest/) |\n| ESP32_C6_THREAD | USB jtag -> VS, OpenThread, Display | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_C6_THREAD/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_C6_THREAD/latest/) |\n| ESP32_H2_THREAD | USB jtag -> VS, OpenThread | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_H2_THREAD/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_H2_THREAD/latest/) |\n| ESP32_P4_UART | Beta, Uart -> VS, wifi ESP32_C6, No Display | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ESP32_P4_UART/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ESP32_P4_UART/latest/) |\n\n### M5Stack\n\n| Target | Note | Version |\n|:---|---|---|\n| [M5Core](https://docs.m5stack.com/en/core/gray) | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/M5Core/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/M5Core/latest/) |\n| [M5StickC](https://docs.m5stack.com/en/core/m5stickc) | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/M5StickC/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/M5StickC/latest/) |\n| [M5StickCPlus](https://docs.m5stack.com/en/core/m5stickc_plus) | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/M5StickCPlus/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/M5StickCPlus/latest/) |\n| [M5Core2](https://docs.m5stack.com/en/core/core2) | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/M5Core2/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/M5Core2/latest/) |\n| [AtomS3](https://docs.m5stack.com/en/core/AtomS3) | | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/AtomS3/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/AtomS3/latest/) |\n\n### STM32 boards and chip based\n\n| Target | Version |\n|:---|---|\n| MXCHIP_AZ3166 | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/MXCHIP_AZ3166/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/MXCHIP_AZ3166/latest/) |\n| ST_STM32F429I_DISCOVERY (B01) | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ST_STM32F429I_DISCOVERY/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ST_STM32F429I_DISCOVERY/latest/) |\n| ST_NUCLEO64_F091RC | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ST_NUCLEO64_F091RC/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ST_NUCLEO64_F091RC/latest/) |\n| ST_STM32F769I_DISCOVERY | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ST_STM32F769I_DISCOVERY/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ST_STM32F769I_DISCOVERY/latest/) |\n| ORGPAL_PALTHREE | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ORGPAL_PALTHREE/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ORGPAL_PALTHREE/latest/) |\n| ORGPAL_PALX | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/ORGPAL_PALX/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/ORGPAL_PALX/latest/) |\n\n### Silicon Labs Giant Gecko boards\n\n| Target | Version |\n|:---|---|\n| SL_STK3701A | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/SL_STK3701A/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/SL_STK3701A/latest/) |\n| SL_STK3701A_REVB | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/SL_STK3701A_REVB/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/SL_STK3701A_REVB/latest/) |\n\n### NXP boards\n\n| Target | Version |\n|:---|---|\n| NXP_MIMXRT1060_EVK | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/NXP_MIMXRT1060_EVK/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/NXP_MIMXRT1060_EVK/latest/) |\n\n### TI boards\n\n| Target | Version |\n|:---|---|\n| TI_CC1352R1_LAUNCHXL_868 | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/TI_CC1352R1_LAUNCHXL_868/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/TI_CC1352R1_LAUNCHXL_868/latest/) |\n| TI_CC1352R1_LAUNCHXL_915 | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/TI_CC1352R1_LAUNCHXL_915/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/TI_CC1352R1_LAUNCHXL_915/latest/) |\n| TI_CC3220SF_LAUNCHXL | [![Latest Version @ Cloudsmith](https://api-prd.cloudsmith.io/v1/badges/version/net-nanoframework/nanoframework-images/raw/TI_CC3220SF_LAUNCHXL/latest/x/?render=true)](https://cloudsmith.io/~net-nanoframework/repos/nanoframework-images/packages/detail/raw/TI_CC3220SF_LAUNCHXL/latest/) |\n\nThe above .NET nanoFramework interpreter builds include support for the class libraries and features marked below.\n\n<details>\n  <summary>Click to expand!</summary>\n\n   | Target                  | Gpio               | Spi                | I2c                | Pwm                | Adc                | Dac                | Serial             | OneWire            | Events             | SWO                | Networking         | Bluetooth BLE    | Large Heap         | UI         |\n  |:-:                      |:-:                 |:-:                 |:-:                 |:-:                 |:-:                 |:-:                 |:-:                 |:-:                 |:-:                 |:-:                 |:-:                 |:-:                 |:-:                 |:-:                 |\n  | ESP32_PSRAM_REV0          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    | :heavy_check_mark: |                    |\n  | ESP32_REV0          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    | :heavy_check_mark: |                    |\n  | ESP32_PSRAM_XTAL26_REV0          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    | :heavy_check_mark: |                    |\n  | ESP32_PSRAM_REV3          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    | :heavy_check_mark: |                    |\n  | ESP32_REV3          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    | :heavy_check_mark: |                    |\n  | ESP32_BLE_REV0      | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: |                    |                    |\n  | ESP32_BLE_REV3      | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: |                    |                    |\n  | ESP_WROVER_KIT          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: |\n  | ESP32_PICO          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |   |                    |                    |\n  | ESP32_LILYGO          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: Wi-Fi + Ethernet |  |             |                    |\n  | ESP32_S2_USB      | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |  |                    |                    |\n  | ESP32_S2_UART     | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |  |                    | :heavy_check_mark: |\n  | ESP32_C3          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    | | |\n  | XIAO_ESP32C3     | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    | | |\n  | ESP32_C6          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    | | |\n  | ESP32_H2          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    | | |\n  | ESP32_ETHERNET_KIT_1.2| :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: Wi-Fi + Ethernet  |  | :heavy_check_mark: |                    |\n  | ESP32_OLIMEX          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: Wi-Fi + Ethernet  |  | :heavy_check_mark: |                    |\n  | ESP32_WT32_ETH01      | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: Wi-Fi + Ethernet  |  | :heavy_check_mark: |    \n  | ESP32_WESP32      | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: Wi-Fi + Ethernet  |  | :heavy_check_mark: |    \n  | M5Core          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: Wi-Fi  |  | :heavy_check_mark: |                    |\n  | M5StickC          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: Wi-Fi |  | :heavy_check_mark: |                    |\n  | M5StickCPlus          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |  :heavy_check_mark: Wi-Fi  |  | :heavy_check_mark: |                    |\n  | M5Core2          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: Wi-Fi |  | :heavy_check_mark: |                    |\n  | ESP32_GenericDisplay_REV0 | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: Wi-Fi  |  | :heavy_check_mark: |                    |\n  | ESP32_PSRAM_BLE_GenericGraphic_REV3          | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: Wi-Fi |  | :heavy_check_mark: |                    |\n  | MXCHIP_AZ3166 | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |  |  | :heavy_check_mark: |  |  |  |  |  |  |  |\n  | ST_STM32F429I_DISCOVERY (B01) | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    | :heavy_check_mark: |                    |\n  | ST_NUCLEO64_F091RC      | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |\n  | ST_STM32F769I_DISCOVERY | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: |\n  | ORGPAL_PALTHREE | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    |\n  | ORGPAL_PALX | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    |\n  | SL_STK3701A_REVB | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: |  | :heavy_check_mark: | :heavy_check_mark: |  |                    |  |  |\n  | SL_STK3701A | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |  | :heavy_check_mark: |  | :heavy_check_mark: | :heavy_check_mark: |  |                    |  |  |\n  | TI_CC1352R1_LAUNCHXL    | :heavy_check_mark: |  |  |  |  |                    |                    |                    |  |                    |  |                    |                    |                    |\n  | TI_CC3220SF_LAUNCHXL    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    | :heavy_check_mark: |                    | :heavy_check_mark: |                    |                    |                    |\n  | NXP_MIMXRT1060_EVK           | :heavy_check_mark: |  |  |  |  |  | :heavy_check_mark:  |                    | :heavy_check_mark: |                    | :heavy_check_mark: |                    | :heavy_check_mark: |                    |\n</details>\n\n<details>\n  <summary>This repo contains:</summary>\n\n  * **nanoFramework** agnostic blocks\n    * [CLR](src/CLR)\n    * [HAL](src/HAL)\n    * [PAL](src/PAL)\n  * Target reference for CMSIS OS\n    * [ChibiOS](targets/ChibiOS)\n      * Reference target boards\n        * [OrgPal PalThree](targets/ChibiOS/ORGPAL_PALTHREE)\n        * [OrgPal PalX](targets/ChibiOS/ORGPAL_PALX)\n        * [ST NUCLEO64 F091RC](targets/ChibiOS/ST_NUCLEO64_F091RC)\n        * [ST STM32F429I DISCOVERY (B01)](targets/ChibiOS/ST_STM32F429I_DISCOVERY)\n        * [ST STM32F769I DISCOVERY](targets/ChibiOS/ST_STM32F769I_DISCOVERY)\n      * ChibiOS overlay for **nanoFramework**\n        * [STM32 1.Wire driver](targets/ChibiOS/_nf-overlay/os/hal/src/stm32_onewire)\n        * [STM32 CRC32 driver](targets/ChibiOS/_nf-overlay/os/hal/src/stm32_crc)\n        * [STM32 Flash driver](targets/ChibiOS/_nf-overlay/os/hal/src/stm32_flash)\n        * [STM32 Flexible Memory Controller driver](targets/ChibiOS/_nf-overlay/os/hal/src/stm32_fsmc)\n        * [STM32 Random number generator driver](targets/ChibiOS/_nf-overlay/os/hal/src/stm32_rng)\n  * Target reference for FreeRTOS\n    * [ESP32_REV0](targets/ESP32/ESP32_REV0)\n    * [NXP_MIMXRT1060_EVK](targets/FreeRTOS/NXP/NXP_MIMXRT1060_EVK)\n  * Target references for Azure RTOS\n    * [Silabs Giant Gecko EVB](targets/AzureRTOS/Silabs/SL_STK3701A)\n  * Target references for TI SimpleLink\n    * [TI CC1352R1_LAUNCHXL](targets/TI_SimpleLink/TI_CC1352R1_LAUNCHXL)\n    * [TI CC3220SF_LAUNCHXL](targets/TI_SimpleLink/TI_CC3220SF_LAUNCHXL)\n  * Target reference for other OSes\n    * [Win32 OS (test project only at this time)](targets/os/win32)\n  * [CMake files for the build system](CMake)\n</details>\n\n## Feedback and documentation\n\nFor documentation, providing feedback, issues and finding out how to contribute please refer to the [Home repo](https://github.com/nanoframework/Home).\n\nJoin our Discord community [here](https://discord.gg/gCyBu8T).\n\n## Credits\n\nThe list of contributors to this project can be found at [CONTRIBUTORS](https://github.com/nanoframework/Home/blob/main/CONTRIBUTORS.md).\n\n## License\n\nThe nanoFramework Interpreter is licensed under the [MIT license](LICENSE.md).\n\n## Code of Conduct\n\nThis project has adopted the code of conduct defined by the Contributor Covenant to clarify expected behavior in our community.\nFor more information see the [.NET Foundation Code of Conduct](https://dotnetfoundation.org/code-of-conduct).\n\n### .NET Foundation\n\nThis project is supported by the [.NET Foundation](https://dotnetfoundation.org).\n",
    "readme_length": 33263
  },
  {
    "name": "EMBO",
    "full_name": "parezj/EMBO",
    "description": "EMBO - Scope for only $6. Embedded instruments: Oscilloscope, Logic Analyzer, Voltmeter, Counter, PWM and Signal Generator on STM32 MCUs (F1, F3, L4, G4 ....). PC app for Windows, Ubuntu and macOS. Firmware is in C, PC GUI app is in C++ Qt 5.",
    "stars": 324,
    "forks": 73,
    "language": "C",
    "url": "https://github.com/parezj/EMBO",
    "topics": [
      "bluepill-board",
      "c-language",
      "c-plus-plus",
      "counter",
      "ctu",
      "cubemx",
      "electronics",
      "embedded",
      "embo",
      "generator",
      "logicanalyzer",
      "nucleo-board",
      "oscilloscope",
      "pwm",
      "qt",
      "signal",
      "st",
      "stm32",
      "stm32f103",
      "voltmeter"
    ],
    "created_at": "2020-11-10T09:19:48Z",
    "updated_at": "2025-12-01T15:48:37Z",
    "homepage": "https://embo.jakubparez.com",
    "license": "MIT License",
    "readme": "<p float=\"left\" align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/img/icon.png\" alt=\"EMBO\" width=\"150\" />\n  &nbsp;&nbsp;&nbsp;&nbsp;\n  <a href=\"https://meas.fel.cvut.cz/\" alt=\"CTU\"><img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/img/ctu_meas.png\" height=\"150\" /></a>\n</p>\n\n# EMBO - EMBedded Oscilloscope\n> Download **[HERE](https://github.com/parezj/EMBO/releases)**  \n\nEMBO consists of 3 primary devices (oscilloscope, logic analyzer, voltmeter) and 3 secondary devices (counter, PWM and signal generator). The multiplatform PC application can be run on Windows, Linux and macOS operating systems. Stable firmware is available for STM32 series F1, F3 and L4 in HEX format, other series L0 and G0 are currently supported experimentally. The EMBO oscilloscope is intended to serve as a cheap and accessible tool for all electronics enthusiasts. \n\nThis work was created as part of my master thesis at [FEE CTU](https://meas.fel.cvut.cz/) in Prague (Department of Measurement) under the supervision of doc. Fischer and with the help of Ing. HladÃ­k from STMicroelectronics. I would like to thank them for perfect support. Whole EMBO is published under the MIT license. \n\nSupported MCUs:\n- **STM32F103C8**\n- **STM32F103RE**\n- **STM32F303RE**\n- **STM32L412KB**\n\n*More yet to come... (L0, G0, G4, F4)*\n\n1. [Parameters](#1-Parameters)\n2. [Connection](#2-Connection)\n3. [Pinout](#3-Pinout)\n4. [PC App Description](#4-PC-App-Description)\n5. [PC App Class Diagram](#5-PC-App-Class-Diagram)\n6. [FW Block Diagram](#6-FW-Block-Diagram)\n7. [Used IP](#7-Used-IP)\n\n\n## 1. Parameters\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/img/params1.png\" alt=\"EMBO params 1\" width=\"700\">\n</div>\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/img/params2.png\" alt=\"EMBO params 2\" width=\"700\">\n</div>\n\n\\* SCOPE - Oscilloscope, LA - Logic Analyzer, VM - Voltmeter, CNTR - Counter, PWM - PWM Generator, SGEN - Signal Generator, DAQ - total memory for SCOPE or LA\n\n## 2. Connection\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/img/comm_types_png_en.png\" alt=\"EMBO comm\" width=\"700\">\n</div>\n\n## 3. Pinout\n\n### STM32F103C8 (Blue Pill)\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/img/pinout_bluepill_png_en.png\" alt=\"EMBO pinout bluepill\" width=\"500\">\n</div>\n\n### STM32F303RE (Nucleo-64)\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/img/pinout_f303_png_en.png\" alt=\"EMBO pinout F303\" width=\"600\">\n</div>\n\n## 4. PC App Description\n\n### Main Window \nAfter starting the PC application, the user gets to the main window with the selection of individual devices. On the left is a list of available ports (in the case of Windows OS these are COM ports, in the case of a UNIX-like system these are tty ports. The user selects the port and connects to the oscilloscope. The three panels on the right are displayed. about the microcontroller and firmware, in the middle panel there are primary devices and their parameters and in the bottom panel secondary devices are available.\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/screenshots/main2.png\" alt=\"EMBO main2\" width=\"700\">\n</div>\n\nThe bottom bar (status bar) contains information about the current session, which is ideally updated with a period of 10 ms. Latency indicates the response time of the oscilloscope and Uptime indicates the time since power-up. If the communication is busy (the oscilloscope sends a lot of data), the latency can increase up to hundreds of ms when using UART (ST-LINK).\n\nDescription of device parameters: \n- **Fs** â€” sampling frequency in samples per second\n- **Mem** â€” memory depth in samples \n- **Bits** â€” resolution in bits \n- **Mode** â€” device mode (4ch 4ADC = 4 channels per 4 ADC) \n- **Port** â€” assignment of microcontroller ports to device channels (C0 = PC0) \n- **Max freq** â€” maximum frequency of input or output signal\n\n### Oscilloscope \nThe oscilloscope is a key device of the whole system. The oscilloscope window is divided into two main blocks. In the left block there is an interactive graph, under which lie measuring elements and cursor controls. The most important controls are concentrated in the right blocks. Auxiliary and additional functions are controlled from the top menu. The lower part of the screen is set aside for the status bar.\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/screenshots/scope.png\" alt=\"EMBO scope\" width=\"800\">\n</div>\n\nDescription of control panels: \n- **Main panel** â€” located at the top right, basic modes (Run, Stop, Single) \n- **Trigger** â€” trigger settings \n- **Horizontal** â€” time base setting, resp. sampling frequency and memory \n- **Vertical** â€” channel settings and their gain and shift \n- **Utils** â€” additional functions (averaging, resolution, FFT) \n- **Cursors** â€” control of cursors \n- **Measure** â€” indicators of measured parameters\n\n#### Main panel \nAs with most oscilloscopes, EMBO has 3 main modes: Run, Stop and Single. The implementation of these modes is inspired by desktop digital oscilloscopes (Agilent, Tektronix). Run mode is the primary mode with continuous data acquisition. Stop mode is used to stop acquisition and signal analysis. The special Single mode is used to capture a single event. Furthermore, in this panel, the user has the option to reset the zoom or the entire device.\n\n#### Trigger\nThe trigger setting is also inspired by desktop devices. The default is always Auto mode, which combines the functionality of Normal mode with a timeout that allows you to see the signal in all cases. There is also an adjustable Slope, Channel, Level and pre-trigger. In Normal and Single mode, the user has the option to force a trigger, which serves as a quick preview. The indicator LED named TrigÂ´d lights up green if the trigger was successful (flashes with each new message).\n\n#### Horizontal\nHorizontal parameters can be set in 2 modes. In manual mode, the user sets the sampling frequency (Fs) and memory depth per channel (Mem). In automatic mode, these 2 parameters are optimized based on the total length of the displayed signal (Width). This approach was chosen because the classic grid and settings (second / division) do not make sense here, because the graph is interactive and the user can freely zoom and move with it. Below are 2 important pieces of information. Real sampling frequency (Real Fs) with an accuracy of 6 orders of magnitude is required for sampling at the equivalent time and subsequent frequency recalculation. The maximum permissible impedance of the source signal is described in detail in the chapter.\n\n#### Vertical \nVertical adjustment allows you to turn on individual channels, adjust their gain and offset.\n\n#### Utils, Cursors and Measure \nAveraging is suitable for smoothing the interfered signal. Optional 8-bit mode speeds up data transfer at the expense of reduced resolution, and FFT turns on spectral analysis mode. Cursors are used to measure the signal in both axes and the measured parameters (Measure) represent the basic information about the signal.\n\nDescription of the top menu: \n- **Export** â€” export data in CSV, TXT format or as a screenshot \n- **Plot** â€” graph display and interpolation settings \n- **Measure** â€” setting of measured parameters \n- **FFT** â€” melting of FFT spectral analysis \n- **Math** â€” control of special modes (difference, XY)\n\n### Logic Analyzer \nThe logic analyzer provides additional oscilloscope functionality for fast digital signals. Like the oscilloscope, the instrument is divided into two main blocks. In the left block there is an interactive graph that dynamically displays the appropriate number of channels, unlike the oscilloscope but below it. In the right blocks are the main controls coming from the oscilloscope. Additional functions are controlled again from the top menu. The lower part of the screen is set aside for the status bar.\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/screenshots/la.png\" alt=\"EMBO la\" width=\"800\">\n</div>\n\nMost controls share the same functionality and design with the oscilloscope. The difference is with the trigger, where the trigger level is missing, on the contrary, the possibility to trigger on both edges (Both) has been added. Furthermore, the vertical gain and offset are missing because they do not make sense for the logic analyzer. Similarly, averaging mode, bit resolution, and FFT are also missing. \n\nDescription of control panels: \n- **Main panel** â€” located at the top right, basic modes (Run, Stop, Single) \n- **Trigger** â€” trigger settings \n- **Horizontal** â€” time base setting, resp. sampling frequency and memory \n- **Vertical** â€” selection of displayed channels \n- **Utils** â€” cursor control, data analysis (to be added later)\n\n### Voltmeter \nThe voltmeter is the last primary instrument of the EMBO oscilloscope. Its window is again divided into two main blocks. In the left block there is an interactive graph, under which the measured parameters and the cursor driver are displayed. In the right block there are 4 channel panels below them and below them is the main control panel (Settings).\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/screenshots/vm.png\" alt=\"EMBO vm\" width=\"800\">\n</div>\n\nDescription of adjustable parameters: \n- **Scale** â€” a constant by which the channel voltage is multiplied \n- **Average** â€” number of averaged values (1 means off) \n- **Plot Points** â€” the number of displayed points in the graph\n\nThe sampling frequency is fixed at 100 Sps. This is twice the frequency of the mains voltage. When averaging mode is off, the NPLC (Number of Power Line Cycles) value is 0.5. If we increase this value (by increasing the Average value), we will significantly suppress the induced 50 Hz noise. The higher the value of Average, resp. NPLC, the slower the voltmeter measures and the more noise is suppressed. The Average mode therefore increases the accuracy of constant signal measurements. \n\nIn addition to the main settings, you can use additional options from the top menu, which is similar to the oscilloscope. In addition to standard settings, such as interpolation or setting of measured parameters, for example, continuous recording to a file (CSV or TXT) can be switched on. The recorded signal will have a sampling frequency of 100 Sps.\n\n### Counter \nThe counter is a simple secondary device that can be operated in parallel with other devices. Slow mode (for signals up to approx. 1 MHz) and Fast mode (for signals from approx. 1 MHz) can be set. For slow signals, the measurement takes longer. If no signal is detected within 2 seconds, a timeout occurs. When used with an oscilloscope or logic analyzer at the maximum sampling frequency, DMA overload and sample dropping may occur.\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/screenshots/cntr.png\" alt=\"EMBO cntr\" width=\"250\">\n</div>\n\n### PWM Generator \nThe PWM signal generator exists with either 1 or 2 channels. In the case of 2 channels, it is a synchronous generator that can be used to control motors or to simulate signals from a quadrature encoder. The frequency is set the same for both channels. You can also set the shift separately for the channel and the offset of the second channel. Again, it should be emphasized that at high values of the sampling frequency of the oscilloscope, the measured signal may be adversely affected.\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/screenshots/pwm.png\" alt=\"EMBO pwm\" width=\"300\">\n</div>\n\n### Signal Generator \nThe signal generator is a secondary optional device that uses a 12-bit DAC. It is only available for those microcontrollers that have a DAC, such as the F303RE. The user can set the frequency, amplitude and offset. At high frequencies, the size of the output buffer is dynamically reduced, ie the samples per period (Size), so that the DAC manages to generate. The real frequency of the generated signal and the magnitude of the amplitude in millivolts are also shown. In Constant mode, the generator acts as a voltage source. \n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/screenshots/sgen.png\" alt=\"EMBO sgen\" width=\"450\">\n</div>\n\nOperation modes: \n- **Constant**\n- **Sine**\n- **Triangle**\n- **Sawtooth**\n- **Square**\n- **Noise**\n\n## 5. PC App Class Diagram\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/img/qt_classes_png_en.png\" alt=\"EMBO PC class diagram\" width=\"700\">\n</div>\n\n## 6. FW Block Diagram\n\n<div align=\"center\" margin=\"0\" padding=\"0\">\n<img src=\"https://raw.githubusercontent.com/parezj/EMBO/master/img/periferie_png_en.png\" alt=\"EMBO fw block diagram\" width=\"700\">\n</div>\n\n## 7. Used IP\n- **[SCPI Parser](https://github.com/j123b567/scpi-parser)**  (Jan Breuer)\n- **[FreeRTOS](https://www.freertos.org/)**\n- **[SEGGER SystemView](https://www.segger.com/products/development-tools/systemview/)**\n- **[QCustomPlot](https://www.qcustomplot.com/)**\n- **[FFTW3](http://www.fftw.org/)**\n- **[qBreakpad](https://github.com/buzzySmile/qBreakpad)**\n- **[Google Breakpad](https://github.com/google/breakpad)**\n- **[QSimpleUpdater](https://github.com/alex-spataru/QSimpleUpdater)**\n\n\n",
    "readme_length": 13768
  },
  {
    "name": "fan2go",
    "full_name": "markusressel/fan2go",
    "description": "A simple daemon providing dynamic fan speed control based on temperature sensors.",
    "stars": 303,
    "forks": 27,
    "language": "Go",
    "url": "https://github.com/markusressel/fan2go",
    "topics": [
      "daemon",
      "fan",
      "fan-speed",
      "fancontrol",
      "golang",
      "hacktoberfest",
      "pwm",
      "rpm",
      "temperature"
    ],
    "created_at": "2021-01-17T09:19:22Z",
    "updated_at": "2025-11-27T07:36:13Z",
    "homepage": "",
    "license": "GNU Affero General Public License v3.0",
    "readme": "<h1 align=\"center\">\n  <img src=\"screenshots/fan2go_icon.svg\" width=\"144\" height=\"144\" alt=\"fan2go icon\">\n  <br>\n  fan2go\n  <br>\n</h1>\n\n<h4 align=\"center\">A daemon to control the fans of your computer.</h4>\n\n<div align=\"center\">\n\n[![Programming Language](https://img.shields.io/badge/Go-00ADD8?logo=go&logoColor=white)]()\n[![Latest Release](https://img.shields.io/github/release/markusressel/fan2go.svg)](https://github.com/markusressel/fan2go/releases)\n[![License](https://img.shields.io/badge/license-AGPLv3-blue.svg)](/LICENSE)\n\n</div>\n\n<p align=\"center\"><img src=\"screenshots/graph.png\" width=90% alt=\"Screenshot of Pyrra\"></p>\n\n# Features\n\n* [x] Intuitive YAML based configuration\n* [x] Massive range of supported devices\n    * [x] lm-sensors (hwmon) based sensors and fans\n    * [x] Fans and temperature sensor on NVIDIA GPUs ([nvml](https://developer.nvidia.com/management-library-nvml))\n    * [x] File based fan/sensor for control/measurement of custom devices\n    * [x] Command based fan/sensor\n* [x] Per fan user-defined speed curves\n* [x] Fully customizable and composable curve definitions\n* [x] Works after resume from suspend\n* [x] **Stable** device paths after reboot\n* [x] Automatic analysis of fan properties, like:\n    * [x] RPM curve\n    * [x] minimum and maximum PWM\n* [x] Error notifications\n* [x] Prometheus exporter\n* [x] (optional) REST Api\n\n# UI\n\nfan2go is first and foremost a daemon that runs in the background. However, it also provides\na small set of CLI commands (see [CLI Commands](#cli-commands)) as well as an optional local HTTP API to allow\nexternal programs to interact with it. This API can be used to create UI clients or other tools.\n\n## UI Clients\n\n[fan2go-tui](https://github.com/markusressel/fan2go-tui) - (Official) Terminal UI for fan2go.\n\n[![asciicast](https://asciinema.org/a/612087.svg)](https://asciinema.org/a/612087)\n\n# How to use\n\nfan2go relies on [lm-sensors](https://github.com/lm-sensors/lm-sensors) to get both temperature and RPM sensor readings,\nas well as PWM controls, so you will have\nto [set it up first](https://wiki.archlinux.org/index.php/Lm_sensors#Installation).\n\n## Installation\n\n### Arch Linux ![](https://img.shields.io/badge/Arch_Linux-1793D1?logo=arch-linux&logoColor=white)\n\n```shell\nyay -S fan2go-git\n```\n\n<details>\n<summary>Community Maintained Packages</summary>\n\n### Nix OS ![](https://img.shields.io/badge/nixpkgs-5277C3?logo=nixos&logoColor=white)\n\n- Nix with [Flakes](https://nixos.wiki/wiki/Flakes):\n\n```shell\nnix profile install nixpkgs#fan2go\n```\n\n- Nix stable:\n\n```shell\nnix-env -f '<nixpkgs>' -iA fan2go\n```\n\n### Debian\n\nSee: https://github.com/johnwbyrd/fan2go-package/\n\n</details>\n\n### Manual\n\nDownload the latest release from GitHub:\n\n```shell\n# Install dependencies\nsudo pacman -S libnotify\n\ncurl -L -o fan2go https://github.com/markusressel/fan2go/releases/latest/download/fan2go-linux-amd64\nchmod +x fan2go\nsudo cp ./fan2go /usr/bin/fan2go\nfan2go -h\n```\n\nOr compile yourself:\n\n```shell\ngit clone https://github.com/markusressel/fan2go.git\ncd fan2go\nmake build\nsudo cp ./bin/fan2go /usr/bin/fan2go\nsudo chmod ug+x /usr/bin/fan2go\n```\n\n## Configuration\n\nThen configure fan2go by creating a YAML configuration file in **one** of the following locations:\n\n* `/etc/fan2go/fan2go.yaml` (recommended)\n* `/root/.fan2go/fan2go.yaml`\n* `./fan2go.yaml`\n\n```shell\nsudo mkdir /etc/fan2go\nsudo nano /etc/fan2go/fan2go.yaml\n```\n\nThe most important configuration options you need to define are the `fans:`, `sensors:` and `curves:` sections.\n\n### Fans\n\nUnder `fans:` you need to define a list of fan devices that you want to control using fan2go. To detect fans on your\nsystem run `fan2go detect`, which will print a list of devices exposed by the hwmon filesystem backend:\n\n```shell\n$ fan2go detect\n=========== hwmon: ============\n\n> Platform: nct6798-isa-0290\n Fans     Index  Channel  Label        RPM   PWM  Mode\n          1      1        hwmon4/fan1  0     153  Manual\n          2      2        hwmon4/fan2  1223  104  Manual\n          3      3        hwmon4/fan3  677   107  Manual\n Sensors   Index   Label    Value\n           1       SYSTIN   41000\n           2       CPUTIN   64000\n\n> Platform: amdgpu-pci-0031\n Fans     Index  Channel  Label        RPM   PWM  Mode\n          1      1        hwmon8/fan1  561   43   Manual\n Sensors   Index   Label      Value\n           1       edge       58000\n           2       junction   61000\n           3       mem        56000\n\n=========== nvidia: ===========\n\n> Device: nvidia-10DE2489-0800\n  Fans     Index  Label  PWM  RPM   Mode\n           1      Fan 1  36   1300  Auto\n           2      Fan 2  36   1298  Auto\n  Sensors  Index  Label        Value\n           1      Temperature  59000\n```\n\nThe `hwmon` fan index is based on device enumeration and is not stable for a given fan if hardware configuration\nchanges.\nThe Linux kernel hwmon channel is a better identifier for configuration as it is largely based on the fan headers\nin use.\n\nFan RPM, PWM, and temperature sensors are independent and Linux does not associate them automatically. A given PWM\nmay control more than one fan, and a fan may not be under the control of a PWM. By default, fan2go guesses and sets\nthe pwm channel number for a given fan to the fan's RPM sensor channel. You can override this in the config.\n\nFor `nvidia` devices, the fan index *is* stable.\n\nNote that it can happen that the hardware only gives limited control over a fan and it, for example,\nalways runs with at least 30% speed - even if in automatic mode the hardware may make it stop entirely\nif the temperature is low enough.\n\n#### HwMon\n\nTo use detected hwmon devices in your configuration, use the `hwmon` fan type:\n\n```yaml\n# A list of fans to control\nfans:\n  # A user defined ID.\n  # Used for logging only\n  - id: cpu\n    # The type of fan configuration, one of: hwmon | file\n    hwmon:\n      # A regex matching a controller platform displayed by `fan2go detect`, f.ex.:\n      # \"nouveau\", \"coretemp\", \"it8620\", \"corsaircpro-.*\" etc.\n      platform: nct6798\n      # The channel of this fan's RPM sensor as displayed by `fan2go detect`\n      rpmChannel: 1\n      # The pwm channel that controls this fan; fan2go defaults to same channel number as fan RPM\n      pwmChannel: 1\n    # Indicates whether this fan should never stop rotating, regardless of\n    # how low the curve value is\n    neverStop: true\n    # The curve ID that should be used to determine the\n    # speed of this fan\n    curve: cpu_curve\n```\n\n#### NVIDIA\n\nTo use detected NVIDIA GPUs in your configuration, use the `nvidia` fan type:\n\n```yaml\nfans:\n  - id: gpufan1\n    nvidia:\n      # A regex matching a nvidia device as displayed by `fan2go detect`\n      # the following matches all nvidia devices in your system\n      # (good enough if you only have one), otherwise you could\n      # also use nvidia-10DE2489-0800 or similar\n      device: nvidia\n      # The fan's index as shown by `fan2go detect`\n      index: 1\n    curve: gpu_curve\n\n  # same for the second fan\n  - id: gpufan2\n    nvidia:\n      device: nvidia\n      index: 2\n    curve: gpu_curve\n```\n\nNote that the fan speed can only be controlled for GPUs of the \"Maxwell\" generation\n(Geforce 9xx) and newer, and that reading the fan speed in RPM requires nvidia driver 565\nor newer.\n\n#### File\n\n```yaml\nfans:\n  - id: file_fan\n    file:\n      # Path to a file to get/set the PWM target for this fan\n      path: /tmp/file_fan\n      # Path to a file to read the current RPM value of this fan\n      rpmPath: /tmp/file_fan_rpm\n```\n\n```shell\n> cat /tmp/file_fan\n255\n> cat /tmp/file_fan_rpm\n3421\n```\n\n#### CMD\n\nPlease also make sure to read the section about\n[considerations for using the cmd sensor/fan](#using-external-commands-for-sensorsfans).\n\n```yaml\nfans:\n  - id: cmd_fan\n    cmd:\n      # Command to apply a new PWM value (0..255)\n      # Use \"%pwm%\" to specify where the target pwm value should be used within the arguments\n      setPwm:\n        exec: /usr/bin/some-program\n        args: [ \"--set\", \"%pwm%\" ]\n      # Command to retrieve the current PWM value (0..255)\n      getPwm:\n        exec: /usr/bin/nvidia-settings\n        args: [ \"-a\", \"someargument\" ]\n      # (optional) Command to retrieve the current RPM value\n      getRpm:\n        exec: /usr/bin/nvidia-settings\n        args: [ \"-a\", \"someargument\" ]\n```\n\n#### Advanced Options\n\nIf the automatic fan curve analysis doesn't provide a good enough estimation\nfor how the fan behaves, you can use the following configuration options (per fan definition)\nto correct it:\n\n```yaml\nfans:\n  - id: ...\n    ...\n    # (Optional) Override for the lowest PWM value at which the\n    # fan is able to maintain rotation if it was spinning previously.\n    minPwm: 30\n    # (Optional) Override for the lowest PWM value at which the\n    # fan will still be able to start rotating.\n    # Note: Settings this to a value that is too small\n    #       may damage your fans. Use at your own risk!\n    startPwm: 30\n    # (Optional) Override for the highest PWM value which still yields\n    # an increased rotational speed compared to lower values.\n    # Note: you can also use this to limit the max speed of a fan.\n    maxPwm: 255\n    # (Optional) Override for the PWM map used by fan2go for\n    # mapping the expected [0..255] value range to values actually supported by this fan.\n    # This can be used to compensate for fans with a very limited set of supported values\n    # (f.ex. off, low, high). If not set manually, fan2go will try to compute this mapping\n    # automatically during fan initialization. This process is not perfect though and may\n    # result in suboptimal fan control.\n    # Note: The values of the mapping must be strictly monotonically increasing. The Key-Set must \n    # be in [0..255] but may omit values. If keys are missing, fan2go will select a key that most \n    # closely matches the required target value (computed by the referenced curve) during operation.\n    pwmMap:\n      0: 0\n      64: 128\n      192: 255\n    # (Optional) Configuration options for sanity checks\n    sanityCheck:\n      # (Optional) Control the behavior of the \"pwmValueChangedByThirdParty\" sanity check\n      # This check is used to detect if the PWM value of a fan has changed between two consecutive\n      # control loop cycles, which is usually an indication that an external program is trying to control the fan\n      # at the same time as fan2go. This can lead to unexpected behavior and is usually not desired, so \n      # fan2go will log a warning if this happens.\n      pwmValueChangedByThirdParty:\n        # (Optional) Whether to enable this check or not\n        enabled: true\n```\n\n### Sensors\n\nUnder `sensors:` you need to define a list of temperature sensor devices that you want to monitor and use to adjust\nfanspeeds. Like with fans, you can find usable devices using `fan2go detect`.\n\n#### HwMon\n\n```yaml\n# A list of sensors to monitor\nsensors:\n  # A user defined ID, which is used to reference\n  # a sensor in a curve configuration (see below)\n  - id: cpu_package\n    # The type of sensor configuration, one of: hwmon | nvidia | file | cmd\n    hwmon:\n      # A regex matching a controller platform displayed by `fan2go detect`, f.ex.:\n      # \"coretemp\", \"it8620\", \"corsaircpro-*\" etc.\n      platform: coretemp\n      # The index of this sensor as displayed by `fan2go detect`\n      index: 1\n```\n\n#### NVIDIA\n\n```yaml\nsensors:\n  - id: gpu_temp\n    nvidia:\n      # A regex matching a nvidia device as displayed by `fan2go detect`\n      device: nvidia-10DE2489-0800\n      # The index of this sensor as displayed by `fan2go detect`\n      # (currently NVIDIA/nvml exposes only a single temperature sensor per GPU)\n      index: 1\n```\n\n#### File\n\n```yaml\nsensors:\n  - id: file_sensor\n    file:\n      # Path to the file containing sensor values\n      path: /tmp/file_sensor\n```\n\nThe file contains a value in milli-units, like f.ex. milli-degrees.\n\n```bash\n> cat /tmp/file_sensor\n10000\n```\n\n#### CMD\n\nPlease also make sure to read the section about\n[considerations for using the cmd sensor/fan](#using-external-commands-for-sensorsfans).\n\nJust like the `file` sensor, the command must output the sensor value in milli-units,\nlike f.ex. milli-degrees.\n\n```yaml\nsensors:\n  - id: cmd_fan\n    cmd:\n      # Path to the executable to run to retrieve the current sensor value\n      exec: /usr/bin/bash\n      # (optional) arguments to pass to the executable\n      args: [ '/home/markus/myscript.sh' ]\n```\n\n### Curves\n\nUnder `curves:` you need to define a list of fan speed curves, which represent the speed of a fan based on one or more\ntemperature sensors.\n\n#### Linear\n\nTo create a simple, linear speed curve, use a curve of type `linear`.\n\nThis curve type can be used with a min/max sensor value, where the min temp will result in a curve value of `0` and the\nmax temp will result in a curve value of `255`:\n\n```yaml\ncurves:\n  - id: cpu_curve\n    # The type of the curve, one of: linear | function\n    linear:\n      # The sensor ID to use as a temperature input\n      sensor: cpu_package\n      # Sensor input value (in degrees Celsius)\n      # at which the curve is at minimum speed\n      min: 40\n      # Sensor input value at which the curve is at maximum speed\n      max: 80\n```\n\nYou can also define the curve in multiple, linear sections using the `steps` parameter:\n\n```yaml\ncurves:\n  - id: cpu_curve\n    # The type of the curve\n    linear:\n      # The sensor ID to use as a temperature input\n      sensor: cpu_package\n      # Steps to define a section-wise defined speed curve function.\n      steps:\n        # Sensor value (in degrees Celsius) -> Speed (0-255)\n        - 40: 0\n        - 50: 50\n        - 80: 255\n```\n\n#### PID\n\nIf you want to get your hands dirty and use a PID based curve, you can use `pid`:\n\n```yaml\ncurves:\n  - id: pid_curve\n    pid:\n      sensor: cpu_package\n      setPoint: 60\n      p: -0.05\n      i: -0.005\n      d: -0.005\n```\n\nUnlike the other curve types, this one does not use the average of the sensor data\nto calculate its value, which allows you to create a completely custom behaviour.\nKeep in mind though that the fan controller may also be PID based (depending on the\nspecified `controlAlgorithm`) which would also affect how the curve is applied to the fan.\n\nSee: [PID controller on Wikipedia](https://en.wikipedia.org/wiki/Proportional%E2%80%93integral%E2%80%93derivative_controller)\nfor more information on what a PID controller is.\n\n#### Function\n\nTo create more complex curves you can combine existing curves using a curve of type `function`:\n\n```yaml\ncurves:\n  - id: case_avg_curve\n    function:\n      # Type of aggregation function to use, one of: minimum | maximum | average | delta | sum | difference\n      type: average\n      # A list of curve IDs to use\n      curves:\n        - cpu_curve\n        - mainboard_curve\n        - ssd_curve\n```\n\n### Example\n\nAn example configuration file including more detailed documentation can be found in [fan2go.yaml](/fan2go.yaml).\n\n### Verify your Configuration\n\nTo check whether your configuration is correct before actually running fan2go you can use:\n\n```shell\n> sudo fan2go config validate\n INFO  Using configuration file at: /etc/fan2go/fan2go.yaml\n SUCCESS  Config looks good! :)\n```\n\nor to validate a specific config file:\n\n```shell\n> fan2go -c \"./my_config.yaml\" config validate\n INFO  Using configuration file at: ./my_config.yaml\n WARNING  Unused curve configuration: m2_first_ssd_curve\n  ERROR   Validation failed: Curve m2_ssd_curve: no curve definition with id 'm2_first_ssd_curve123' found\n```\n\n## Using external commands for sensors/fans\n\nfan2go supports using external executables for use as both sensor input, as well as fan output (and rpm input). There\nare some considerations you should take into account before using this feature though:\n\n### Security\n\nSince fan2go requires root permissions to interact with lm-sensors, executables run by fan2go are also executed as root.\nTo prevent some malicious actor from taking advantage of this fan2go will only allow the execution of files that only\nallow the root user (UID 0) to modify the file.\n\n### Side effects\n\nRunning external commands repeatedly through fan2go can have unintended side effects. F.ex., on a laptop using hybrid\ngraphics, running `nvidia-settings` can cause the dedicated GPU to wake up, resulting in substantial increase in power\nconsumption while on battery. Also, fan2go expects to be able to update sensor values with a minimal delay, so using a\nlong running script or some network call with a long timeout could also cause problems. With great power comes great\nresponsibility, always remember that :)\n\n## Run\n\nAfter successfully verifying your configuration you can launch fan2go from the CLI and make sure the initial setup is\nworking as expected. Assuming you put your configuration file in `/etc/fan2go/fan2go.yaml` run:\n\n```shell\n> fan2go help\nfan2go is a simple daemon that controls the fans\non your computer based on temperature sensors.\n\nUsage:\n  fan2go [flags]\n  fan2go [command]\n\nAvailable Commands:\n  completion  Generate the autocompletion script for the specified shell\n  config      Configuration related commands\n  curve       Curve related commands\n  detect      Detect fans and sensors\n  fan         Fan related commands\n  help        Help about any command\n  sensor      Sensor related commands\n  version     Print the version number of fan2go\n\nFlags:\n  -c, --config string   config file (default is $HOME/.fan2go.yaml)\n  -h, --help            help for fan2go\n      --no-color        Disable all terminal output coloration\n      --no-style        Disable all terminal output styling\n  -v, --verbose         More verbose output\n\nUse \"fan2go [command] --help\" for more information about a command.\n> sudo fan2go\n```\n\nAlternatively you can specify the path to your configuration file like this:\n\n```shell\n> sudo fan2go -c /home/markus/my_fan2go_config.yaml\n```\n\n## As a Service\n\n### Systemd\n\nWhen installing fan2go using a package, it comes with a [systemd unit file](./fan2go.service). To enable it simply run:\n\n```shell\nsudo systemctl daemon-reload\nsudo systemctl enable --now fan2go\n# follow logs\njournalctl -u fan2go -f\n```\n\n> NOTE: If you want to use a config path that differs from the default one, make sure to edit the\n> unit file and point the `-c` flag to the correct path.\n\n## CLI Commands\n\nAlthough fan2go is a fan controller daemon at heart, it also provides some handy cli commands to interact with the\ndevices that you have specified within your config.\n\n### Fans interaction\n\n```shell\n> fan2go fan --id cpu speed 100\n\n> fan2go fan --id cpu speed\n255\n\n> fan2go fan --id cpu rpm\n546\n\n> fan2go fan --id cpu mode\nNo control, 100% all the time (0)\n\n> fan2go fan --id cpu mode auto\nAutomatic control by integrated hardware (2)\n```\n\n### Sensors\n\n```shell\n> fan2go sensor --id cpu_package\n46000\n```\n\n### Print fan curve data\n\nFor each newly configured fan **fan2go** measures its fan curve and stores it in a db for future reference. You can take\na look at this measurement using the following command:\n\n```shell\n> sudo fan2go fan --id cpu curve \ncpu\n                \n  Min PWM    30 \n  Start PWM  30 \n  Max PWM    255\n\nNo fan curve data yet...\n\n> sudo fan2go fan --id in_front curve \nnct6798 -> pwm2\n\n  Min PWM     0 \n  Start PWM   0\n  Max PWM     194\n\n 1994 â”¤                                                                          â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n 1900 â”¤                                                                       â•­â”€â”€â•¯\n 1805 â”¤                                                                  â•­â”€â”€â”€â”€â•¯\n 1711 â”¤                                                             â•­â”€â”€â”€â”€â•¯\n 1616 â”¤                                                        â•­â”€â”€â”€â”€â•¯\n 1522 â”¤                                                    â•­â”€â”€â”€â•¯\n 1427 â”¤                                               â•­â”€â”€â”€â”€â•¯\n 1333 â”¤                                          â•­â”€â”€â”€â”€â•¯\n 1238 â”¤                                    â•­â”€â”€â”€â”€â”€â•¯\n 1144 â”¤                               â•­â”€â”€â”€â”€â•¯\n 1049 â”¤                         â•­â”€â”€â”€â”€â”€â•¯\n  955 â”¤                   â•­â”€â”€â”€â”€â”€â•¯\n  860 â”¤             â•­â”€â”€â”€â”€â”€â•¯\n  766 â”¤       â•­â”€â”€â”€â”€â”€â•¯\n  671 â”¤ â•­â”€â”€â”€â”€â”€â•¯\n  577 â”¼â”€â•¯\n                                                    RPM / PWM\n```\n\n## Statistics\n\nfan2go has a prometheus exporter built in, which you can use to extract data over time. Simply enable it in your\nconfiguration and you are good to go:\n\n```yaml\nstatistics:\n  # Whether to enable the prometheus exporter or not\n  enabled: true\n  # The port to expose the exporter on\n  port: 9000\n```\n\nYou can then see the metrics on [http://localhost:9000/metrics](http://localhost:9000/metrics) while the fan2go daemon\nis\nrunning.\n\n## API\n\nfan2go comes with a built-in REST Api. This API can be used by third party tools to display (and in the future possibly\nmodify) the state of fans, sensors and curves within fan2go.\n\n```yaml\napi:\n  # Whether to enable the API or not\n  enabled: false\n  # The host to listen for connections\n  host: localhost\n  # The port to listen for connections\n  port: 9001\n```\n\n### Endpoints\n\nCurrently, this API is read-only and only provides REST endpoints. If there is demand for it, this might be expanded to\nalso support realtime\ncommunication via websockets.\n\n#### Fans\n\n| Endpoint    | Type | Description                                       |\n|-------------|------|---------------------------------------------------|\n| `/fan`      | GET  | Returns a list of all currently configured fans   |\n| `/fan/<id>` | GET  | Returns the fan with the given `id`, if it exists |\n\n#### Sensors\n\n| Endpoint       | Type | Description                                          |\n|----------------|------|------------------------------------------------------|\n| `/sensor`      | GET  | Returns a list of all currently configured sensors   |\n| `/sensor/<id>` | GET  | Returns the sensor with the given `id`, if it exists |\n\n#### Curves\n\n| Endpoint      | Type | Description                                         |\n|---------------|------|-----------------------------------------------------|\n| `/curve`      | GET  | Returns a list of all currently configured curves   |\n| `/curve/<id>` | GET  | Returns the curve with the given `id`, if it exists |\n\n# How it works\n\n## Device detection\n\nfan2go uses [gosensors](https://github.com/md14454/gosensors) to directly interact with lm-sensors.\n\n## Initialization\n\nTo properly control a fan which fan2go has not seen before, its speed curve is analyzed. This means\n\n* spinning down the fans to 0\n* slowly ramping up the speed and monitoring RPM changes along the way\n\n**Note that this takes approx. 8 1/2 minutes**, since we have to wait for the fan speed to settle before taking\nmeasurements. Measurements taken during this process will then be used to determine the lowest PWM value at which the\nfan is still running, as well as the highest PWM value that still yields a change in RPM.\n\nAll of this is saved to a local database (path given by the `dbPath` config option), so it is only needed once per fan\nconfiguration.\n\nTo reduce the risk of running the whole system on low fan speeds for such a long period of time, you can force fan2go to\ninitialize only one fan at a time, using the `runFanInitializationInParallel: false` config option.\n\nSome PWM controllers or fans may require more time to react to PWM changes. If fan2go is failing to characterize a fan,\nyou can try increasing the fan response delay by passing `--fan-response-delay <seconds>` to the `fan init` command or\nby setting `fanResponseDelay` in the config. The default value is 2 seconds.\n\n## Monitoring\n\nTemperature and RPM sensors are polled continuously at the rate specified by the `tempSensorPollingRate` config option.\n`tempRollingWindowSize`/`rpmRollingWindowSize` amount of measurements are always averaged and stored as the average\nsensor value.\n\n## Fan Controllers\n\nThe speed of a Fan is controlled using a combination of its curve, a control algorithm and the properties of\nthe fan controller itself.\n\nThe curve is used as the target value for the control algorithm to reach. The control algorithm then calculates the\nnext PWM value to apply to the fan to reach this target value. The fan controller then applies this PWM value to the\nfan, while respecting constraints like the minimum and maximum PWM values, as well as the `neverStop` flag.\n\n### Control Algorithms\n\nA control algorithm\nis a function that returns the next PWM value to apply based on the target value calculated by the curve. The simplest\ncontrol algorithm is the direct control algorithm, which simply forwards the target value to the fan.\n\n#### Direct Control Algorithm\n\nThe simplest control algorithm is the direct control algorithm. It simply forwards the curve value to the fan\ncontroller.\n\n```yaml\nfans:\n  - id: some_fan\n    ...\n    controlAlgorithm: direct\n```\n\nThis control algorithm can also be used to approach the curve value more slowly:\n\n```yaml\nfans:\n  - id: some_fan\n    ...\n    controlAlgorithm:\n      direct:\n        maxPwmChangePerCycle: 10\n```\n\n### PID Control Algorithm\n\nThe PID control algorithm uses a PID loop to approach the target value. The default\nconfiguration is pretty non-aggressive using the following values:\n\n| P     | I      | D       |\n|-------|--------|---------|\n| `0.3` | `0.02` | `0.005` |\n\nIf you don't like the default behaviour you can configure your own in the config:\n\n```yaml\nfans:\n  - id: some_fan\n    ...\n    controlAlgorithm:\n      pid:\n        p: 0.3\n        i: 0.02\n        d: 0.005\n```\n\nThe loop is advanced at a constant rate, specified by the `controllerAdjustmentTickRate` config option, which\ndefaults to `200ms`.\n\nSee [PID controller on Wikipedia](https://en.wikipedia.org/wiki/Proportional%E2%80%93integral%E2%80%93derivative_controller)\nfor more information on what a PID controller is.\n\n# FAQ\n\n## Why are my SATA HDD drives not detected?\n\n**TL;DR**: `modprobe drivetemp`\n\nWhile _lm-sensors_ doesn't provide temperature sensors of SATA drives by default, you can use the kernel module\n`drivetemp` to enable this. See [here](https://wiki.archlinux.org/title/Lm_sensors#S.M.A.R.T._drive_temperature)\n\n## WARNING: PWM of `<fan>` was changed by third party!\n\nIf you see this log message while running fan2go, fan2go detected a change of the PWM value for the given fan\nthat was not caused by fan2go itself. This usually means that fan2go is not the only program controlling the fan\nand something else (like f.ex. the mainboard or another fan control software) is also running and changing the speed\nof the fan, competing with fan2go. Since fan2go cannot figure out what other software is, you have to investigate\nthis yourself.\n\nAnother common reason this message can occur is when the driver of the fan in question does not actually support\nsetting the PWM directly and uses some kind of virtual PWM instead. This has been a problem mostly on AMD graphics\ncards but is probably not limited to them. See #64 for more detail.\n\nIf you are sure that this warning is not justified, you can disable this check on a per-fan basis.\nSee the `sanityCheck` section in the [fan configuration](#advanced-options) for more details.\n\n## My components are overheating during initialization, what can I do about this?\n\n**TL;DR**: Skip the initialization and configure your fans manually.\n\nThe initialization phase measures the RPM curve of each fan and tries to estimate the minimum and maximum\nboundaries. This can take quite a while though and can lead to overheating of components if they are\nunder load. To avoid this use the `minPwm` and `maxPwm` fan config options to set the boundaries yourself.\nThat way the initialization phase will be skipped and the control algorithm will start right away.\n\n# Dependencies\n\nSee [go.mod](go.mod)\n\n# Similar Projects\n\n* [nbfc-linux](https://github.com/nbfc-linux/nbfc-linux)\n* [thinkfan](https://github.com/vmatare/thinkfan)\n\n# License\n\n```\nfan2go\nCopyright (C) 2021  Markus Ressel\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see <https://www.gnu.org/licenses/>.\n```\n",
    "readme_length": 28521
  },
  {
    "name": "SnailHeater",
    "full_name": "ClimbSnail/SnailHeater",
    "description": "å¤šåŠŸèƒ½ç„Šå°ï¼ˆT12/JBC245ã€çƒ¤ç®±å›žæµç„Šã€åŠ çƒ­å°ã€é£Žæžªã€å¯è°ƒç”µæºã€ç®€æ˜“åŒé€šé“ç¤ºæ³¢å™¨ã€å‡½æ•°å‘ç”Ÿå™¨ã€é«˜ç²¾åº¦PWMè„‰å†²ï¼‰",
    "stars": 282,
    "forks": 48,
    "language": "C",
    "url": "https://github.com/ClimbSnail/SnailHeater",
    "topics": [],
    "created_at": "2021-09-25T12:21:54Z",
    "updated_at": "2025-11-24T07:22:45Z",
    "homepage": "",
    "license": "GNU General Public License v3.0",
    "readme": "# SnailHeaterç®€ä»‹\n\n#### _é¡¹ç›®å¼€æºçš„ç›®çš„ï¼Œåšä¸€æ¬¾é€‚åˆåˆ›å®¢çš„å¼€æºç„Šå°ï¼Œè®©è€å¤–ä¹Ÿèƒ½ç”¨ä¸Šä¸­å›½äººå¼€æºçš„ç„Šå°ï¼_\n\nå¤šåŠŸèƒ½ç„Šå°ï¼Œç®€ç§° __\"èœ—ç‰›å°\"__ ã€‚å¹³æ¿åŠ çƒ­å°ã€é£Žæžªã€çƒ™é“ã€å¯è°ƒç”µæºå¤šæŽ§ã€‚æœ¬é¡¹ç›®å†…æœ‰220Vé«˜åŽ‹ï¼ŒåŠ¡å¿…å°å¿ƒï¼ŒåŽæžœè‡ªè´Ÿã€‚\n\n* æœ¬é¡¹ç›®çš„åœ°å€ https://github.com/ClimbSnail/SnailHeater ï¼ˆgithubä¸ºæœ€æ–°èµ„æ–™ï¼‰\n* æˆ–è€… https://gitee.com/ClimbSnailQ/SnailHeater ï¼ˆgiteeéžæœ€æ–°ï¼Œä»…ä¸ºé¢„è§ˆï¼‰\n* ä¸Šä¸€ä»£æ—§ç‰ˆæŽ§åˆ¶å™¨é¡¹ç›® https://github.com/ClimbSnail/HeatPlatform_SMT ï¼ˆåº•æˆæœ¬æ–¹æ¡ˆï¼‰\n\næœ¬é¡¹ç›®ä½¿ç”¨ESP32S2æ¨¡ç»„å¼€å‘ã€‚åŒæ—¶æŽ§åˆ¶åŠ çƒ­æ¿ã€é£Žæžªã€çƒ™é“ã€æ•°å­—å¯è°ƒç”µæºå·¥ä½œ ã€‚æœ¬é¡¹ç›®å°†æŒç»­ä¼˜åŒ–ï¼Œä¸åšåŠåŠå­ã€‚ä¸ºé˜²æ­¢åŽæœŸå’¸é±¼å€’å–ï¼Œåªæä¾›ä¸€ä¸ªç®€å•çš„demoï¼Œä¸»çº¿æºç ä¸å¼€æºã€‚æä¾›å¤åˆ»æ‰€éœ€è¦çš„æ‰€æœ‰èµ„æ–™ï¼ˆGerberã€BOMã€ç»„è£…æ•™ç¨‹ç­‰ç­‰ï¼‰ã€‚ç›®å‰æœºå™¨åˆ·æœºåŽéœ€è¦æ¿€æ´»ç ï¼Œæ¿€æ´»ç çš„èŽ·å–è§„åˆ™å‚è€ƒäº¤æµç¾¤å†…æœ€æ–°åŠ¨æ€ã€‚å¦å¤–ï¼Œæœ¬äººä¹Ÿä¼šä¸å®šæœŸç”Ÿäº§æ•´æœºæˆå“å‡ºå”®ç»™æœ‰éœ€è¦çš„ç¾¤å‹ã€‚\n\nå¦å¤–ï¼šä¸ºäº†å……åˆ†åˆ©ç”¨ç¡¬ä»¶èµ„æºï¼Œå°†ä¼šé™„å¸¦åŒè·¯ç¤ºæ³¢å™¨ï¼ˆé‡‡æ ·2Mï¼‰ã€å‡½æ•°å‘ç”Ÿå™¨ã€é«˜ç²¾åº¦è„‰å†²è¾“å‡ºï¼ˆå¯ç”¨äºŽç‚¹ç„Šæœºï¼‰ã€‚é¢å¤–ä¼šæœ‰é”å±æ—¶é’Ÿç­‰ç­‰è¶…å¤šåŠŸèƒ½ã€‚\n\n## SnailHeaterç¡¬ä»¶æ”¯æŒ\n#### V2.0.Xç‰ˆæœ¬ç¡¬ä»¶ï¼ˆä¸€è½¦ï¼‰\n1. çƒ™é“æ”¯æŒT12ã€JBC245ï¼ˆä¸æ”¯æŒJBC210ï¼‰ï¼Œå†…ç½®ç”µæºè¾¾200WåŠŸçŽ‡ï¼Œä¸Žä¸»æŽ§ä¾›ç”µéš”ç¦»ã€‚\n2. æ”¯æŒ858Dç›¸å…³ç±»åž‹çš„é£Žæžªï¼ŒåŒæ—¶æŽ¥å£ä¹Ÿæ”¯æŒåŠ çƒ­æ¿ï¼ˆåˆ†ä½“å¼ï¼‰ï¼Œæœ€å¤§æ”¯æŒ2000Wã€‚å…±ä¸¤ä¸ªGX16æŽ¥å£ï¼Œæ”¯æŒé£ŽæžªåŠ çƒ­æ¿ç›²æ’ã€‚\n3. å¯è°ƒç”µæºï¼šå…¨æ•°å­—å¯è°ƒï¼Œè¾“å‡º0.8V~22.8Vä¹‹é—´çš„ç”µåŽ‹ï¼Œç”µæµå¯è¾¾5Aï¼ˆä¸æ”¯æŒæ’æµï¼‰ï¼Œ3Aä»¥ä¸Šè´Ÿè½½æ–‡æ³¢80mvå†…ã€‚å†…ç½®è½¯ä»¶è¿‡æµä¿æŠ¤ï¼ˆå¯è‡ªå®šä¹‰é˜ˆå€¼ï¼‰ï¼Œå†…ç½®ç¡¬ä»¶çŸ­è·¯ä¿æŠ¤ã€‚ï¼ˆç¡¬ä»¶çº¹æ³¢ä¸ç¨³å®šï¼Œå¾…ç»Ÿä¸€é€šçŸ¥ä¿®å¤æ–¹æ¡ˆï¼‰\n4. å†…ç½®NTCçŽ¯å¢ƒè¡¥å¿ã€‚å¦å¤–è¿˜è®¾ç½®å…¨å¥—æ¸©åº¦æ ¡å‡†æ–¹æ¡ˆã€‚ï¼ˆæ³¨ï¼šå¾…å›ºä»¶æ›´æ–°æ”¯æŒï¼‰\n5. ä½¿ç”¨TFT 1.69å¯¸240*280çš„tftå½©å±ï¼Œä½¿ç”¨LVGLåšç•Œé¢ï¼Œå¢žå¼ºæ“ä½œçš„åŠ¨ç”»æ•ˆæžœã€‚\n6. é…å¥—ä¸Šä½æœºè½¯ä»¶ï¼Œç”¨äºŽæ›´æ–°å›ºä»¶å’Œç»˜åˆ¶æ¸©æŽ§æ›²çº¿ç­‰ç­‰ã€‚\n7. DACè¾“å‡ºç”µè·¯ï¼Œç”¨ä½œå•è·¯å‡½æ•°å‘ç”Ÿå™¨ä½¿ç”¨ã€‚\n8. é«˜ç²¾åº¦PWMè¾“å‡ºæŽ¥å£ï¼Œå¯å¤–æŽ¥ç‚¹ç„Šæœºä½œä¸ºå¯è°ƒè„‰å†²ä¿¡å·ã€‚ï¼ˆç‚¹ç„Šæœºå¥—ä»¶å¼€æºåœ°å€ https://oshwhub.com/climbsnail/spotwelderï¼‰\n9. è®¾å¤‡ä¼‘çœ æ—¶ï¼Œå¸¦æœ‰å¤©æ°”æ—¶é’Ÿæ˜¾ç¤ºã€‚\n10. è¿˜ä¼šæœ‰æ›´å¤šçš„åŠŸèƒ½åŠ å…¥ã€‚ã€‚ã€‚ã€‚\n\n#### V2.5.Xç‰ˆæœ¬ç¡¬ä»¶ï¼ˆäºŒè½¦ï¼‰\n1. çƒ™é“æ”¯æŒT12ã€JBC245ã€JBC210ç­‰ç­‰ï¼Œå†…ç½®åŒç”µåŽ‹ç”µæº24V/8Aã€12V6Aï¼Œä¸Žä¸»æŽ§ä¾›ç”µéš”ç¦»ã€‚\n2. æ”¯æŒ858Dç›¸å…³ç±»åž‹çš„é£Žæžªï¼ŒåŒæ—¶æŽ¥å£ä¹Ÿæ”¯æŒåŠ çƒ­æ¿ï¼ˆåˆ†ä½“å¼ï¼‰ï¼Œæœ€å¤§æ”¯æŒ2000Wã€‚å…±ä¸¤ä¸ªGX16æŽ¥å£ï¼Œæ”¯æŒé£ŽæžªåŠ çƒ­æ¿ç›²æ’ã€‚\n3. å¯è°ƒç”µæºï¼šå…¨æ•°å­—ç”µåŽ‹ç”µæµå¯è°ƒï¼Œè¾“å‡º0.8V~22.8Vä¹‹é—´çš„ç”µåŽ‹ï¼Œç”µæµå¯è¾¾0~5Aï¼Œ1Aä»¥ä¸Šè´Ÿè½½æ–‡æ³¢25mvã€‚å†…ç½®è½¯ä»¶è¿‡æµä¿æŠ¤ï¼ˆå¯è‡ªå®šä¹‰é˜ˆå€¼ï¼‰ï¼Œå†…ç½®ç¡¬ä»¶çŸ­è·¯ä¿æŠ¤ã€‚\n4. å†…ç½®NTCçŽ¯å¢ƒè¡¥å¿ã€‚å¦å¤–è¿˜è®¾ç½®å…¨å¥—æ¸©åº¦æ ¡å‡†æ–¹æ¡ˆã€‚ï¼ˆæ³¨ï¼šå¾…å›ºä»¶æ›´æ–°æ”¯æŒï¼‰\n5. ä½¿ç”¨TFT 1.69å¯¸240*280çš„tftå½©å±ï¼Œä½¿ç”¨LVGLåšç•Œé¢ï¼Œå¢žå¼ºæ“ä½œçš„åŠ¨ç”»æ•ˆæžœã€‚\n6. é…å¥—ä¸Šä½æœºè½¯ä»¶ï¼Œç”¨äºŽæ›´æ–°å›ºä»¶å’Œç»˜åˆ¶æ¸©æŽ§æ›²çº¿ç­‰ç­‰ã€‚\n7. DACè¾“å‡ºç”µè·¯ï¼Œç”¨ä½œå•è·¯å‡½æ•°å‘ç”Ÿå™¨ä½¿ç”¨ã€‚\n8. é«˜ç²¾åº¦PWMè¾“å‡ºæŽ¥å£ï¼Œå¯å¤–æŽ¥ç‚¹ç„Šæœºä½œä¸ºå¯è°ƒè„‰å†²ä¿¡å·ã€‚ï¼ˆç‚¹ç„Šæœºå¥—ä»¶å¼€æºåœ°å€ https://oshwhub.com/climbsnail/spotwelderï¼‰\n9.  åŽæœŸå°†ä¼šæ·»åŠ ä¼‘çœ åŠŸèƒ½ï¼Œå¯ç”¨äºŽæ˜¾ç¤ºå¤©æ°”ã€ç›‘æŽ§\n10. è¿˜ä¼šæœ‰æ›´å¤šçš„åŠŸèƒ½åŠ å…¥ã€‚ã€‚ã€‚ã€‚\n\n# äº¤æµç¾¤\nåŠ å…¥èœ—ç‰›å°è®¨è®ºqqç¾¤`148563337`æˆ–`686756592`ã€‚ä¸€èµ·DIYæˆ–è€…è·Ÿå›¢ã€‚\n\n# PCBå±•ç¤ºï¼ˆV2.6.xç‰ˆæœ¬ï¼‰\n##### å½“å‰æµ‹è¯•å›¾\n![SnailHeater](Images/SnailHeateré•‚ç©º.png)\n![SnailHeater](Images/ä¸»æœºæˆå“2.jpg)\n\n##### å±å¹•ç‰ˆ\n![SnailHeater](Images/å±å¹•ç‰ˆæ­£é¢.jpg)\n\n![SnailHeater](Images/å±å¹•ç‰ˆå’ŒæŽ¥çº¿æ¿æŽ¥çº¿1.jpg)\n\n##### ä¸»æŽ§åˆ¶æ¿\n![SnailHeater](Images/æ ¸å¿ƒæ¿åé¢æ¸²æŸ“å›¾.png)\n\n##### é©±åŠ¨æ¿\n![SnailHeater](Images/é©±åŠ¨æ¿æ­£é¢.jpg)\n\n##### æŽ¥çº¿æ¿\n![SnailHeater](Images/æ‹“å±•æ¿åé¢æ¸²æŸ“å›¾ï¼ˆç»„è£…æœ€åŽæ‰ç„Šèˆªç©ºå¤´ï¼‰.png)\n\n##### åˆ·æœºå·¥å…·\n![SnailHeater](Images/é…å¥—åˆ·æœºå·¥å…·.png)\n\n# æŽ¨èçº¿åºï¼ˆè¯¦ç»†è¯·å‚è€ƒç»„è£…æ•™ç¨‹ï¼‰\n\n![T12æŽ¥çº¿å›¾](Images/T12æŽ¥çº¿å›¾.png)\n![JBC245æŽ¥çº¿å›¾](Images/JBC245æŽ¥çº¿å›¾.png)\n\n![JBC210æŽ¥çº¿å›¾](Images/JBC210æŽ¥çº¿å›¾.png)\n\n![858DæŽ¥çº¿å›¾](Images/858DæŽ¥çº¿å›¾.png)\n\n***\n\n### åŠ çƒ­å°æœ¬ä½“ï¼ˆå¯å‚è€ƒé£Žæžªæ‰‹æŸ„æŽ¥çº¿å›¾ï¼‰\n![SnailHeater](Images/åŠ çƒ­æ¿æ­£é¢.jpg)\n![SnailHeater](Images/åŠ çƒ­æ¿èƒŒé¢.jpg)\n\næ³¨ï¼šSnailHeateråŠ çƒ­å°ä¸Žçƒ­é£Žæžªçš„ç‰©ç†æŽ¥å£ä¸€è‡´ï¼Œåªæ˜¯å°‘äº†é£Žæ‰‡å’Œå¼€å…³ä¿¡å·ï¼Œä½†æ•´ä½“å’Œçƒ­é£Žæžªçš„çº¿åºæ˜¯ä¸€è‡´çš„ã€‚çƒ­é£Žæžª\\åŠ çƒ­å°ä¸ºAC220ä¾›ç”µï¼Œæ•…æŽ¥å£å°†ä¼šæœ‰é«˜åŽ‹è¾“å‡ºï¼Œè¯·å‹¿åœ¨é€šç”µçŠ¶æ€ä¸‹è§¦æ‘¸èˆªç©ºå¤´çš„`å†…é’ˆ`ã€‚\n\nSnailHeaterçš„åŠ çƒ­å°å¯ä»¥ä½¿ç”¨ç¾¤å†…å®šåˆ¶çš„å‘çƒ­æ¿ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨æ·˜å®çŽ°æˆçš„é“¸é“å‘çƒ­æ¿ï¼Œç”šè‡³å¯ä»¥ä½¿ç”¨å»‰ä»·çš„PTCå‘çƒ­æ¿ã€‚å‰ææ˜¯éƒ½éœ€è¦æœ‰çƒ­ç”µå¶æµ‹æ¸©ã€‚\n\n### binæ–‡ä»¶è·¯å¾„\n* .platformio\\packages\\framework-arduinoespressif32\\variants\\adafruit_feather_esp32s2\n* .platformio\\packages\\framework-arduinoespressif32\\tools\\partitions\\boot_app0.bin\n\n### æ›´å¤šå†…å®¹ä¹‹åŽè¡¥å……\nç›®å‰å¤„äºŽå¼€å‘é˜¶æ®µï¼ŒæŒç»­å®Œå–„ã€‚\n\nè°ƒè¯•è¿‡ç¨‹ä¸­æŠ¥é”™å®šä½ä»£ç ï¼š`xtensa-esp32s2-elf-addr2line -pfiaC -e å›ºä»¶å.elf Backtraceåœ°å€ä¿¡æ¯`\n\nplatformIOæ¨¡æ‹Ÿå™¨ https://github.com/lvgl/lv_platformio\n\nåº”ç”¨å›¾æ ‡(128*128)ï¼šå¯ä»¥ä¸‹è½½é˜¿é‡ŒçŸ¢é‡å›¾ https://www.iconfont.cn/\n\n\n\n### è‡´è°¢\n* T12ç”µè·¯è®¾è®¡å‚è€ƒ https://github.com/wagiminator/ATmega-Soldering-Station\n* PWMå®žçŽ°DACç”µè·¯è®¾è®¡ https://www.elecfans.com/d/590628.html\n* PWMè½¬DACå¦‚ä½•å®žçŽ° https://blog.csdn.net/shileiwu0505/article/details/124053815\n* è¿æ”¾çš„ç”µæµæ£€æµ‹ https://blog.csdn.net/qq997758497/article/details/79374599\n* ä¿®æ”¹åˆ†åŒºè¡¨æ•°æ® https://www.bilibili.com/read/mobile?id=23149188\n* ç¤ºæ³¢å™¨ä»£ç  https://www.bilibili.com/read/mobile?id=23076062\n* lvglçš„çŽ°å®žä¸­åŽŸç† https://blog.csdn.net/weixin_49947521/article/details/132866129\n* æŽ¥åœ°è®¾è®¡ https://blog.csdn.net/sinat_15677011/article/details/126088471\n* UIè°ƒè‰² https://colorhunt.co\n* UIè°ƒè‰² https://aicolors.co/\n* UIè°ƒè‰² https://color.adobe.com/zh/create/color-wheel\n* ESP32ä¸ŽESP32S2çš„ADCå¿ƒå¾— https://blog.csdn.net/qq_42437017/article/details/127796716\n* ä½¿ç”¨`figma`ç»˜åˆ¶UIå›¾\n* ESP32ä¸ŽESP32S2çš„ADCå¿ƒå¾— https://blog.csdn.net/qq_42437017/article/details/127796716\n* åœ¨çº¿ éžå¯¹ç§°åŠ å¯†æµ‹è¯•å·¥å…·ï¼Œ è®°ä½ï¼šç”¨ç§é’¥åŠ å¯†ï¼Œå…¬é’¥è§£å¯†!!! http://tool.chacuo.net/cryptrsapubkey",
    "readme_length": 3734
  },
  {
    "name": "nanoFramework.IoT.Device",
    "full_name": "nanoframework/nanoFramework.IoT.Device",
    "description": "ðŸ“¦ This repo includes .NET nanoFramework implementations for various sensors, chips, displays, hats and drivers",
    "stars": 281,
    "forks": 116,
    "language": "C#",
    "url": "https://github.com/nanoframework/nanoFramework.IoT.Device",
    "topics": [
      "csharp",
      "dotnet",
      "drivers",
      "hacktoberfest",
      "i2c",
      "iot",
      "lcd",
      "led",
      "pwm",
      "screen",
      "sensor",
      "servo-motor",
      "spi"
    ],
    "created_at": "2021-04-27T15:24:33Z",
    "updated_at": "2025-11-26T14:12:32Z",
    "homepage": "https://www.nanoframework.net",
    "license": "MIT License",
    "readme": "[![#yourfirstpr](https://img.shields.io/badge/first--timers--only-friendly-blue.svg)](https://github.com/nanoframework/.github/blob/main/CONTRIBUTING.md) [![Discord](https://img.shields.io/discord/478725473862549535.svg?logo=discord&logoColor=white&label=Discord&color=7289DA)](https://discord.gg/gCyBu8T)\r\n\r\n![nanoFramework logo](https://github.com/nanoframework/Home/blob/main/resources/logo/nanoFramework-repo-logo.png)\r\n\r\n-----\r\n\r\n### English | [ä¸­æ–‡](README.zh-cn.md)\r\n\r\n-----\r\n\r\n# Welcome to the **nanoFramework** IoT.Device Library repository!\r\n\r\nThis repository contains bindings which can be sensors, small screen and anything else that you can connect to your nanoFramework chip!\r\n\r\nMost of the bindings have been migrated from [.NET IoT repository](https://github.com/dotnet/iot/tree/main/src/devices). Not all the bindings make sense to migrate to .NET nanoFramework, so the effort of migration has been placed into devices that can work with .NET nanoFramework. Please note as well that some devices have been migrated without been tested, so they main contain problems.\r\n\r\n## List of devices\r\n\r\n<devices>\r\n\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Uln2003.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Uln2003/) [28BYJ-48 Stepper Motor 5V 4-Phase 5-Wire & ULN2003 Driver Board](devices/Uln2003)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.A4988.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.A4988/) [4-Wire stepper motor & A4988 driver board](devices/A4988)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Drv8825.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Drv8825/) [4-Wire stepper motor & Drv8825 driver board](devices/Drv8825)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.AD5328.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.AD5328/) [AD5328 - Digital to Analog Convertor](devices/AD5328)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Seesaw.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Seesaw/) [Adafruit Seesaw - extension board (ADC, PWM, GPIO expander)](devices/Seesaw)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Adc128D818.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Adc128D818/) [ADC128D818 - Analog to Digital Converter](devices/Adc128D818)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ads1115.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ads1115/) [ADS1115 - Analog to Digital Converter](devices/Ads1115)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Adxl343.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Adxl343/) [ADXL343 - Accelerometer](devices/Adxl343)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Adxl345.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Adxl345/) [ADXL345 - Accelerometer](devices/Adxl345)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Adxl357.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Adxl357/) [ADXL357 - Accelerometer](devices/Adxl357)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ags01db.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ags01db/) [AGS01DB - MEMS VOC Gas Sensor](devices/Ags01db)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ahtxx.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ahtxx/) [AHT10/15/20 - Temperature and humidity sensor modules](devices/Ahtxx)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ak8963.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ak8963/) [AK8963 - Magnetometer](devices/Ak8963)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Am2320.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Am2320/) [AM2320 - Temperature and Humidity sensor](devices/Am2320)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Amg88xx.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Amg88xx/) [AMG8833/AMG8834/AMG8853/AMG8854 Infrared Array Sensor Family](devices/Amg88xx)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Apa102.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Apa102/) [APA102 - Double line transmission integrated control LED](devices/Apa102)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.At24cxx.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.At24cxx/) [AT24C32/AT24C64/AT24C128/AT24C256 family of I2C EEPROM](devices/At24cxx)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Axp192.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Axp192/) [AXP192 - Enhanced single Cell Li-Battery and Power System Management IC](devices/Axp192)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Bh1745.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Bh1745/) [Bh1745 - RGB Sensor](devices/Bh1745)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Bh1750fvi.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Bh1750fvi/) [BH1750FVI - Ambient Light Sensor](devices/Bh1750fvi)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Bmm150.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Bmm150/) [Bmm150 - Magnetometer](devices/Bmm150)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Bmp180.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Bmp180/) [BMP180 - barometer, altitude and temperature sensor](devices/Bmp180)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Bmxx80.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Bmxx80/) [BMP280/BME280/BME680 Device Family](devices/Bmxx80)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Bno055.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Bno055/) [BNO055 - inertial measurement unit](devices/Bno055)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Bq2579x.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Bq2579x/) [BQ2579x/BQ25792/BQ25798 - Buck-boost battery charger](devices/Bq2579x)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Button.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Button/) [Button](devices/Button)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Buzzer.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Buzzer/) [Buzzer - Piezo Buzzer Controller](devices/Buzzer)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ccs811.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ccs811/) [CCS811 Gas sensor](devices/Ccs811)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.CharacterLcd.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.CharacterLcd/) [Character LCD (Liquid Crystal Display)](devices/CharacterLcd)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Charlieplex.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Charlieplex/) [Charlieplex Segment binding](devices/Charlieplex)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Chsc6540.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Chsc6540/) [CHSC6540 - Touch screen controller](devices/Chsc6540)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Dac63004.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Dac63004/) [DAC63004/DAC63004W - Ultra-low-power quad-channel 12-bit smart DAC with IÂ²C, SPI and PWM](devices/Dac63004)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.DCMotor.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.DCMotor/) [DC Motor Controller](devices/DCMotor)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.DhcpServer.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.DhcpServer/) [DHCP Server](devices/DhcpServer)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Dhtxx.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Dhtxx/) [DHT10/DHT11/DHT12/DHT21/DHT22 - Digital-Output Relative Humidity & Temperature Sensor Module](devices/Dhtxx)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Dhtxx.Esp32.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Dhtxx.Esp32/) [DHT10/DHT11/DHT12/DHT21/DHT22 for Esp32 using RMT - Digital-Output Relative Humidity & Temperature Sensor Module](devices/Dhtxx.Esp32)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.LiquidLevel.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.LiquidLevel/) [Digital liquid level switch](devices/LiquidLevel)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.DnsServer.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.DnsServer/) [DNS Server](devices/DnsServer)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ds1302.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ds1302/) [DS1302 - Realtime Clock](devices/Ds1302)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ds1621.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ds1621/) [Ds1621 - 1-Wire Digital Thermometer with Programmable Resolution](devices/Ds1621)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ds18b20.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ds18b20/) [Ds18b20 - Temperature Sensor](devices/Ds18b20)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.ePaper.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.ePaper/) [ePaper drivers for .NET nanoFramework](devices/ePaper)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ft6xx6x.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ft6xx6x/) [Ft6xx6x/Ft6336GU - Touch screen controller](devices/Ft6xx6x)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.AtModem.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.AtModem/) [Generic AT Modem SIM800 and SIM7070, SIM7080, SIM7090 - Dual Mode Wireless Module CatM, LTE modems](devices/AtModem)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.ShiftRegister.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.ShiftRegister/) [Generic shift register](devices/ShiftRegister)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Common.GnssDevice.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Common.GnssDevice/) [Global Navigation Satellite System Device NMEA 0183 - Including Generic Serial Module with GPS, GNSS, BeiDou - NEO6-M, NEO-M8P-2, NEO-M9N from u-blox, ATGM336H, Minewsemi, ZED-F9P, ZOE-M8Q, SAM-M8Q, SARA-R5 and many many more](devices/GnssDevice)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Hcsr04.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Hcsr04/) [HC-SR04 - Ultrasonic Ranging Module](devices/Hcsr04)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Hcsr04.Esp32.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Hcsr04.Esp32/) [HC-SR04 for ESP32 with RMT - Ultrasonic Ranging Module](devices/Hcsr04.Esp32)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Hcsr501.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Hcsr501/) [HC-SR501 - PIR Motion Sensor](devices/Hcsr501)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Hdc1080.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Hdc1080/) [Hdc1080 - temperature and humidity sensor](devices/Hdc1080)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ld2410.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ld2410/) [HLK-LD2410 24Ghz Human Presence Radar Sensor Module](devices/Ld2410)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Hmc5883l.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Hmc5883l/) [HMC5883L - 3 Axis Digital Compass](devices/Hmc5883l)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Hts221.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Hts221/) [HTS221 - Capacitive digital sensor for relative humidity and temperature](devices/Hts221)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Hx711.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Hx711/) [Hx711 (M5Stack WEIGHT)](devices/Hx711)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ina219.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ina219/) [INA219 - Bidirectional Current/Power Monitor](devices/Ina219)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Multiplexing.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Multiplexing/) [Iot.Device.Multiplexing](devices/Multiplexing)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Common.NumberHelper.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Common.NumberHelper/) [Iot.Device.NumberHelper](devices/NumberHelper)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Common.WeatherHelper.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Common.WeatherHelper/) [Iot.Device.WeatherHelper](devices/WeatherHelper)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ip5306.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ip5306/) [IP5306 - Power management](devices/Ip5306)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.KeyMatrix.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.KeyMatrix/) [Key Matrix](devices/KeyMatrix)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.SparkFunLcd.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.SparkFunLcd/) [LCD library for SparkFun RGB Serial Open LCD display (sizes 20x4 or 16x2) with I2C connection](devices/SparkFunLcd)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.LidarLiteV3.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.LidarLiteV3/) [LidarLiteV3 - LIDAR Time of Flight Sensor](devices/LidarLiteV3)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Lis2Mdl.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Lis2Mdl/) [LIS2MDL - Ultra-low-power, high-performance 3-axis digital magnetic sensor](devices/Lis2Mdl)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Lm75.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Lm75/) [LM75 - Digital Temperature Sensor](devices/Lm75)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Lp3943.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Lp3943/) [Lp3943 LED driver](devices/Lp3943)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Lps22Hb.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Lps22Hb/) [LPS22HB - MEMS nano pressure sensor: 260-1260 hPa absolute digital output barometer](devices/Lps22Hb)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Lps25h.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Lps25h/) [LPS25H - Piezoresistive pressure and thermometer sensor](devices/Lps25h)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Lsm9Ds1.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Lsm9Ds1/) [LSM9DS1 - 3D accelerometer, gyroscope and magnetometer](devices/Lsm9Ds1)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.AtomQrCode.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.AtomQrCode/) [M5Stack ATOM QR Code reader](devices/AtomQrCode)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Max1704x.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Max1704x/) [MAX1704x/MAX17043/MAX17044/MAX17048/MAX17049 - Battery gauge](devices/Max1704x)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Max31856.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Max31856/) [Max31856 - cold-junction compensated thermocouple to digital converter](devices/Max31856)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Max31865.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Max31865/) [MAX31865 - Resistance Temperature Detector Amplifier](devices/Max31865)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Max44009.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Max44009/) [MAX44009 - Ambient Light Sensor](devices/Max44009)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Max7219.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Max7219/) [Max7219 (LED Matrix driver)](devices/Max7219)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mbi5027.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mbi5027/) [MBI5027 -- 16-bit shift register with error detection](devices/Mbi5027)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mcp23xxx.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mcp23xxx/) [Mcp23xxx/MCP23008/MCP23009/MCP23017/MCP23018 - I/O Expander device family](devices/Mcp23xxx)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mcp25xxx.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mcp25xxx/) [Mcp25xxx/MCP2515/MCP2565 device family - CAN bus](devices/Mcp25xxx)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mcp3xxx.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mcp3xxx/) [MCP3001/MCP3002/MCP3004/MCP3008/MCP3201/MCP3202/MCP3204/MCP3208/MCP3301/MCP3302/MCP3304 family of Analog to Digital Converters](devices/Mcp3xxx)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mcp3428.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mcp3428/) [Mcp3428 - Analog to Digital Converter (I2C)](devices/Mcp3428)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mcp7940xx.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mcp7940xx/) [Mcp7940xx/MCP79400/MCP79401/MCP79402 - I2C Real-Time Clock/Calendar with SRAM](devices/Mcp7940xx)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mcp960x.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mcp960x/) [MCP960X/MCP9600/MCP9601 - device family of cold-junction compensated thermocouple to digital converter](devices/Mcp960x)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mcp9808.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mcp9808/) [MCP9808 - Digital Temperature Sensor](devices/Mcp9808)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mfrc522.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mfrc522/) [MFRC522 - RFID reader](devices/Mfrc522)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mhz19b.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mhz19b/) [MH-Z19B CO2-Sensor](devices/Mhz19b)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mlx90614.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mlx90614/) [MLX90614 - Infra Red Thermometer](devices/Mlx90614)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Modbus.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Modbus/) [Modbus - Machine to machine communication protocol](devices/Modbus)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Relay4.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Relay4/) [Module and Unit 4 Relay - I2C relay](devices/Relay4)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mpr121.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mpr121/) [MPR121 - Proximity Capacitive Touch Sensor Controller](devices/Mpr121)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mpu9250.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mpu9250/) [MPU6050/MPU6500/MPU9250 - Gyroscope, Accelerometer, Temperature and Magnetometer (MPU9250 only)](devices/Mpu9250)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Mpu6886.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Mpu6886/) [Mpu6886 - accelerometer and gyroscope](devices/Mpu6886)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ms5611.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ms5611/) [Ms5611 in GY-63 module - temperature and pressure sensor](devices/MS5611)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.MulticastDns.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.MulticastDns/) [Multicast DNS](devices/MulticastDns)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Nrf24l01.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Nrf24l01/) [nRF24L01 - Single Chip 2.4 GHz Transceiver](devices/Nrf24l01)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Pca95x4.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Pca95x4/) [Pca95x4/PCA9534/PCA9534A/PCA9554/PCA9554A - I2C GPIO Expander](devices/Pca95x4)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Pcd8544.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Pcd8544/) [PCD8544 - 48 Ã— 84 pixels matrix LCD, famous Nokia 5110 screen](devices/Pcd8544)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Pcx857x.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Pcx857x/) [PCx857x/PCF8574/PCF8575/PCA8574/PCA8575 - NXP/TI GPIO expansion](devices/Pcx857x)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Pn5180.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Pn5180/) [PN5180 - RFID and NFC reader](devices/Pn5180)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Pn532.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Pn532/) [PN532 - RFID and NFC reader](devices/Pn532)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.QtrSensors.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.QtrSensors/) [QTR Sensors - Pololu QTR Reflectance Sensors](devices/QtrSensors)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.RotaryEncoder.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.RotaryEncoder/) [Quadrature Rotary Encoder](devices/RotaryEncoder)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.RotaryEncoder.Esp32.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.RotaryEncoder.Esp32/) [Quadrature Rotary Encoder (ESP32)](devices/RotaryEncoder.Esp32)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.RadioReceiver.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.RadioReceiver/) [Radio Receiver](devices/RadioReceiver)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.RadioTransmitter.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.RadioTransmitter/) [Radio Transmitter](devices/RadioTransmitter)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Rtc.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Rtc/) [Realtime Clock](devices/Rtc)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Card.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Card/) [RFID shared elements](devices/Card)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.RgbDiode.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.RgbDiode/) [RGB diode - PWM](devices/RgbDiode)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Scd4x.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Scd4x/) [SCD4x - Temperature & Humidity & CO2 Sensor](devices/Scd4x)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Scd30.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Scd30/) [Sensirion SCD30 Particulate Matter Sensor](devices/Scd30)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Sen5x.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Sen5x/) [Sensirion SEN5x series module](devices/Sen5x)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Sps30.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Sps30/) [Sensirion SPS30 Particulate Matter Sensor](devices/Sps30)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.ServoMotor.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.ServoMotor/) [Servo Motor](devices/ServoMotor)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Sht3x.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Sht3x/) [SHT3x/SHT30/SHT31/SHT35 - Temperature & Humidity Sensor](devices/Sht3x)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Sht4x.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Sht4x/) [Sht4x/SHT40/SHT41/SHT45 - Temperature & Humidity Sensor with internal heater](devices/Sht4x)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Shtc3.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Shtc3/) [SHTC3 - Temperature & Humidity Sensor](devices/Shtc3)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Si7021.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Si7021/) [Si7021 - Temperature & Humidity Sensor](devices/Si7021)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Sn74hc595.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Sn74hc595/) [SN74HC595 -- 8-bit shift register](devices/Sn74hc595)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ssd13xx.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ssd13xx/) [SSD13xx/SSD1306/SSD1327 & SSH1106 - OLED display family](devices/Ssd13xx)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.SwarmTile.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.SwarmTile/) [Swarm Tile](devices/SwarmTile)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.System.Buffers.Binary.BinaryPrimitives.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.System.Buffers.Binary.BinaryPrimitives/) [System.Buffers.Binary.BinaryPrimitives](devices/System.Buffers.Binary.BinaryPrimitives)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.System.Buffers.Helpers.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.System.Buffers.Helpers/) [System.Buffers.Helpers](devices/System.Buffers.Helpers)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.System.Device.Model.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.System.Device.Model/) [System.Device.Model - attributes for device bindings](devices/System.Device.Model)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.System.Diagnostics.Stopwatch.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.System.Diagnostics.Stopwatch/) [System.Diagnostics.Stopwatch and DelayHelper](devices/System.Diagnostics.Stopwatch)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.System.Drawing.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.System.Drawing/) [System.Drawing](devices/System.Drawing)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.System.Numerics.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.System.Numerics/) [System.Numerics](devices/System.Numerics)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Tcs3472x.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Tcs3472x/) [TCS3472x/TCS34721/TCS34723/TCS34725/TCS34727 Sensors](devices/Tcs3472x)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Tlc1543.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Tlc1543/) [TLC1543 - 10-bit ADC with 11 input channels](devices/Tlc1543)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Tm1637.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Tm1637/) [TM1637 - Segment Display](devices/Tm1637)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Tsl256x.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Tsl256x/) [TSL256x/TSL2560/TSL2561 - Illuminance sensor](devices/Tsl256x)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Vl53L0X.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Vl53L0X/) [VL53L0X - distance sensor](devices/Vl53L0X)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Vl6180X.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Vl6180X/) [Vl6180X - distance sensor](devices/Vl6180X)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ws28xx.Esp32.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ws28xx.Esp32/) [Ws28xx/WS2812B/WS2815B/WS2808/SK6812/Neo pixel for ESP32 using RMT - LED drivers](devices/Ws28xx.Esp32)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Ws28xx.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Ws28xx/) [Ws28xx/WS2812B/WS2815B/WS2808/SK6812/Neo pixel using SPI - LED drivers](devices/Ws28xx)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.XPT2046.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.XPT2046/) [XPT2046 - Touch screen controller](devices/XPT2046)\r\n* [![NuGet](https://img.shields.io/nuget/v/nanoFramework.Iot.Device.Yx5300.svg?label=NuGet&style=flat&logo=nuget)](https://www.nuget.org/packages/nanoFramework.Iot.Device.Yx5300/) [YX5200/YX5300 - MP3 Player](devices/Yx5300)\r\n</devices>\r\n\r\n## Folder Structure\r\n\r\n[/devices/](./devices/) contains devices that were cleaned up and should be working out of the box.\r\n\r\n[/src/devices_generated/](./src/devices_generated/) contains devices that were automatically ported from [the NET Core IoT Libraries devices](https://github.com/dotnet/iot/tree/main/src/devices). They might not work or compile at this point, but are a good starting point if you need support for one of the devices contained here but missing from the [/devices/](./devices/) folder.\r\n\r\n[/src/nanoFramework.IoT.Device.CodeConverter](./src/nanoFramework.IoT.Device.CodeConverter) contains the tool used to generate the devices from [the NET Core IoT Libraries devices](https://github.com/dotnet/iot/tree/main/src/devices).\r\n\r\nOther folders in [/src](./src) contain nanoFramework projects that you can reference when creating/updating devices with provide functionality such as a StopWatach, a DelayHelper, BinaryPrimitives or various System.Device.Model Attributes.\r\n\r\n## Contributing\r\n\r\n**Important:** If you plan to clean up the code in [/src/devices_generated/](./src/devices_generated/), please copy your work to the [/devices/](./devices/) folder as the content of [/src/devices_generated/](./src/devices_generated/) will be overwritten by the generator tool.\r\n\r\nPlease check the [detail list of tips and tricks](./tips-trick.md) to facilitate the migration. The generator takes care of some heavy lifting but there is always some manual adjustments needed.\r\n\r\nWe are using the following structure for the bindings:\r\n\r\n```text\r\n/devices\r\n  /Binding1\r\n    /samples\r\n      Binding1.Samples.nfproj\r\n      AssicateFile.cs\r\n      Program.cs\r\n    /test\r\n      BindingA.Test.nfproj\r\n      AssociatedTestFile.cs\r\n    Binding1.nfproj\r\n    Binding1.nuspec\r\n    version.json\r\n    OtherFiles.cs\r\n    OtherFiles.anythingelse\r\n    Readme.md\r\n```\r\n\r\n## Using the Code Converter\r\n\r\nThe Code Converter allows to facilitate migration of .NET Core/.NET 5.0 code into .NET nanoFramework. More information and how to [customize and run it here](./src/nanoFramework.IoT.Device.CodeConverter/README.md).\r\n\r\n## Porting a .NET nanoFramework binding to .NET IoT\r\n\r\nDid you know that with minimal efforts you can make a nanoFramework binding available for .NET IoT as well? More information and guidance on the steps to take, can be found in [this article](migrate-binding-to-dotnetiot.md).\r\n\r\n## Feedback and documentation\r\n\r\nFor documentation, providing feedback, issues and finding out how to contribute please refer to the [Home repo](https://github.com/nanoframework/Home).\r\n\r\nJoin our Discord community [here](https://discord.gg/gCyBu8T).\r\n\r\n## Credits\r\n\r\nThe list of contributors to this project can be found at [CONTRIBUTORS](https://github.com/nanoframework/Home/blob/main/CONTRIBUTORS.md).\r\n\r\n## License\r\n\r\nThe **nanoFramework** Class Libraries are licensed under the [MIT license](LICENSE.md).\r\n\r\n## Code of Conduct\r\n\r\nThis project has adopted the code of conduct defined by the Contributor Covenant to clarify expected behavior in our community.\r\nFor more information see the [.NET Foundation Code of Conduct](https://dotnetfoundation.org/code-of-conduct).\r\n\r\n### .NET Foundation\r\n\r\nThis project is supported by the [.NET Foundation](https://dotnetfoundation.org).\r\n",
    "readme_length": 37795
  },
  {
    "name": "CHIP_IO",
    "full_name": "xtacocorex/CHIP_IO",
    "description": "A CHIP IO library for Python: IO+PWM+SPWM+ADC+Utilities",
    "stars": 276,
    "forks": 62,
    "language": "C",
    "url": "https://github.com/xtacocorex/CHIP_IO",
    "topics": [],
    "created_at": "2016-02-23T00:54:52Z",
    "updated_at": "2025-08-25T09:42:02Z",
    "homepage": "",
    "license": "MIT License",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "Adafruit_Python_PCA9685",
    "full_name": "adafruit/Adafruit_Python_PCA9685",
    "description": "Python code to use the PCA9685 PWM servo/LED controller with a Raspberry Pi or BeagleBone black.",
    "stars": 271,
    "forks": 167,
    "language": "Python",
    "url": "https://github.com/adafruit/Adafruit_Python_PCA9685",
    "topics": [],
    "created_at": "2016-04-20T07:30:30Z",
    "updated_at": "2025-10-21T06:33:18Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "!! DEPRECATED !!\n================\n\nThis library is deprecated and has been archived. Please use the new library here:\nhttps://github.com/adafruit/Adafruit_CircuitPython_PCA9685\n\n# Adafruit Python PCA9685\nPython code to use the PCA9685 PWM servo/LED controller with a Raspberry Pi or BeagleBone black.\n\n## Installation\n\nTo install the library from source (recommended) run the following commands on a Raspberry Pi or other Debian-based OS system:\n\n    sudo apt-get install git build-essential python-dev\n    cd ~\n    git clone https://github.com/adafruit/Adafruit_Python_PCA9685.git\n    cd Adafruit_Python_PCA9685\n    sudo python setup.py install\n\nAlternatively you can install from pip with:\n\n    sudo pip install adafruit-pca9685\n\nNote that the pip install method **won't** install the example code.\n",
    "readme_length": 801
  },
  {
    "name": "CRServoF",
    "full_name": "CapnBry/CRServoF",
    "description": "CRSF to PWM Servo converter for STM32F103",
    "stars": 268,
    "forks": 67,
    "language": "C++",
    "url": "https://github.com/CapnBry/CRServoF",
    "topics": [],
    "created_at": "2021-05-19T00:54:20Z",
    "updated_at": "2025-11-24T22:50:37Z",
    "homepage": null,
    "license": "GNU General Public License v3.0",
    "readme": "## CRServoF - The CSRF serial protocol to PWM servo converter\n\nI wanted to create a small project to mess around with PWM servo output for ExpressLRS, and thought this might be of use for other people.\n\n[![YouTube Demo](https://img.youtube.com/vi/WrQQ0svOxig/hqdefault.jpg)](https://youtu.be/WrQQ0svOxig)\n\n### What it does\n\nIf you have a receiver that outputs CRSF serial protocol (ExpressLRS, Crossfire, Tracer) but want to directly drive servos without a flight controller, I guess you're in the right place. That's exactly what this does. Hook up a CRSF RX to UART2 and your servos to various pins of an STM32F103C8 \"blue pill\" board and away you go. Not much to it other than that.\n\n### Wiring and Flashing\n\nSee the wiki [Flashing and Wiring](https://github.com/CapnBry/CRServoF/wiki/Wiring)\n\n### Channel Mapping\n\nTo change the channel mapping, use the `OUTPUT_MAP[]` array at the top. These are 1-based channels from the CRSF output, so 1 is usually Roll, 2 is Pitch and so on. 5 is AUX1 up to 12 is AUX8 for ExpressLRS, or up to 16 AUX12 for Crossfire models. The default map is `[ Roll, Pitch, Throttle, Yaw, AUX2, AUX3, AUX4, AUX12 ]` for my radio setup. To invert the channel output, +100% becomes -100%, just use a negative number for the channel (e.g. -12 for AUX8 inverted).\n\n### Failsafe\n\nThe code has failsafe detection which happens if no channel packets are received for a short time (300ms currently). The default failsafe setting is to set CH1-4 to `1500, 1500, 988, 1500`, CH4-7 to hold their last position, and CH8 to stop putting out pulses. To change the failsafe behavior, modify the `OUTPUT_FAILSAFE[]` array with either the microseconds position to set on failsafe or `fsaNoPulses` (stop outputting PWM) or `fsaHold` (hold last received value).\n\n### Arming / Disarming\n\nCRServoF includes an optional feature to require an arming signal for other channels to be processed. To use this feature, include the buildflag `USE_ARMSWITCH`. CRServoF expects a \"high\" value (>1500us) on CH5 to arm. If disarmed, the failsafe values mentioned above will be sent, make sure that you use the correct values applicable to your use case.\n\n### VBAT\n\nThe code sends a BATTERY telemetry item back to the CRSF RX, using A0 as the input value. **You can not plug VBAT directly in**. The maximum input voltage is 3.3V so the voltage needs to be scaled down. The code expects a resistor divider `VBAT -- 8.2kohm -A0- 1.2kohm -- GND` with VBAT on one end, GND on the other, and A0 connected in the middle. That should be good up to 6S voltage if I did my math right. The voltage can be calibrated using the `VBAT_SCALE` define in the top of main.cpp, and different resistors can be used by changing the `VBAT_R1` and `VBAT_R2` defines.\n\n### ExpressLRS_via_BetaflightPassthrough\n\nThe serial UART will attempt to emulate a Betaflight CLI so ExpressLRS can flash the connected RX with yet another RC version. This works, I dunno, like 80% of the time? It is hard to get all the timing just right, but if it fails, you will likely need to repower the whole device because the RX is in the bootloader and probably at the wrong autobaud.\n\n\n\n\n",
    "readme_length": 3138
  },
  {
    "name": "stm32-ws2811-ws2812-ws2812b-ws281x-tim-pwm-dma-timer",
    "full_name": "MaJerle/stm32-ws2811-ws2812-ws2812b-ws281x-tim-pwm-dma-timer",
    "description": "WS2811, WS2812, WS2812B or compatible leds driver with STM32, TIM, PWM and DMA with minimum RAM required",
    "stars": 268,
    "forks": 55,
    "language": "C",
    "url": "https://github.com/MaJerle/stm32-ws2811-ws2812-ws2812b-ws281x-tim-pwm-dma-timer",
    "topics": [
      "dma",
      "led",
      "leds",
      "memory",
      "pwm",
      "stm32",
      "tim",
      "timer",
      "timers",
      "ws2811",
      "ws2812b"
    ],
    "created_at": "2018-06-03T17:35:04Z",
    "updated_at": "2025-11-21T01:44:18Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# WS2811 and WS2812B driver for STM32 with TIM, PWM and DMA\n\nThis application note aims to explain architecture and understanding how to develop driver for addressable LEDs, such as WS2811, WS2812, WS2812B or any other matching protocol specifications.\n\n## Table of Contents\n\nGithub supports ToC by default. It is available in the top-left corner of this document.\n\n## Abbreviations\n\n- DMA: Direct Memory Access controller in STM32\n- TIM: Timer peripheral, general or advanced, w/ or w/o external channel (output compare or input capture)\n- PWM: Pulse-Width-Modulation generated by TIM peripheral\n- HT: Half-Transfer Complete DMA event/flag\n- TC: Transfer Complete DMA event/flag\n- IRQ: Interrupt\n\n## Understanding LED protocol\n\nWS2811 and WS2812 protocol is specific one and has defined values:\n\n- Transfer rate is `800 kHz`, or `1.25us` pulse length for each bit\n- Transfer length is `24` pulses for each led, that's `30us` for one LED\n- Each logical bit (`1` or `0`) consists of high and low part, with different length\n- Reset pulse is needed prior updating led strip, to synchronize sequence\n\n![WS2811 & WS2812 & WS2812B LED protocol](https://raw.githubusercontent.com/MaJerle/stm32-ws2812b-tim-pwm-dma/master/docs/ws-protocol.svg?sanitize=true)\n\n> Minimum reset pulse length depends on WS281x device. Check datasheet for your particular unit. WS2812B says `> 50us`, while WS2811 says `> 280us`.\n\n## STM32 TIM and PWM\n\nSeveral STM32 timers (TIM) have support for Capture/Compare channels, connect to output pins, and being able to generate PWM pulses.\nAccording to protocol (also explained above), timer should be able to generate `3` type of pulses:\n\n- All-time zero to emulate reset pulse\n- Logical bit `1` with `~3/4` of time being high and low for the rest of the time\n- Logical bit `0` with `~1/4` of time being high and low for the rest of the time\n\nFurthemore, timer should be configured to generate update event for `800 kHz`, with auto-reload register (`TIMx->ARR`) set to `TIMx->ARR = timer_kernel_clock / 800000 - 1` and `prescaler = 0` for maximum resolution.\n\nTimer does not support a look-up table to know if logical bit `1` or `0` is next, hence we will utilize DMA feature to implement this task.\nTimer channel allows DMA requests to *load* next value to channel compare register, and effectively implement look-up table with data transfer from memory to timer peripheral.\n\n> [STM32 Timer Cookbook](https://www.st.com/resource/en/application_note/dm00236305-generalpurpose-timer-cookbook-for-stm32-microcontrollers-stmicroelectronics.pdf) is a great starting point to understand how timers work in STM32s.\n\n## STM32 DMA\n\nDMA controllers in STM32s support various operations, one of them being super handy for our WS LED driver, called *circular operation mode*.\n*Circular mode* will continuously transmit data from memory to peripheral (or, in general, can also go opposite direction) and periodically send *transfer-complete* or *half-transfer-complete* interrupts to the application.\n\n![STM32 DMA circular mode](https://raw.githubusercontent.com/MaJerle/stm32-ws2812b-tim-pwm-dma/master/docs/stm32-dma-circular.svg?sanitize=true)\n\nWe will use *HT* and *TC* events extensively, as they will be use to *prepare data* for next operations to transfer all bits for all leds.\nMore explained in the later sections.\n\nList of some useful STM32 DMA application notes\n\n- [AN4031 - Using the STM32F2, STM32F4 and STM32F7 Series DMA controller](https://www.st.com/resource/en/application_note/dm00046011-using-the-stm32f2-stm32f4-and-stm32f7-series-dma-controller-stmicroelectronics.pdf)\n- [AN2548 - Using the STM32F0/F1/F3/Gx/Lx Series DMA controller\n](https://www.st.com/resource/en/application_note/an2548-using-the-stm32f0f1f3gxlx-series-dma-controller-stmicroelectronics.pdf)\n- [AN5593 - How to use the GPDMA for STM32U575/585 microcontrollers](https://www.st.com/resource/en/application_note/an5593-how-to-use-the-gpdma-for-stm32u575585-microcontrollers-stmicroelectronics.pdf)\n- [AN5224 - STM32 DMAMUX: the DMA request router](https://www.st.com/resource/en/application_note/an5224-stm32-dmamux-the-dma-request-router-stmicroelectronics.pdf)\n- [STM32H7 DMA](https://www.st.com/content/ccc/resource/training/technical/product_training/group0/86/28/2f/08/4d/ad/49/61/STM32H7-System-Direct_memory_access_controller_DMA/files/STM32H7-System-Direct_memory_access_controller_DMA.pdf/_jcr_content/translations/en.STM32H7-System-Direct_memory_access_controller_DMA.pdf)\n\n## Memory requirement\n\nMemory requirement for one LED strip is split to:\n\n- `3 * leds_count` to store *read, green & blue* colors for each led\n- `1` working buffer with size of `2` leds (`48` elements), each element of `16/32` bits, depending on used TIM peripheral\n\n## Putting it all together\n\nTo put all together, application developer must:\n\n- Decide to use one timer with PWM generation support with one channel\n- Select one DMA channel/stream/dmamux (depends on STM32) for selected timer and channel\n- Put DMA in circular mode and enable HT and TC interrupts\n- Use HT and TC events to prepare data for next operation\n  - HT event is used to prepare data at the beginning of memory (part of memory just completed with transmision)\n  - TC event is used to prepare data for second part of memory\n\n> Examples are extensively commented and should provide necessary understanding for application development\n\n## Examples\n\nExamples can be used as reference code to implement your own LED driver with your own STM32.\n\n- Developed in [STM32CubeIDE](https://www.st.com/en/development-tools/stm32cubeide.html) for easier evaluation on STM32 boards\n- Supports CMake and VSCode development\n- Uses LL or HAL drivers\n- Demos for various STM32\n\n| STM32 family | Board name        | TIM & CH   | GPIO   | DMA settings                    |\n|--------------|-------------------|------------|--------|---------------------------------|\n| STM32G0xx    | `NUCLEO-G0B1RE`   | `TIM2 CH4` | `PA3`  | *`DMA2`, `Channel 5`*           |\n",
    "readme_length": 5993
  },
  {
    "name": "fan-control",
    "full_name": "wiiznokes/fan-control",
    "description": "Control your fans with different behaviors",
    "stars": 266,
    "forks": 19,
    "language": "Rust",
    "url": "https://github.com/wiiznokes/fan-control",
    "topics": [
      "fan-control",
      "fancontrol",
      "iced",
      "librefancontrol",
      "librehardwaremonitor",
      "libsensors",
      "linux",
      "pwm",
      "rust",
      "sensors"
    ],
    "created_at": "2023-08-04T02:29:01Z",
    "updated_at": "2025-12-01T12:03:50Z",
    "homepage": "",
    "license": "GNU General Public License v3.0",
    "readme": "<h1 align=\"center\">Fan Control</h1>\n\n<div>\n    <a href=\"https://flathub.org/apps/io.github.wiiznokes.fan-control\"><img align=center height=\"50\" alt=\"Download on Flathub\" src=\"https://flathub.org/api/badge?svg&locale=en\"/></a>&nbsp;&nbsp;\n    <a href=\"https://github.com/wiiznokes/fan-control/releases/latest\"><img align=center alt=\"Download on Github release\" src=\"https://img.shields.io/github/release/wiiznokes/fan-control.svg\"/></a>&nbsp;&nbsp;\n<div>\n\n## Features\n\n- Display sensors data on real time\n- Control fans based on custom behaviors\n- Save configuration\n- Multiplatform (Linux/Windows)\n\n![screenshot of Fan Control](https://media.githubusercontent.com/media/wiiznokes/fan-control/master/res/screenshots/app.png)\n\n## Usage\n\n- You can add items with the buttons on the right of the app.\n- To save a configuration, write a name in the \"Configuration name\" field, and click on the `+`.\n- To modify the value of a fan, you must select it in a `Control` item (the left column), select a `Behavior`, and activate the switch.\n\n## Installation\n\n### Windows\n\n1. Install Fan Control from [the release section](https://github.com/wiiznokes/fan-control/releases/latest)\n\n_The configuration files will be in [`C:\\Users\\wiiz\\AppData\\Roaming\\wiiznokes\\fan-control\\config`](file:///C:\\Users\\wiiz\\AppData\\Roaming\\wiiznokes\\fan-control\\config)._\n\n### Flatpak (Linux)\n\n1. [Install the required udev rules](./res/linux/udev_rules.md)\n2. Install fan-control from [Flathub](https://flathub.org/apps/io.github.wiiznokes.fan-control)\n\n_The configuration files will be in [`~/.var/app/io.github.wiiznokes.fan-control/config/fan-control/`](file://~/.var/app/io.github.wiiznokes.fan-control/config/fan-control/)._\n\n<ins>To ensure the application detects the maximum number of sensors, follow these steps</ins>\n\n1. Install `lm-sensors`  \n   For Debian-based systems, run: `sudo apt install lm-sensors`  \n   For Fedora-based systems, run: `sudo dnf install lm_sensors`\n2. Run `sudo sensors-detect` to detect available sensors\n\n## Troubleshooting\n\nSee [this file](./TROUBLESHOOTING.md).\n\n## Repo structure\n\n- [hardware](./hardware/README.md): define an abstraction around the hardware.\n- [data](./data/README.md): define structures used in the app (Node, Config), and there logic. Depend on [hardware](./hardware/README.md)\n- [ui](./ui/README.md): implement the UI. Depend on [data](./data/README.md) and [hardware](./hardware/README.md)\n- the app: integrate all this crates in one executable\n\n## Build\n\nSee instructions [here](./BUILD.md).\n\n## Contributing\n\nSee [this file](./CONTRIBUTING.md).\n",
    "readme_length": 2576
  },
  {
    "name": "FTC-Skystone-Dark-Angels-Romania-2020",
    "full_name": "chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020",
    "description": "NOTICE This repository contains the public FTC SDK for the SKYSTONE (2019-2020) competition season. If you are looking for the current season's FTC SDK software, please visit the new and permanent home of the public FTC SDK:  FtcRobotController repository  Welcome! This GitHub repository contains the source code that is used to build an Android app to control a FIRST Tech Challenge competition robot. To use this SDK, download/clone the entire project to your local computer.  Getting Started If you are new to robotics or new to the FIRST Tech Challenge, then you should consider reviewing the FTC Blocks Tutorial to get familiar with how to use the control system:        FTC Blocks Online Tutorial  Even if you are an advanced Java programmer, it is helpful to start with the FTC Blocks tutorial, and then migrate to the OnBot Java Tool or to Android Studio afterwards.  Downloading the Project If you are an Android Studio programmer, there are several ways to download this repo. Note that if you use the Blocks or OnBot Java Tool to program your robot, then you do not need to download this repository.  If you are a git user, you can clone the most current version of the repository:             git clone https://github.com/FIRST-Tech-Challenge/SKYSTONE.git  Or, if you prefer, you can use the \"Download Zip\" button available through the main repository page. Downloading the project as a .ZIP file will keep the size of the download manageable.  You can also download the project folder (as a .zip or .tar.gz archive file) from the Downloads subsection of the Releases page for this repository.  Once you have downloaded and uncompressed (if needed) your folder, you can use Android Studio to import the folder (\"Import project (Eclipse ADT, Gradle, etc.)\").  Getting Help User Documentation and Tutorials FIRST maintains online documentation with information and tutorials on how to use the FIRST Tech Challenge software and robot control system. You can access this documentation using the following link:        SKYSTONE Online Documentation  Note that the online documentation is an \"evergreen\" document that is constantly being updated and edited. It contains the most current information about the FIRST Tech Challenge software and control system.  Javadoc Reference Material The Javadoc reference documentation for the FTC SDK is now available online. Click on the following link to view the FTC SDK Javadoc documentation as a live website:        FTC Javadoc Documentation  Documentation for the FTC SDK is also included with this repository. There is a subfolder called \"doc\" which contains several subfolders:  The folder \"apk\" contains the .apk files for the FTC Driver Station and FTC Robot Controller apps. The folder \"javadoc\" contains the JavaDoc user documentation for the FTC SDK. Online User Forum For technical questions regarding the Control System or the FTC SDK, please visit the FTC Technology forum:        FTC Technology Forum  Release Information Version 5.5 (20200824-090813) Version 5.5 requires Android Studio 4.0 or later.  New features Adds support for calling custom Java classes from Blocks OpModes (fixes SkyStone issue #161). Classes must be in the org.firstinspires.ftc.teamcode package. Methods must be public static and have no more than 21 parameters. Parameters declared as OpMode, LinearOpMode, Telemetry, and HardwareMap are supported and the argument is provided automatically, regardless of the order of the parameters. On the block, the sockets for those parameters are automatically filled in. Parameters declared as char or java.lang.Character will accept any block that returns text and will only use the first character in the text. Parameters declared as boolean or java.lang.Boolean will accept any block that returns boolean. Parameters declared as byte, java.lang.Byte, short, java.lang.Short, int, java.lang.Integer, long, or java.lang.Long, will accept any block that returns a number and will round that value to the nearest whole number. Parameters declared as float, java.lang.Float, double, java.lang.Double will accept any block that returns a number. Adds telemetry API method for setting display format Classic Monospace HTML (certain tags only) Adds blocks support for switching cameras. Adds Blocks support for TensorFlow Object Detection with a custom model. Adds support for uploading a custom TensorFlow Object Detection model in the Manage page, which is especially useful for Blocks and OnBotJava users. Shows new Control Hub blink codes when the WiFi band is switched using the Control Hub's button (only possible on Control Hub OS 1.1.2) Adds new warnings which can be disabled in the Advanced RC Settings Mismatched app versions warning Unnecessary 2.4 GHz WiFi usage warning REV Hub is running outdated firmware (older than version 1.8.2) Adds support for Sony PS4 gamepad, and reworks how gamepads work on the Driver Station Removes preference which sets gamepad type based on driver position. Replaced with menu which allows specifying type for gamepads with unknown VID and PID Attempts to auto-detect gamepad type based on USB VID and PID If gamepad VID and PID is not known, use type specified by user for that VID and PID If gamepad VID and PID is not known AND the user has not specified a type for that VID and PID, an educated guess is made about how to map the gamepad Driver Station will now attempt to automatically recover from a gamepad disconnecting, and re-assign it to the position it was assigned to when it dropped If only one gamepad is assigned and it drops: it can be recovered If two gamepads are assigned, and have different VID/PID signatures, and only one drops: it will be recovered If two gamepads are assigned, and have different VID/PID signatures, and BOTH drop: both will be recovered If two gamepads are assigned, and have the same VID/PID signatures, and only one drops: it will be recovered If two gamepads are assigned, and have the same VID/PID signatures, and BOTH drop: neither will be recovered, because of the ambiguity of the gamepads when they re-appear on the USB bus. There is currently one known edge case: if there are two gamepads with the same VID/PID signature plugged in, but only one is assigned, and they BOTH drop, it's a 50-50 chance of which one will be chosen for automatic recovery to the assigned position: it is determined by whichever one is re-enumerated first by the USB bus controller. Adds landscape user interface to Driver Station New feature: practice timer with audio cues New feature (Control Hub only): wireless network connection strength indicator (0-5 bars) New feature (Control Hub only): tapping on the ping/channel display will switch to an alternate display showing radio RX dBm and link speed (tap again to switch back) The layout will NOT autorotate. You can switch the layout from the Driver Station's settings menu. Breaking changes Removes support for Android versions 4.4 through 5.1 (KitKat and Lollipop). The minSdkVersion is now 23. Removes the deprecated LinearOpMode methods waitOneFullHardwareCycle() and waitForNextHardwareCycle() Enhancements Handles RS485 address of Control Hub automatically The Control Hub is automatically given a reserved address Existing configuration files will continue to work All addresses in the range of 1-10 are still available for Expansion Hubs The Control Hub light will now normally be solid green, without blinking to indicate the address The Control Hub will not be shown on the Expansion Hub Address Change settings page Improves REV Hub firmware updater The user can now choose between all available firmware update files Version 1.8.2 of the REV Hub firmware is bundled into the Robot Controller app. Text was added to clarify that Expansion Hubs can only be updated via USB. Firmware update speed was reduced to improve reliability Allows REV Hub firmware to be updated directly from the Manage webpage Improves log viewer on Robot Controller Horizontal scrolling support (no longer word wrapped) Supports pinch-to-zoom Uses a monospaced font Error messages are highlighted New color scheme Attempts to force-stop a runaway/stuck OpMode without restarting the entire app Not all types of runaway conditions are stoppable, but if the user code attempts to talk to hardware during the runaway, the system should be able to capture it. Makes various tweaks to the Self Inspect screen Renames \"OS version\" entry to \"Android version\" Renames \"WiFi Direct Name\" to \"WiFi Name\" Adds Control Hub OS version, when viewing the report of a Control Hub Hides the airplane mode entry, when viewing the report of a Control Hub Removes check for ZTE Speed Channel Changer Shows firmware version for all Expansion and Control Hubs Reworks network settings portion of Manage page All network settings are now applied with a single click The WiFi Direct channel of phone-based Robot Controllers can now be changed from the Manage page WiFi channels are filtered by band (2.4 vs 5 GHz) and whether they overlap with other channels The current WiFi channel is pre-selected on phone-based Robot Controllers, and Control Hubs running OS 1.1.2 or later. On Control Hubs running OS 1.1.2 or later, you can choose to have the system automatically select a channel on the 5 GHz band Improves OnBotJava New light and dark themes replace the old themes (chaos, github, chrome,...) the new default theme is light and will be used when you first update to this version OnBotJava now has a tabbed editor Read-only offline mode Improves function of \"exit\" menu item on Robot Controller and Driver Station Now guaranteed to be fully stopped and unloaded from memory Shows a warning message if a LinearOpMode exists prematurely due to failure to monitor for the start condition Improves error message shown when the Driver Station and Robot Controller are incompatible with each other Driver Station OpMode Control Panel now disabled while a Restart Robot is in progress Disables advanced settings related to WiFi direct when the Robot Controller is a Control Hub. Tint phone battery icons on Driver Station when low/critical. Uses names \"Control Hub Portal\" and \"Control Hub\" (when appropriate) in new configuration files Improve I2C read performance Very large improvement on Control Hub; up to ~2x faster with small (e.g. 6 byte) reads Not as apparent on Expansion Hubs connected to a phone Update/refresh build infrastructure Update to 'androidx' support library from 'com.android.support:appcompat', which is end-of-life Update targetSdkVersion and compileSdkVersion to 28 Update Android Studio's Android plugin to latest Fix reported build timestamp in 'About' screen Add sample illustrating manual webcam use: ConceptWebcam Bug fixes Fixes SkyStone issue #248 Fixes SkyStone issue #232 and modifies bulk caching semantics to allow for cache-preserving MANUAL/AUTO transitions. Improves performance when REV 2M distance sensor is unplugged Improves readability of Toast messages on certain devices Allows a Driver Station to connect to a Robot Controller after another has disconnected Improves generation of fake serial numbers for UVC cameras which do not provide a real serial number Previously some devices would assign such cameras a serial of 0:0 and fail to open and start streaming Fixes ftc_app issue #638. Fixes a slew of bugs with the Vuforia camera monitor including: Fixes bug where preview could be displayed with a wonky aspect ratio Fixes bug where preview could be cut off in landscape Fixes bug where preview got totally messed up when rotating phone Fixes bug where crosshair could drift off target when using webcams Fixes issue in UVC driver on some devices (ftc_app 681) if streaming was started/stopped multiple times in a row Issue manifested as kernel panic on devices which do not have this kernel patch. On affected devices which do have the patch, the issue was manifest as simply a failure to start streaming. The Tech Team believes that the root cause of the issue is a bug in the Linux kernel XHCI driver. A workaround was implemented in the SDK UVC driver. Fixes bug in UVC driver where often half the frames from the camera would be dropped (e.g. only 15FPS delivered during a streaming session configured for 30FPS). Fixes issue where TensorFlow Object Detection would show results whose confidence was lower than the minimum confidence parameter. Fixes a potential exploitation issue of CVE-2019-11358 in OnBotJava Fixes changing the address of an Expansion Hub with additional Expansion Hubs connected to it Preserves the Control Hub's network connection when \"Restart Robot\" is selected Fixes issue where device scans would fail while the Robot was restarting Fix RenderScript usage Use androidx.renderscript variant: increased compatibility Use RenderScript in Java mode, not native: simplifies build Fixes webcam-frame-to-bitmap conversion problem: alpha channel wasn't being initialized, only R, G, & B Fixes possible arithmetic overflow in Deadline Fixes deadlock in Vuforia webcam support which could cause 5-second delays when stopping OpMode Version 5.4 (20200108-101156) Fixes SkyStone issue #88 Adds an inspection item that notes when a robot controller (Control Hub) is using the factory default password. Fixes SkyStone issue #61 Fixes SkyStone issue #142 Fixes ftc_app issue #417 by adding more current and voltage monitoring capabilities for REV Hubs. Fixes a crash sometimes caused by OnBotJava activity Improves OnBotJava autosave functionality ftc_app #738 Fixes system responsiveness issue when an Expansion Hub is disconnected Fixes issue where IMU initialization could prevent Op Modes from stopping Fixes issue where AndroidTextToSpeech.speak() would fail if it was called too early Adds telemetry.speak() methods and blocks, which cause the Driver Station (if also updated) to speak text Adds and improves Expansion Hub-related warnings Improves Expansion Hub low battery warning Displays the warning immediately after the hub reports it Specifies whether the condition is current or occurred temporarily during an OpMode run Displays which hubs reported low battery Displays warning when hub loses and regains power during an OpMode run Fixes the hub's LED pattern after this condition Displays warning when Expansion Hub is not responding to commands Specifies whether the condition is current or occurred temporarily during an OpMode run Clarifies warning when Expansion Hub is not present at startup Specifies that this condition requires a Robot Restart before the hub can be used. The hub light will now accurately reflect this state Improves logging and reduces log spam during these conditions Syncs the Control Hub time and timezone to a connected web browser programming the robot, if a Driver Station is not available. Adds bulk read functionality for REV Hubs A bulk caching mode must be set at the Hub level with LynxModule#setBulkCachingMode(). This applies to all relevant SDK hardware classes that reference that Hub. The following following Hub bulk caching modes are available: BulkCachingMode.OFF (default): All hardware calls operate as usual. Bulk data can read through LynxModule#getBulkData() and processed manually. BulkCachingMode.AUTO: Applicable hardware calls are served from a bulk read cache that is cleared/refreshed automatically to ensure identical commands don't hit the same cache. The cache can also be cleared manually with LynxModule#clearBulkCache(), although this is not recommended. (advanced users) BulkCachingMode.MANUAL: Same as BulkCachingMode.AUTO except the cache is never cleared automatically. To avoid getting stale data, the cache must be manually cleared at the beginning of each loop body or as the user deems appropriate. Removes PIDF Annotation values added in Rev 5.3 (to AndyMark, goBILDA and TETRIX motor configurations). The new motor types will still be available but their Default control behavior will revert back to Rev 5.2 Adds new ConceptMotorBulkRead sample Opmode to demonstrate and compare Motor Bulk-Read modes for reducing I/O latencies. Version 5.3 (20191004-112306) Fixes external USB/UVC webcam support Makes various bugfixes and improvements to Blocks page, including but not limited to: Many visual tweaks Browser zoom and window resize behave better Resizing the Java preview pane works better and more consistently across browsers The Java preview pane consistently gets scrollbars when needed The Java preview pane is hidden by default on phones Internet Explorer 11 should work Large dropdown lists display properly on lower res screens Disabled buttons are now visually identifiable as disabled A warning is shown if a user selects a TFOD sample, but their device is not compatible Warning messages in a Blocks op mode are now visible by default. Adds goBILDA 5201 and 5202 motors to Robot Configurator Adds PIDF Annotation values to AndyMark, goBILDA and TETRIX motor configurations. This has the effect of causing the RUN_USING_ENCODERS and RUN_TO_POSITION modes to use PIDF vs PID closed loop control on these motors. This should provide more responsive, yet stable, speed control. PIDF adds Feedforward control to the basic PID control loop. Feedforward is useful when controlling a motor's speed because it \"anticipates\" how much the control voltage must change to achieve a new speed set-point, rather than requiring the integrated error to change sufficiently. The PIDF values were chosen to provide responsive, yet stable, speed control on a lightly loaded motor. The more heavily a motor is loaded (drag or friction), the more noticable the PIDF improvement will be. Fixes startup crash on Android 10 Fixes ftc_app issue #712 (thanks to FROGbots-4634) Fixes ftc_app issue #542 Allows \"A\" and lowercase letters when naming device through RC and DS apps. Version 5.2 (20190905-083277) Fixes extra-wide margins on settings activities, and placement of the new configuration button Adds Skystone Vuforia image target data. Includes sample Skystone Vuforia Navigation op modes (Java). Includes sample Skystone Vuforia Navigation op modes (Blocks). Adds TensorFlow inference model (.tflite) for Skystone game elements. Includes sample Skystone TensorFlow op modes (Java). Includes sample Skystone TensorFlow op modes (Blocks). Removes older (season-specific) sample op modes. Includes 64-bit support (to comply with Google Play requirements). Protects against Stuck OpModes when a Restart Robot is requested. (Thanks to FROGbots-4634) (ftc_app issue #709) Blocks related changes: Fixes bug with blocks generated code when hardware device name is a java or javascript reserved word. Shows generated java code for blocks, even when hardware items are missing from the active configuration. Displays warning icon when outdated Vuforia and TensorFlow blocks are used (SkyStone issue #27) Version 5.1 (20190820-222104) Defines default PIDF parameters for the following motors: REV Core Hex Motor REV 20:1 HD Hex Motor REV 40:1 HD Hex Motor Adds back button when running on a device without a system back button (such as a Control Hub) Allows a REV Control Hub to update the firmware on a REV Expansion Hub via USB Fixes SkyStone issue #9 Fixes ftc_app issue #715 Prevents extra DS User clicks by filtering based on current state. Prevents incorrect DS UI state changes when receiving new OpMode list from RC Adds support for REV Color Sensor V3 Adds a manual-refresh DS Camera Stream for remotely viewing RC camera frames. To show the stream on the DS, initialize but do not run a stream-enabled opmode, select the Camera Stream option in the DS menu, and tap the image to refresh. This feature is automatically enabled when using Vuforia or TFODâ€”no additional RC configuration is required for typical use cases. To hide the stream, select the same menu item again. Note that gamepads are disabled and the selected opmode cannot be started while the stream is open as a safety precaution. To use custom streams, consult the API docs for CameraStreamServer#setSource and CameraStreamSource. Adds many Star Wars sounds to RobotController resources. Added SKYSTONE Sounds Chooser Sample Program. Switches out startup, connect chimes, and error/warning sounds for Star Wars sounds Updates OnBot Java to use a WebSocket for communication with the robot The OnBot Java page no longer has to do a full refresh when a user switches from editing one file to another Known issues:  Camera Stream The Vuforia camera stream inherits the issues present in the phone preview (namely ftc_app issue #574). This problem does not affect the TFOD camera stream even though it receives frames from Vuforia. The orientation of the stream frames may not always match the phone preview. For now, these frames may be rotated manually via a custom CameraStreamSource if desired. OnBotJava Browser back button may not always work correctly It's possible for a build to be queued, but not started. The OnBot Java build console will display a warning if this occurs. A user might not realize they are editing a different file if the user inadvertently switches from one file to another since this switch is now seamless. The name of the currently open file is displayed in the browser tab. Version 5.0 (built on 19.06.14) Support for the REV Robotics Control Hub. Adds a Java preview pane to the Blocks editor. Adds a new offline export feature to the Blocks editor. Display wifi channel in Network circle on Driver Station. Adds calibration for Logitech C270 Updates build tooling and target SDK. Compliance with Google's permissions infrastructure (Required after build tooling update). Keep Alives to mitigate the Motorola wifi scanning problem. Telemetry substitute no longer necessary. Improves Vuforia error reporting. Fixes ftctechnh/ftc_app issues 621, 713. Miscellaneous bug fixes and improvements. Version 4.3 (built on 18.10.31) Includes missing TensorFlow-related libraries and files. Version 4.2 (built on 18.10.30) Includes fix to avoid deadlock situation with WatchdogMonitor which could result in USB communication errors. Comm error appeared to require that user disconnect USB cable and restart the Robot Controller app to recover. robotControllerLog.txt would have error messages that included the words \"E RobotCore: lynx xmit lock: #### abandoning lock:\" Includes fix to correctly list the parent module address for a REV Robotics Expansion Hub in a configuration (.xml) file. Bug in versions 4.0 and 4.1 would incorrect list the address module for a parent REV Robotics device as \"1\". If the parent module had a higher address value than the daisy-chained module, then this bug would prevent the Robot Controller from communicating with the downstream Expansion Hub. Added requirement for ACCESS_COARSE_LOCATION to allow a Driver Station running Android Oreo to scan for Wi-Fi Direct devices. Added google() repo to build.gradle because aapt2 must be downloaded from the google() repository beginning with version 3.2 of the Android Gradle Plugin. Important Note: Android Studio users will need to be connected to the Internet the first time build the ftc_app project. Internet connectivity is required for the first build so the appropriate files can be downloaded from the Google repository. Users should not need to be connected to the Internet for subsequent builds. This should also fix buid issue where Android Studio would complain that it \"Could not find com.android.tools.lint:lint-gradle:26.1.4\" (or similar). Added support for REV Spark Mini motor controller as part of the configuration menu for a servo/PWM port on the REV Expansion Hub. Provide examples for playing audio files in an Op Mode. Block Development Tool Changes Includes a fix for a problem with the Velocity blocks that were reported in the FTC Technology forum (Blocks Programming subforum). Change the \"Save completed successfully.\" message to a white color so it will contrast with a green background. Fixed the \"Download image\" feature so it will work if there are text blocks in the op mode. Introduce support for Google's TensorFlow Lite technology for object detetion for 2018-2019 game. TensorFlow lite can recognize Gold Mineral and Silver Mineral from 2018-2019 game. Example Java and Block op modes are included to show how to determine the relative position of the gold block (left, center, right). Version 4.1 (released on 18.09.24) Changes include:  Fix to prevent crash when deprecated configuration annotations are used. Change to allow FTC Robot Controller APK to be auto-updated using FIRST Global Control Hub update scripts. Removed samples for non supported / non legal hardware. Improvements to Telemetry.addData block with \"text\" socket. Updated Blocks sample op mode list to include Rover Ruckus Vuforia example. Update SDK library version number. Version 4.0 (released on 18.09.12) Changes include:  Initial support for UVC compatible cameras  If UVC camera has a unique serial number, RC will detect and enumerate by serial number. If UVC camera lacks a unique serial number, RC will only support one camera of that type connected. Calibration settings for a few cameras are included (see TeamCode/src/main/res/xml/teamwebcamcalibrations.xml for details). User can upload calibration files from Program and Manage web interface. UVC cameras seem to draw a fair amount of electrical current from the USB bus. This does not appear to present any problems for the REV Robotics Control Hub. This does seem to create stability problems when using some cameras with an Android phone-based Robot Controller. FTC Tech Team is investigating options to mitigate this issue with the phone-based Robot Controllers. Updated sample Vuforia Navigation and VuMark Op Modes to demonstrate how to use an internal phone-based camera and an external UVC webcam. Support for improved motor control.  REV Robotics Expansion Hub firmware 1.8 and greater will support a feed forward mechanism for closed loop motor control. FTC SDK has been modified to support PIDF coefficients (proportional, integral, derivative, and feed forward). FTC Blocks development tool modified to include PIDF programming blocks. Deprecated older PID-related methods and variables. REV's 1.8.x PIDF-related changes provide a more linear and accurate way to control a motor. Wireless  Added 5GHz support for wireless channel changing for those devices that support it. Tested with Moto G5 and E4 phones. Also tested with other (currently non-approved) phones such as Samsung Galaxy S8. Improved Expansion Hub firmware update support in Robot Controller app  Changes to make the system more robust during the firmware update process (when performed through Robot Controller app). User no longer has to disconnect a downstream daisy-chained Expansion Hub when updating an Expansion Hub's firmware. If user is updating an Expansion Hub's firmware through a USB connection, he/she does not have to disconnect RS485 connection to other Expansion Hubs. The user still must use a USB connection to update an Expansion Hub's firmware. The user cannot update the Expansion Hub firmware for a downstream device that is daisy chained through an RS485 connection. If an Expansion Hub accidentally gets \"bricked\" the Robot Controller app is now more likely to recognize the Hub when it scans the USB bus. Robot Controller app should be able to detect an Expansion Hub, even if it accidentally was bricked in a previous update attempt. Robot Controller app should be able to install the firmware onto the Hub, even if if accidentally was bricked in a previous update attempt. Resiliency  FTC software can detect and enable an FTDI reset feature that is available with REV Robotics v1.8 Expansion Hub firmware and greater. When enabled, the Expansion Hub can detect if it hasn't communicated with the Robot Controller over the FTDI (USB) connection. If the Hub hasn't heard from the Robot Controller in a while, it will reset the FTDI connection. This action helps system recover from some ESD-induced disruptions. Various fixes to improve reliability of FTC software. Blocks  Fixed errors with string and list indices in blocks export to java. Support for USB connected UVC webcams. Refactored optimized Blocks Vuforia code to support Rover Ruckus image targets. Added programming blocks to support PIDF (proportional, integral, derivative and feed forward) motor control. Added formatting options (under Telemetry and Miscellaneous categories) so user can set how many decimal places to display a numerical value. Support to play audio files (which are uploaded through Blocks web interface) on Driver Station in addition to the Robot Controller. Fixed bug with Download Image of Blocks feature. Support for REV Robotics Blinkin LED Controller. Support for REV Robotics 2m Distance Sensor. Added support for a REV Touch Sensor (no longer have to configure as a generic digital device). Added blocks for DcMotorEx methods. These are enhanced methods that you can use when supported by the motor controller hardware. The REV Robotics Expansion Hub supports these enhanced methods. Enhanced methods include methods to get/set motor velocity (in encoder pulses per second), get/set PIDF coefficients, etc.. Modest Improvements in Logging  Decrease frequency of battery checker voltage statements. Removed non-FTC related log statements (wherever possible). Introduced a \"Match Logging\" feature. Under \"Settings\" a user can enable/disable this feature (it's disabled by default). If enabled, user provides a \"Match Number\" through the Driver Station user interface (top of the screen). The Match Number is used to create a log file specifically with log statements from that particular Op Mode run. Match log files are stored in /sdcard/FIRST/matlogs on the Robot Controller. Once an op mode run is complete, the Match Number is cleared. This is a convenient way to create a separate match log with statements only related to a specific op mode run. New Devices  Support for REV Robotics Blinkin LED Controller. Support for REV Robotics 2m Distance Sensor. Added configuration option for REV 20:1 HD Hex Motor. Added support for a REV Touch Sensor (no longer have to configure as a generic digital device). Miscellaneous  Fixed some errors in the definitions for acceleration and velocity in our javadoc documentation. Added ability to play audio files on Driver Station When user is configuring an Expansion Hub, the LED on the Expansion Hub will change blink pattern (purple-cyan) to indicate which Hub is currently being configured. Renamed I2cSensorType to I2cDeviceType. Added an external sample Op Mode that demonstrates localization using 2018-2019 (Rover Ruckus presented by QualComm) Vuforia targets. Added an external sample Op Mode that demonstrates how to use the REV Robotics 2m Laser Distance Sensor. Added an external sample Op Mode that demonstrates how to use the REV Robotics Blinkin LED Controller. Re-categorized external Java sample Op Modes to \"TeleOp\" instead of \"Autonomous\". Known issues:  Initial support for UVC compatible cameras  UVC cameras seem to draw significant amount of current from the USB bus. This does not appear to present any problems for the REV Robotics Control Hub. This does seem to create stability problems when using some cameras with an Android phone-based Robot Controller. FTC Tech Team is investigating options to mitigate this issue with the phone-based Robot Controllers. There might be a possible deadlock which causes the RC to become unresponsive when using a UVC webcam with a Nougat Android Robot Controller. Wireless  When user selects a wireless channel, this channel does not necessarily persist if the phone is power cycled. Tech Team is hoping to eventually address this issue in a future release. Issue has been present since apps were introduced (i.e., it is not new with the v4.0 release). Wireless channel is not currently displayed for WiFi Direct connections. Miscellaneous  The blink indication feature that shows which Expansion Hub is currently being configured does not work for a newly created configuration file. User has to first save a newly created configuration file and then close and re-edit the file in order for blink indicator to work. Version 3.6 (built on 17.12.18) Changes include:  Blocks Changes Uses updated Google Blockly software to allow users to edit their op modes on Apple iOS devices (including iPad and iPhone). Improvement in Blocks tool to handle corrupt op mode files. Autonomous op modes should no longer get switched back to tele-op after re-opening them to be edited. The system can now detect type mismatches during runtime and alert the user with a message on the Driver Station. Updated javadoc documentation for setPower() method to reflect correct range of values (-1 to +1). Modified VuforiaLocalizerImpl to allow for user rendering of frames Added a user-overrideable onRenderFrame() method which gets called by the class's renderFrame() method. Version 3.5 (built on 17.10.30) Changes with version 3.5 include:  Introduced a fix to prevent random op mode stops, which can occur after the Robot Controller app has been paused and then resumed (for example, when a user temporarily turns off the display of the Robot Controller phone, and then turns the screen back on). Introduced a fix to prevent random op mode stops, which were previously caused by random peer disconnect events on the Driver Station. Fixes issue where log files would be closed on pause of the RC or DS, but not re-opened upon resume. Fixes issue with battery handler (voltage) start/stop race. Fixes issue where Android Studio generated op modes would disappear from available list in certain situations. Fixes problem where OnBot Java would not build on REV Robotics Control Hub. Fixes problem where OnBot Java would not build if the date and time on the Robot Controller device was \"rewound\" (set to an earlier date/time). Improved error message on OnBot Java that occurs when renaming a file fails. Removed unneeded resources from android.jar binaries used by OnBot Java to reduce final size of Robot Controller app. Added MR_ANALOG_TOUCH_SENSOR block to Blocks Programming Tool. Version 3.4 (built on 17.09.06) Changes with version 3.4 include:  Added telemetry.update() statement for BlankLinearOpMode template. Renamed sample Block op modes to be more consistent with Java samples. Added some additional sample Block op modes. Reworded OnBot Java readme slightly. Version 3.3 (built on 17.09.04) This version of the software includes improves for the FTC Blocks Programming Tool and the OnBot Java Programming Tool.  Changes with verion 3.3 include:  Android Studio ftc_app project has been updated to use Gradle Plugin 2.3.3. Android Studio ftc_app project is already using gradle 3.5 distribution. Robot Controller log has been renamed to /sdcard/RobotControllerLog.txt (note that this change was actually introduced w/ v3.2). Improvements in I2C reliability. Optimized I2C read for REV Expansion Hub, with v1.7 firmware or greater. Updated all external/samples (available through OnBot and in Android project folder). Vuforia Added support for VuMarks that will be used for the 2017-2018 season game. Blocks Update to latest Google Blockly release. Sample op modes can be selected as a template when creating new op mode. Fixed bug where the blocks would disappear temporarily when mouse button is held down. Added blocks for Range.clip and Range.scale. User can now disable/enable Block op modes. Fix to prevent occasional Blocks deadlock. OnBot Java Significant improvements with autocomplete function for OnBot Java editor. Sample op modes can be selected as a template when creating new op mode. Fixes and changes to complete hardware setup feature. Updated (and more useful) onBot welcome message. Known issues:  Android Studio After updating to the new v3.3 Android Studio project folder, if you get error messages indicating \"InvalidVirtualFileAccessException\" then you might need to do a File->Invalidate Caches / Restart to clear the error. OnBot Java Sometimes when you push the build button to build all op modes, the RC returns an error message that the build failed. If you press the build button a second time, the build typically suceeds. Version 3.2 (built on 17.08.02) This version of the software introduces the \"OnBot Java\" Development Tool. Similar to the FTC Blocks Development Tool, the FTC OnBot Java Development Tool allows a user to create, edit and build op modes dynamically using only a Javascript-enabled web browser.  The OnBot Java Development Tool is an integrated development environment (IDE) that is served up by the Robot Controller. Op modes are created and edited using a Javascript-enabled browser (Google Chromse is recommended). Op modes are saved on the Robot Controller Android device directly.  The OnBot Java Development Tool provides a Java programming environment that does NOT need Android Studio.  Changes with version 3.2 include:  Enhanced web-based development tools  Introduction of OnBot Java Development Tool. Web-based programming and management features are \"always on\" (user no longer needs to put Robot Controller into programming mode). Web-based management interface (where user can change Robot Controller name and also easily download Robot Controller log file). OnBot Java, Blocks and Management features available from web based interface. Blocks Programming Development Tool:  Changed \"LynxI2cColorRangeSensor\" block to \"REV Color/range sensor\" block. Fixed tooltip for ColorSensor.isLightOn block. Added blocks for ColorSensor.getNormalizedColors and LynxI2cColorRangeSensor.getNormalizedColors. Added example op modes for digital touch sensor and REV Robotics Color Distance sensor.  User selectable color themes.  Includes many minor enhancements and fixes (too numerous to list).  Known issues:  Auto complete function is incomplete and does not support the following (for now): Access via this keyword Access via super keyword Members of the super cloass, not overridden by the class Any methods provided in the current class Inner classes Can't handle casted objects Any objects coming from an parenthetically enclosed expression Version 3.10 (built on 17.05.09) This version of the software provides support for the REV Robotics Expansion Hub. This version also includes improvements in the USB communication layer in an effort to enhance system resiliency. If you were using a 2.x version of the software previously, updating to version 3.1 requires that you also update your Driver Station software in addition to updating the Robot Controller software.  Also note that in version 3.10 software, the setMaxSpeed and getMaxSpeed methods are no longer available (not deprecated, they have been removed from the SDK). Also note that the the new 3.x software incorporates motor profiles that a user can select as he/she configures the robot.  Changes include:  Blocks changes Added VuforiaTrackableDefaultListener.getPose and Vuforia.trackPose blocks. Added optimized blocks support for Vuforia extended tracking. Added atan2 block to the math category. Added useCompetitionFieldTargetLocations parameter to Vuforia.initialize block. If set to false, the target locations are placed at (0,0,0) with target orientation as specified in https://github.com/gearsincorg/FTCVuforiaDemo/blob/master/Robot_Navigation.java tutorial op mode. Incorporates additional improvements to USB comm layer to improve system resiliency (to recover from a greater number of communication disruptions). Additional Notes Regarding Version 3.00 (built on 17.04.13)  In addition to the release changes listed below (see section labeled \"Version 3.00 (built on 17.04.013)\"), version 3.00 has the following important changes:  Version 3.00 software uses a new version of the FTC Robocol (robot protocol). If you upgrade to v3.0 on the Robot Controller and/or Android Studio side, you must also upgrade the Driver Station software to match the new Robocol. Version 3.00 software removes the setMaxSpeed and getMaxSpeed methods from the DcMotor class. If you have an op mode that formerly used these methods, you will need to remove the references/calls to these methods. Instead, v3.0 provides the max speed information through the use of motor profiles that are selected by the user during robot configuration. Version 3.00 software currently does not have a mechanism to disable extra i2c sensors. We hope to re-introduce this function with a release in the near future. Version 3.00 (built on 17.04.13) *** Use this version of the software at YOUR OWN RISK!!! ***  This software is being released as an \"alpha\" version. Use this version at your own risk!  This pre-release software contains SIGNIFICANT changes, including changes to the Wi-Fi Direct pairing mechanism, rewrites of the I2C sensor classes, changes to the USB/FTDI layer, and the introduction of support for the REV Robotics Expansion Hub and the REV Robotics color-range-light sensor. These changes were implemented to improve the reliability and resiliency of the FTC control system.  Please note, however, that version 3.00 is considered \"alpha\" code. This code is being released so that the FIRST community will have an opportunity to test the new REV Expansion Hub electronics module when it becomes available in May. The developers do not recommend using this code for critical applications (i.e., competition use).  *** Use this version of the software at YOUR OWN RISK!!! ***  Changes include:  Major rework of sensor-related infrastructure. Includes rewriting sensor classes to implement synchronous I2C communication. Fix to reset Autonomous timer back to 30 seconds. Implementation of specific motor profiles for approved 12V motors (includes Tetrix, AndyMark, Matrix and REV models). Modest improvements to enhance Wi-Fi P2P pairing. Fixes telemetry log addition race. Publishes all the sources (not just a select few). Includes Block programming improvements Addition of optimized Vuforia blocks. Auto scrollbar to projects and sounds pages. Fixed blocks paste bug. Blocks execute after while-opModeIsActive loop (to allow for cleanup before exiting op mode). Added gyro integratedZValue block. Fixes bug with projects page for Firefox browser. Added IsSpeaking block to AndroidTextToSpeech. Implements support for the REV Robotics Expansion Hub Implements support for integral REV IMU (physically installed on I2C bus 0, uses same Bosch BNO055 9 axis absolute orientation sensor as Adafruit 9DOF abs orientation sensor). - Implements support for REV color/range/light sensor. Provides support to update Expansion Hub firmware through FTC SDK. Detects REV firmware version and records in log file. Includes support for REV Control Hub (note that the REV Control Hub is not yet approved for FTC use). Implements FTC Blocks programming support for REV Expansion Hub and sensor hardware. Detects and alerts when I2C device disconnect. Version 2.62 (built on 17.01.07) Added null pointer check before calling modeToByte() in finishModeSwitchIfNecessary method for ModernRoboticsUsbDcMotorController class. Changes to enhance Modern Robotics USB protocol robustness. Version 2.61 (released on 16.12.19) Blocks Programming mode changes: Fix to correct issue when an exception was thrown because an OpticalDistanceSensor object appears twice in the hardware map (the second time as a LightSensor). Version 2.6 (released on 16.12.16) Fixes for Gyro class: Improve (decrease) sensor refresh latency. fix isCalibrating issues. Blocks Programming mode changes: Blocks now ignores a device in the configuration xml if the name is empty. Other devices work in configuration work fine. Version 2.5 (internal release on released on 16.12.13) Blocks Programming mode changes: Added blocks support for AdafruitBNO055IMU. Added Download Op Mode button to FtcBocks.html. Added support for copying blocks in one OpMode and pasting them in an other OpMode. The clipboard content is stored on the phone, so the programming mode server must be running. Modified Utilities section of the toolbox. In Programming Mode, display information about the active connections. Fixed paste location when workspace has been scrolled. Added blocks support for the android Accelerometer. Fixed issue where Blocks Upload Op Mode truncated name at first dot. Added blocks support for Android SoundPool. Added type safety to blocks for Acceleration. Added type safety to blocks for AdafruitBNO055IMU.Parameters. Added type safety to blocks for AnalogInput. Added type safety to blocks for AngularVelocity. Added type safety to blocks for Color. Added type safety to blocks for ColorSensor. Added type safety to blocks for CompassSensor. Added type safety to blocks for CRServo. Added type safety to blocks for DigitalChannel. Added type safety to blocks for ElapsedTime. Added type safety to blocks for Gamepad. Added type safety to blocks for GyroSensor. Added type safety to blocks for IrSeekerSensor. Added type safety to blocks for LED. Added type safety to blocks for LightSensor. Added type safety to blocks for LinearOpMode. Added type safety to blocks for MagneticFlux. Added type safety to blocks for MatrixF. Added type safety to blocks for MrI2cCompassSensor. Added type safety to blocks for MrI2cRangeSensor. Added type safety to blocks for OpticalDistanceSensor. Added type safety to blocks for Orientation. Added type safety to blocks for Position. Added type safety to blocks for Quaternion. Added type safety to blocks for Servo. Added type safety to blocks for ServoController. Added type safety to blocks for Telemetry. Added type safety to blocks for Temperature. Added type safety to blocks for TouchSensor. Added type safety to blocks for UltrasonicSensor. Added type safety to blocks for VectorF. Added type safety to blocks for Velocity. Added type safety to blocks for VoltageSensor. Added type safety to blocks for VuforiaLocalizer.Parameters. Added type safety to blocks for VuforiaTrackable. Added type safety to blocks for VuforiaTrackables. Added type safety to blocks for enums in AdafruitBNO055IMU.Parameters. Added type safety to blocks for AndroidAccelerometer, AndroidGyroscope, AndroidOrientation, and AndroidTextToSpeech. Version 2.4 (released on 16.11.13) Fix to avoid crashing for nonexistent resources. Blocks Programming mode changes: Added blocks to support OpenGLMatrix, MatrixF, and VectorF. Added blocks to support AngleUnit, AxesOrder, AxesReference, CameraDirection, CameraMonitorFeedback, DistanceUnit, and TempUnit. Added blocks to support Acceleration. Added blocks to support LinearOpMode.getRuntime. Added blocks to support MagneticFlux and Position. Fixed typos. Made blocks for ElapsedTime more consistent with other objects. Added blocks to support Quaternion, Velocity, Orientation, AngularVelocity. Added blocks to support VuforiaTrackables, VuforiaTrackable, VuforiaLocalizer, VuforiaTrackableDefaultListener. Fixed a few blocks. Added type checking to new blocks. Updated to latest blockly. Added default variable blocks to navigation and matrix blocks. Fixed toolbox entry for openGLMatrix_rotation_withAxesArgs. When user downloads Blocks-generated op mode, only the .blk file is downloaded. When user uploads Blocks-generated op mode (.blk file), Javascript code is auto generated. Added DbgLog support. Added logging when a blocks file is read/written. Fixed bug to properly render blocks even if missing devices from configuration file. Added support for additional characters (not just alphanumeric) for the block file names (for download and upload). Added support for OpMode flavor (â€œAutonomousâ€ or â€œTeleOpâ€) and group. Changes to Samples to prevent tutorial issues. Incorporated suggested changes from public pull 216 (â€œReplace .. pathsâ€). Remove Servo Glitches when robot stopped. if user hits â€œCancelsâ€ when editing a configuration file, clears the unsaved changes and reverts to original unmodified configuration. Added log info to help diagnose why the Robot Controller app was terminated (for example, by watch dog function). Added ability to transfer log from the controller. Fixed inconsistency for AngularVelocity Limit unbounded growth of data for telemetry. If user does not call telemetry.update() for LinearOpMode in a timely manner, data added for telemetry might get lost if size limit is exceeded. Version 2.35 (released on 16.10.06) Blockly programming mode - Removed unnecesary idle() call from blocks for new project. Version 2.30 (released on 16.10.05) Blockly programming mode: Mechanism added to save Blockly op modes from Programming Mode Server onto local device To avoid clutter, blocks are displayed in categorized folders Added support for DigitalChannel Added support for ModernRoboticsI2cCompassSensor Added support for ModernRoboticsI2cRangeSensor Added support for VoltageSensor Added support for AnalogInput Added support for AnalogOutput Fix for CompassSensor setMode block Vuforia Fix deadlock / make camera data available while Vuforia is running. Update to Vuforia 6.0.117 (recommended by Vuforia and Google to close security loophole). Fix for autonomous 30 second timer bug (where timer was in effect, even though it appeared to have timed out). opModeIsActive changes to allow cleanup after op mode is stopped (with enforced 2 second safety timeout). Fix to avoid reading i2c twice. Updated sample Op Modes. Improved logging and fixed intermittent freezing. Added digital I/O sample. Cleaned up device names in sample op modes to be consistent with Pushbot guide. Fix to allow use of IrSeekerSensorV3. Version 2.20 (released on 16.09.08) Support for Modern Robotics Compass Sensor. Support for Modern Robotics Range Sensor. Revise device names for Pushbot templates to match the names used in Pushbot guide. Fixed bug so that IrSeekerSensorV3 device is accessible as IrSeekerSensor in hardwareMap. Modified computer vision code to require an individual Vuforia license (per legal requirement from PTC). Minor fixes. Blockly enhancements: Support for Voltage Sensor. Support for Analog Input. Support for Analog Output. Support for Light Sensor. Support for Servo Controller. Version 2.10 (released on 16.09.03) Support for Adafruit IMU. Improvements to ModernRoboticsI2cGyro class Block on reset of z axis. isCalibrating() returns true while gyro is calibration. Updated sample gyro program. Blockly enhancements support for android.graphics.Color. added support for ElapsedTime. improved look and legibility of blocks. support for compass sensor. support for ultrasonic sensor. support for IrSeeker. support for LED. support for color sensor. support for CRServo prompt user to configure robot before using programming mode. Provides ability to disable audio cues. various bug fixes and improvements. Version 2.00 (released on 16.08.19) This is the new release for the upcoming 2016-2017 FIRST Tech Challenge Season. Channel change is enabled in the FTC Robot Controller app for Moto G 2nd and 3rd Gen phones. Users can now use annotations to register/disable their Op Modes. Changes in the Android SDK, JDK and build tool requirements (minsdk=19, java 1.7, build tools 23.0.3). Standardized units in analog input. Cleaned up code for existing analog sensor classes. setChannelMode and getChannelMode were REMOVED from the DcMotorController class. This is important - we no longer set the motor modes through the motor controller. setMode and getMode were added to the DcMotor class. ContinuousRotationServo class has been added to the FTC SDK. Range.clip() method has been overloaded so it can support this operation for int, short and byte integers. Some changes have been made (new methods added) on how a user can access items from the hardware map. Users can now set the zero power behavior for a DC motor so that the motor will brake or float when power is zero. Prototype Blockly Programming Mode has been added to FTC Robot Controller. Users can place the Robot Controller into this mode, and then use a device (such as a laptop) that has a Javascript enabled browser to write Blockly-based Op Modes directly onto the Robot Controller. Users can now configure the robot remotely through the FTC Driver Station app. Android Studio project supports Android Studio 2.1.x and compile SDK Version 23 (Marshmallow). Vuforia Computer Vision SDK integrated into FTC SDK. Users can use sample vision targets to get localization information on a standard FTC field. Project structure has been reorganized so that there is now a TeamCode package that users can use to place their local/custom Op Modes into this package. Inspection function has been integrated into the FTC Robot Controller and Driver Station Apps (Thanks Team HazMatâ€¦ 9277 & 10650!). Audio cues have been incorporated into FTC SDK. Swap mechanism added to FTC Robot Controller configuration activity. For example, if you have two motor controllers on a robot, and you misidentified them in your configuration file, you can use the Swap button to swap the devices within the configuration file (so you do not have to manually re-enter in the configuration info for the two devices). Fix mechanism added to all user to replace an electronic module easily. For example, suppose a servo controller dies on your robot. You replace the broken module with a new module, which has a different serial number from the original servo controller. You can use the Fix button to automatically reconfigure your configuration file to use the serial number of the new module. Improvements made to fix resiliency and responsiveness of the system. For LinearOpMode the user now must for a telemetry.update() to update the telemetry data on the driver station. This update() mechanism ensures that the driver station gets the updated data properly and at the same time. The Auto Configure function of the Robot Controller is now template based. If there is a commonly used robot configuration, a template can be created so that the Auto Configure mechanism can be used to quickly configure a robot of this type. The logic to detect a runaway op mode (both in the LinearOpMode and OpMode types) and to abort the run, then auto recover has been improved/implemented. Fix has been incorporated so that Logitech F310 gamepad mappings will be correct for Marshmallow users. Release 16.07.08 For the ftc_app project, the gradle files have been modified to support Android Studio 2.1.x. Release 16.03.30 For the MIT App Inventor, the design blocks have new icons that better represent the function of each design component. Some changes were made to the shutdown logic to ensure the robust shutdown of some of our USB services. A change was made to LinearOpMode so as to allow a given instance to be executed more than once, which is required for the App Inventor. Javadoc improved/updated. Release 16.03.09 Changes made to make the FTC SDK synchronous (significant change!) waitOneFullHardwareCycle() and waitForNextHardwareCycle() are no longer needed and have been deprecated. runOpMode() (for a LinearOpMode) is now decoupled from the system's hardware read/write thread. loop() (for an OpMode) is now decoupled from the system's hardware read/write thread. Methods are synchronous. For example, if you call setMode(DcMotorController.RunMode.RESET_ENCODERS) for a motor, the encoder is guaranteed to be reset when the method call is complete. For legacy module (NXT compatible), user no longer has to toggle between read and write modes when reading from or writing to a legacy device. Changes made to enhance reliability/robustness during ESD event. Changes made to make code thread safe. Debug keystore added so that user-generated robot controller APKs will all use the same signed key (to avoid conflicts if a team has multiple developer laptops for example). Firmware version information for Modern Robotics modules are now logged. Changes made to improve USB comm reliability and robustness. Added support for voltage indicator for legacy (NXT-compatible) motor controllers. Changes made to provide auto stop capabilities for op modes. A LinearOpMode class will stop when the statements in runOpMode() are complete. User does not have to push the stop button on the driver station. If an op mode is stopped by the driver station, but there is a run away/uninterruptible thread persisting, the app will log an error message then force itself to crash to stop the runaway thread. Driver Station UI modified to display lowest measured voltage below current voltage (12V battery). Driver Station UI modified to have color background for current voltage (green=good, yellow=caution, red=danger, extremely low voltage). javadoc improved (edits and additional classes). Added app build time to About activity for driver station and robot controller apps. Display local IP addresses on Driver Station About activity. Added I2cDeviceSynchImpl. Added I2cDeviceSync interface. Added seconds() and milliseconds() to ElapsedTime for clarity. Added getCallbackCount() to I2cDevice. Added missing clearI2cPortActionFlag. Added code to create log messages while waiting for LinearOpMode shutdown. Fix so Wifi Direct Config activity will no longer launch multiple times. Added the ability to specify an alternate i2c address in software for the Modern Robotics gyro. Release 16.02.09 Improved battery checker feature so that voltage values get refreshed regularly (every 250 msec) on Driver Station (DS) user interface. Improved software so that Robot Controller (RC) is much more resilient and â€œself-healingâ€ to USB disconnects: If user attempts to start/restart RC with one or more module missing, it will display a warning but still start up. When running an op mode, if one or more modules gets disconnected, the RC & DS will display warnings,and robot will keep on working in spite of the missing module(s). If a disconnected module gets physically reconnected the RC will auto detect the module and the user will regain control of the recently connected module. Warning messages are more helpful (identifies the type of module thatâ€™s missing plus its USB serial number). Code changes to fix the null gamepad reference when users try to reference the gamepads in the init() portion of their op mode. NXT light sensor output is now properly scaled. Note that teams might have to readjust their light threshold values in their op modes. On DS user interface, gamepad icon for a driver will disappear if the matching gamepad is disconnected or if that gamepad gets designated as a different driver. Robot Protocol (ROBOCOL) version number info is displayed in About screen on RC and DS apps. Incorporated a display filter on pairing screen to filter out devices that donâ€™t use the â€œ-â€œ format. This filter can be turned off to show all WiFi Direct devices. Updated text in License file. Fixed formatting error in OpticalDistanceSensor.toString(). Fixed issue on with a blank (â€œâ€) device name that would disrupt WiFi Direct Pairing. Made a change so that the WiFi info and battery info can be displayed more quickly on the DS upon connecting to RC. Improved javadoc generation. Modified code to make it easier to support language localization in the future. Release 16.01.04 Updated compileSdkVersion for apps Prevent Wifi from entering power saving mode removed unused import from driver station Corrrected \"Dead zone\" joystick code. LED.getDeviceName and .getConnectionInfo() return null apps check for ROBOCOL_VERSION mismatch Fix for Telemetry also has off-by-one errors in its data string sizing / short size limitations error User telemetry output is sorted. added formatting variants to DbgLog and RobotLog APIs code modified to allow for a long list of op mode names. changes to improve thread safety of RobocolDatagramSocket Fix for \"missing hardware leaves robot controller disconnected from driver station\" error fix for \"fast tapping of Init/Start causes problems\" (toast is now only instantiated on UI thread). added some log statements for thread life cycle. moved gamepad reset logic inside of initActiveOpMode() for robustness changes made to mitigate risk of race conditions on public methods. changes to try and flag when WiFi Direct name contains non-printable characters. fix to correct race condition between .run() and .close() in ReadWriteRunnableStandard. updated FTDI driver made ReadWriteRunnableStanard interface public. fixed off-by-one errors in Command constructor moved specific hardware implmentations into their own package. moved specific gamepad implemnatations to the hardware library. changed LICENSE file to new BSD version. fixed race condition when shutting down Modern Robotics USB devices. methods in the ColorSensor classes have been synchronized. corrected isBusy() status to reflect end of motion. corrected \"back\" button keycode. the notSupported() method of the GyroSensor class was changed to protected (it should not be public). Release 15.11.04.001 Added Support for Modern Robotics Gyro. The GyroSensor class now supports the MR Gyro Sensor. Users can access heading data (about Z axis) Users can also access raw gyro data (X, Y, & Z axes). Example MRGyroTest.java op mode included. Improved error messages More descriptive error messages for exceptions in user code. Updated DcMotor API Enable read mode on new address in setI2cAddress Fix so that driver station app resets the gamepads when switching op modes. USB-related code changes to make USB comm more responsive and to display more explicit error messages. Fix so that USB will recover properly if the USB bus returns garbage data. Fix USB initializtion race condition. Better error reporting during FTDI open. More explicit messages during USB failures. Fixed bug so that USB device is closed if event loop teardown method was not called. Fixed timer UI issue Fixed duplicate name UI bug (Legacy Module configuration). Fixed race condition in EventLoopManager. Fix to keep references stable when updating gamepad. For legacy Matrix motor/servo controllers removed necessity of appending \"Motor\" and \"Servo\" to controller names. Updated HT color sensor driver to use constants from ModernRoboticsUsbLegacyModule class. Updated MR color sensor driver to use constants from ModernRoboticsUsbDeviceInterfaceModule class. Correctly handle I2C Address change in all color sensors Updated/cleaned up op modes. Updated comments in LinearI2cAddressChange.java example op mode. Replaced the calls to \"setChannelMode\" with \"setMode\" (to match the new of the DcMotor method). Removed K9AutoTime.java op mode. Added MRGyroTest.java op mode (demonstrates how to use MR Gyro Sensor). Added MRRGBExample.java op mode (demonstrates how to use MR Color Sensor). Added HTRGBExample.java op mode (demonstrates how to use HT legacy color sensor). Added MatrixControllerDemo.java (demonstrates how to use legacy Matrix controller). Updated javadoc documentation. Updated release .apk files for Robot Controller and Driver Station apps. Release 15.10.06.002 Added support for Legacy Matrix 9.6V motor/servo controller. Cleaned up build.gradle file. Minor UI and bug fixes for driver station and robot controller apps. Throws error if Ultrasonic sensor (NXT) is not configured for legacy module port 4 or 5. Release 15.08.03.001 New user interfaces for FTC Driver Station and FTC Robot Controller apps. An init() method is added to the OpMode class. For this release, init() is triggered right before the start() method. Eventually, the init() method will be triggered when the user presses an \"INIT\" button on driver station. The init() and loop() methods are now required (i.e., need to be overridden in the user's op mode). The start() and stop() methods are optional. A new LinearOpMode class is introduced. Teams can use the LinearOpMode mode to create a linear (not event driven) program model. Teams can use blocking statements like Thread.sleep() within a linear op mode. The API for the Legacy Module and Core Device Interface Module have been updated. Support for encoders with the Legacy Module is now working. The hardware loop has been updated for better performance.",
    "stars": 249,
    "forks": 75,
    "language": "Java",
    "url": "https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020",
    "topics": [],
    "created_at": "2021-03-08T11:34:11Z",
    "updated_at": "2025-12-01T02:23:11Z",
    "homepage": null,
    "license": "N/A",
    "readme": "## NOTICE\n\nThis repository contains the public FTC SDK for the SKYSTONE (2019-2020) competition season.  \n\nFormerly this software project was hosted [here](https://github.com/ftctechnh/ftc_app).  Teams who are competing in the SKYSTONE Challenge should use this [new SKYSTONE repository](https://github.com/FIRST-Tech-Challenge/SKYSTONE) instead of the older (and no longer updated) ftc_app repository.\n\n## Welcome!\nThis GitHub repository contains the source code that is used to build an Android app to control a *FIRST* Tech Challenge competition robot.  To use this SDK, download/clone the entire project to your local computer.\n\n## Getting Started\nIf you are new to robotics or new to the *FIRST* Tech Challenge, then you should consider reviewing the [FTC Blocks Tutorial](https://github.com/FIRST-Tech-Challenge/SKYSTONE/wiki/Blocks-Tutorial) to get familiar with how to use the control system:  \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[FTC Blocks Online Tutorial](https://github.com/FIRST-Tech-Challenge/SKYSTONE/wiki/Blocks-Tutorial)\n\nEven if you are an advanced Java programmer, it is helpful to start with the [FTC Blocks tutorial](https://github.com/FIRST-Tech-Challenge/SKYSTONE/wiki/Blocks-Tutorial), and then migrate to the [OnBot Java Tool](https://github.com/FIRST-Tech-Challenge/SKYSTONE/wiki/OnBot-Java-Tutorial) or to [Android Studio](https://github.com/FIRST-Tech-Challenge/SKYSTONE/wiki/Android-Studio-Tutorial) afterwards.\n\n## Downloading the Project\nIf you are an Android Studio programmer, there are several ways to download this repo.  Note that if you use the Blocks or OnBot Java Tool to program your robot, then you do not need to download this repository.\n\n* If you are a git user, you can clone the most current version of the repository:\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;git clone https://github.com/FIRST-Tech-Challenge/SKYSTONE.git</p>\n\n* Or, if you prefer, you can use the \"Download Zip\" button available through the main repository page.  Downloading the project as a .ZIP file will keep the size of the download manageable.\n\n* You can also download the project folder (as a .zip or .tar.gz archive file) from the Downloads subsection of the [Releases](https://github.com/FIRST-Tech-Challenge/SKYSTONE/releases) page for this repository.\n\nOnce you have downloaded and uncompressed (if needed) your folder, you can use Android Studio to import the folder  (\"Import project (Eclipse ADT, Gradle, etc.)\").\n\n## Getting Help\n### User Documentation and Tutorials\n*FIRST* maintains online documentation with information and tutorials on how to use the *FIRST* Tech Challenge software and robot control system.  You can access this documentation using the following link:\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[SKYSTONE Online Documentation](https://github.com/FIRST-Tech-Challenge/SKYSTONE/wiki)\n\nNote that the online documentation is an \"evergreen\" document that is constantly being updated and edited.  It contains the most current information about the *FIRST* Tech Challenge software and control system.\n\n### Javadoc Reference Material\nThe Javadoc reference documentation for the FTC SDK is now available online.  Click on the following link to view the FTC SDK Javadoc documentation as a live website:\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[FTC Javadoc Documentation](https://first-tech-challenge.github.io/SkyStone/doc/javadoc/index.html)    \n\nDocumentation for the FTC SDK is also included with this repository.  There is a subfolder called \"doc\" which contains several subfolders:\n\n * The folder \"apk\" contains the .apk files for the FTC Driver Station and FTC Robot Controller apps.\n * The folder \"javadoc\" contains the JavaDoc user documentation for the FTC SDK.\n\n### Online User Forum\nFor technical questions regarding the Control System or the FTC SDK, please visit the FTC Technology forum:\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[FTC Technology Forum](https://ftcforum.usfirst.org/forumdisplay.php?156-FTC-Technology)\n\n\n**************************************************************************************\n# Release Information\n**************************************************************************************\n\nVersion 5.2 (20190905-083227)\n\n* Fixes extra-wide margins on settings activities, and placement of the new configuration button\n* Adds Skystone Vuforia image target data.\n   * Includes sample Skystone Vuforia Navigation op modes (Java).\n   * Includes sample Skystone Vuforia Navigation op modes (Blocks).\n* Adds TensorFlow inference model (.tflite) for Skystone game elements.\n   * Includes sample Skystone TensorFlow op modes (Java).\n   * Includes sample Skystone TensorFlow op modes (Blocks).\n* Removes older (season-specific) sample op modes.\n* Includes 64-bit support (to comply with [Google Play requirements](https://android-developers.googleblog.com/2019/01/get-your-apps-ready-for-64-bit.html)).\n* Protects against Stuck OpModes when a Restart Robot is requested. (Thanks to FROGbots-4634) ([ftc_app issue #709](https://github.com/ftctechnh/ftc_app/issues/709))\n* Blocks related changes:\n   * Fixes bug with blocks generated code when hardware device name is a java or javascript reserved word.\n   * Shows generated java code for blocks, even when hardware items are missing from the active configuration.\n   * Displays warning icon when outdated Vuforia and TensorFlow blocks are used ([SkyStone issue #27](https://github.com/FIRST-Tech-Challenge/SkyStone/issues/27))\n\n**************************************************************************************\n# Release Information\n**************************************************************************************\n\nVersion 5.1 (20190820-222104)\n\n* Defines default PIDF parameters for the following motors:\n    * REV Core Hex Motor\n    * REV 20:1 HD Hex Motor\n    * REV 40:1 HD Hex Motor\n* Adds back button when running on a device without a system back button (such as a Control Hub) \n* Allows a REV Control Hub to update the firmware on a REV Expansion Hub via USB\n* Fixes [SkyStone issue #9](https://github.com/FIRST-Tech-Challenge/SkyStone/issues/9)\n* Fixes [ftc_app issue #715](https://github.com/ftctechnh/ftc_app/issues/715)\n* Prevents extra DS User clicks by filtering based on current state.\n* Prevents incorrect DS UI state changes when receiving new OpMode list from RC\n* Adds support for REV Color Sensor V3\n* Adds a manual-refresh DS Camera Stream for remotely viewing RC camera frames.\n    * To show the stream on the DS, initialize **but do not run** a stream-enabled opmode, select the Camera Stream option in the DS menu, and tap the image to refresh. This feature is automatically enabled when using Vuforia or TFODâ€”no additional RC configuration is required for typical use cases. To hide the stream, select the same menu item again.\n    * Note that gamepads are disabled and the selected opmode cannot be started while the stream is open as a safety precaution. \n    * To use custom streams, consult the API docs for `CameraStreamServer#setSource` and `CameraStreamSource`.\n* Adds many Star Wars sounds to RobotController resources.\n* Added SKYSTONE Sounds Chooser Sample Program.\n* Switches out startup, connect chimes, and error/warning sounds for Star Wars sounds\n* Updates OnBot Java to use a WebSocket for communication with the robot\n    * The OnBot Java page no longer has to do a full refresh when a user switches from editing one file to another\n \nKnown issues:\n* Camera Stream\n    * The Vuforia camera stream inherits the issues present in the phone preview (namely [ftc_app issue #574](https://github.com/ftctechnh/ftc_app/issues/574)). This problem does not affect the TFOD camera stream even though it receives frames from Vuforia.\n    * The orientation of the stream frames may not always match the phone preview. For now, these frames may be rotated manually via a custom `CameraStreamSource` if desired.\n* OnBotJava\n    * Browser back button may not always work correctly\n    * It's possible for a build to be queued, but not started. The OnBot Java build console will display a warning if this occurs.\n    * A user might not realize they are editing a different file if the user inadvertently switches from one file to another since this switch is now seamless. The name of the currently open file is displayed in the browser tab.\n\n**************************************************************************************\n# Release Information\n**************************************************************************************\n\nVersion 5.0 (built on 19.06.14)\n\n * Support for the REV Robotics Control Hub.\n * Adds a Java preview pane to the Blocks editor.\n * Adds a new offline export feature to the Blocks editor.\n * Display wifi channel in Network circle on Driver Station.\n * Adds calibration for Logitech C270\n * Updates build tooling and target SDK.\n * Compliance with Google's permissions infrastructure (Required after build tooling update).\n * Keep Alives to mitigate the Motorola wifi scanning problem.  Telemetry substitute no longer necessary.\n * Improves Vuforia error reporting.\n * Fixes ftctechnh/ftc_app issues 621, 713.\n * Miscellaneous bug fixes and improvements.\n\n**************************************************************************************\n# Release Information\n**************************************************************************************\n\nVersion 4.3 (built on 18.10.31)\n * Includes missing TensorFlow-related libraries and files.\n\n**************************************************************************************\n# Release Information\n**************************************************************************************\n\nVersion 4.2 (built on 18.10.30)\n * Includes fix to avoid deadlock situation with WatchdogMonitor which could result in USB communication errors.\n     - Comm error appeared to require that user disconnect USB cable and restart the Robot Controller app to recover.\n     - robotControllerLog.txt would have error messages that included the words \"E RobotCore: lynx xmit lock: #### abandoning lock:\"\n * Includes fix to correctly list the parent module address for a REV Robotics Expansion Hub in a configuration (.xml) file.\n     - Bug in versions 4.0 and 4.1 would incorrect list the address module for a parent REV Robotics device as \"1\".\n     - If the parent module had a higher address value than the daisy-chained module, then this bug would prevent the Robot Controller from communicating with the downstream Expansion Hub.\n * Added requirement for ACCESS_COARSE_LOCATION to allow a Driver Station running Android Oreo to scan for Wi-Fi Direct devices.\n * Added google() repo to build.gradle because aapt2 must be downloaded from the google() repository beginning with version 3.2 of the Android Gradle Plugin.\n     - Important Note: Android Studio users will need to be connected to the Internet the first time build the ftc_app project.\n     - Internet connectivity is required for the first build so the appropriate files can be downloaded from the Google repository.\n     - Users should not need to be connected to the Internet for subsequent builds.\n     - This should also fix buid issue where Android Studio would complain that it \"Could not find com.android.tools.lint:lint-gradle:26.1.4\" (or similar).\n * Added support for REV Spark Mini motor controller as part of the configuration menu for a servo/PWM port on the REV Expansion Hub.\n * Provide examples for playing audio files in an Op Mode.\n * Block Development Tool Changes\n     - Includes a fix for a problem with the Velocity blocks that were reported in the FTC Technology forum (Blocks Programming subforum).\n     - Change the \"Save completed successfully.\" message to a white color so it will contrast with a green background.\n     - Fixed the \"Download image\" feature so it will work if there are text blocks in the op mode.    \n * Introduce support for Google's TensorFlow Lite technology for object detetion for 2018-2019 game.\n     - TensorFlow lite can recognize Gold Mineral and Silver Mineral from 2018-2019 game.\n     - Example Java and Block op modes are included to show how to determine the relative position of the gold block (left, center, right).\n\n**************************************************************************************\n# Release Information\n**************************************************************************************\n\nVersion 4.1 (released on 18.09.24)\n\nChanges include:\n * Fix to prevent crash when deprecated configuration annotations are used.\n * Change to allow FTC Robot Controller APK to be auto-updated using FIRST Global Control Hub update scripts.\n * Removed samples for non supported / non legal hardware.\n * Improvements to Telemetry.addData block with \"text\" socket.\n * Updated Blocks sample op mode list to include Rover Ruckus Vuforia example.\n * Update SDK library version number.\n     \n**************************************************************************************\n# Release Information\n**************************************************************************************\n\nVersion 4.0 (released on 18.09.12)\n\nChanges include:\n * Initial support for UVC compatible cameras \n    - If UVC camera has a unique serial number, RC will detect and enumerate by serial number.\n    - If UVC camera lacks a unique serial number, RC will only support one camera of that type connected.\n    - Calibration settings for a few cameras are included (see TeamCode/src/main/res/xml/teamwebcamcalibrations.xml for details).\n    - User can upload calibration files from Program and Manage web interface.\n    - UVC cameras seem to draw a fair amount of electrical current from the USB bus.\n         + This does not appear to present any problems for the REV Robotics Control Hub.\n\t + This does seem to create stability problems when using some cameras with an Android phone-based Robot Controller.\n\t + FTC Tech Team is investigating options to mitigate this issue with the phone-based Robot Controllers.\n    - Updated sample Vuforia Navigation and VuMark Op Modes to demonstrate how to use an internal phone-based camera and an external UVC webcam.    \n\n * Support for improved motor control.\n    - REV Robotics Expansion Hub firmware 1.8 and greater will support a feed forward mechanism for closed loop motor control.\n    - FTC SDK has been modified to support PIDF coefficients (proportional, integral, derivative, and feed forward).\n    - FTC Blocks development tool modified to include PIDF programming blocks.\n    - Deprecated older PID-related methods and variables.\n    - REV's 1.8.x PIDF-related changes provide a more linear and accurate way to control a motor.\n\n * Wireless\n    - Added 5GHz support for wireless channel changing for those devices that support it.\n        + Tested with Moto G5 and E4 phones.\n\t+ Also tested with other (currently non-approved) phones such as Samsung Galaxy S8.\n\n* Improved Expansion Hub firmware update support in Robot Controller app\n    - Changes to make the system more robust during the firmware update process (when performed through Robot Controller app).\n    - User no longer has to disconnect a downstream daisy-chained Expansion Hub when updating an Expansion Hub's firmware.\n        + If user is updating an Expansion Hub's firmware through a USB connection, he/she does not have to disconnect RS485 connection to other Expansion Hubs.\n\t+ The user still must use a USB connection to update an Expansion Hub's firmware.\n\t+ The user cannot update the Expansion Hub firmware for a downstream device that is daisy chained through an RS485 connection.\n    - If an Expansion Hub accidentally gets \"bricked\" the Robot Controller app is now more likely to recognize the Hub when it scans the USB bus.\n        + Robot Controller app should be able to detect an Expansion Hub, even if it accidentally was bricked in a previous update attempt.\n\t+ Robot Controller app should be able to install the firmware onto the Hub, even if if accidentally was bricked in a previous update attempt.\n \n * Resiliency\n    - FTC software can detect and enable an FTDI reset feature that is available with REV Robotics v1.8 Expansion Hub firmware and greater.\n        + When enabled, the Expansion Hub can detect if it hasn't communicated with the Robot Controller over the FTDI (USB) connection.\n\t+ If the Hub hasn't heard from the Robot Controller in a while, it will reset the FTDI connection.\n\t+ This action helps system recover from some ESD-induced disruptions.\n    - Various fixes to improve reliability of FTC software.\n     \n * Blocks\n    - Fixed errors with string and list indices in blocks export to java.\n    - Support for USB connected UVC webcams.\n    - Refactored optimized Blocks Vuforia code to support Rover Ruckus image targets.\n    - Added programming blocks to support PIDF (proportional, integral, derivative and feed forward) motor control.\n    - Added formatting options (under Telemetry and Miscellaneous categories) so user can set how many decimal places to display a numerical value.\n    - Support to play audio files (which are uploaded through Blocks web interface) on Driver Station in addition to the Robot Controller.\n    - Fixed bug with Download Image of Blocks feature.\n    - Support for REV Robotics Blinkin LED Controller.\n    - Support for REV Robotics 2m Distance Sensor.\n    - Added support for a REV Touch Sensor (no longer have to configure as a generic digital device).\n    - Added blocks for DcMotorEx methods.\n        + These are enhanced methods that you can use when supported by the motor controller hardware.\n\t+ The REV Robotics Expansion Hub supports these enhanced methods.\n\t+ Enhanced methods include methods to get/set motor velocity (in encoder pulses per second), get/set PIDF coefficients, etc..\n\n * Modest Improvements in Logging\n    - Decrease frequency of battery checker voltage statements.\n    - Removed non-FTC related log statements (wherever possible).\n    - Introduced a \"Match Logging\" feature.\n        + Under \"Settings\" a user can enable/disable this feature (it's disabled by default).\n\t+ If enabled, user provides a \"Match Number\" through the Driver Station user interface (top of the screen).\n\t    * The Match Number is used to create a log file specifically with log statements from that particular Op Mode run.\n\t    * Match log files are stored in /sdcard/FIRST/matlogs on the Robot Controller.\n\t    * Once an op mode run is complete, the Match Number is cleared.\n\t    * This is a convenient way to create a separate match log with statements only related to a specific op mode run.\n \n * New Devices\n    - Support for REV Robotics Blinkin LED Controller.\n    - Support for REV Robotics 2m Distance Sensor.\n    - Added configuration option for REV 20:1 HD Hex Motor.\n    - Added support for a REV Touch Sensor (no longer have to configure as a generic digital device).\n    \n * Miscellaneous\n    - Fixed some errors in the definitions for acceleration and velocity in our javadoc documentation.\n    - Added ability to play audio files on Driver Station\n    - When user is configuring an Expansion Hub, the LED on the Expansion Hub will change blink pattern (purple-cyan)  to indicate which Hub is currently being configured.\n    - Renamed I2cSensorType to I2cDeviceType.\n    - Added an external sample Op Mode that demonstrates localization using 2018-2019 (Rover Ruckus presented by QualComm) Vuforia targets.\n    - Added an external sample Op Mode that demonstrates how to use the REV Robotics 2m Laser Distance Sensor.\n    - Added an external sample Op Mode that demonstrates how to use the REV Robotics Blinkin LED Controller.\n    - Re-categorized external Java sample Op Modes to \"TeleOp\" instead of \"Autonomous\".\n    \nKnown issues:\n * Initial support for UVC compatible cameras\n    - UVC cameras seem to draw significant amount of current from the USB bus.\n        + This does not appear to present any problems for the REV Robotics Control Hub.\n\t+ This does seem to create stability problems when using some cameras with an Android phone-based Robot Controller.\n\t+ FTC Tech Team is investigating options to mitigate this issue with the phone-based Robot Controllers.\n    - There might be a possible deadlock which causes the RC to become unresponsive when using a UVC webcam with a Nougat Android Robot Controller.\n\n * Wireless\n    - When user selects a wireless channel, this channel does not necessarily persist if the phone is power cycled.\n        + Tech Team is hoping to eventually address this issue in a future release.\n\t+ Issue has been present since apps were introduced (i.e., it is not new with the v4.0 release).\n    - Wireless channel is not currently displayed for WiFi Direct connections.\n\n * Miscellaneous\n    - The blink indication feature that shows which Expansion Hub is currently being configured does not work for a newly created configuration file.\n        + User has to first save a newly created configuration file and then close and re-edit the file in order for blink indicator to work.\n\n\n**************************************************************************************\n# Release Information\n**************************************************************************************\n\nVersion 3.6 (built on 17.12.18)\n\nChanges include:\n * Blocks Changes\n     - Uses updated Google Blockly software to allow users to edit their op modes on Apple iOS devices (including iPad and iPhone).\n     - Improvement in Blocks tool to handle corrupt op mode files.\n     - Autonomous op modes should no longer get switched back to tele-op after re-opening them to be edited.\n     - The system can now detect type mismatches during runtime and alert the user with a message on the Driver Station.\n * Updated javadoc documentation for setPower() method to reflect correct range of values (-1 to +1).\n * Modified VuforiaLocalizerImpl to allow for user rendering of frames\n     - Added a user-overrideable onRenderFrame() method which gets called by the class's renderFrame() method.\n\n**************************************************************************************\n# Release Information\n**************************************************************************************\n\nVersion 3.5 (built on 17.10.30)\n\nChanges with version 3.5 include:\n * Introduced a fix to prevent random op mode stops, which can occur after the Robot Controller app has been paused and then resumed (for example, when a user temporarily turns off the display of the Robot Controller phone, and then turns the screen back on).\n * Introduced a fix to prevent random op mode stops, which were previously caused by random peer disconnect events on the Driver Station.\n * Fixes issue where log files would be closed on pause of the RC or DS, but not re-opened upon resume.\n * Fixes issue with battery handler (voltage) start/stop race.\n * Fixes issue where Android Studio generated op modes would disappear from available list in certain situations.\n * Fixes problem where OnBot Java would not build on REV Robotics Control Hub.\n * Fixes problem where OnBot Java would not build if the date and time on the Robot Controller device was \"rewound\" (set to an earlier date/time).\n * Improved error message on OnBot Java that occurs when renaming a file fails.\n * Removed unneeded resources from android.jar binaries used by OnBot Java to reduce final size of Robot Controller app.\n * Added MR_ANALOG_TOUCH_SENSOR block to Blocks Programming Tool.\n\n**************************************************************************************\n# Release Information\n**************************************************************************************\n\nVersion 3.4 (built on 17.09.06)\n\nChanges with version 3.4 include:\n * Added telemetry.update() statement for BlankLinearOpMode template.\n * Renamed sample Block op modes to be more consistent with Java samples.\n * Added some additional sample Block op modes.\n * Reworded OnBot Java readme slightly.\n\n**************************************************************************************\n\nVersion 3.3 (built on 17.09.04)\n\nThis version of the software includes improves for the FTC Blocks Programming Tool and the OnBot Java Programming Tool.\n\nChanges with verion 3.3 include:\n * Android Studio ftc_app project has been updated to use Gradle Plugin 2.3.3.\n * Android Studio ftc_app project is already using gradle 3.5 distribution.\n * Robot Controller log has been renamed to /sdcard/RobotControllerLog.txt (note that this change was actually introduced w/ v3.2).\n * Improvements in I2C reliability.\n * Optimized I2C read for REV Expansion Hub, with v1.7 firmware or greater.\n * Updated all external/samples (available through OnBot and in Android project folder).\n * Vuforia\n    - Added support for VuMarks that will be used for the 2017-2018 season game.\n * Blocks\n    - Update to latest Google Blockly release.\n    - Sample op modes can be selected as a template when creating new op mode.\n    - Fixed bug where the blocks would disappear temporarily when mouse button is held down.\n    - Added blocks for Range.clip and Range.scale.\n    - User can now disable/enable Block op modes.\n    - Fix to prevent occasional Blocks deadlock.\n * OnBot Java\n    - Significant improvements with autocomplete function for OnBot Java editor.\n    - Sample op modes can be selected as a template when creating new op mode.\n    - Fixes and changes to complete hardware setup feature.\n    - Updated (and more useful) onBot welcome message.\n    \nKnown issues:\n * Android Studio\n    - After updating to the new v3.3 Android Studio project folder, if you get error messages indicating \"InvalidVirtualFileAccessException\" then you might need to do a File->Invalidate Caches / Restart to clear the error.\n * OnBot Java\n    - Sometimes when you push the build button to build all op modes, the RC returns an error message that the build failed.  If you press the build button a second time, the build typically suceeds.\n    \n**************************************************************************************\n\nVersion 3.2 (built on 17.08.02)\n\nThis version of the software introduces the \"OnBot Java\" Development Tool.  Similar to the FTC Blocks Development Tool, the FTC OnBot Java Development Tool allows a user to create, edit and build op modes dynamically using only a Javascript-enabled web browser.\n\nThe OnBot Java Development Tool is an integrated development environment (IDE) that is served up by the Robot Controller.  Op modes are created and edited using a Javascript-enabled browser (Google Chromse is recommended).  Op modes are saved on the Robot Controller Android device directly.  \n\nThe OnBot Java Development Tool provides a Java programming environment that does NOT need Android Studio.\n\n\n\nChanges with version 3.2 include:\n * Enhanced web-based development tools\n    - Introduction of OnBot Java Development Tool.\n    - Web-based programming and management features are \"always on\" (user no longer needs to put Robot Controller into programming mode).\n    - Web-based management interface (where user can change Robot Controller name and also easily download Robot Controller log file).\n    - OnBot Java, Blocks and Management features available from web based interface.\n\n* Blocks Programming Development Tool:\n    - Changed \"LynxI2cColorRangeSensor\" block to \"REV Color/range sensor\" block.\n    - Fixed tooltip for ColorSensor.isLightOn block.\n    Added blocks for ColorSensor.getNormalizedColors and LynxI2cColorRangeSensor.getNormalizedColors.\n\n* Added example op modes for digital touch sensor and REV Robotics Color Distance sensor.\n* User selectable color themes.\n* Includes many minor enhancements and fixes (too numerous to list).\n\nKnown issues:\n* Auto complete function is incomplete and does not support the following (for now):\n     - Access via *this* keyword\n     - Access via *super* keyword\n     - Members of the super cloass, not overridden by the class\n     - Any methods provided in the current class\n     - Inner classes\n     - Can't handle casted objects\n     - Any objects coming from an parenthetically enclosed expression\n\n**************************************************************************************\n\nVersion 3.10 (built on 17.05.09)\n\nThis version of the software provides support for the REV Robotics Expansion Hub.  This version also includes improvements in the USB communication layer in an effort to enhance system resiliency.  If you were using a 2.x version of the software previously, updating to version 3.1 requires that you also update your Driver Station software in addition to updating the Robot Controller software.\n\nAlso note that in version 3.10 software, the setMaxSpeed and getMaxSpeed methods are no longer available (not deprecated, they have been removed from the SDK). Also note that the the new 3.x software incorporates motor profiles that a user can select as he/she configures the robot.\n\nChanges include:\n * Blocks changes\n    - Added VuforiaTrackableDefaultListener.getPose and Vuforia.trackPose blocks.\n    - Added optimized blocks support for Vuforia extended tracking.\n    - Added atan2 block to the math category.\n    - Added useCompetitionFieldTargetLocations parameter to Vuforia.initialize block.  If set to false, the target locations are placed at (0,0,0) with target orientation as specified in https://github.com/gearsincorg/FTCVuforiaDemo/blob/master/Robot_Navigation.java tutorial op mode.\n * Incorporates additional improvements to USB comm layer to improve system resiliency (to recover from a greater number of communication disruptions).\n\n**************************************************************************************\n\nAdditional Notes Regarding Version 3.00 (built on 17.04.13)\n\nIn addition to the release changes listed below (see section labeled \"Version 3.00 (built on 17.04.013)\"), version 3.00 has the following important changes:\n\n1. Version 3.00 software uses a new version of the FTC Robocol (robot protocol).  If you upgrade to v3.0 on the Robot Controller and/or Android Studio side, you must also upgrade the Driver Station software to match the new Robocol.\n2. Version 3.00 software removes the setMaxSpeed and getMaxSpeed methods from the DcMotor class.  If you have an op mode that formerly used these methods, you will need to remove the references/calls to these methods.  Instead, v3.0 provides the max speed information through the use of motor profiles that are selected by the user during robot configuration.\n3. Version 3.00 software currently does not have a mechanism to disable extra i2c sensors.  We hope to re-introduce this function with a release in the near future.\n\n**************************************************************************************\n\nVersion 3.00 (built on 17.04.13)\n\n*** Use this version of the software at YOUR OWN RISK!!! ***\n\nThis software is being released as an \"alpha\" version.  Use this version at your own risk!\n\nThis pre-release software contains SIGNIFICANT changes, including changes to the Wi-Fi Direct pairing mechanism, rewrites of the I2C sensor classes, changes to the USB/FTDI layer, and the introduction of support for the REV Robotics Expansion Hub and the REV Robotics color-range-light sensor.  These changes were implemented to improve the reliability and resiliency of the FTC control system.\n\nPlease note, however, that version 3.00 is considered \"alpha\" code.  This code is being released so that the FIRST community will have an opportunity to test the new REV Expansion Hub electronics module when it becomes available in May.  The developers do not recommend using this code for critical applications (i.e., competition use).\n\n*** Use this version of the software at YOUR OWN RISK!!! ***\n\nChanges include:\n * Major rework of sensor-related infrastructure.  Includes rewriting sensor classes to implement synchronous I2C communication.\n * Fix to reset Autonomous timer back to 30 seconds.\n * Implementation of specific motor profiles for approved 12V motors (includes Tetrix, AndyMark, Matrix and REV models).\n * Modest improvements to enhance Wi-Fi P2P pairing.\n * Fixes telemetry log addition race.\n * Publishes all the sources (not just a select few).\n * Includes Block programming improvements\n    - Addition of optimized Vuforia blocks.\n    - Auto scrollbar to projects and sounds pages.\n    - Fixed blocks paste bug.\n    - Blocks execute after while-opModeIsActive loop (to allow for cleanup before exiting op mode).\n    - Added gyro integratedZValue block.\n    - Fixes bug with projects page for Firefox browser.\n    - Added IsSpeaking block to AndroidTextToSpeech.  \n * Implements support for the REV Robotics Expansion Hub\n    - Implements support for integral REV IMU (physically installed on I2C bus 0, uses same Bosch BNO055 9 axis absolute orientation sensor as Adafruit 9DOF abs orientation sensor).    - Implements support for REV color/range/light sensor.\n    - Provides support to update Expansion Hub firmware through FTC SDK.\n    - Detects REV firmware version and records in log file.\n    - Includes support for REV Control Hub (note that the REV Control Hub is not yet approved for FTC use).\n    - Implements FTC Blocks programming support for REV Expansion Hub and sensor hardware.\n    - Detects and alerts when I2C device disconnect.\n\n**************************************************************************************\n\nVersion 2.62 (built on 17.01.07)\n  * Added null pointer check before calling modeToByte() in finishModeSwitchIfNecessary method for ModernRoboticsUsbDcMotorController class.\n  * Changes to enhance Modern Robotics USB protocol robustness.\n\n**************************************************************************************\n\nVersion 2.61 (released on 16.12.19)\n  * Blocks Programming mode changes:\n     - Fix to correct issue when an exception was thrown because an OpticalDistanceSensor object appears twice in the hardware map (the second time as a LightSensor).\n\n**************************************************************************************\n\nVersion 2.6 (released on 16.12.16)\n  * Fixes for Gyro class:\n     - Improve (decrease) sensor refresh latency.\n     - fix isCalibrating issues.\n  * Blocks Programming mode changes:\n     - Blocks now ignores a device in the configuration xml if the name is empty. Other devices work in configuration work fine.\n\n**************************************************************************************\n\nVersion 2.5 (internal release on released on 16.12.13)\n  * Blocks Programming mode changes:\n     - Added blocks support for AdafruitBNO055IMU.\n     - Added Download Op Mode button to FtcBocks.html.\n     - Added support for copying blocks in one OpMode and pasting them in an other OpMode. The clipboard content is stored on the phone, so the programming mode server must be running.\n     - Modified Utilities section of the toolbox.\n     - In Programming Mode, display information about the active connections.\n     - Fixed paste location when workspace has been scrolled.\n     - Added blocks support for the android Accelerometer.\n     - Fixed issue where Blocks Upload Op Mode truncated name at first dot.\n     - Added blocks support for Android SoundPool.\n     - Added type safety to blocks for Acceleration.\n     - Added type safety to blocks for AdafruitBNO055IMU.Parameters.\n     - Added type safety to blocks for AnalogInput.\n     - Added type safety to blocks for AngularVelocity.\n     - Added type safety to blocks for Color.\n     - Added type safety to blocks for ColorSensor.\n     - Added type safety to blocks for CompassSensor.\n     - Added type safety to blocks for CRServo.\n     - Added type safety to blocks for DigitalChannel.\n     - Added type safety to blocks for ElapsedTime.\n     - Added type safety to blocks for Gamepad.\n     - Added type safety to blocks for GyroSensor.\n     - Added type safety to blocks for IrSeekerSensor.\n     - Added type safety to blocks for LED.\n     - Added type safety to blocks for LightSensor.\n     - Added type safety to blocks for LinearOpMode.\n     - Added type safety to blocks for MagneticFlux.\n     - Added type safety to blocks for MatrixF.     \n     - Added type safety to blocks for MrI2cCompassSensor.\n     - Added type safety to blocks for MrI2cRangeSensor.\n     - Added type safety to blocks for OpticalDistanceSensor.\n     - Added type safety to blocks for Orientation.\n     - Added type safety to blocks for Position.\n     - Added type safety to blocks for Quaternion.\n     - Added type safety to blocks for Servo.\n     - Added type safety to blocks for ServoController.\n     - Added type safety to blocks for Telemetry.\n     - Added type safety to blocks for Temperature.\n     - Added type safety to blocks for TouchSensor.\n     - Added type safety to blocks for UltrasonicSensor.\n     - Added type safety to blocks for VectorF.\n     - Added type safety to blocks for Velocity.\n     - Added type safety to blocks for VoltageSensor.\n     - Added type safety to blocks for VuforiaLocalizer.Parameters.\n     - Added type safety to blocks for VuforiaTrackable.\n     - Added type safety to blocks for VuforiaTrackables.\n     - Added type safety to blocks for enums in AdafruitBNO055IMU.Parameters.\n     - Added type safety to blocks for AndroidAccelerometer, AndroidGyroscope, AndroidOrientation, and AndroidTextToSpeech.\n\n**************************************************************************************\n\nVersion 2.4 (released on 16.11.13)\n  * Fix to avoid crashing for nonexistent resources.\n  * Blocks Programming mode changes:\n     - Added blocks to support OpenGLMatrix, MatrixF, and VectorF.\n     - Added blocks to support AngleUnit, AxesOrder, AxesReference, CameraDirection, CameraMonitorFeedback, DistanceUnit, and TempUnit.\n     - Added blocks to support Acceleration.\n     - Added blocks to support LinearOpMode.getRuntime.\n     - Added blocks to support MagneticFlux and Position.\n     - Fixed typos.\n     - Made blocks for ElapsedTime more consistent with other objects.\n     - Added blocks to support Quaternion, Velocity, Orientation, AngularVelocity.\n     - Added blocks to support VuforiaTrackables, VuforiaTrackable, VuforiaLocalizer, VuforiaTrackableDefaultListener.\n     - Fixed a few blocks.\n     - Added type checking to new blocks.\n     - Updated to latest blockly.\n     - Added default variable blocks to navigation and matrix blocks.\n     - Fixed toolbox entry for openGLMatrix_rotation_withAxesArgs.\n     - When user downloads Blocks-generated op mode, only the .blk file is downloaded.\n     - When user uploads Blocks-generated op mode (.blk file), Javascript code is auto generated.\n     - Added DbgLog support.\n     - Added logging when a blocks file is read/written.\n     - Fixed bug to properly render blocks even if missing devices from configuration file.\n     - Added support for additional characters (not just alphanumeric) for the block file names (for download and upload).\n     - Added support for OpMode flavor (â€œAutonomousâ€ or â€œTeleOpâ€) and group.\n  * Changes to Samples to prevent tutorial issues.\n  * Incorporated suggested changes from public pull 216 (â€œReplace .. pathsâ€).\n  * Remove Servo Glitches when robot stopped.\n  * if user hits â€œCancelsâ€ when editing a configuration file, clears the unsaved changes and reverts to original unmodified configuration.\n  * Added log info to help diagnose why the Robot Controller app was terminated (for example, by watch dog function).\n  * Added ability to transfer log from the controller.\n  * Fixed inconsistency for AngularVelocity\n  * Limit unbounded growth of data for telemetry.  If user does not call telemetry.update() for LinearOpMode in a timely manner, data added for telemetry might get lost if size limit is exceeded.\n\n**************************************************************************************\n\nVersion 2.35 (released on 16.10.06)\n  * Blockly programming mode - Removed unnecesary idle() call from blocks for new project.\n\n**************************************************************************************\n\nVersion 2.30 (released on 16.10.05)\n  * Blockly programming mode:\n     - Mechanism added to save Blockly op modes from Programming Mode Server onto local device\n     - To avoid clutter, blocks are displayed in categorized folders\n     - Added support for DigitalChannel\n     - Added support for ModernRoboticsI2cCompassSensor\n     - Added support for ModernRoboticsI2cRangeSensor\n     - Added support for VoltageSensor\n     - Added support for AnalogInput\n     - Added support for AnalogOutput\n     - Fix for CompassSensor setMode block\n  * Vuforia\n     - Fix deadlock / make camera data available while Vuforia is running.\n     - Update to Vuforia 6.0.117 (recommended by Vuforia and Google to close security loophole).\n  * Fix for autonomous 30 second timer bug (where timer was in effect, even though it appeared to have timed out).\n  * opModeIsActive changes to allow cleanup after op mode is stopped (with enforced 2 second safety timeout).\n  * Fix to avoid reading i2c twice.\n  * Updated sample Op Modes.\n  * Improved logging and fixed intermittent freezing.\n  * Added digital I/O sample.\n  * Cleaned up device names in sample op modes to be consistent with Pushbot guide.\n  * Fix to allow use of IrSeekerSensorV3.\n\n**************************************************************************************\n\nVersion 2.20 (released on 16.09.08)\n  * Support for Modern Robotics Compass Sensor.\n  * Support for Modern Robotics Range Sensor.\n  * Revise device names for Pushbot templates to match the names used in Pushbot guide.\n  * Fixed bug so that IrSeekerSensorV3 device is accessible as IrSeekerSensor in hardwareMap.\n  * Modified computer vision code to require an individual Vuforia license (per legal requirement from PTC).\n  * Minor fixes.\n  * Blockly enhancements:\n     - Support for Voltage Sensor.\n     - Support for Analog Input.\n     - Support for Analog Output.\n     - Support for Light Sensor.\n     - Support for Servo Controller.\n\n**************************************************************************************\n\nVersion 2.10 (released on 16.09.03)\n * Support for Adafruit IMU.\n * Improvements to ModernRoboticsI2cGyro class\n    - Block on reset of z axis.\n    - isCalibrating() returns true while gyro is calibration.\n * Updated sample gyro program.\n * Blockly enhancements\n    - support for android.graphics.Color.\n    - added support for ElapsedTime.\n    - improved look and legibility of blocks.\n    - support for compass sensor.\n    - support for ultrasonic sensor.\n    - support for IrSeeker.\n    - support for LED.\n    - support for color sensor.\n    - support for CRServo\n    - prompt user to configure robot before using programming mode.\n * Provides ability to disable audio cues.\n * various bug fixes and improvements.\n\n**************************************************************************************\n\nVersion 2.00 (released on 16.08.19)\n * This is the new release for the upcoming 2016-2017 FIRST Tech Challenge Season.\n * Channel change is enabled in the FTC Robot Controller app for Moto G 2nd and 3rd Gen phones.\n * Users can now use annotations to register/disable their Op Modes.\n * Changes in the Android SDK, JDK and build tool requirements (minsdk=19, java 1.7, build tools 23.0.3).\n * Standardized units in analog input.\n * Cleaned up code for existing analog sensor classes.\n * setChannelMode and getChannelMode were REMOVED from the DcMotorController class.  This is important - we no longer set the motor modes through the motor controller.\n * setMode and getMode were added to the DcMotor class.  \n * ContinuousRotationServo class has been added to the FTC SDK.\n * Range.clip() method has been overloaded so it can support this operation for int, short and byte integers.\n * Some changes have been made (new methods added) on how a user can access items from the hardware map.\n * Users can now set the zero power behavior for a DC motor so that the motor will brake or float when power is zero.\n * Prototype Blockly Programming Mode has been added to FTC Robot Controller.  Users can place the Robot Controller into this mode, and then use a device (such as a laptop) that has a Javascript enabled browser to write Blockly-based Op Modes directly onto the Robot Controller.\n * Users can now configure the robot remotely through the FTC Driver Station app.\n * Android Studio project supports Android Studio 2.1.x and compile SDK Version 23 (Marshmallow).\n * Vuforia Computer Vision SDK integrated into FTC SDK.  Users can use sample vision targets to get localization information on a standard FTC field.\n * Project structure has been reorganized so that there is now a TeamCode package that users can use to place their local/custom Op Modes into this package.\n * Inspection function has been integrated into the FTC Robot Controller and Driver Station Apps (Thanks Team HazMatâ€¦ 9277 & 10650!).\n * Audio cues have been incorporated into FTC SDK.\n * Swap mechanism added to FTC Robot Controller configuration activity.  For example, if you have two motor controllers on a robot, and you misidentified them in your configuration file, you can use the Swap button to swap the devices within the configuration file (so you do not have to manually re-enter in the configuration info for the two devices).\n * Fix mechanism added to all user to replace an electronic module easily.  For example, suppose a servo controller dies on your robot. You replace the broken module with a new module, which has a different serial number from the original servo controller.  You can use the Fix button to automatically reconfigure your configuration file to use the serial number of the new module.\n * Improvements made to fix resiliency and responsiveness of the system.\n * For LinearOpMode the user now must for a telemetry.update() to update the telemetry data on the driver station.  This update() mechanism ensures that the driver station gets the updated data properly and at the same time.\n * The Auto Configure function of the Robot Controller is now template based.  If there is a commonly used robot configuration, a template can be created so that the Auto Configure mechanism can be used to quickly configure a robot of this type.\n * The logic to detect a runaway op mode (both in the LinearOpMode and OpMode types) and to abort the run, then auto recover has been improved/implemented.\n * Fix has been incorporated so that Logitech F310 gamepad mappings will be correct for Marshmallow users.\n\n**************************************************************************************\n\nRelease 16.07.08\n\n * For the ftc_app project, the gradle files have been modified to support Android Studio 2.1.x.\n\n\n\n**************************************************************************************\n\nRelease 16.03.30\n\n * For the MIT App Inventor, the design blocks have new icons that better represent the function of each design component.\n * Some changes were made to the shutdown logic to ensure the robust shutdown of some of our USB services.\n * A change was made to LinearOpMode so as to allow a given instance to be executed more than once, which is required for the App Inventor.\n * Javadoc improved/updated.\n\n**************************************************************************************\n\nRelease 16.03.09\n\n * Changes made to make the FTC SDK synchronous (significant change!)\n    - waitOneFullHardwareCycle() and waitForNextHardwareCycle() are no longer needed and have been deprecated.\n    - runOpMode() (for a LinearOpMode) is now decoupled from the system's hardware read/write thread.\n    - loop() (for an OpMode) is now decoupled from the system's hardware read/write thread.\n    - Methods are synchronous.\n    - For example, if you call setMode(DcMotorController.RunMode.RESET_ENCODERS) for a motor, the encoder is guaranteed to be reset when the method call is complete.\n    - For legacy module (NXT compatible), user no longer has to toggle between read and write modes when reading from or writing to a legacy device.\n * Changes made to enhance reliability/robustness during ESD event.\n * Changes made to make code thread safe.\n * Debug keystore added so that user-generated robot controller APKs will all use the same signed key (to avoid conflicts if a team has multiple developer laptops for example).\n * Firmware version information for Modern Robotics modules are now logged.\n * Changes made to improve USB comm reliability and robustness.\n * Added support for voltage indicator for legacy (NXT-compatible) motor controllers.\n * Changes made to provide auto stop capabilities for op modes.\n    - A LinearOpMode class will stop when the statements in runOpMode() are complete.  User does not have to push the stop button on the driver station.\n    - If an op mode is stopped by the driver station, but there is a run away/uninterruptible thread persisting, the app will log an error message then force itself to crash to stop the runaway thread.\n * Driver Station UI modified to display lowest measured voltage below current voltage (12V battery).\n * Driver Station UI modified to have color background for current voltage (green=good, yellow=caution, red=danger, extremely low voltage).\n * javadoc improved (edits and additional classes).\n * Added app build time to About activity for driver station and robot controller apps.\n * Display local IP addresses on Driver Station About activity.\n * Added I2cDeviceSynchImpl.\n * Added I2cDeviceSync interface.\n * Added seconds() and milliseconds() to ElapsedTime for clarity.\n * Added getCallbackCount() to I2cDevice.\n * Added missing clearI2cPortActionFlag.\n * Added code to create log messages while waiting for LinearOpMode shutdown.\n * Fix so Wifi Direct Config activity will no longer launch multiple times.\n * Added the ability to specify an alternate i2c address in software for the Modern Robotics gyro.\n\n**************************************************************************************\n\nRelease 16.02.09\n\n * Improved battery checker feature so that voltage values get refreshed regularly (every 250 msec) on Driver Station (DS) user interface.\n * Improved software so that Robot Controller (RC) is much more resilient and â€œself-healingâ€ to USB disconnects:\n    - If user attempts to start/restart RC with one or more module missing, it will display a warning but still start up.\n    - When running an op mode, if one or more modules gets disconnected, the RC & DS will display warnings,and robot will keep on working in spite of the missing module(s).\n    - If a disconnected module gets physically reconnected the RC will auto detect the module and the user will regain control of the recently connected module.\n    - Warning messages are more helpful (identifies the type of module thatâ€™s missing plus its USB serial number).   \n * Code changes to fix the null gamepad reference when users try to reference the gamepads in the init() portion of their op mode.\n * NXT light sensor output is now properly scaled.  Note that teams might have to readjust their light threshold values in their op modes.\n * On DS user interface, gamepad icon for a driver will disappear if the matching gamepad is disconnected or if that gamepad gets designated as a different driver.\n * Robot Protocol (ROBOCOL) version number info is displayed in About screen on RC and DS apps.\n * Incorporated a display filter on pairing screen to filter out devices that donâ€™t use the â€œ<TEAM NUMBER>-â€œ format. This filter can be turned off to show all WiFi Direct devices.\n * Updated text in License file.\n * Fixed formatting error in OpticalDistanceSensor.toString().\n * Fixed issue on with a blank (â€œâ€) device name that would disrupt WiFi Direct Pairing.\n * Made a change so that the WiFi info and battery info can be displayed more quickly on the DS upon connecting to RC.\n * Improved javadoc generation.\n * Modified code to make it easier to support language localization in the future.\n\n**************************************************************************************\n\nRelease 16.01.04\n\n * Updated compileSdkVersion for apps\n * Prevent Wifi from entering power saving mode\n * removed unused import from driver station\n * Corrrected \"Dead zone\" joystick code.\n * LED.getDeviceName and .getConnectionInfo() return null\n * apps check for ROBOCOL_VERSION mismatch\n * Fix for Telemetry also has off-by-one errors in its data string sizing / short size limitations error\n * User telemetry output is sorted.\n * added formatting variants to DbgLog and RobotLog APIs\n * code modified to allow for a long list of op mode names.\n * changes to improve thread safety of RobocolDatagramSocket\n * Fix for \"missing hardware leaves robot controller disconnected from driver station\" error\n * fix for \"fast tapping of Init/Start causes problems\" (toast is now only instantiated on UI thread).\n * added some log statements for thread life cycle.\n * moved gamepad reset logic inside of initActiveOpMode() for robustness\n * changes made to mitigate risk of race conditions on public methods.\n * changes to try and flag when WiFi Direct name contains non-printable characters.\n * fix to correct race condition between .run() and .close() in ReadWriteRunnableStandard.\n * updated FTDI driver\n * made ReadWriteRunnableStanard interface public.\n * fixed off-by-one errors in Command constructor\n * moved specific hardware implmentations into their own package.\n * moved specific gamepad implemnatations to the hardware library.\n * changed LICENSE file to new BSD version.\n * fixed race condition when shutting down Modern Robotics USB devices.\n * methods in the ColorSensor classes have been synchronized.\n * corrected isBusy() status to reflect end of motion.\n * corrected \"back\" button keycode.\n * the notSupported() method of the GyroSensor class was changed to protected (it should not be public).\n\n\n**************************************************************************************\n\nRelease 15.11.04.001\n\n * Added Support for Modern Robotics Gyro.\n  - The GyroSensor class now supports the MR Gyro Sensor.\n  - Users can access heading data (about Z axis)\n  - Users can also access raw gyro data (X, Y, & Z axes).\n  - Example MRGyroTest.java op mode included.\n * Improved error messages\n  - More descriptive error messages for exceptions in user code.\n * Updated DcMotor API\n * Enable read mode on new address in setI2cAddress\n * Fix so that driver station app resets the gamepads when switching op modes.\n * USB-related code changes to make USB comm more responsive and to display more explicit error messages.\n  - Fix so that USB will recover properly if the USB bus returns garbage data.\n  - Fix USB initializtion race condition.\n  - Better error reporting during FTDI open.\n  - More explicit messages during USB failures.\n  - Fixed bug so that USB device is closed if event loop teardown method was not called.\n * Fixed timer UI issue\n * Fixed duplicate name UI bug (Legacy Module configuration).\n * Fixed race condition in EventLoopManager.\n * Fix to keep references stable when updating gamepad.\n * For legacy Matrix motor/servo controllers removed necessity of appending \"Motor\" and \"Servo\" to controller names.\n * Updated HT color sensor driver to use constants from ModernRoboticsUsbLegacyModule class.\n * Updated MR color sensor driver to use constants from ModernRoboticsUsbDeviceInterfaceModule class.\n * Correctly handle I2C Address change in all color sensors\n * Updated/cleaned up op modes.\n  - Updated comments in LinearI2cAddressChange.java example op mode.\n  - Replaced the calls to \"setChannelMode\" with \"setMode\" (to match the new of the DcMotor  method).\n  - Removed K9AutoTime.java op mode.\n  - Added MRGyroTest.java op mode (demonstrates how to use MR Gyro Sensor).\n  - Added MRRGBExample.java op mode (demonstrates how to use MR Color Sensor).\n  - Added HTRGBExample.java op mode (demonstrates how to use HT legacy color sensor).\n  - Added MatrixControllerDemo.java (demonstrates how to use legacy Matrix controller).\n * Updated javadoc documentation.\n * Updated release .apk files for Robot Controller and Driver Station apps.\n\nT. Eng\nNovember 5, 2015\n\n**************************************************************************************\n\nRelease 15.10.06.002\n\n * Added support for Legacy Matrix 9.6V motor/servo controller.\n * Cleaned up build.gradle file.\n * Minor UI and bug fixes for driver station and robot controller apps.\n * Throws error if Ultrasonic sensor (NXT) is not configured for legacy module port 4 or 5.\n\nT. Eng\nOctober 6, 2015\n\n**************************************************************************************\n\nIn this latest version of the FTC SDK (20150803_001) the following changes should be noted:\n\n * New user interfaces for FTC Driver Station and FTC Robot Controller apps.\n * An init() method is added to the OpMode class.\n   - For this release, init() is triggered right before the start() method.\n   - Eventually, the init() method will be triggered when the user presses an \"INIT\" button on driver station.\n   - The init() and loop() methods are now required (i.e., need to be overridden in the user's op mode).\n   - The start() and stop() methods are optional.\n * A new LinearOpMode class is introduced.\n   - Teams can use the LinearOpMode mode to create a linear (not event driven) program model.\n   - Teams can use blocking statements like Thread.sleep() within a linear op mode.\n * The API for the Legacy Module and Core Device Interface Module have been updated.\n   - Support for encoders with the Legacy Module is now working.\n * The hardware loop has been updated for better performance.\n\n\nT. Eng\nAugust 3, 2015\n",
    "readme_length": 57542
  },
  {
    "name": "rpi_gpio",
    "full_name": "ClockVapor/rpi_gpio",
    "description": "Ruby conversion of RPi.GPIO Python module",
    "stars": 222,
    "forks": 33,
    "language": "C",
    "url": "https://github.com/ClockVapor/rpi_gpio",
    "topics": [
      "gpio",
      "gpio-pins",
      "pwm",
      "raspberry-pi",
      "raspberry-pi-3",
      "raspberry-pi-4",
      "raspberrypi",
      "rpi",
      "rpi-gpio",
      "ruby"
    ],
    "created_at": "2014-12-31T05:04:56Z",
    "updated_at": "2025-11-16T15:18:19Z",
    "homepage": "https://rubygems.org/gems/rpi_gpio",
    "license": "MIT License",
    "readme": "# rpi_gpio v0.5.0\n\nRuby conversion of [RPi.GPIO Python module](https://pypi.python.org/pypi/RPi.GPIO)\n\n## Features\n\nManipulate your Raspberry Pi's GPIO pins from Ruby!\n\n- Boolean input/output\n- Software-driven PWM (written in C for speed)\n- Event-driven input (blocking and non-blocking)\n\nUp-to-date with RPi.GPIO Python module version 0.7.0, so it works on all Raspberry Pi models!\n\n## Sample Usage\n\nI aimed to make the gem's usage exactly the same as its Python counterpart -- only with a few semantic differences to utilize Ruby's readability. If anything is confusing, you can always check [here](http://sourceforge.net/p/raspberry-gpio-python/wiki/Examples/) for the original Python module's documentation.\n\n#### Download the gem\n\nThe easiest way to download the gem is to use [Bundler](http://bundler.io/) with a Gemfile. In your Gemfile, include the line \n```ruby\ngem 'rpi_gpio'\n```\nThen you can run `bundle install` to automatically download and compile the gem for your system. To include the gem in a Ruby file, use the line `require 'rpi_gpio'`.\n\n#### Pin numbering\n\nBefore you can do anything with the GPIO pins, you need to specify how you want to number them.\n```ruby\nRPi::GPIO.set_numbering :board\n# or\nRPi::GPIO.set_numbering :bcm\n````\n`:board` numbering refers to the physical pin numbers on the Pi, whereas `:bcm` numbering refers to the Broadcom SOC channel numbering. Note that `:bcm` numbering differs between Pi models, while `:board` numbering does not.\n\n#### Input\n\nTo receive input from a GPIO pin, you must first initialize it as an input pin:\n```ruby\nRPi::GPIO.setup PIN_NUM, :as => :input\n# or\nRPi::GPIO.setup [PIN1_NUM, PIN2_NUM, ...], :as => :input\n```\nThe pin number will differ based on your selected numbering system and which pin you want to use.\n\nYou can use the additional hash argument `:pull` to apply a pull-up or pull-down resistor to the input pin like so:\n```ruby\nRPi::GPIO.setup PIN_NUM, :as => :input, :pull => :down\n# or\nRPi::GPIO.setup PIN_NUM, :as => :input, :pull => :up\n# or (not necessary; :off is the default value)\nRPi::GPIO.setup PIN_NUM, :as => :input, :pull => :off\n```\n\nNow you can use the calls\n```ruby\nRPi::GPIO.high? PIN_NUM\nRPi::GPIO.low? PIN_NUM\n```\nto receive either `true` or `false`.\n\nIf you prefer to use a callback when a pin edge is detected, you can use the `watch` method:\n```ruby\nRPi::GPIO.watch PIN_NUM, :on => :rising do |pin, value| # :on supports :rising, :falling, and :both\n  ...\nend\n```\n\n`watch` also supports the optional `bounce_time` parameter found in the Python module to prevent duplicate events from firing:\n```ruby\nRPi::GPIO.watch PIN_NUM, :on => :falling, :bounce_time => 200 do |pin, value|\n  ...\nend\n```\n\nTo stop watching a pin, use `stop_watching`:\n```ruby\nRPi::GPIO.stop_watching PIN_NUM\n```\n\nIf you want to block execution until a pin edge is detected, there's `wait_for_edge`:\n```ruby\nputs 'Waiting to start...'\nRPi::GPIO.wait_for_edge PIN_NUM, :rising # :rising, :falling, and :both are also supported here\nputs 'Here we go!'\n```\n\n`wait_for_edge` accepts optional `bounce_time` and `timeout` arguments too:\n```ruby\nputs 'Waiting to start...'\nvalue = RPi::GPIO.wait_for_edge PIN_NUM, :falling, :bounce_time => 200, :timeout => 5000\nif value.nil? # nil is returned if the timeout is reached\n  print 'You took too long. '\nend\nputs 'Here we go!'\n```\n\n#### Output\n\nTo send output to a GPIO pin, you must first initialize it as an output pin:\n```ruby\nRPi::GPIO.setup PIN_NUM, :as => :output\n# or\nRPi::GPIO.setup [PIN1_NUM, PIN2_NUM, ...], :as => :output\n```\nNow you can use the calls\n```ruby\nRPi::GPIO.set_high PIN_NUM\nRPi::GPIO.set_low PIN_NUM\n```\nto set the pin either high or low.\n\nYou can use the additional hash argument `:initialize` to set the pin's initial state like so:\n```ruby\nRPi::GPIO.setup PIN_NUM, :as => :output, :initialize => :high\n# or\nRPi::GPIO.setup PIN_NUM, :as => :output, :initialize => :low\n```\n\n#### PWM (pulse-width modulation)\n\nPulse-width modulation is a useful tool for controlling things like LED brightness or motor speed. To utilize PWM, first create a PWM object for an [output pin](#output).\n```ruby\npwm = RPi::GPIO::PWM.new(PIN_NUM, PWM_FREQ)\n```\nThe `PWM_FREQ` is a value in hertz that specifies the amount of pulse cycles per second.\n\nNow you can call the following method to start PWM:\n```ruby\npwm.start DUTY_CYCLE\n```\n`DUTY_CYCLE` is a value from `0.0` to `100.0` indicating the percent of the time that the signal will be high.\n\nOnce running, you can get/set the PWM duty cycle with\n```ruby\npwm.duty_cycle # get\npwm.duty_cycle = NEW_DUTY_CYCLE # set\n```\nget/set the PWM frequency with\n```ruby\npwm.frequency # get\npwm.frequency = NEW_FREQUENCY # set\n```\nand get the PWM GPIO number with\n```ruby\npwm.gpio\n```\nNote that this number corresponds to `:bcm` numbering of the GPIO pins, so it will be different than pin number you used if you created the PWM with `:board` numbering.\n\nTo stop PWM, use\n```ruby\npwm.stop\n```\n\nTo check if a PWM object is currently running, use\n```ruby\npwm.running?\n```\n\n#### Cleaning up\n\nAfter your program is finished using the GPIO pins, it's a good idea to release them so other programs can use them later. Simply call\n```ruby\nRPi::GPIO.clean_up PIN_NUM\n```\nto release a specific pin, or\n```ruby\nRPi::GPIO.clean_up\n```\nto release all allocated pins.\n\nAlternatively, you can call\n```ruby\nRPi::GPIO.reset\n```\nto clean up all pins and to also reset the selected numbering mode.\n\n## Credits\n\nOriginal Python code by Ben Croston modified for Ruby by Nick Lowery\n\nCopyright (c) 2014-2020 [Nick Lowery](https://github.com/ClockVapor)\n\nView LICENSE for full license.\n\n",
    "readme_length": 5615
  },
  {
    "name": "fanpico",
    "full_name": "tjko/fanpico",
    "description": "Fanpico: Open Source Programmable PWM (PC) Fan Controller",
    "stars": 201,
    "forks": 15,
    "language": "C",
    "url": "https://github.com/tjko/fanpico",
    "topics": [
      "diy",
      "electronics",
      "fan-control",
      "firmware",
      "hardware",
      "home-assistant",
      "mqtt",
      "mqtt-client",
      "open-source",
      "open-source-hardware",
      "oshw",
      "pcb",
      "pwm",
      "pwm-fan-controller",
      "raspberry-pi-pico",
      "rp2040",
      "scpi",
      "snmp",
      "ssh-server",
      "temperature"
    ],
    "created_at": "2022-01-31T06:40:19Z",
    "updated_at": "2025-11-29T18:43:34Z",
    "homepage": "",
    "license": "GNU General Public License v3.0",
    "readme": "\n# Fanpico: Programmable PWM (PC) Fan Controller\n\n[![CI](https://github.com/tjko/fanpico/actions/workflows/cmake.yml/badge.svg)](https://github.com/tjko/fanpico/actions/workflows/cmake.yml)\n[![CodeQL](https://github.com/tjko/fanpico/actions/workflows/codeql.yml/badge.svg)](https://github.com/tjko/fanpico/actions/workflows/codeql.yml)\n\n<p><a href=\"https://certification.oshwa.org/us002599.html\" title=\"Open Source Hardware Association Certificate\"><img align=\"right\" width=\"10%\" src=\"images/fanpico-oshw.svg\" alt=\"[OSHW] US002599 | Certified open source hardware | oshwa.org/cert\"></a></p>\n\n\nFanpico is a smart PWM (PC) fan controller based around [Raspberry Pi Pico](https://www.raspberrypi.com/documentation/microcontrollers/raspberry-pi-pico.html) (RP2040 MCU). Fanpico operates as standalone controller that can be configured and left running. It does not require any drivers and doesn't care what OS is being used. FanPico is an Open-source hardware (OSHW) project.\n\n[![FanPico-0804D](images/fanpico-0804D-v1.3-small.jpg)](images/fanpico-0804D-v1.3-large.jpg?raw=true)\n[![FanPico-0401D](images/fanpico-0401D-small.jpg)](images/fanpico-0401D-large.jpg?raw=true)\n\n## What is it for?\n* Fanpico is for customizing fan \"curves\" to fine tune how your motherboard controlled fans run.\n* Fine tuning fan speeds to easily balance airflow.\n* Controlling fan based on temperature sensor input.\n* Monitoring fan speeds and behaviour as well as monitoring what exactly motherboard is doing.\n* Fanpico works also as standalong controller for other things than a PC, it's been used  controlling fans on 3D printers and on enclosed network equipment racks, etc..\n\n## Features\n* Controls up to 8 fans.\n* Connect up to 4 motherboard fan outputs.\n* Connect up to 2 remote temperature probes (plus onboard \"ambient\" temperature sensor).\n* Support for [1-Wire Temperature sensors](https://github.com/tjko/fanpico/wiki/1%E2%80%90Wire-Temperature-Sensors-with-FanPico) (up to 8 sensors)\n* Support for [I2C Temperature sensors](https://github.com/tjko/fanpico/wiki/I2C-Temperature-Sensors-with-FanPico) (up to 8 sensors)\n* [OLED](https://github.com/tjko/fanpico/wiki/OLED-(I2C)-Display-Support) and [LCD](https://github.com/tjko/fanpico/wiki/LCD-(SPI)-Panel-Connection) display support (boards where model name ends with \"D\").\n* Can be powered from motherboard fan outputs or using auxiliary power connector (4-Pin Floppy or DC Barrel jack connector).\n* Ability to define custom fan \"curves\" for each fan.\n* Ability to provide custom tachometer (fan RPM) output signal back to motherboard.\n* Support for Locked Rotor (Alarm) signal fans (allows using regular tachometer fans with system expecting Locked Rotor Alarm fans and vice versa).\n* Control fans from any motherboard PWM signal or from temperature sensors.\n* OS Independent, no drivers or software needed.\n* Configuration stored on the device itself (in the flash memory).\n* SCPI \"like\" programming interface (see [Command Reference](commands.md))\n* Monitor each fan and motherboard output signals as well as temperatures.\n* [WiFi support](https://github.com/tjko/fanpico/wiki/FanPico-Network-Interface) if opting to mount Pico W on the board.\n  * [HTTP server](https://github.com/tjko/fanpico/wiki/Fanpico-HTTP-Server) with TLS (HTTPS) support.\n  * [MQTT client](https://github.com/tjko/fanpico/wiki/FanPico-MQTT-Tutorial) with TLS support for logging data and to receive commands.\n  * [Home Assistant](https://github.com/tjko/fanpico/wiki/FanPico-Home-Assistant-(MQTT-Discovery)-Support) support using MQTT Discovery feature.\n  * Telnet server for configuration and monitoring.\n  * [SSH server](https://github.com/tjko/fanpico/wiki/Fanpico-Using-SSH-Server) for configuration and monitoring (experimental).\n  * [SNMP Agent](https://github.com/tjko/fanpico/wiki/SNMP-Agent-Usage) for monitoring.\n\n\n### Interfaces\n\n* Serial \"Console\". Primary method for configuring/monitoring FanPico units is via (USB) Serial console (some units also have 3.3V TTL Serial connection).\n* Desktop application (that polls FanPico using SCPI commands over the console connection): [FanPico Monitor](https://github.com/tjko/fanpico-monitor)\n* Web Interface (available when using _Pico W_) over WiFi\n\n   [![FanPico](images/fanpico-web-small.png)](images/fanpico-web.png?raw=true)\n\nFor more documentation check [FanPico Wiki](https://github.com/tjko/fanpico/wiki)\n\n## Sponsors\nI would like to thank following sponsors, who have helped FanPico project:\n\n* PCBWay\n\n\n## Where can I get one?\nCurrently Fanpico is available as a DIY project.\nCheck discussion forums for places to purchase Kits or PCBs: [Places to Purchase FanPico](https://github.com/tjko/fanpico/discussions/12)\n\n(Instructions for building the kit are in [FanPico Wiki](https://github.com/tjko/fanpico/wiki))\n\nHowever, if there is sufficient interest, then ready-made units may be made available for sale.\nIf you'd be interested in purchasing FanPico pease indicate your interest here: [FanPico Forum](https://github.com/tjko/fanpico/discussions/9)\n\n### DIY Option\nYou can upload the Kerber files (found under boards directory) to your favorite PCB maker website and get your boards produced in matter of days.\nComponent list (BOM or bill of materials) is found under boards directory as well along with the electrical schematics.\n\nNOTE, check [discussions forum](https://github.com/tjko/fanpico/discussions/) for giveaways for free PCBs.\n\n### Review Units\nIf you are member of press (or YouTuber) and would like to review/test Fanpico. Please contact me via email.\n\n\n## How Can I help?\n* Build your own FanPico and provide feedback, suggestions (see discussion forums).\n* If you're into electronics new/improved board designs are most welcome.\n* If you're into programming:\n  - Create cool (desktop) program that allows easily configuring FanPico boards (with ability to \"draw\" fan curves, etc..) and to monitor Fan statuses.\n  - Help improve the firmware.\n* If you're into graphics (or web) design:\n  - Help create cool (but lightweight) web interface\n  - Create better logo for FanPico\n* As this is Open (Source) Hardware Project, if you like to build (and sell) assembled boards or DIY kits, you'd be most welcome...\n* You can always donate (and get your name added to the Sponsors section, if you so wish).\n\n\n## Hardware\nFanpico is Open Source Hardware, reference design is provided for the \"0804\" model (8 fan outputs and 4 motherboard fan inputs), and \"0804D\" model that adds OLED display support (hence the \"D\" suffix).\n\n\nAdditional models with different combinations of fan inputs/outputs could be easily designed (takining into account limitations of Raspberry Pi Pico I/O limits). New and improved PCB models/designs are most welcome.\n\nModel \"0840D\" with (with right-angle connectors and DC power connector):\n\n[![Fanpico-0804D](images/fanpico-0804D-v1.3-dc.jpg)](images/fanpico-0804D-v1.3-dc-large.jpg?raw=true)\n\nModel \"0804\" without display (with straight connectors):\n\n![Fanpico-0804](images/fanpico-0804.jpg)\n\n\n\n### Hardware Design\nFanpico (reference design) utilizes all available I/O pins on a Raspberry Pi Pico.\n* Fan PWM outputs are driven by the Pico's PWM hardware.\n* Motherboard Fan PWM inputs are read using Pico's PWM hardware.\n* Tacho signal output (for motherboard connectors) is generated using Pico's PIO hardware, providing extremely stable tachometer signal.\n* Tacho signal inputs (from fans) are read differently in model 0804 and 0804D:\n  - 0804: signals are read using GPIO interrupts, measuring all fans simultaneously by counting number of pulses received over a period of time.\n  - 0804D: signals are read through multiplexer measuring one fan at a time, by measuring pulse length.\n* Temperature readings are done using ADC, with help of a accurrate 3V voltage reference (LM4040). Any NTC (10k or 100k) thermistors can be used as themperature sensors.\n* Each FAN output has jumper to select whether fan gets its power from associated MBFAN connector or from the AUX connector\n* There is a jumper to select whether power the Fanpico itself from MBFAN1 or AUX connector.\n\nTo help design and test Fanpico couple other projects were born:\n* [Tiny PicoProbe](https://github.com/tjko/tiny-picoprobe/) - tiny PicoProbe implementation.\n* [Fan Test Adapter](https://github.com/tjko/fan-test-adapter/) - adapter to help simulate motherboard fan outputs.\n\nProjects based on FanPico firmware:\n* [BrickPico](https://github.com/tjko/brickpico/) - (LEGO) LED Light Kit Controller.\n\n### Models (PCB designs)\nCurrently following models are available:\n\n|Model|Fans (Outputs)|MB Fan (Inputs)|External Temperature Sensors|Display|Notes|\n|-----|--------------|---------------|-------------------|--------|-----|\n|[FANPICO-0804](boards/fanpico-0804/)|8|4|2|N|First publicly available board (reference design).|\n|[FANPICO-0804D](boards/fanpico-0804D/)|8|4|2|Y|Support for OLED (128x64 or 128x128) displays|\n|[FANPICO-0401D](boards/fanpico-0401D/)|4|1|2|Y|Adds QWIIC and 1-Wire connectors.|\n|[FANPICO-0200](boards/fanpico-0200/)|2|0|2|Y|OLED display supported via QWIIC connector. PCB under development.\n\n(all boards have one additional 'on-board' temperature sensor on the RP2040 MCU itself)\n\nModels ending with \"D\" have connector to attach OLED display module to see 'real-time' status of fans and temperatures.\n\n\nOLED panel directly mounted on the PCB:\n\n![FanPico OLED Display](images/oled-status-screen-2.jpg)\n\nLCD Panel attached via a short cable:\n\n![FanPico LCD Display](images/theme-default-480x320-small.jpg)\n\n\n#### Accessory Boards (PCB designs)\nFollowing accessories are available:\n\n|Model|Info|\n|-----|----|\n|[OLED-Adapter](boards/oled-adapter/)|Mounting adapter for OLED boards with different pinouts.|\n|[Power-Board](boards/power-board/)|Adapter for using high-power fans with FanPico.|\n\n\n## Firmware\nFirmware is developed in C using the Pico SDK. Pre-compiled firmware is released when there is new major features or bug fixes.\n\nLatest pre-compiled firmware image can be found here: [Releases](https://github.com/tjko/fanpico/releases)\n\nTo get latest firmware with latest updates/fixes you must compile the firmware from the sources.\n\n\n### Installing firmware image\nFirmware can be installed via the built-in UF2 bootloader on the Raspberry Pi Pico or using the debug header with Picoprobe, etc...\n\n#### Selecting Right Firmware to use\nEach release (zip file) contains multiple different firmware files.\nMake sure to select firmware for the board you're using and for the pico model (\"pico_w\" if using Pico W).\n\nFirmware file names have format: fanpico-<board_model>-<pico_model>.uf2\n```\nfanpico-0401D-pico.uf2\nfanpico-0401D-pico_w.uf2\nfanpico-0804-pico.uf2\nfanpico-0804-pico_w.uf2\nfanpico-0804D-pico.uf2\nfanpico-0804D-pico_w.uf2\n```\n\n#### Upgrading Firmware\nFirmware upgrade steps:\n* Boot Pico into UF2 bootloader. This can be done in two ways:\n  1)  Press and hold \"bootsel\" button and then press and release \"reset\" button.\n  2)  Issue command: SYS:UPGRADE\n* Copy firmware file to the USB mass storage device that appears.\n* As soon as firmware copy is complete, Pico will reboot and run the fanpico firmware.\n\n### Building Firmware Images\n\nRaspberry Pi Pico C/C++ SDK is required for compiling the firmware:\n\n#### Requirements / Dependencies\n* [Raspberry Pi Pico C/C++ SDK](https://www.raspberrypi.com/documentation/microcontrollers/c_sdk.html)\n* [cJSON](https://github.com/DaveGamble/cJSON)\n* [pico-lfs](https://github.com/tjko/pico-lfs)\n* [ss_oled-lib](https://github.com/tjko/ss_oled-lib)\n* [bb_spi_lcd-lib](https://github.com/tjko/bb_spi_lcd-lib)\n* [libb64](https://github.com/libb64/libb64)\n* [pico-telnetd](https://github.com/tjko/pico-telnetd)\n* [pico-1wire-lib](https://github.com/tjko/pico-1wire-lib)\n* [pico-sensor-lib](https://github.com/tjko/pico-sensor-lib)\n* [wolfSSL](https://github.com/wolfSSL/wolfssl)\n* [wolfSSH](https://github.com/wolfSSL/wolfssh)\n* [pico-sshd](https://github.com/tjko/pico-sshd)\n\n##### Install Pico SDK\nPico SDK must be installed working before you can compile fanpico.\n\nInstructions on installing Pico SDK see: [Getting started with Raspberry Pi Pico](https://datasheets.raspberrypi.com/pico/getting-started-with-pico.pdf)\n\n(Make sure PICO_SDK_PATH environment variable is set)\n\n##### Downloading sources\n\nCreate some directory for building fanpico ('src' used in this example):\n```\n$ mkdir src\n$ cd src\n$ git clone https://github.com/tjko/fanpico.git\n$ git submodule update --init --recursive\n```\n\n##### Building fanpico firmware\n\nTo build fanpico firmware, first create a build directory:\n```\n$ cd fanpico\n$ mkdir build\n```\n\nSelect which board to build firmware for (default is \"0804\") and which Pico is to be used (default is \"pico\"):\n```\n$ cd build\n$ cmake -DFANPICO_BOARD=0804D -DPICO_BOARD=pico_w ..\n```\n\nThen compile fanpico:\n```\n$ make -j\n```\n\nAfter successful compile you should see firmware binary in the build directory:\nsubdirectory:\n\n```\n$ ls *.uf2\nfanpico.uf2\n```\n\nIf you have picotool installed you can check the firmware image information:\n```\n$ picotool info -a fanpico.uf2\nFile fanpico.uf2 family ID 'rp2040':\n\nProgram Information\n name:              fanpico\n version:           1.7.6\n web site:          https://kokkonen.net/fanpico/\n description:       FanPico-0804D - Smart PWM Fan Controller\n features:          USB stdin / stdout\n boot settings:     bootdelay = 0\n                    safemode = 0\n binary start:      0x10000000\n binary end:        0x1012edc0\n embedded drive:    0x101c0000-0x10200000 (256K): littlefs\n\nFixed Pin Information\n 0:                 TX (Serial) / MISO (SPI)\n 1:                 RX (Serial) / CS (SPI)\n 2:                 SDA (I2C) / SCK (SPI)\n 3:                 SCL (I2C) / MOSI (SPI)\n 4:                 Fan1 PWM signal (output)\n 5:                 Fan2 PWM signal (output)\n 6:                 Fan3 PWM signal (output)\n 7:                 Fan4 PWM signal (output)\n 8:                 Fan5 PWM signal (output)\n 9:                 Fan6 PWM signal (output)\n 10:                Fan7 PWM signal (output)\n 11:                Fan8 PWM signal (output)\n 12:                MB Fan1 tacho signal (output)\n 13:                MB Fan1 PWM signal (input)\n 14:                MB Fan2 tacho signal (output)\n 15:                MB Fan2 PWM signal (input)\n 16:                MB Fan3 tacho signal (output)\n 17:                MB Fan3 PWM signal (input)\n 18:                MB Fan4 tacho signal (output)\n 19:                MB Fan4 PWM signal (input)\n 20:                Multiplexer S2 (output)\n 21:                Multiplexer S1 (output)\n 22:                Multiplexer S0 (output)\n 26:                Multiplexer A [tacho signal] (input)\n 27:                Temperature Sensor1 (input)\n 28:                Temperature Sensor2 (input)\n\nBuild Information\n sdk version:       2.1.1\n pico_board:        pico_w\n boot2_name:        boot2_w25q080\n build date:        Jul 10 2025\n build attributes:  Release\n\nMetadata Blocks\n none\n```\n",
    "readme_length": 14910
  },
  {
    "name": "lua-periphery",
    "full_name": "vsergeev/lua-periphery",
    "description": "A Lua library for peripheral I/O (GPIO, LED, PWM, SPI, I2C, MMIO, Serial) in Linux.",
    "stars": 201,
    "forks": 42,
    "language": "C",
    "url": "https://github.com/vsergeev/lua-periphery",
    "topics": [],
    "created_at": "2014-05-14T17:19:35Z",
    "updated_at": "2025-11-20T05:18:21Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# lua-periphery [![Build Status](https://github.com/vsergeev/lua-periphery/actions/workflows/build.yml/badge.svg)](https://github.com/vsergeev/lua-periphery/actions/workflows/build.yml) [![GitHub release](https://img.shields.io/github/release/vsergeev/lua-periphery.svg?maxAge=7200)](https://github.com/vsergeev/lua-periphery) [![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/vsergeev/lua-periphery/blob/master/LICENSE)\n\n## Linux Peripheral I/O (GPIO, LED, PWM, SPI, I2C, MMIO, Serial) with Lua\n\nlua-periphery is a library for GPIO, LED, PWM, SPI, I2C, MMIO, and Serial peripheral I/O interface access in userspace Linux. It is useful in embedded Linux environments (including Raspberry Pi, BeagleBone, etc. platforms) for interfacing with external peripherals. lua-periphery is compatible with Lua 5.1 (including LuaJIT), Lua 5.2, Lua 5.3, and Lua 5.4, has no dependencies outside the standard C library and Linux, is portable across architectures, and is MIT licensed.\n\nUsing Python or C? Check out the [python-periphery](https://github.com/vsergeev/python-periphery) and [c-periphery](https://github.com/vsergeev/c-periphery) projects.\n\nContributed libraries: [java-periphery](https://github.com/sgjava/java-periphery), [dart_periphery](https://github.com/pezi/dart_periphery)\n\n## Examples\n\n### GPIO\n\n``` lua\nlocal GPIO = require('periphery').GPIO\n\n-- Open GPIO /dev/gpiochip0 line 10 with input direction\nlocal gpio_in = GPIO(\"/dev/gpiochip0\", 10, \"in\")\n-- Open GPIO /dev/gpiochip0 line 12 with output direction\nlocal gpio_out = GPIO(\"/dev/gpiochip0\", 12, \"out\")\n\nlocal value = gpio_in:read()\ngpio_out:write(not value)\n\ngpio_in:close()\ngpio_out:close()\n```\n\n[Go to GPIO documentation.](docs/gpio.md)\n\n### LED\n\n``` lua\nlocal LED = require('periphery').LED\n\n-- Open LED led0\nlocal led = LED(\"led0\")\n\n-- Turn on LED (set max brightness)\nled:write(true)\n\n-- Set half brightness\nled:write(math.floor(led.max_brightness / 2))\n\n-- Turn off LED (set zero brightness)\nled:write(false)\n\nled:close()\n```\n\n[Go to LED documentation.](docs/led.md)\n\n### PWM\n\n``` lua\nlocal PWM = require('periphery').PWM\n\n-- Open PWM chip 0, channel 10\nlocal pwm = PWM(0, 10)\n\n-- Set frequency to 1 kHz\npwm.frequency = 1e3\n-- Set duty cycle to 75%\npwm.duty_cycle = 0.75\n\n-- Enable PWM output\npwm:enable()\n\npwm:close()\n```\n\n[Go to PWM documentation.](docs/pwm.md)\n\n### SPI\n\n``` lua\nlocal SPI = require('periphery').SPI\n\n-- Open spidev1.0 with mode 0 and max speed 1MHz\nlocal spi = SPI(\"/dev/spidev1.0\", 0, 1e6)\n\nlocal data_out = {0xaa, 0xbb, 0xcc, 0xdd}\nlocal data_in = spi:transfer(data_out)\n\nprint(string.format(\"shifted out {0x%02x, 0x%02x, 0x%02x, 0x%02x}\", unpack(data_out)))\nprint(string.format(\"shifted in  {0x%02x, 0x%02x, 0x%02x, 0x%02x}\", unpack(data_in)))\n\nspi:close()\n```\n\n[Go to SPI documentation.](docs/spi.md)\n\n### I2C\n\n``` lua\nlocal I2C = require('periphery').I2C\n\n-- Open i2c-0 controller\nlocal i2c = I2C(\"/dev/i2c-0\")\n\n-- Read byte at address 0x100 of EEPROM at 0x50\nlocal msgs = { {0x01, 0x00}, {0x00, flags = I2C.I2C_M_RD} }\ni2c:transfer(0x50, msgs)\nprint(string.format(\"0x100: 0x%02x\", msgs[2][1]))\n\ni2c:close()\n```\n\n[Go to I2C documentation.](docs/i2c.md)\n\n### MMIO\n\n``` lua\nlocal MMIO = require('periphery').MMIO\n\n-- Open am335x real-time clock subsystem page\nlocal rtc_mmio = MMIO(0x44E3E000, 0x1000)\n\n-- Read current time\nlocal rtc_secs = rtc_mmio:read32(0x00)\nlocal rtc_mins = rtc_mmio:read32(0x04)\nlocal rtc_hrs = rtc_mmio:read32(0x08)\n\nprint(string.format(\"hours: %02x minutes: %02x seconds: %02x\", rtc_hrs, rtc_mins, rtc_secs))\n\nrtc_mmio:close()\n\n-- Open am335x control module page\nlocal ctrl_mmio = MMIO(0x44E10000, 0x1000)\n\n-- Read MAC address\nlocal mac_id0_lo = ctrl_mmio:read32(0x630)\nlocal mac_id0_hi = ctrl_mmio:read32(0x634)\n\nprint(string.format(\"MAC address: %04x%08x\", mac_id0_lo, mac_id0_hi))\n\nctrl_mmio:close()\n```\n\n[Go to MMIO documentation.](docs/mmio.md)\n\n### Serial\n\n``` lua\nlocal Serial = require('periphery').Serial\n\n-- Open /dev/ttyUSB0 with baudrate 115200, and defaults of 8N1, no flow control\nlocal serial = Serial(\"/dev/ttyUSB0\", 115200)\n\nserial:write(\"Hello World!\")\n\n-- Read up to 128 bytes with 500ms timeout\nlocal buf = serial:read(128, 500)\nprint(string.format(\"read %d bytes: _%s_\", #buf, buf))\n\nserial:close()\n```\n\n[Go to Serial documentation.](docs/serial.md)\n\n### Error Handling\n\nlua-periphery errors are descriptive table objects with an error code string, C errno, and a user message.\n\n``` lua\n-- Example of error caught with pcall()\n> status, err = pcall(function () spi = periphery.SPI(\"/dev/spidev1.0\", 0, 1e6) end)\n> =status\nfalse\n> dump(err)\n{\n  message = \"Opening SPI device \\\"/dev/spidev1.0\\\": Permission denied [errno 13]\",\n  c_errno = 13,\n  code = \"SPI_ERROR_OPEN\"\n}\n> \n\n-- Example of error propagated to user\n> periphery = require('periphery')\n> spi = periphery.SPI('/dev/spidev1.0', 0, 1e6)\nOpening SPI device \"/dev/spidev1.0\": Permission denied [errno 13]\n> \n```\n\n#### Note about Lua 5.1\n\nLua 5.1 does not automatically render the string representation of error\nobjects that are reported to the console, and instead shows the following error\nmessage:\n\n``` lua\n> periphery = require('periphery')\n> gpio = periphery.GPIO(14, 'in')\n(error object is not a string)\n> \n```\n\nThese errors can be caught with `pcall()` and rendered in their string\nrepresentation with `tostring()`, `print()`, or by evaluation in the\ninteractive console:\n\n``` lua\n> periphery = require('periphery')\n> gpio, err = pcall(periphery.GPIO, 14, 'in')\n> =tostring(err)\nOpening GPIO: opening 'export': Permission denied [errno 13]\n> print(err)\nOpening GPIO: opening 'export': Permission denied [errno 13]\n> =err\nOpening GPIO: opening 'export': Permission denied [errno 13]\n> \n```\n\nThis only applies to Lua 5.1. LuaJIT and Lua 5.2 onwards automatically render\nthe string representation of error objects that are reported to the console.\n\n## Documentation\n\n`man` page style documentation for each interface is available in [docs](docs/) folder.\n\n## Installation\n\n#### Build and install with LuaRocks\n\n``` console\n$ sudo luarocks install lua-periphery\n```\n\n#### Build and install from source\n\nClone lua-periphery recursively to also fetch [c-periphery](https://github.com/vsergeev/c-periphery), which lua-periphery is built on.\n\n``` console\n$ git clone --recursive https://github.com/vsergeev/lua-periphery.git\n$ cd lua-periphery\n$ make clean all\n$ sudo make install\n```\n\nlua-periphery can then be loaded in lua with `periphery = require('periphery')`.\n\n#### Cross-compiling from Source\n\nSet the `CROSS_COMPILE` environment variable with the cross-compiler prefix when calling make. Your target's sysroot must provide the Lua includes.\n\n``` console\n$ CROSS=arm-linux- make clean all\ncd c-periphery && make clean\nmake[1]: Entering directory '/home/anteater/projects/software/lua-periphery/c-periphery'\nrm -rf periphery.a obj tests/test_serial tests/test_i2c tests/test_gpio_sysfs tests/test_mmio tests/test_spi tests/test_gpio\nmake[1]: Leaving directory '/home/anteater/projects/software/lua-periphery/c-periphery'\nrm -rf periphery.so\ncd c-periphery; make\nmake[1]: Entering directory '/home/anteater/projects/software/lua-periphery/c-periphery'\nmkdir obj\narm-linux-gcc -std=gnu99 -pedantic -Wall -Wextra -Wno-unused-parameter  -fPIC -DPERIPHERY_VERSION_COMMIT=\\\"v2.0.1\\\"  -c src/gpio.c -o obj/gpio.o\narm-linux-gcc -std=gnu99 -pedantic -Wall -Wextra -Wno-unused-parameter  -fPIC -DPERIPHERY_VERSION_COMMIT=\\\"v2.0.1\\\"  -c src/spi.c -o obj/spi.o\narm-linux-gcc -std=gnu99 -pedantic -Wall -Wextra -Wno-unused-parameter  -fPIC -DPERIPHERY_VERSION_COMMIT=\\\"v2.0.1\\\"  -c src/i2c.c -o obj/i2c.o\narm-linux-gcc -std=gnu99 -pedantic -Wall -Wextra -Wno-unused-parameter  -fPIC -DPERIPHERY_VERSION_COMMIT=\\\"v2.0.1\\\"  -c src/mmio.c -o obj/mmio.o\narm-linux-gcc -std=gnu99 -pedantic -Wall -Wextra -Wno-unused-parameter  -fPIC -DPERIPHERY_VERSION_COMMIT=\\\"v2.0.1\\\"  -c src/serial.c -o obj/serial.o\narm-linux-gcc -std=gnu99 -pedantic -Wall -Wextra -Wno-unused-parameter  -fPIC -DPERIPHERY_VERSION_COMMIT=\\\"v2.0.1\\\"  -c src/version.c -o obj/version.o\narm-linux-ar rcs periphery.a obj/gpio.o obj/spi.o obj/i2c.o obj/mmio.o obj/serial.o obj/version.o\nmake[1]: Leaving directory '/home/anteater/projects/software/lua-periphery/c-periphery'\narm-linux-gcc  -std=c99 -pedantic -D_XOPEN_SOURCE=700 -Wall -Wextra -Wno-unused-parameter  -fPIC -I.  -Iinc  -shared src/lua_periphery.c src/lua_mmio.c src/lua_\ngpio.c src/lua_spi.c src/lua_i2c.c src/lua_serial.c c-periphery/periphery.a -o periphery.so\n$ file periphery.so\nperiphery.so: ELF 32-bit LSB shared object, ARM, EABI5 version 1 (SYSV), dynamically linked, not stripped\n$\n```\n\n## Testing\n\nThe tests located in the [tests](tests/) folder may be run under Lua to test the correctness and functionality of lua-periphery. Some tests require interactive probing (e.g. with an oscilloscope), the installation of a physical loopback, or the existence of a particular device on a bus. See the usage of each test for more details on the required setup.\n\n## License\n\nlua-periphery is MIT licensed. See the included [LICENSE](LICENSE) file.\n\n",
    "readme_length": 9098
  },
  {
    "name": "ESP8266_new_pwm",
    "full_name": "StefanBruens/ESP8266_new_pwm",
    "description": "This is a drop-in replacement for the ESP8266 SDK PWM",
    "stars": 198,
    "forks": 46,
    "language": "C",
    "url": "https://github.com/StefanBruens/ESP8266_new_pwm",
    "topics": [],
    "created_at": "2016-06-28T21:12:56Z",
    "updated_at": "2025-09-09T06:15:29Z",
    "homepage": null,
    "license": "GNU General Public License v2.0",
    "readme": "# ESP8266_new_pwm\nThis is a drop-in replacement for the ESP8266 SDK PWM\n\nIf you like this project and want to support this and my other works, consider donating on Liberapay:\n\n<img src=\"https://img.shields.io/liberapay/patrons/StefanB.svg?logo=liberapay\"> <a href=\"https://liberapay.com/StefanB/donate\"><img alt=\"Donate using Liberapay\" src=\"https://liberapay.com/assets/widgets/donate.svg\"></a>\n\nThe software PWM provided in the ESP8266 SDK from Espressif has several drawbacks:\n\n1. Duty cycle limited to 90% (at 1kHz PWM period)\n2. usable PWM period at most ~2KHz.\n3. Incomplete documentation\n \nThis replacement allows duty cycles from 0% to 100%, with a stepsize of 200ns.\nThis is 5000 steps for a 1kHz PWM, and 256 steps (8 bit of resolution) at 19kHz.\n\nIf all channels are in steady state (either 0% of 100% in any combination),\nthe implementation goes to full idle, e.g. no interrupts.\n\nThe code is a drop-in replacement for the SDK, it provides the same functions\nas the SDK libpwm.a closed binary library. Just add pwm.c to your project.\n\nBy default there is one small difference to the SDK. The code uses a unit of\n200ns for both period and duty. E.g. for 10% duty cycle at 1kHz you need to\nspecify a period value of 5000 and a duty cycle value of 500, a duty cycle of\n5000 or above switches the channel to full on.\n\nTo have full compatibility with the SDK, you have to set the\nSDK_PWM_PERIOD_COMPAT_MODE define to 1. If set, the code will use 1us for PWM\nperiod and 40ns for the duty cycle. E.g. 10% duty cycle at 1kHz is set by a\nperiod value of 1000 and a duty cycle value of 2500, full duty at 25000 and\nabove.\n\nExample usage:\n\n\t#define PWM_CHANNELS 5\n\tconst uint32_t period = 5000; // * 200ns ^= 1 kHz\n\n\t// PWM setup\n\tuint32 io_info[PWM_CHANNELS][3] = {\n\t\t// MUX, FUNC, PIN\n\t\t{PERIPHS_IO_MUX_MTDI_U,  FUNC_GPIO12, 12},\n\t\t{PERIPHS_IO_MUX_MTDO_U,  FUNC_GPIO15, 15},\n\t\t{PERIPHS_IO_MUX_MTCK_U,  FUNC_GPIO13, 13},\n\t\t{PERIPHS_IO_MUX_MTMS_U,  FUNC_GPIO14, 14},\n\t\t{PERIPHS_IO_MUX_GPIO5_U, FUNC_GPIO5 ,  5},\n\t};\n\n\t// initial duty: all off\n\tuint32 pwm_duty_init[PWM_CHANNELS] = {0, 0, 0, 0, 0};\n\n\tpwm_init(period, pwm_duty_init, PWM_CHANNELS, io_info);\n\tpwm_start();\n\n\t// do something like this whenever you want to change duty\n\tpwm_set_duty(500, 1);  // GPIO15: 10%\n\tpwm_set_duty(5000, 1); // GPIO15: 100%\n\tpwm_start();           // commit\n\n**CAVEATS**\n\n- **To set 100% duty, the duty must be *equal* to the period**\n- **The code uses the TIMER1 interrupt. If you use e.g. the\n  softtimer, there is a conflict. You can use NM1 for the PWM\n  instead.**\n",
    "readme_length": 2557
  },
  {
    "name": "pwManager",
    "full_name": "KalleHallden/pwManager",
    "description": null,
    "stars": 182,
    "forks": 74,
    "language": "Python",
    "url": "https://github.com/KalleHallden/pwManager",
    "topics": [],
    "created_at": "2020-11-13T07:34:49Z",
    "updated_at": "2025-11-10T16:40:24Z",
    "homepage": null,
    "license": "N/A",
    "readme": "\n\nTodo:\n- Find out general structure of password managers\n- Find what hashing algo to use (probably sha256)\n\n- Create an SQL database of some kind (mysql)\n\t- tables: password, username, user_email, app_name, url\n- Refresh memory on how to create, store and retrieve things from an SQL database\n\n- create a terminal menu\n\t- needs to allow the user to type in a domain name, url or app name\n\t- user should get back password, email used, and if applicable username\n\n- create master password\n",
    "readme_length": 488
  },
  {
    "name": "101Things",
    "full_name": "dawsonjon/101Things",
    "description": "A collection of cool projects to make!",
    "stars": 180,
    "forks": 24,
    "language": "G-code",
    "url": "https://github.com/dawsonjon/101Things",
    "topics": [
      "audio",
      "clock",
      "cnc",
      "effects",
      "guitar",
      "ham",
      "msf",
      "pedal",
      "pi",
      "pico",
      "pwm",
      "radio",
      "raspberry",
      "time"
    ],
    "created_at": "2023-10-01T18:06:23Z",
    "updated_at": "2025-11-29T11:53:55Z",
    "homepage": "",
    "license": "MIT License",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "Navio2",
    "full_name": "emlid/Navio2",
    "description": "C++ and Python sensor examples for developers",
    "stars": 179,
    "forks": 127,
    "language": "C++",
    "url": "https://github.com/emlid/Navio2",
    "topics": [
      "adc",
      "emlid",
      "navio",
      "pwm",
      "raspberrypi",
      "ublox"
    ],
    "created_at": "2015-11-30T14:25:21Z",
    "updated_at": "2025-11-10T09:40:46Z",
    "homepage": "",
    "license": "BSD 3-Clause \"New\" or \"Revised\" License",
    "readme": "Navio 2\n=====\n\nCollection of drivers and examples for Navio 2 - autopilot shield for Raspberry Pi.\n\n## Repository structure\n\n### C++\n\n#### Examples\n\nBasic examples showing how to work with Navio's onboard devices using C++.\n\n* AccelGyroMag\n* ADC\n* AHRS\n* Barometer\n* GPS\n* LED 2\n* RCInput\n* Servo\n\n#### Navio 2\n\nC++ drivers for Navio 2's onboard devices and peripheral interfaces.\n\n* MPU9250 SPI\n* LSM9DS1 SPI\n* U-blox SPI\n* MS5611 I2C\n* I2C driver\n* SPI driver\n\n### Python\n\nBasic examples showing how to work with Navio's onboard devices using Python.\n\n* AccelGyroMag\n* ADC\n* Barometer\n* GPS\n* LED\n* RCInput\n* Servo\n\n\n### Utilities\n\nApplications and utilities for Navio.\n\n* 3D IMU visualizer\n* U-blox SPI to PTY bridge utility\n* U-blox SPI to TCP bridge utility\n* ROS packages installation script\n\n### Cross-compilation\n\n#### Requirements\n\n* Install the toolchain `gcc-arm-linux-gnueabihf g++-arm-linux-gnueabihf` (`sudo apt-get install gcc-arm-linux-gnueabihf g++-arm-linux-gnueabihf` for Debian based systems)\n\n#### Usage\n\n* `export CXX=arm-linux-gnueabihf-g++`\n* Compile the examples via `make`\n",
    "readme_length": 1099
  },
  {
    "name": "ShiftPWM",
    "full_name": "elcojacobs/ShiftPWM",
    "description": "Arduino Library for software PWM with shift registers",
    "stars": 177,
    "forks": 86,
    "language": "C++",
    "url": "https://github.com/elcojacobs/ShiftPWM",
    "topics": [],
    "created_at": "2012-05-05T16:36:36Z",
    "updated_at": "2025-03-20T12:50:51Z",
    "homepage": null,
    "license": "MIT License",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "mp-flipper",
    "full_name": "ofabel/mp-flipper",
    "description": "Python support for Flipper Zero",
    "stars": 173,
    "forks": 11,
    "language": "C",
    "url": "https://github.com/ofabel/mp-flipper",
    "topics": [
      "adc",
      "f0",
      "flipper-app",
      "flipper-plugin",
      "flipper-zero",
      "flipperzero",
      "gpio",
      "infrared",
      "micropython",
      "pwm",
      "python"
    ],
    "created_at": "2024-04-01T15:11:13Z",
    "updated_at": "2025-11-30T20:23:08Z",
    "homepage": "https://ofabel.github.io/mp-flipper/",
    "license": "MIT License",
    "readme": "![License](https://img.shields.io/github/license/ofabel/mp-flipper)\n![Version](https://img.shields.io/github/v/tag/ofabel/mp-flipper)\n![](https://img.shields.io/github/issues/ofabel/mp-flipper)\n\n# MicroPython Flipper Zero\n\nAllows you to use the power of Python natively on your Flipper Zero.\nThe application is available on the official [Flipper Lab](https://lab.flipper.net/apps/upython).\nFor details on how to programm your Flipper with Python, check out the [documentation](https://ofabel.github.io/mp-flipper/) on GitHub pages.\n\n![MicroPython REPL](./docs/pages/assets/repl.gif)\n\n## Disclaimer\n\nThis FAP version requires about 80 kB from SRAM to start (needed for the Python runtime and compiler).\nDue to memory fragmentation it's possible, that the application crashes when you start it.\nIf this happens, just try again (the crash doesn't harm your device).\n\n> [!IMPORTANT]\n> This problem is already addressed to the firmware developers in [this issue](https://github.com/flipperdevices/flipperzero-firmware/issues/3927).\n> Nevertheless, running the uPython application from the SD card is still a heavy task for the Flipper.\n\n_I'm thinking about publishing a fork of the original firmware with uPython bundled as a core service._\n_This would mitigate all the memory problems._\n_The SD card version would still be there and maintained, but possibly with a limited set of features._\n\n## Development\n\nThis section is only relevant, if you want to build the FAP on your own.\n\n### Branches\n\nThis branch contains the [FAP](https://developer.flipper.net/flipperzero/doxygen/apps_on_sd_card.html) version.\nThe results of the preceding research phase is still available in the [poc](https://github.com/ofabel/mp-flipper/tree/poc) branch.\nThe [lib](https://github.com/ofabel/mp-flipper/tree/lib) branch of this repository contains just the [MicroPython](https://github.com/micropython/micropython) library.\nThe progress of further research on what can be achieved when moving functionality to the firmware can be found in the [fork of the original firmware](https://github.com/ofabel/flipperzero-firmware/tree/ofa/micropython).\n\n### Requirements\n\n* [Git](https://git-scm.com/)\n* [Make](https://www.gnu.org/software/make/)\n* [uFBT](https://pypi.org/project/ufbt/) available in your `PATH` (or you have to adjust the [Makefile](./Makefile))\n* [Flipper Zero](https://flipperzero.one/)\n\n### Setup\n\n```bash\ngit clone --recurse-submodules git@github.com:ofabel/mp-flipper.git\n```\n\n### Build\n\nJust open a terminal and run the Makefile targets:\n\n```bash\nmake build\n```\n\nYou can also build an launch the application on the attached Flipper Zero device in one command:\n\n```bash\nmake launch\n```\n",
    "readme_length": 2680
  },
  {
    "name": "Arduino-Atmel-sPWM",
    "full_name": "Irev-Dev/Arduino-Atmel-sPWM",
    "description": "Implementation of an sPWM signal",
    "stars": 167,
    "forks": 81,
    "language": "C++",
    "url": "https://github.com/Irev-Dev/Arduino-Atmel-sPWM",
    "topics": [],
    "created_at": "2015-11-10T20:47:50Z",
    "updated_at": "2025-12-01T14:34:11Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# Arduino-Atmel-sPWM \n\n#### Implementation of an sPWM signal on Ardunio and Atmel micros\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/pulses_1.JPG?raw=true \"Figure\")\n\n## Introduction\n\nThe aim of this repo is to help the hobbyist or student make rapid progress in implementing an sPWM signal on a arduino or atmel micro, while making sure that the theory behind the sPWM and the code itself is understood. \n\nPlease also note that:\n\n * It's assumed the reader has a basic understanding of C programming\n * If you plan on making an inverter please read the safety section\n * Feel free to collaborate on improving this repo\n\n\n## Table of Contents\n<!-- toc -->\n* [Brief Theory](#brief-theory)\n    - [Basic PWM](#basic-pwm)\n    - [Typical micro implementation](#typical-microcontroller-pwm-implementation)\n    - [Sinusoidal PWM](#sinusoidal-pwm)\n* [Code & Explanation](#code-and-explanation)\n    - [sPWM_Basic](#spwm_basic)\n    - [sPWM_Generate_Lookup_Table](#spwm_generate_lookup_table)\n* [Testing the Signal](#testing-the-signal)\n    - [Viewing Pulse Widths with an Osilloscope](#viewing-pulse-widths-with-an-osilloscope)\n    - [Viewing the Filtered Signal with an Oscilloscope](#viewing-the-filtered-signal-with-an-oscilloscope)\n    - [Listening to the Signal](#listening-to-the-signal)\n* [Compatibility](#compatibility)\n* [Safety](#safety)\n\n<!-- tocstop -->\n\n## Brief Theory\n### Basic PWM\n\nPulse width modulationâ€™s (PWM) main use is to control the voltage supplied to electric circuits, it does this by rapidly switching a load on and off. Another way of thinking of it is to consider it as a method for a digital system to output an analogue signal. The figure below shows an example of a PWM signal.\n\n![Figure 1-1](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/basicPWM_3.png?raw=true \"Figure 1.1\")\n\nThere are two properties to a PWM signal, the frequency which is determined by the period of the signal and the duty cycle which is determined by the high-time of the signal. The signal in figure above has a period of 250Î¼S which means it switches at 4KHz. The duty-cycle is the percent high time in each period, in the last figure the duty-cycle is 60% because of the high-time and period of 150Î¼S and 250Î¼S respectively. It is the duty-cycle that determines average output voltage. In figure above the duty-cycle of 60% with 5V switching voltage results in 3V average output as shown by the red line. After filtering the output, a stable analogue output can be achieved. the figure below shows PWM signals with 80% and 10% duty-cycles. By dynamically changing the duty-cycle, signals other than a flat output voltage can be achieved.\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/basicPWM_4.png?raw=true \"Figure\")\n\n### Typical microcontroller PWM implementation\n\nThis section describes how micro-controllers use timers/counters to implement a PWM signal, this description relies heavily on the figure below. Here the blue line represents a counter that resets after 16000, this gives the period of the PWM and also the switching frequency (fs), if this micro has a clock source of 16MHz, then fs will be 16Ã—10^6/16Ã—10^3 = 1KHz.\n\nThe two other values of 11200 and 8000 shown as the green and red line, they represent compare output values of the microcontroller. When the counter reaches this values, the microcontroller can change the output of some pins. In this case two pins are set to HIGH when the counter resets and then Set low when the counter reaches each of the output compare values. This determines the high time and therefore duty-cycle of the PWM. It's important to understand how a PWM is typically set up in a microcontroller, as the explanation of the code in the next section will not make sense otherwise.\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/sawtooth_counter_1.png?raw=true \"Figure\")\n\n### Sinusoidal PWM\n\nA sinusoidal PWM (sPWM) signal can be constructed by dynamically changing the duty- cycle. The result is short pulses at the zero-crossings and long pulses at the wave peaks. This can be seen in the figure below.\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/PWMsin_2.png?raw=true \"Figure\")\n\nThe figure shows negative pulses which is not possible on most micro-controllers. Instead normally this is implemented with two pins, one pulsing the positive half of the sin wave and the second pulsing the negative half, this is how it is implemented in this repo as having the signal split across two pins makes sense if the signal is going to be used to control a H-bridge.\n\n## Code and Explanation\n\nIn this chapter we'll step through the code found in the folder sPWM_basic, and then the difference between it and the code found in the folder sPWM_generate_lookup_table will be discussed. Code found in sPWM_atmel is for use on an atmel chip without using the arduino IDE.The code toggles a pin for every period of the sine output in order to make it osilliscope friendy. \n\n### sPWM_Basic\n\nThe following C code implements an sPWM on an Atmel micro-controller. The signal is generated on two pins, one responsible for the positive half of the sine wave and the other pin the negative half. The sPWM is generated by running an (ISR) every period of the PWM in order to dynamically change the duty-cycle. This is done by changing the values in the registers OCR1A and OCR1B from values in a look up table. There are two look up tables for each of the two pins, lookUpTable1 and lookUpTable2 and both have 200 values. The first half of lookUpTable1 has sin values from 0 to Ï€ and the second half is all zeroes. The first half of lookUpTable2 is all zeroes and the second half has sin values from 0 to Ï€ as shown in figure below.\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/lookup_2.png?raw=true \"Figure\")\n\nThe code assumes implementation on an Arduino Uno but is likely compatible with other boards. see Compatibility.\n\nLets walk through the code.\n\n```c\n#include <avr/io.h>\n#include <avr/interrupt.h>\n\n// Look up tables with 200 entries each, normalised to have max value of 1600 which is the period of the PWM loaded into register ICR1.\nint lookUp1[] = {50 ,100 ,151 ,201 ,250 ,300 ,349 ,398 ,446 ,494 ,542 ,589 ,635 ,681 ,726 ,771 ,814 ,857 ,899 ,940 ,981 ,1020 ,1058 ,1095 ,1131 ,1166 ,1200 ,1233 ,1264 ,1294 ,1323 ,1351 ,1377 ,1402 ,1426 ,1448 ,1468 ,1488 ,1505 ,1522 ,1536 ,1550 ,1561 ,1572 ,1580 ,1587 ,1593 ,1597 ,1599 ,1600 ,1599 ,1597 ,1593 ,1587 ,1580 ,1572 ,1561 ,1550 ,1536 ,1522 ,1505 ,1488 ,1468 ,1448 ,1426 ,1402 ,1377 ,1351 ,1323 ,1294 ,1264 ,1233 ,1200 ,1166 ,1131 ,1095 ,1058 ,1020 ,981 ,940 ,899 ,857 ,814 ,771 ,726 ,681 ,635 ,589 ,542 ,494 ,446 ,398 ,349 ,300 ,250 ,201 ,151 ,100 ,50 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0};\nint lookUp2[] = {0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,50 ,100 ,151 ,201 ,250 ,300 ,349 ,398 ,446 ,494 ,542 ,589 ,635 ,681 ,726 ,771 ,814 ,857 ,899 ,940 ,981 ,1020 ,1058 ,1095 ,1131 ,1166 ,1200 ,1233 ,1264 ,1294 ,1323 ,1351 ,1377 ,1402 ,1426 ,1448 ,1468 ,1488 ,1505 ,1522 ,1536 ,1550 ,1561 ,1572 ,1580 ,1587 ,1593 ,1597 ,1599 ,1600 ,1599 ,1597 ,1593 ,1587 ,1580 ,1572 ,1561 ,1550 ,1536 ,1522 ,1505 ,1488 ,1468 ,1448 ,1426 ,1402 ,1377 ,1351 ,1323 ,1294 ,1264 ,1233 ,1200 ,1166 ,1131 ,1095 ,1058 ,1020 ,981 ,940 ,899 ,857 ,814 ,771 ,726 ,681 ,635 ,589 ,542 ,494 ,446 ,398 ,349 ,300 ,250 ,201 ,151 ,100 ,50 ,0};\n\n```\n\nWe are going to address the registers on the atmel chip as well as using interrupts so the <avr/io.h> and <avr/interrupt.h> headers are necessary. From there we have two arrays which have a two half sinusoidal signal entered in\n\n```c\nvoid setup(){\n    // Register initilisation, see datasheet for more detail.\n    TCCR1A = 0b10100010;\n       /*10 clear on match, set at BOTTOM for compA.\n         10 clear on match, set at BOTTOM for compB.\n         00\n         10 WGM1 1:0 for waveform 15.\n       */\n    TCCR1B = 0b00011001;\n       /*000\n         11 WGM1 3:2 for waveform 15.\n         001 no prescale on the counter.\n       */\n    TIMSK1 = 0b00000001;\n       /*0000000\n         1 TOV1 Flag interrupt enable. \n       */\n```\nHere the timer register have been initialised. If you are interested in the details you can look up the ATMEGA328p datasheet, but for now what's important is that we have set up a PWM for two pins and it call in interrupt routine for every period of the PWM.\n\n```c\n    ICR1   = 1600;     // Period for 16MHz crystal, for a switching frequency of 100KHz for 200 subdevisions per 50Hz sin wave cycle.\n    sei();             // Enable global interrupts.\n    DDRB = 0b00000110; // Set PB1 and PB2 as outputs.\n    pinMode(13,OUTPUT);\n}\n```\nICR1 is another register that contains the length of the counter before resetting, since we have no prescale on our clock, this defines the period of the PWM to 1600 clock cycles. Then we enable interrupts. Next the two pins are set as outputs, the reason why pinMode() is not used is because the pins might change on different arduinos, they might also change on different Atmel micros, however you are using an arduino with a 328p, then this code will work. Lastly pinMode() is used to set pin 13 as an output, we will use this later as a trigger for the oscilloscope, however it is not necessary.\n\n```c\nvoid loop(){; /*Do nothing . . . . forever!*/}\n```\nNothing is implemented in the loop.\n\n```c\nISR(TIMER1_OVF_vect){\n    static int num;\n    static char trig;\n    // change duty-cycle every period.\n    OCR1A = lookUp1[num];\n    OCR1B = lookUp2[num];\n    \n    if(++num >= 200){ // Pre-increment num then check it's below 200.\n       num = 0;       // Reset num.\n       trig = trig^0b00000001;\n       digitalWrite(13,trig);\n     }\n}\n```\nThis interrupt service routine is call every period of the PWM, and every period the duty-cycle is change. This done by changing the value in the registers OCR1A and OCR1B to the next value of the look up table as this registers hold the compare values that set the output pins low when reached as per figure .\n\nTherefore in each period the registers OCR1x are loaded with the next value of their look up tables by using num to point to the next value in the array, as each period num is incremented and checked that it is below 200, if it is not below 200 in is reset to 0. The other two lines involving trig and digitalWrite are there two toggle a pin as a trigger for an osilloscope and does not impact the sPWM code.\n\n### sPWM_generate_lookup_table\n\nThe rest of this section discusses some modifications to this code, namely we can make generate the lookup table at the start of the code, the benefit of this is we change the switching frequency as well as the sPWM frequency. Code for this can be found in the sPWM_generate_lookup_table folder. The start of the code looks like this:\n\n```c\n#include <avr/io.h>\n#include <avr/interrupt.h>\n\n#define SinDivisions (200)// Sub divisions of sisusoidal wave.\n\nstatic int microMHz = 16; // Micro clock frequency\nstatic int freq = 50;     // Sinusoidal frequency\nstatic long int period;   // Period of PWM in clock cycles.\nstatic unsigned int lookUp[SinDivisions];\nstatic char theTCCR1A = 0b10000010; //varible for TCCR1A\n\nvoid setup(){\n  double temp; //Double varible for <math.h> functions.\n  \n  period = microMHz*1e6/freq/SinDivisions;// Period of PWM in clock cycles\n  \n  for(int i = 0; i < SinDivisions/2; i++){ // Generating the look up table.\n    temp = sin(i*2*M_PI/SinDivisions)*period;\n    lookUp[i] = (int)(temp+0.5);       // Round to integer.    \n  }\n```\nNotice that only the first half of the sine wave is generated, because of the way this code implements the sPMW where each of the two pins are responsible for different halves of the signal, only half the sine wave is needed. However it does require a modification to the interrupt service routine.\n\n```c\nISR(TIMER1_OVF_vect){\n    static int num;\n    static int delay1;\n    static char trig;\n    \n    if(delay1 == 1){/*delay by one period because the high time loaded into OCR1A:B values are buffered but can be disconnected immediately by TCCR1A. */\n      theTCCR1A ^= 0b10100000;// Toggle connect and disconnect of compare output A and B.\n      TCCR1A = theTCCR1A;\n      delay1 = 0;             // Reset delay1\n    } else if(num >= SinDivisions/2){\n      num = 0;                // Reset num\n      delay1++;\n      trig ^=0b00000001;\n      digitalWrite(13,trig);\n    }\n    // change duty-cycle every period.\n    OCR1A = lookUp[num];\n    OCR1B = lookUp[num];\n    num++;\n}\n``` \nBoth output compare registers ORC1x have the same values loaded into them each period, however the output compare for each pin is enabled and disable in turns by toggling two bits in the TCCR1A register. It is toggle each time the look up table index (num) is reset, however it is delayed by one clock cycle, this is because when values are loaded into OCR1x registered, is buffered whereas changes in TCCR1A is implemented immediately, see the 328p datasheet for more details.\n\n## Testing the Signal\n\nThis chapter discusses ways to test and monitor the sPWM signal, the first section discusses using as oscilloscope which is the best method to verify the signal however an alternate and cheaper method is to use a small speaker. The 50hz signal and the underlying switching frequency is easy to hear.\n\n### Viewing Pulse Widths with an Oscilloscope\n\nHere we aim to view the individual pulses that make up the sPWM to see if they look how we would expect, that is thin pulses becoming thick and then thin again. If you are using the code sPWM_generate _lookup_table I recommend lowering the switching frequency of the sPWM by changing #define SinDivisions(200) to #define SinDivisions (50). This will make the pulses easier to see as there will be fewer of them. If you are using the Uno pin 13 will be toggled every period of the sine and so can be used as a trigger for the oscilloscope. the figure below shows the wiring to the Arduino Uno.\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/PulseOscill_1.png?raw=true \"Figure\")\n\nNotice we are only looking at half the signal from one of the output pins. The wave form produced should look like the figure below. Notice the change from thin to thick back to thin pulses. \n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/pulses_2.png?raw=true \"Figure\")\n\n### Viewing the Filtered Signal with an Oscilloscope\n\nHere we aim to view a smooth sinusoidal wave by smoothing the output of the micro with a simple low-pass RC filter as shown in Figure 4.3. \n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/filterOscill_2.png?raw=true \"Figure\")\n\nThe purpose of the filter is to remove the much higher switching frequency (fs) and leave only the 50Hz sine wave. The cut-off frequency (fc) of the filter determined is by fc = 1/2Ï€RC, and it should lie somewhere between fs and 50Hz, closer to 50Hz is more optimal. fs is determined by the line of code #define SinDivisions (a number) in sPWM_generate_lookup_table. The switching frequency is given by fs = SinDivisionsÃ—50Hz. Using an arbitrary number for SinDivisions may produce unexpected results, I recommend using 50,200 and 400 to produced fsâ€™s of 2.5, 10 and 20 kHz respectively. The filtered signal is shown in the figure below.\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/smoothed_2.png?raw=true \"Figure\")\n\nAlternatively a potentiometer can be used as a variable resistor as shown in the figure below.\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/trimpot_RCfilter_2.png?raw=true \"Figure\")\n\n This allows the fc to be varied. The following five figures show the result of changing the potentiometer and therefore the amount of filtering.\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/smothing_1.png?raw=true \"Figure\")\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/smothing_2.png?raw=true \"Figure\")\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/smothing_3.png?raw=true \"Figure\")\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/smothing_4.png?raw=true \"Figure\")\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/smothing_5.png?raw=true \"Figure\")\n\n### Listening to the Signal\n\nIf you donâ€™t have an oscilloscope, listening to the signal is a useful way to determine if the sPWM is working since it is easy to hear both the 50Hz hum and the switching frequency. We can use the micro to drive a small pair of head-phones directly, putting them in series with a 1KÎ© resister should protect most head-phones, as seen in Figure 4.6.\n\n![Figure what](https://github.com/Terbytes/Arduino-Atmel-sPWM/blob/master/im/speaker_2.png?raw=true \"Figure\")\n\nIt is recommended to change #define SinDivisions (200) to 50, 200 and 400 in order to hear the difference in switching frequencies. Note depending on your age and hearing you might not be able to hear the switching frequency with #define SinDivisions (400), as this produces a switching frequency of 20KHz, which is on the limit of human hearing.\n\n## Compatibility\n\nPlease let me know if you got the code to work on a device that's not listed here\n\nComparability list:\n\n* Arduino Uno\n* Arduino Nano\n* Arduino mega2560\n\n## Safety\n\nThis section is to briefly discuss safety in regards to making an inverter that steps up to  mains voltage whether that be 110 or 230. First of all, I don't encourage it, I'd prefer that you didn't and I take no responsibility for your actions. Remember 30mA can be lethal, mains voltage deserves respect.\n\nIf you still choose to do so, take basic precautionary steps like: Invest in some terminals and make sure that any high voltage part of the circuit is not touchable; don't modify it while it's power up; Don't do it alone.\n",
    "readme_length": 18478
  },
  {
    "name": "charge-controller-firmware",
    "full_name": "LibreSolar/charge-controller-firmware",
    "description": "Firmware for Libre Solar MPPT/PWM charge controllers",
    "stars": 163,
    "forks": 75,
    "language": "C++",
    "url": "https://github.com/LibreSolar/charge-controller-firmware",
    "topics": [],
    "created_at": "2016-08-02T12:03:35Z",
    "updated_at": "2025-11-06T23:10:33Z",
    "homepage": "https://libre.solar/charge-controller-firmware/",
    "license": "Apache License 2.0",
    "readme": "# Libre Solar Charge Controller Firmware\n\n![build badge](https://github.com/LibreSolar/charge-controller-firmware/actions/workflows/zephyr.yml/badge.svg)\n\nThis repository contains the firmware for the different Libre Solar Charge Controllers based on [Zephyr RTOS](https://www.zephyrproject.org/).\n\nCoding style is described [here](https://github.com/LibreSolar/coding-style).\n\n## Development and release model\n\nThe `main` branch is used for ongoing development of the firmware.\n\nReleases are created from `main` after significant updates have been introduced to the firmware. Each release has to pass tests with multiple boards.\n\nA release is tagged with a version number consisting of the release year and a release count for that year (starting at zero). For back-porting of bug-fixes, a branch named after the release followed by `-branch` is created, e.g. `v21.0-branch`.\n\n## Documentation\n\nThe firmware documentation including build instructions and API reference can be found under [libre.solar/charge-controller-firmware](https://libre.solar/charge-controller-firmware/).\n\nIn order to build the documentation locally you need to install Doxygen, Sphinx and Breathe and run `make html` in the `docs` folder.\n\n## License\n\nThis firmware is released under the [Apache-2.0 License](LICENSE).\n",
    "readme_length": 1295
  },
  {
    "name": "MasteringMCU2",
    "full_name": "niekiran/MasteringMCU2",
    "description": "This repository holds all documents and source codes related to udemy course \"Mastering Microcontroller 2: TIMERS, PWM, CAN, RTC,LOW POWER\"",
    "stars": 159,
    "forks": 126,
    "language": "C",
    "url": "https://github.com/niekiran/MasteringMCU2",
    "topics": [],
    "created_at": "2018-09-25T14:35:08Z",
    "updated_at": "2025-11-30T01:01:37Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# MasteringMCU2\nThis repository holds all documents and source codes related to udemy course \"Mastering Microcontroller 2: TIMERS, PWM, CAN, RTC,LOW POWER\"\n\nyou can get the course coupon by clicking the below link\nhttps://www.udemy.com/microcontroller-programming-stm32-timers-pwm-can-bus-protocol/?couponCode=MCU2ONLY10\n",
    "readme_length": 321
  },
  {
    "name": "riscduino",
    "full_name": "dineshannayya/riscduino",
    "description": "Arduino compatible Risc-V Based SOC",
    "stars": 157,
    "forks": 29,
    "language": "Tcl",
    "url": "https://github.com/dineshannayya/riscduino",
    "topics": [
      "analog",
      "ar",
      "i2c",
      "pwm",
      "qspi",
      "riscv",
      "usb-host"
    ],
    "created_at": "2021-09-11T09:12:43Z",
    "updated_at": "2025-10-20T08:55:30Z",
    "homepage": "",
    "license": "Apache License 2.0",
    "readme": "```\n  Riscduino Single Risc Core SOC\n\n\nPermission to use, copy, modify, and/or distribute this soc for any\npurpose with or without fee is hereby granted, provided that the above\ncopyright notice and this permission notice appear in all copies.\n\nTHE SOC IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\nWITH REGARD TO THIS SOC INCLUDING ALL IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\nANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\nWHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\nACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\nOR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOC.\n```\n\n# Table of contents\n- [Overview](#overview)\n- [Riscduino Block Diagram](#Riscduino-block-diagram)\n- [Key Feature](#key-features)\n- [Riscduino derivatives](#riscduino-derivatives)\n- [MPW Shuttle on Riscduino](#mpw-shuttle-on-riscduino)\n- [Sub IP Feature](#sub-ip-features)\n- [SOC Memory Map](#soc-memory-map)\n- [Pin Mapping](#soc-pin-mapping)\n- [Repository contents](#repository-contents)\n- [Prerequisites](#prerequisites)\n- [Tests preparation](#tests-preparation)\n    - [Test Cases](#test-cases)\n    - [Running Simuation](#running-simulation)\n- [Tool sets](#tool-sets)\n- [News](#news)\n- [Contacts](#contacts)\n- [How To Contribute](#how-to-contribute)\n- [Documentation](#documentation)\n\n\n# Overview\n\nRiscduino is a Single 32 bit RISC V based SOC design pin compatible to arduino platform and this soc targeted for efabless Shuttle program.  This project uses only open source tool set for simulation,synthesis and backend tools.  The SOC flow follow the openlane methodology and SOC environment is compatible with efabless/carvel methodology.\n<table>\n  <tr>\n    <td  align=\"center\"><img src=\"./docs/source/_static/Riscduino_Integration.png\" ></td>\n  </tr>\n\n</table>\n# Riscduino Block Diagram\n\n<table>\n  <tr>\n    <td  align=\"center\"><img src=\"./docs/source/_static/Riscduino_Soc.png\" ></td>\n  </tr>\n\n</table>\n\n\n# Key features\n```\n    * Open sourced under Apache-2.0 License (see LICENSE file) - unrestricted commercial use allowed.\n    * Single 32 Bit RISC-V core\n    * 2KB SRAM for instruction cache \n    * 2KB SRAM for data cache\n    * 2KB SRAM for Tightly coupled memory - For Data Memory\n    * Quad SPI Master with 4 Chip select, supports both SPI flash and SRAM interface\n    * 2 x UART with 16Byte FIFO\n    * USB 1.1 Host\n    * I2C Master\n    * UART Master\n    * Simple SPI Master with 4 Chip select\n    * 6 Channel ADC (in Progress)\n    * 6 x PWM\n    * 3 x Timer (16 Bit), 1us/1ms/1second resolution\n    * 2 x ws281x driver\n    * 16 Hardware Semaphore\n    * FPU (SP) Core\n    * AES 128 Bit Core\n    * RTC Core\n    * Pin Compatbible to arduino uno\n    * Wishbone compatible design\n    * Written in System Verilog\n    * Open-source tool set\n       * simulation - iverilog\n       * synthesis  - yosys\n       * backend/sta - openlane tool set\n    * Verification suite provided.\n```\n\n# Riscduino derivatives\n\n<table>\n  <tr>\n    <td  align=\"center\"><img src=\"./docs/source/_static/Riscduino-derivatives.png\" ></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"><img src=\"./docs/source/_static/Riscduino_Series_placement.png\" ></td>\n  </tr>\n\n</table>\n\n# MPW Shuttle on Riscduino \n\n<table>\n  <tr>\n    <td  align=\"center\"> MPW</td> \n    <td  align=\"center\"> Tape-out</td>\n    <td  align=\"center\"> Project Name</td>\n    <td  align=\"center\"> Project Details</td>\n    <td  align=\"center\"> Github</td>\n    <td  align=\"center\"> Efabless</td>\n    <td  align=\"center\"> Tapeout Link</td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-2 </td> \n    <td  align=\"center\"> 18-June-2021  </td>\n    <td  align=\"center\"> YiFive</td>\n    <td  align=\"center\"> Single 32bit RISCV core without cache + SDRAM Controller + WB Interconnect</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/yifive\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://platform.efabless.com/projects/152\">Link</a></td>\n    <td  align=\"center\"> <a https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-002/slot-007\">Link</a></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-3 </td> \n    <td  align=\"center\"> 15-Nov-2021  </td>\n    <td  align=\"center\"> Riscduino</td>\n    <td  align=\"center\"> Single 32bit RISCV core without cache + Onchip SRAM + WB Interconnect</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/riscduino\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://platform.efabless.com/projects/385\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-003/slot-013\">Link</a></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-4 </td> \n    <td  align=\"center\"> 31-Dec-2021  </td>\n    <td  align=\"center\"> Riscduino-R1</td>\n    <td  align=\"center\"> Single 32bit RISCV core with cache + Onchip SRAM + WB Inter Connect</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/riscduino\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://platform.efabless.com/projects/575\">Link</a></td>\n    <td  align=\"center\"> <a https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-004/slot-014\">Link</a></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-5 </td> \n    <td  align=\"center\"> 21-Mar-2022  </td>\n    <td  align=\"center\"> Riscduino-DCORE (D0)</td>\n    <td  align=\"center\"> Dual 32bit RISCV core with cache + Onchip SRAM+ WB Cross Bar</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/riscduino_dcore\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://platform.efabless.com/projects/718\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-005/slot-013\">Link</a></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-5 </td> \n    <td  align=\"center\"> 21-Mar-2022  </td>\n    <td  align=\"center\"> Riscduino-QCORE (Q0)</td>\n    <td  align=\"center\"> Quad 32bit RISCV core with cache + Onchip SRAM+ WB Cross Bar</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/riscduino_qcore\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://platform.efabless.com/projects/782\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-005/slot-014\">Link</a></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-6 </td> \n    <td  align=\"center\"> 07-June-2022  </td>\n    <td  align=\"center\"> Riscduino-SCORE (S3)</td>\n    <td  align=\"center\"> Single 32bit RISCV core with cache + Onchip SRAM+ WB Cross Bar</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/riscduino\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://platform.efabless.com/projects/1047\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-006/slot-006\">Link</a></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-6 </td> \n    <td  align=\"center\"> 07-June-2022  </td>\n    <td  align=\"center\"> Riscduino-DCORE (D1)</td>\n    <td  align=\"center\"> Dual 32bit RISCV core with cache + Onchip SRAM+ WB Cross Bar</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/riscduino_dcore\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://platform.efabless.com/projects/838\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-006/slot-004\">Link</a></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-6 </td> \n    <td  align=\"center\"> 07-June-2022 </td>\n    <td  align=\"center\"> Riscduino-QCORE (Q1)</td>\n    <td  align=\"center\"> Quad 32bit RISCV core with cache + Onchip SRAM+ WB Cross Bar</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/riscduino_qcore\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://platform.efabless.com/projects/839\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-006/slot-005\">Link</a></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-7 </td> \n    <td  align=\"center\"> 12-Sept-2022  </td>\n    <td  align=\"center\"> Riscduino-SCORE (S4)</td>\n    <td  align=\"center\"> Single 32bit RISCV core with cache + Onchip SRAM+ WB Cross Bar</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/riscduino\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://platform.efabless.com/projects/1166\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-007/slot-008\">Link</a></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-7 </td> \n    <td  align=\"center\"> 12-Sept-2022  </td>\n    <td  align=\"center\"> Riscduino-DCORE (D3)</td>\n    <td  align=\"center\"> Dual 32bit RISCV core with cache + Onchip SRAM+ WB Cross Bar</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/riscduino_dcore\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://platform.efabless.com/projects/1167\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-007/slot-006\">Link</a></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-7 </td> \n    <td  align=\"center\"> 12-Sept-2022 </td>\n    <td  align=\"center\"> Riscduino-QCORE (Q1)</td>\n    <td  align=\"center\"> Quad 32bit RISCV core with cache + Onchip SRAM+ WB Cross Bar</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/riscduino_qcore\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://platform.efabless.com/projects/1168\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-007/slot-007\">Link</a></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-8 </td> \n    <td  align=\"center\"> 31-Dec-2022  </td>\n    <td  align=\"center\"> Riscduino-DCORE (D4)</td>\n    <td  align=\"center\"> Dual 32bit RISCV core with cache + Onchip SRAM+ WB Cross Bar</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/riscduino_dcore\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://platform.efabless.com/projects/1632\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-008/slot-004\">Link</a></td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> MPW-8 </td> \n    <td  align=\"center\"> 12-Sept-2022 </td>\n    <td  align=\"center\"> Riscduino-QCORE (Q3)</td>\n    <td  align=\"center\"> Quad 32bit RISCV core with cache + Onchip SRAM+ WB Cross Bar</td>\n    <td  align=\"center\"> <a href=\"https://github.com/dineshannayya/riscduino_qcore\">Link</a></td>\n    <td  align=\"center\"> <a https://platform.efabless.com/projects/1633\">Link</a></td>\n    <td  align=\"center\"> <a href=\"https://foss-eda-tools.googlesource.com/third_party/shuttle/sky130/mpw-008/slot-005\">Link</a></td>\n  </tr>\n</table>\n\n# SOC Pin Mapping\nCarvel SOC provides 38 GPIO pins for user functionality. Riscduino SOC GPIO Pin Mapping as follows vs ATMEGA328 and Arudino\n<table>\n  <tr>\n    <td  align=\"center\"><img src=\"./docs/source/_static/RiscDunio-PinMapping.png\" ></td>\n  </tr>\n\n</table>\n\n<table>\n  <tr align=\"center\"> <td> ATMGA328 Pin No</td> <td> Functionality           </td> <td> Arudino Pin Name</td> <td> Carvel Pin Mapping                   </td></tr>\n  <tr align=\"center\"> <td> Pin-1           </td> <td> PC6/RESET              </td> <td>  D20            </td> <td> digital_io[5]                        </td></tr>\n  <tr align=\"center\"> <td> Pin-2           </td> <td> PD0/RXD[0]             </td> <td>  D0             </td> <td> digital_io[6]                        </td></tr>\n  <tr align=\"center\"> <td> Pin-3           </td> <td> PD1/TXD[0]             </td> <td>  D1             </td> <td> digital_io[7]/analog_io[0]           </td></tr>\n  <tr align=\"center\"> <td> Pin-4           </td> <td> PD2/RXD[1]/INT0        </td> <td>  D2             </td> <td> digital_io[8]/analog_io[1]           </td></tr>\n  <tr align=\"center\"> <td> Pin-5           </td> <td> PD3/INT1/OC2B(PWM0)    </td> <td>  D3             </td> <td> digital_io[9]/analog_io[2]           </td></tr>\n  <tr align=\"center\"> <td> Pin-6           </td> <td> PD4/TXD[1]             </td> <td>  D4             </td> <td> digital_io[10]/analog_io[3]          </td></tr>\n  <tr align=\"center\"> <td> Pin-7           </td> <td> VCC                    </td> <td>                 </td> <td>  -                                   </td></tr>\n  <tr align=\"center\"> <td> Pin-8           </td> <td> GND                    </td> <td>                 </td> <td>  -                                   </td></tr>\n  <tr align=\"center\"> <td> Pin-9           </td> <td> PB6/XTAL1/TOSC1        </td> <td> D21             </td> <td> digital_io[11]/analog_io[4]          </td></tr>\n  <tr align=\"center\"> <td> Pin-10          </td> <td> PB7/XTAL2/TOSC2        </td> <td> D22             </td> <td> digital_io[12]/analog_io[5]          </td></tr>\n  <tr align=\"center\"> <td> Pin-11          </td> <td> PD5/SS[3]/OC0B(PWM1)/T1      </td> <td> D5        </td> <td> digital_io[13]/analog_io[6]          </td></tr>\n  <tr align=\"center\"> <td> Pin-12          </td> <td> PD6/SS[2]/OC0A(PWM2)/AIN0    </td> <td> D6        </td> <td> digital_io[14]/analog_io[7]          </td></tr>\n  <tr align=\"center\"> <td> Pin-13          </td> <td> PD7/A1N1               </td> <td> D7              </td> <td> digital_io[15]/analog_io[8]          </td></tr>\n  <tr align=\"center\"> <td> Pin-14          </td> <td> PB0/CLKO/ICP1          </td> <td> D8              </td> <td> digital_io[16]/analog_io[9]          </td></tr>\n  <tr align=\"center\"> <td> Pin-15          </td> <td> PB1/SS[1]OC1A(PWM3)         </td> <td> D9         </td> <td> digital_io[17]/analog_io[10]         </td></tr>\n  <tr align=\"center\"> <td> Pin-16          </td> <td> PB2/SS[0]/OC1B(PWM4)      </td> <td> D10          </td> <td> digital_io[18]/analog_io[11]         </td></tr>\n  <tr align=\"center\"> <td> Pin-17          </td> <td> PB3/MOSI/OC2A(PWM5)    </td> <td> D11             </td> <td> digital_io[19]/analog_io[12]         </td></tr>\n  <tr align=\"center\"> <td> Pin-18          </td> <td> PB4/MISO               </td> <td> D12             </td> <td> digital_io[20]/analog_io[13]         </td></tr>\n  <tr align=\"center\"> <td> Pin-19          </td> <td> PB5/SCK                </td> <td> D13             </td> <td> digital_io[21]/analog_io[14]         </td></tr>\n  <tr align=\"center\"> <td> Pin-20          </td> <td> AVCC                   </td> <td>                 </td> <td> -                                    </td></tr>\n  <tr align=\"center\"> <td> Pin-21          </td> <td> AREF                   </td> <td>                 </td> <td> analog_io[23]                        </td></tr>\n  <tr align=\"center\"> <td> Pin-22          </td> <td> GND                    </td> <td>                 </td> <td> -                                    </td></tr>\n  <tr align=\"center\"> <td> Pin-23          </td> <td> PC0/uartm_rxd/ADC0     </td> <td>  D14/A0         </td> <td> digital_io[22]/analog_io[15]         </td></tr>\n  <tr align=\"center\"> <td> Pin-24          </td> <td> PC1/uartm/ADC1         </td> <td>  D15/A1         </td> <td> digital_io[23]/analog_io[16]         </td></tr>\n  <tr align=\"center\"> <td> Pin-25          </td> <td> PC2/usb_dp/ADC2        </td> <td>  D16/A2         </td> <td> digital_io[24]/analog_io[17]         </td></tr>\n  <tr align=\"center\"> <td> Pin-26          </td> <td> PC3/usb_dn/ADC3        </td> <td>  D17/A3         </td> <td> digital_io[25]/analog_io[18]         </td></tr>\n  <tr align=\"center\"> <td> Pin-27          </td> <td> PC4/ADC4/SDA           </td> <td>  D18/A4         </td> <td> digital_io[26]/analog_io[19]         </td></tr>\n  <tr align=\"center\"> <td> Pin-28          </td> <td> PC5/ADC5/SCL           </td> <td>  D19/A5         </td> <td> digital_io[27]/analog_io[20]         </td></tr>\n  <tr align=\"center\"> <td colspan=\"4\">   Additional Pad used for Externam ROM/RAM/USB </td></tr>\n  <tr align=\"center\"> <td> Sflash          </td> <td> sflash_sck             </td> <td>                 </td> <td> digital_io[28]/Analog[21]            </td></tr>\n  <tr align=\"center\"> <td> SFlash          </td> <td> sflash_ss0             </td> <td>                 </td> <td> digital_io[29]/Analog[22]            </td></tr>\n  <tr align=\"center\"> <td> SFlash          </td> <td> sflash_ss1/AREF        </td> <td>                 </td> <td> digital_io[30]/Analog[23]            </td></tr>\n  <tr align=\"center\"> <td> SFlash          </td> <td> sflash_ss2             </td> <td>                 </td> <td> digital_io[31]/Analog[24]            </td></tr>\n  <tr align=\"center\"> <td> SFlash          </td> <td> sflash_ss3             </td> <td>                 </td> <td> digital_io[32]/Analog[25]            </td></tr>\n  <tr align=\"center\"> <td> SFlash          </td> <td> sflash_io0             </td> <td>                 </td> <td> digital_io[33]/Analog[26]            </td></tr>\n  <tr align=\"center\"> <td> SFlash          </td> <td> sflash_io1             </td> <td>                 </td> <td> digital_io[34]/Analog[27]            </td></tr>\n  <tr align=\"center\"> <td> SFlash          </td> <td> sflash_io2             </td> <td>                 </td> <td> digital_io[35]/Analog[28]            </td></tr>\n  <tr align=\"center\"> <td> SFlash          </td> <td> sflash_io3             </td> <td>                 </td> <td> digital_io[36]                       </td></tr>\n  <tr align=\"center\"> <td> DEBUG           </td> <td> dbg_clk_mon            </td> <td>                 </td> <td> digital_io[37]                       </td></tr>\n  <tr align=\"center\"> <td> SPARE           </td> <td> PA0                    </td> <td>                 </td> <td> digital_io[0]                       </td></tr>\n  <tr align=\"center\"> <td> SPARE           </td> <td> PA1                    </td> <td> D23             </td> <td> digital_io[1]                       </td></tr>\n  <tr align=\"center\"> <td> SPARE           </td> <td> PA2                    </td> <td> D24             </td> <td> digital_io[2]                       </td></tr>\n  <tr align=\"center\"> <td> SPARE           </td> <td> PA3                    </td> <td>                 </td> <td> digital_io[3]                       </td></tr>\n  <tr align=\"center\"> <td> SPARE           </td> <td> PA4                     </td> <td> D25            </td> <td> digital_io[4]                       </td></tr>\n</table>\n\n\n# Riscduino documentation\n*  Riscduino documentation available at : <https://riscduino.readthedocs.io/en/latest/>\n\n# Arduino ide integration\n*  We are in initial phase of Riscduino board integration into arduino and integration details are available at : <https://github.com/dineshannayya/riscduino_tools/>\n\n\n# Sub IP features\n\n## RISC V Core\n\nRiscduino SOC Integrated 32 Bits RISC V core. Initial version of Single core RISC-V core is picked from \nSyntacore SCR1 (https://github.com/syntacore/scr1)\n### RISC V core customization for Riscduino SOC\nFollowing Design changes are done on the basic version of syntacore RISC core\n```\n   * Some of the sv syntex are changed to standard verilog format to make compatibile with opensource tool iverilog & yosys\n   * local Instruction Memory depth increased from 4 to 8 location\n   * Instruction Mem Request are changed from Single word to 4 Word Burst\n   * Multiplication and Divsion are changed to improve timing\n   * Additional pipe line stages added to improve the RISC timing closure near to 50Mhz\n   * 2KB instruction cache \n   * 2KB data cache\n   * Additional router are added towards instruction cache\n   * Additional router are added towards data cache\n   * Modified AXI/AHB interface to wishbone interface for instruction and data memory interface\n```\n### Block Diagram\n<table>\n  <tr>\n    <td  align=\"center\"><img src=\"./docs/source/_static/riscvcore_blockdiagram.png\" ></td>\n  </tr>\n</table>\n\n### RISC V Core Key feature\n```\n   * RV32I or RV32E ISA base + optional RVM and RVC standard extensions\n   * Machine privilege mode only\n   * 2 to 5 stage pipeline\n   * 2KB icache\n   * 2KB dcache\n   * Optional Integrated Programmable Interrupt Controller with 16 IRQ lines\n   * Optional RISC-V Debug subsystem with JTAG interface\n   * Optional on-chip Tightly-Coupled Memory\n```\n\n\n# 6 Channel SAR ADC\n In Process - Looking for community help ...\n\n<table>\n  <tr>\n    <td  align=\"center\"><img src=\"./docs/source/_static/6-Channel-SAR-ADC.png\" ></td>\n  </tr>\n\n</table>\n\n# SOC Memory Map\n\n<table>\n  <tr>\n    <td  align=\"center\"> RISC IMEM</td> \n    <td  align=\"center\"> RISC DMEM</td>\n    <td  align=\"center\"> EXT MAP</td>\n    <td  align=\"center\"> Target IP</td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> 0x0000_0000 to 0x0FFF_FFFF  </td> \n    <td  align=\"center\"> 0x0000_0000 to 0x0FFF_FFFF  </td>\n    <td  align=\"center\"> 0x0000_0000 to 0x0FFF_FFFF</td>\n    <td  align=\"center\"> QSPI FLASH MEMORY</td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> 0x1000_0000 to 0x1000_00FF</td> \n    <td  align=\"center\"> 0x1000_0000 to 0x1000_00FF</td>\n    <td  align=\"center\"> 0x1000_0000 to 0x1000_00FF</td>\n    <td  align=\"center\"> QSPI Config Reg</td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> 0x1001_0000 to 0x1001_003F</td> \n    <td  align=\"center\"> 0x1001_0000 to 0x1001_003F</td>\n    <td  align=\"center\"> 0x1001_0000 to 0x1001_003F</td>\n    <td  align=\"center\"> UART</td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> 0x1001_0040 to 0x1001_007F</td> \n    <td  align=\"center\"> 0x1001_0040 to 0x1001_007F</td>\n    <td  align=\"center\"> 0x1001_0040 to 0x1001_007F</td>\n    <td  align=\"center\"> I2C</td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> 0x1001_0080 to 0x1001_00BF</td> \n    <td  align=\"center\"> 0x1001_0080 to 0x1001_00BF</td>\n    <td  align=\"center\"> 0x1001_0080 to 0x1001_00BF</td>\n    <td  align=\"center\"> USB</td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> 0x1001_00C0 to 0x1001_00FF</td> \n    <td  align=\"center\"> 0x1001_00C0 to 0x1001_00FF</td>\n    <td  align=\"center\"> 0x1001_00C0 to 0x1001_00FF</td>\n    <td  align=\"center\"> SSPI</td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> 0x1001_01C0 to 0x1001_013F</td> \n    <td  align=\"center\"> 0x1001_01C0 to 0x1001_013F</td>\n    <td  align=\"center\"> 0x1001_01C0 to 0x1001_013F</td>\n    <td  align=\"center\"> SSPI</td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> 0x1002_0080 to 0x1002_00FF</td> \n    <td  align=\"center\"> 0x1002_0080 to 0x1002_00FF</td>\n    <td  align=\"center\"> 0x1002_0080 to 0x1002_00FF</td>\n    <td  align=\"center\"> PINMUX</td>\n  </tr>\n  <tr>\n    <td  align=\"center\"> -</td> \n    <td  align=\"center\"> -</td>\n    <td  align=\"center\"> 0x3080_0000 to 0x3080_00FF</td>\n    <td  align=\"center\"> WB HOST</td>\n  </tr>\n</table>\n\n# SOC Size\n\n| Block             | Total Cell | Combo   | Seq      |\n| ------            | ---------  | -----   | -------- |\n| RISC              |  47218     | 41230   | 5988     |\n| QSPI              |  9039      |  7526   | 1513     |\n| UART_I2C_USB_SPI  | 11793      |  8932   | 2861     |\n| WB_HOST           |  6508      |  5356   | 1152     |\n| WB_INTC           |  6949      |  5538   | 1411     |\n| PINMUX            | 11932      |  9327   | 2605     |\n| PERIPHERAL        |  5852      |  4791   | 1061     |\n| FPU               | 12831      | 11394   | 1437     |\n| AES               | 21549      | 17960   | 3589     |\n| BUS-REPEATER      |   922      |   922   | 0        |\n|                   |            |         |          |\n| TOTAL             | 134593     | 112976  | 21617    |\n\n\n\n# Prerequisites\n   - Docker (ensure docker daemon is running) -- tested with version 19.03.12, but any recent version should suffice.\n## Step-1: Docker in ubuntu 20.04 version\n```bash\n   sudo apt update\n   sudo apt-get install apt-transport-https curl rtificates -agent software-properties-common\n   curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n   sudo apt-get install apt-transport-https curl ca-certificates software-properties-common\n   sudo apt update\n   apt-cache policy docker-ce\n   sudo apt install docker-ce\n\n   #Add User Name to docker\n   sudo usermod -aG docker <your user name>\n   # Reboot the system to enable the docker setup\n```\n##  Step-2: Clone , update the Submodule, unzip the content\n```bash\n   git clone https://github.com/dineshannayya/riscduino.git\n   cd riscduino\n   git submodule init\n   git submodule update\n   make unzip\n```\n\n## Note-1: RTL to GDS Docker\n    - Required openlane and pdk are moved inside the riscduino docker to avoid the external dependency. \n    - flow automatically pull the required docker based on MPW version.\n    - RTL to gds docker is hardcoded inside File: openlane/Makefile\n```bash\n     OPENLANE_TAG = mpw9\n     OPENLANE_IMAGE_NAME = riscduino/openlane:$(OPENLANE_TAG)\n```\n## Note-1.1: View the RTL to GDS Docker content\n    - for MPW-9 caravel pdk and openlane avaible inside riscduino/openlane:mpw9 docker \n    - caravel, openlane and pdk envionment are automatically pointed to internal docker pointer\n    - To view the docker contents\n```bash\n    docker run -ti --rm riscduino/openlane:mpw9  bash\n    cd /opt/pdk_mpw9     -  pdk folder\n    cd /opt/caravel      -  caravel folder \n    cd /openlane         -  openlane folder\n    env   - Show the internally defined env's\n        CARAVEL_ROOT=/opt/caravel\n        PDK_ROOT=/opt/pdk_mpw9\n```\n\n## Note-2: RTL Simulation Docker\n    - Required caravel and pdk are moved inside the riscduino docker to avoid the external dependency. \n    - flow automatically pull the required docker based on MPW version.\n    - To view the docker contents\n    - RTL simulation docker hardcoded inside File: Makefile\n        simenv:\n\t    docker pull riscduino/dv_setup:mpw6\n\n## Note-2.1: View the RTL Simulation Docker content\n    - for MPW-9 caravel and pdk avaible inside riscduino/dv_setup:mpw9 docker this is used for RTL to gds flows\n    - caravel and pdk envionment are automatically pointed to internal docker pointer\n    - To view the docker contents\n```bash\n    docker run -ti --rm riscduino/dv_setup:mpw9  bash\n    cd /opt/pdk_mpw9     -  pdk folder\n    cd /opt/caravel      -  caravel folder \n    env   - Show the internally defined env's\n        CARAVEL_ROOT=/opt/caravel\n        PDK_ROOT=/opt/pdk_mpw6\n```\n## Note-3: Clock Gate Plugin integration\n1. **Hack-1: Modify following files for synthesis script update**\n```\n   Directory: OpenSTA (hacks/src/OpenSTA)\n   Source Files:\n\t   OpenLane/scripts/yosys/synth.tcl\n\t   OpenLane/configuration/synthesis.tcl\n\t   OpenLane/scripts/utils/utils.tcl\n    Modifield File: \n       hacks/clk_gating/synth.tcl\n       hacks/clk_gating/synthesis.tcl\n       hacks/clk_gating/utils.tcl\n```\n2. **Hack-2: Create a pluging directory in openlane and add following files**\n```\nplugin\n   â””â”€â”€ yosys\n       â””â”€â”€ Lighter\n           â”œâ”€â”€ clock_gating_plugin.cc\n           â”œâ”€â”€ README.md\n           â”œâ”€â”€ sky130_clkg_blackbox.v\n           â””â”€â”€ sky130_ff_map.v\n```\n\n3. **Hack-3: Add ENV to enable clock gating in config.tcl**\n```\n   set ::env(SYNTH_ENB_CG) 1\n```\n4. **Hack-4: Check Clock gate insertion in Generated netlist file**\n```\n   grep sky130_fd_sc_hd__dlclkp  <your netlist file>\n```\n\n# Tests preparation\n\nThe simulation package includes the following tests:\n\n## Standalone Riscduino SOC Specific Test case \n* **1.user_basic**             - Basic test case to validate strap and clocks\n* **2.user_uart**              - Standalone Risc with UART-0 Test\n* **3.user_uart1**             - Standalone Risc with UART-1 Test\n* **4.user_risc_boot**         - Standalone User Risc core boot\n* **4.risc_boot**              - Complete caravel User Risc core boot \n* **5.user_qspi**              - Standalone Quad SPI test\n* **6.user_sspi**              - Standalone SSPI test\n* **7.user_i2c**               - Standalone I2C test\n* **8.user_usb**               - Standalone USB Host test\n* **9.user_gpio**              - Standalone GPIO Test\n* **10.user_aes**              - AES computation through Riscv core\n* **11.user_spi_isp**          - Device boot with SPI as ISP\n* **12.user_timer**            - Standalone timer Test\n* **13.user_uart_master**      - Standalone uart master test\n* **14.user_sram_exec**        - Riscv Boot with code running in SRAM\n* **15.user_cache_bypass**     - Riscv Boot without icache and dcache\n* **16.user_pwm**               -Standalone pwm Test\n* **17.user_sema**              -Standalone validation of hardware Semaphore function\n* **18.riscv_regress**          -Standalone riscv compliance and regression test suite\n* **19.user_rtc**               -Standalone RTC core test\n* **20.user_aes_core**          -Standalone AES Core test\n* **21.user_fpu_core**          -Standalone FPU(SP) Core test\n* **22.user_rtc**               -Standalone RTC core test\n* **24.user_ir_tx**             -Standalone IR Transmitted Test\n* **25.user_ir_rx**             -Standalone IR Receiver Test\n* **26.user_random**            - User Random core test\n* **27.step_motor_controller**  - Standalone Stepper Motor Controller\n\n## Caravel+RISCDUINO Integrated Specific Test case \n* **1.wb_port**             - Complete caravel User Wishbone validation\n* **2.uart_master_test1**   - complete caravel user uart master test with baud control from LA port\n* **3.uart_master_test2**   - complete caravel user uart master test based on auto baud detection\n* **4.risc_boot**           - Complete caravel User Risc core boot \n* **5.caravel_hkspi**       - Caravel House Keeping SPI test case\n\n## Arduino Based Test Case\n* **1.arduino_arrays**                - Validation of Array function\n* **2.arduino_digital_port_control**  - Validation for AD5206 digital potentiometer through SPI\n* **3.arduino_i2c_scaner**            - I2C port scanner        \n* **4.arduino_risc_boot**             - Riscv Basic Boot\n* **5.arduino_timer_intr**            - Timer Interrupt handling\n* **6.arduino_ascii_table**           - Ascii Table Display\n* **7.arduino_gpio_intr**             - GPIO Interrupt generation\n* **8.arduino_i2c_wr_rd**             - I2C Write and Read access\n* **9.arduino_string**                - Validation of String function\n* **10.arduino_ws281x**               - Validation of ws281x serial protocol \n* **11.arduino_character_analysis**   - uart Input Character analysis\n* **12.arduino_hello_world**          - Basic hello world display\n* **13.arduino_multi_serial**         - Validation of Two Serail port\n* **14.arduino_switchCase2**          - Validation of switch case\n\n# Running Simulation\n\nExamples:\n``` sh\n    make verify-wb_port                        - User Wishbone Test from caravel\n    make verify-risc_boot                      - User Risc core test from caravel\n    make verify-uart_master                    - User uart master test from caravel\n    make verify-user_basic                     - Standalone Basic signal and clock divider test\n    make verify-user_uart                      - Standalone user uart-0 test using user risc core\n    make verify-user_uart1                     - Standalone user uart-0 test using user risc core\n    make verify-user_i2cm                      - Standalone user i2c test\n    make verify-user_risc_boot                 - standalone user risc core-0 boot test\n    make verify-user_pwm                       - standalone user pwm test\n    make verify-user_timer                     - standalone user timer test\n    make verify-user_sspi                      - standalone user spi test\n    make verify-user_qspi                      - standalone user quad spi test\n    make verify-user_usb                       - standalone user usb host test\n    make verify-user_gpio                      - standalone user gpio test\n    make verify-user_aes                       - standalone aes test with risc core-0\n    make verify-user_cache_bypass              - standalone icache and dcache bypass test with risc core-0\n    make verify-user_uart_master               - standalone user uart master test\n    make verify-user_sram_exec                 - standalone riscv core-0 test with executing code from data memory\n    make verify-riscv_regress                  - standalone riscv compliance test suite\n    make verify-arduino_risc_boot              - standalone riscv core-0 boot using arduino tool set\n    make verify-arduino_hello_world            - standalone riscv core-0 hello world test using arduino tool set\n    make verify-arduino_digital_port_control   - standalone riscv core-0 digital port control using arduino tool set\n    make verify-arduino_ascii_table            - standalone riscv core-0 ascii table using arduino tool set\n    make verify-arduino_character_analysis     - standalone riscv core-0 character analysis using arduino tool set\n    make verify-arduino_multi_serial           - standalone riscv core-0 multi uart test using arduino tool set\n    make verify-arduino_switchCase2            - standalone riscv core-0 switch case using arduino tool set\n    make verify-arduino_risc_boot              - standalone riscv core-0 boot test using arduino tool set\n    make verify-arduino_string                 - standalone riscv core-0 string usage test using arduino tool set\n\n   \n    make verify-user_uart SIM=RTL DUMP=OFF     - Standalone user uart-0 test using user risc core with waveform dump off\n    make verify-user_uart SIM=RTL DUMP=ON      - Standalone user uart-0 test using user risc core with waveform dump on\n    make verify-user_uart SIM=GL DUMP=OFF      - Standalone user uart-0 test using user risc core with gatelevel netlist\n    make verify-user_uart SIM=GL DUMP=ON       - Standalone user uart-0 test using user risc core with gatelevel netlist and waveform on\n\n```\n# Running RTL to GDS flows\n   - First run the individual macro file\n   - Last run the user_project_wrapper\n``` sh\n   cd openlane\n   make pinmux\n   make qspim_top\n   make uart_i2cm_usb_spi_top\n   make wb_host\n   make wb_interconnect\n   make ycr_intf\n   make ycr_core_top\n   make ycr_iconnect\n   make user_project_wrapper\n```\n\n#Timing Analysis\n## Timing Analysis setup\n   \n``` sh\n   make setup-timing-scripts\n   make install\n   make install_mcw\n```\nhis will update Caravel design files and install the scripts for running timing.\n\n## Running Timing Analysis\n\n``` sh\nmake extract-parasitics\nmake create-spef-mapping\nmake caravel-sta\n```\n#Other Miscellaneous Targets\nThe makefile provides a number of useful that targets that can run LVS, DRC, and XOR checks on your hardened design outside of openlaneâ€™s flow.\n\nRun make help to display available targets.\n\nRun lvs on the mag view,\n\n``` sh\nmake lvs-<macro_name>\n```\n\nRun lvs on the gds,\n\n``` sh\nmake lvs-gds-<macro_name>\n```\n\nRun lvs on the maglef,\n\n``` sh\nmake lvs-maglef-<macro_name>\n```\n\nRun drc using magic,\n\n``` sh\nmake drc-<macro_name>\n```\n\nRun antenna check using magic,\n\n``` sh\nmake antenna-<macro_name>\n```\n\nRun XOR check,\n\n``` sh\nmake xor-wrapper\n```\n\n\n\n\n# Tool Sets\n\nRiscduino Soc flow uses Openlane tool sets.\n\n1. **Synthesis**\n    1. `yosys` - Performs RTL synthesis\n    2. `abc` - Performs technology mapping\n    3. `OpenSTA` - Pefroms static timing analysis on the resulting netlist to generate timing reports\n2. **Floorplan and PDN**\n    1. `init_fp` - Defines the core area for the macro as well as the rows (used for placement) and the tracks (used for routing)\n    2. `ioplacer` - Places the macro input and output ports\n    3. `pdn` - Generates the power distribution network\n    4. `tapcell` - Inserts welltap and decap cells in the floorplan\n3. **Placement**\n    1. `RePLace` - Performs global placement\n    2. `Resizer` - Performs optional optimizations on the design\n    3. `OpenPhySyn` - Performs timing optimizations on the design\n    4. `OpenDP` - Perfroms detailed placement to legalize the globally placed components\n4. **CTS**\n    1. `TritonCTS` - Synthesizes the clock distribution network (the clock tree)\n5. **Routing**\n    1. `FastRoute` - Performs global routing to generate a guide file for the detailed router\n    2. `CU-GR` - Another option for performing global routing.\n    3. `TritonRoute` - Performs detailed routing\n    4. `SPEF-Extractor` - Performs SPEF extraction\n6. **GDSII Generation**\n    1. `Magic` - Streams out the final GDSII layout file from the routed def\n    2. `Klayout` - Streams out the final GDSII layout file from the routed def as a back-up\n7. **Checks**\n    1. `Magic` - Performs DRC Checks & Antenna Checks\n    2. `Klayout` - Performs DRC Checks\n    3. `Netgen` - Performs LVS Checks\n    4. `CVC` - Performs Circuit Validity Checks\n\n\n\n# How To Contribute\n\nWe are looking for community help in following activity, interested user can ping me in efabless slack platform\n\n*  **Analog Design**           - ADC, DAC, PLL,\n*  **Digital Design**          - New IP Integration, Encription,DSP, DMA controller, 10Mb MAC, Floating point functions\n*  **Verification**            - Improving the Verification flow\n*  **Linux Porting**           - Build Root integration\n*  **Arudino Software Update** - Tool Customisation for Riscduino, Adding additional plug-in and Riscv compilation support\n*  **Riscv Simulator**         - integration to Riscduino\n*  **Any other ideas**          \n\n\n# Contacts\n\n* **Report an issue**: <https://github.com/dineshannayya/riscduino/issues>\n* **Group Email**: <https://groups.google.com/g/riscduino>\n\n# Reference\n* **Syntacore**     - https://github.com/syntacore/scr1\n* **DCore Riscv**   - https://github.com/dineshannayya/ycr2c\n* **quad spi**      - https://github.com/dineshannayya/qspim\n* **security core** - https://github.com/dineshannayya/security_core\n* **fpu**           - https://github.com/dineshannayya/fpu\n* **rtc**           - https://github.com/dineshannayya/rtc\n* **uart**          - https://github.com/dineshannayya/uart2spi\n* **jtag vpi**      - https://github.com/fjullien/jtag_vpi\n* **clock gating**  - https://github.com/AUCOHL/Lighter\n* **i2c**           - http://www.opencores.org/projects/i2c/\n* **USB1 core**     - https://github.com/ultraembedded/core_usb_host\n* **USB1 phy**      - https://github.com/ultraembedded/core_usb_fs_phy\n* **8 bit DAC**     - https://github.com/pramitpal/8bit_dac\n* **IR-Receiver**   - https://github.com/JulienOury/ChristmasTreeController/\n* **Random Number** - https://github.com/JulienOury/ChristmasTreeController/\n* **Stepper Motor** - https://github.com/JulienOury/ChristmasTreeController/\n\nYoutube video on Riscduino\n===============\n* **Riscduino CI2206Q Bringup Status** - https://youtu.be/EzxGh2Bxo_o\n* **Riscduino Live Demo** - https://youtu.be/FrcUKUgzDW4\n* **Riscduino Aim** - https://www.youtube.com/watch?v=lFVnicPhTI0\n* **Riscduino MPW-2 Bringup Status** - https://www.youtube.com/watch?v=Qo_RZ8Fo--c\n\nNews on Riscduino\n===============\n* **Riscduino OpenRoad Article** - https://theopenroadproject.org/implementation-of-riscduino-core-using-a-hierarchical-design-flow/\n* **riscv.org News** - https://riscv.org/news/2023/01/implementation-of-riscduino-core-using-a-hierarchical-design-flow-dinesh-annayya-openroad/\n\n\n",
    "readme_length": 38842
  },
  {
    "name": "Adafruit_CircuitPython_PCA9685",
    "full_name": "adafruit/Adafruit_CircuitPython_PCA9685",
    "description": "Adafruit CircuitPython driver for PCA9685 16-channel, 12-bit PWM LED & servo driver chip.",
    "stars": 155,
    "forks": 65,
    "language": "Python",
    "url": "https://github.com/adafruit/Adafruit_CircuitPython_PCA9685",
    "topics": [
      "circuitpython",
      "hacktoberfest"
    ],
    "created_at": "2017-01-09T19:39:34Z",
    "updated_at": "2025-12-01T12:32:52Z",
    "homepage": null,
    "license": "MIT License",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "Micro_RC_Receiver",
    "full_name": "TheDIYGuy999/Micro_RC_Receiver",
    "description": "An Atmega328P based 2.4GHz receiver with integrated TB6612FNG motor driver. PWM and SBUS outputs.",
    "stars": 151,
    "forks": 55,
    "language": "C",
    "url": "https://github.com/TheDIYGuy999/Micro_RC_Receiver",
    "topics": [
      "arduino",
      "rc-car",
      "remote-control",
      "sbus"
    ],
    "created_at": "2016-07-26T18:02:03Z",
    "updated_at": "2025-11-24T14:54:12Z",
    "homepage": "https://www.youtube.com/watch?v=Jfju3FHmbbs",
    "license": "Other",
    "readme": "# This is an ATMEL Mega328P / NRF24L01+ based 2.4GHz radio receiver\n## Features:\n- Compatible with this transmitter: https://github.com/TheDIYGuy999/RC_Transmitter\n- Programmable with Arduino IDE\n- 4 RC servo connectors\n- SBUS output\n- Outputs for Headlights, indicators, taillights with brake lights\n- Additional switching output\n- Support for self balancing robot or car with stability control (using an MPU-6050 sensor)\n- Integrated TB6612FNG dual dc motor driver\n- Serial header (with SBUS support)\n- I2C header (for example for the supported MPU-6050)\n- ICSP header for programming with Arduino IDE (use \"TheDIYGuy999 328P\" board definition and set brownout voltage to 1.8V)\n- NRF24L01+ SMD module integrated\n- very small size\n- Up to 20 vehicle addresses selectable, allows to use the same remote controller for 10 vehicles\n- Eagle & Gerber files are also provided, if you want to build your own receiver!\n\nSee: https://www.youtube.com/playlist?list=PLGO5EJJClJBCjIvu8frS7LrEU3H2Yz_so\n\nNew in V 1.3:\n- Vehicle configuration is now stored in \"vehicleConfig.h\", so the main code always stays the same.\n- Crude frequency hopping, if bad receiving quality is detected (needs to be improved).\n\nNew in V 1.4:\n- 10 vehicle addresses instead of 5\n- More vehicle configuration options\n- The \"BACK\" button on the transmitter can now be used (if the menu screen is not displayed on the OLED) as a momentary button, for example for a horn on the vehicle. It is sent as \"data.momentary1\"\n- SimpleTimer library replaced with my own code, which uses significantly less memory\n\nNew in V 1.5:\n- Added steering mixer for semi caterpillar and caterpillar vehicles such as tanks, diggers etc.\n- Channel 3 is throttle, channel 1 is steering\n- More vehicle configuration options in vehicleVonfig.h\n- The left caterpillar is motor 2, the right is motor 1\n- Steering servo 1 can be used in parallel (for semi caterpillar vehicles or vehicles with electronic differential)\n\nNew in V 1.6:\n- Added indicator lights - wired to A4 (SDA) / A5 (SCL) ports\n- enabled and disabled left / right by channel 4 left / right\n- disabled by turning back the steering wheel (channel 1) - just like a real car\n- hazard lights (all indicators flashing), if no connection to the transmitter\n\nNew in V 1.61:\n- Battery cutoff bug fixed\n\nNew in V 1.7 (Support for board revision 1.3):\n- **** TB6612FNG library update to V1.1 is required! ****\n- motor driver pinout has changed in board V1.3. So we are now able to adjust the PWM frequency of motor2. This allows smooth motor operation without the typical PWM whining.\n- vehicleConfig.h changed: \"boardVersion\" and \"HP\" variables added. They allow to stay compatible with older board versions. Just select the correct configuration.\n- The new \"HP\" (High Power) board version has only one motor driver channel, because both TB6612FNG channels are wired in parallel. It's able to supply 2.4A (average) / 6.4A (peak) so we can use bigger motors. This is useful for models, which use a steering servo and don't need a second motor channel.'\n- automatic radio re-initialisation after signal timeout\n\nNew in V 1.71\n- Battery monitoring and cutoff triggering improved. Under load, 0.3V are now added to the battery voltage to compensate the voltage drop. This allows to set the cutoff voltage to 3.6V, even with bigger loads.\n- Vehicle #7 added.\n\nNew in V 1.8\n- New \"engineSound\" variable in \"vehicleconfig.h\". True means, that the servo 3 pulse signal can be switched on and off, using the  channel 2 joystick. This switches the engine sound on my \"Rc Engine Sound\" unit on and off.\n\nNew in V 1.9\n- New vehicle specific configuration section in vehicleConfig.h. The configuration is now done entirely during compilation time.\n\nNew in V 2.0\n- vehicle Config.h: New vehicleType 3 (forklift) added. Motor 2 is used as lifting motor in this case. A servo is required as steering motor.\n- Beacon lights can be wired to servo channel 4. Select \"beacons = true\" in this case. The beacons are flashing during vehicle movement and are switching off, after the vehicle did not move for 10s.\n\nNew in V 2.1\n- Sounds for a STAR WARS R2-D2 robot can now be generated on servo channel 3\n- \"toneOut\" variable added to vehicleConfig.h. True means, that servo 3 channel is generating sounds instead of a servo signal.\n- Do not enable \"toneOut\" and \"engineSound\" at the same time.\n- Note: the Arduino tone() function is blocking. So don't play sounds while driving ;-)\n\nNew in V 2.2\n- **** TB6612FNG library update to V1.2 is required! **** https://github.com/TheDIYGuy999/TB6612FNG\n- **** The PID library is required! **** https://github.com/br3ttb/Arduino-PID-Library/\n- Support for self balancing (inverted pendulum, segway) vehicles. See: https://www.youtube.com/watch?v=zse9-l2Yo3Y)\n- A MPU-6050 accelerometer & gyro module is used for the balancing data communication via SDA & SCL\n- Note, that the receiver will not work, if your vehicleType is 4 (balancing) and no MPU-6050 is connected!\n- Do not enable \"balancing\" and \"indicators\" at the same time.\n\nNew in V 2.21\n- Gyro drift calibration bug fixed\n- Changed the balancing controller refresh rate to 125Hz to reduce jitter on a 8MHz MCU. Reason: one program loop takes 4 - 7ms, so we can't use a time base of 4ms (=250Hz).\n\nNew in V 2.3\n- MRSC (Micro RC Stability Control) added. This functionality is similar to the Traxxas TSM or ESC. See: https://www.youtube.com/watch?v=IPve7QpdLBc&t=5s\n- The steering servo angle is compensated in accordance with the speed, the requested steering angle and the measured yaw rate, which is coming from the MPU-6050\n- Basically the mrsc() function was added. The required yaw rate is already calculated in the balancing.h, which is already existing for self balancing robots\n- If you want to use this functionality, you have to set the vehicleType to 5 in the vehicleConfig.h. Please note, that the receiver will not work, if no MPU-6050 is connected and the vehicleType is set to 4 or 5!\n\nNew in V 2.31\n - MRSC gain factor now depending on the potentiometer 1 value on the transmitter\n - Allows to fine adjust the MRSC during driving, depending on the requirements of the street surface\n - Note, that you need to connect a potentiometer to the analog input A6 of your transmitter!\n \n New in V 2.32\n - Flickering headlights bug in vehicleType 5 fixed\n - The accelerometer vectors are not processed anymore in vehicleType 5. This is only required for the self balancing robot (vehicleType 4)\n \n New in V 2.4\n - The I2C bus is scanned for an MPU-6050 sensor during the setup() sequence. The vehicleType is automatically changed to 0 (car without MRSC), if no sensor is detected.\n - This means, that a car with vehicleType 5 (MRSC Stability Control) can now also be used without an MPU-6050 plugged in.\n - Note, that the self balancing mode (vehicleType 4) always requires an MPU-6050. Otherwise the receiver will not start.\n \n New in V 2.41\n - The \"indicators\" variable is now locked and can't be enabled, if the SDA and SCL pins are in use for the MPU-6050 sensor. This is the case in vehicleMode 4 and 5.\n \n New in V 2.5\n - The MRSC stability control is now also working for cars without a steering servo\n - Code was added to control a DC steering motor (as found in cheap RC cars) in accordance with the MPU-6050 yaw rate\n - This is for example very useful for Coke Can Cars. See: https://www.youtube.com/watch?v=jr5yYBal3vk\n \n New in V 2.6\n - The MRSC stability control can now be used with transmitters without gain adjustment potentiometer\n - The following optional parameters in the vehicleConfig.h allow to use a fixed gain: MRSC_FIXED, mrscGain\n - Allows to add an MPU-6050 gyro to every vehicle. So we always have a closed loop steering control, even without a proper servo with potentiometer feedback\n - On the \"Non-High-Power\" receiver model, 0.3V battery voltage are not added anymore during driving. This is not useful, if there is not much current draw.\n \n New in V 2.7\n - The MRSC stability control gain adjustment knob can now be linked to servo CH4, if \"potentiometer1\" is set to \"true\" in vehicleConfig.h\n - This is useful, if an external MRSC stability control unit for 3 pin servos is used. See: https://github.com/TheDIYGuy999/MRSC_Adapter_3Pin_Servo\n - Servo CH4 is wired to pin 5 in this case. This allows to adjust the MRSC gain on the transmitter, just as with the internal MRSC stability control\n \n New in V 2.8\n - Added support for differential thrust controlled airplanes\n - Tested with a Piper J3 CUB from Banggood\n \n New in V 2.9\n - Added TWO_SPEED_GEARBOX option for mechanical two speed gearboxes, shifted by a servo, connected to CH2, controlled by \"Mode 1\" button\n - Example see vehicle CONFIG_WPL_B-36\n - See video: https://www.youtube.com/watch?v=EaOJE_GU5pk&t=1s\n \n New in V 3.0\n - Added TRACTOR_TRAILER_UNLOCK option. Used to operate the tractor trailer unlocking servo\n - Press the \"Back / Pulse\" button to unlock your trailer\n - See vehicle \"CONFIG_JJRC_Q60\" for example\n - Parts for 3D printing: https://www.thingiverse.com/thing:3399449\n \n New in V 3.1\n - Added THRE_SPEED_GEARBOX option for mechanical three speed gearboxes, shifted by a servo, connected to CH2, controlled by a 3 position switch, connected to CH2 Joystick input of the transmitter. A 10k resistor is reqired between GND and the input pin as well as between the input pin and VCC\n - Three individual servo positions can be programmed: lim2L, lim2C, lim2R\n - Example see vehicle CONFIG_HG_P407\n \n New in V 3.2\n - Added the \"TXO_toggle1\" variable: allows to toggle the TXO pin, using the \"Back / Impulse\" button on the transmitter\n - Useful for example for additional lights\n - Don't activate it in combination with \"TXO_momentary1\". Serial is not usable, if one of these booleans is set to true\n \n New in V 3.3\n - Added experimental support for serial communication with my \"Arduino RC engine sound generator for ESP32\": https://github.com/TheDIYGuy999/Rc_Engine_Sound_ESP32\n - Uncomment \"#define SERIAL_COMMUNICATION\" in Adjustments.h of the sound generator\n - Connect pin 36 \"VP\"of the sound generator  to pin \"TXO\" of the receiver (disable \"TXO_momentary1\", \"TXO_toggle1\" & \"headLights\" in vehicleConfig.h)\n \n New in V 3.31\n - \"headlLights\" must be inactive as well in order to use serial communication!\n \n New in V 3.32\n - \"data.pot1\" properly initialized (cauesd an issue on the ESP32 engine sound generator)\n \n New in V 3.4\n - SBUS support added (variable \"SBUS_SERIAL\" in vehicleConfiguration.h\n - Channel order and comments see sendSbusCommands()\n - You need to install my SBUS library: https://github.com/TheDIYGuy999/SBUS\n - Connect your servos or what ever to pin \"TXO\"  (disable \"TXO_momentary1\", \"TXO_toggle1\" & \"headLights\" in vehicleConfig.h)\n \n New in V 3.5\n - mode1, mode2 and momentary1 properly initialised before SBUS is enabled\n \n New in V 3.6\n - \"vehicleType\" 1 or 2 (tracked or half tracked mode steering mixer) is now working as well, if two ESC are connected to servo pins 2 and 3. In the past, it was only usable with the internal TB6612 dual DC motor driver.\n - Also works in combination with my ESP32 Engine Sound & Light Controller (SBUS & PWM Mode)\n \n New in V 3.7\n - Steering center position separately adjustable, using \"#define STEERING_3_POINT_CAL\" and \"lim1C\". This is useful for badly designed steering linkage geometries. Example see vehicle configuration \"CONFIG_WPL_B_36_MODE1\"\n \n New in V 3.8:\n - 20 vehicle addresses instead of 10\n - Less RAM usage\n - 2 speed transmission not shifting while standing still. This will protect the shifting servo.\n\nNew in V 3.81:\n - Libraries comments added\n \n New in V 3.9:\n - New \"#define VEHICLE_TYPE_3_WITH_ESC\" option for vehicle type 3. Allows to use both TB6612 FNG motor driver channels for other stuff\n - Used in \"CONFIG_MECCANO_DUMPER\"\n\n## Usage\n\nSee pictures\n\nNewest version v1.5:\n![](https://github.com/TheDIYGuy999/Micro_RC_Receiver/blob/master/1.5complete.jpg)\n![](https://github.com/TheDIYGuy999/Micro_RC_Receiver/blob/master/1.5top.jpg)\n![](https://github.com/TheDIYGuy999/Micro_RC_Receiver/blob/master/1.5bottom.jpg)\n\n\nFirst prototype:\n![](https://github.com/TheDIYGuy999/Micro_RC_Receiver/blob/master/1.jpg)\n![](https://github.com/TheDIYGuy999/Micro_RC_Receiver/blob/master/2.jpg)\n![](https://github.com/TheDIYGuy999/Micro_RC_Receiver/blob/master/3.jpg)\n\nReceiver PCB\n\n![](https://github.com/TheDIYGuy999/Micro_RC_Receiver/blob/master/Micro_RC_Receiver.png)\n\nMPU-6050 shield PCB\n\n![](https://github.com/TheDIYGuy999/Micro_RC_Receiver/blob/master/MPU-6050_Shield.png)\n\n(c) 2016 - 2020 TheDIYGuy999\n",
    "readme_length": 12576
  },
  {
    "name": "STM32-PLC",
    "full_name": "DanielMartensson/STM32-PLC",
    "description": "STM32 microcontroller with lots of periferials such as ADC, differential ADC, Input Capture, PWM, USB, Encoder, DAC, Digital Input, RTC, CAN-bus + Alarm etc.",
    "stars": 146,
    "forks": 63,
    "language": "C",
    "url": "https://github.com/DanielMartensson/STM32-PLC",
    "topics": [
      "c",
      "can-bus",
      "embedded-systems",
      "stm32",
      "usb"
    ],
    "created_at": "2021-08-18T14:25:17Z",
    "updated_at": "2025-11-29T05:48:38Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# STM32 PLC\n\n![a](https://raw.githubusercontent.com/DanielMartensson/STM32-PLC/main/Documentation/Pinmap.png)\n\nThis project is a PCB bord that has the following measurement and control input/output:\n\n * 12 x ADC at 16-bit resolution for 0-20mA input with programmable gain\n * 5 x Differential ADC at 16-bit for 0-20mA input with programmable gain\n * 3 x DAC at 12-bit with 0-20mA output\n * 8 x PWM for 0-2.2A with N-channel MOSFET\n * 10 x Digital Input\n * 1 x CAN-bus channel\n * 4 x Input Capture for 0 kHz to 10kHz\n * 3 x Encoder for -32768 to 32767 pulses\n * 1 x USB port for connecting with [OpenSourceLogger](https://github.com/DanielMartensson/OpenSourceLogger) and [GoobySoft](https://github.com/DanielMartensson/GoobySoft)\n * 1 x SPI with 3 chip select for ILI9341 LCD with touch\n * 1 x ST-Link V2 connection\n * 1 x RTC clock with two alarms A(date) and B(week day) and a battery holder so the RTC remembers the date and time\n\n## Protection \n * All ADC, Digital Input, Differential ADC, Input Capture and Encoder are high voltage protected with PTC(fall current 30mA) fuse + 3.6v zener diode.\n * All PWM and DAC are high voltage protected with N-channel MOSFET and PNP-transistor and OP-amp. \n * The CAN-bus channel is high voltage peak protected for 3000V under a short time with a TVS-diode. The CAN-bus transmitter itself can hold against -14V to +14V, but the TVS-diode has a limit around 6V. \n\n## Documentation\nThe documentation for the pin map can be found in the `Documentation` folder. Also all the article numbers for each component can be found in at the `DAC ADC PWM IO.sch` file in the `PCB` folder. Just double click on a PCB symbol and see the `Mouser Electronics` article number of the electrical component. \n\n## Calibration\nYes, it's possible to set the calibration to each input in this project. You need to have the ILI9341 touch LCD with SPI bus. Open the `STM32 PLC Pinouts.pdf` and see the connection for the LCD. You can also set the PWM frequency and analog input gain for the ADC and Differential ADC at 16-bit. \n\n## SAE J1939\nThe STM32 PLC has internal SAE J1939 protocol. Made from [Open-SAE-J1939](https://github.com/DanielMartensson/Open-SAE-J1939) repository.\n\n## OpenSourceLogger\nThis is a QT C++ software that you can connect to your STM32 PLC board via the USB and then you can send signals from OpenSourceLogger and recieve signals.\nOpenSourceLogger is a very easy to use logging and controlling software and it stores data at a SQL server. \n\n## GoobySoft\nThis is a ImGui C++ project that do the same as OpenSourceLogger, but it's much better and have more features. The reason why I moved away from QT C++ to ImGui C++ is because\nit's much easier to write a GUI application in C++ by using ImGui instead of QT. With QT, you are stuck with object oriented programming. Everything is a class. \nBut for ImGui, you can choose which type of lever you want to code, I prefer C-style C++ code with a small dose of object oriented programming (if needed). \n\nConsider that I will work on GoobySoft instead of OpenSourceLogger.\n\n## Program \nThe STM32 PLC has a lot of functions you can select by touching the LCD. \n\n * A:Show measurement and time\n * B:Set analog gain\n * C:Set PWM frequencies\n * D:Set analog input calibration\n * E:Set pulses per encoder revolution\n * F:Set date and time and alarm\n * G:Do a PGN request\n * H:Show ECU addresses\n * I:Commanded address\n * J:Show this ECU DM1 codes\n * K:Show other ECU DM1 codes\n * L:Show this ECU DM2 codes\n * M:Show other ECU DM2 codes\n * N:Show this ECU name\n * O:Show other ECU name\n * P:Show this ECU identifications\n * Q:Show other ECU identifications\n * R:SAE J1939 Auxiliary valve command\n * S:Analog in to PWM\n * T:Analog in to analog out\n * U:About STM32 PLC\n\n## How to build this STM32 PLC\n\n 1. Download this repository\n 2. Download & Install KiCad\n 3. Open the `PCB` folder and open the `.pro` file with KiCAD and greate a `gerber` file of your own choice\n 4. Go to your PCB manufacturer and give them the `gerber` file and let them produce the board for you\n 5. Order the eletrical components from `Mouser Electronics`\n 6. Once you have the eletrical components and your PCB board, it's time to solder them.\n 7. Once the PCB board is finished, then install STM32CubeIDE\n 8. Open the `Code` project and import the `.ioc` project file using STM32CubeIDE\n 9. Flash the board with the `C` code by using ST-Link V2 connection\n 10. Connect the ILI9431 touch LCD and then you are done!\n\n## Software used\n\n * KiCAD: 6.0.7\n * STM32CubeIDE 1.10.1\n\n## Status of the project\nIt's done. I don't plan to update this project. Everything is working and it will remain that way.\n",
    "readme_length": 4654
  },
  {
    "name": "ESP8266_WLAN_speaker",
    "full_name": "chunter1/ESP8266_WLAN_speaker",
    "description": "Using an ESP8266 to playback an audio stream over WiFi and 7-bit PWM.",
    "stars": 140,
    "forks": 35,
    "language": "C++",
    "url": "https://github.com/chunter1/ESP8266_WLAN_speaker",
    "topics": [],
    "created_at": "2017-05-15T06:55:58Z",
    "updated_at": "2025-10-01T21:37:38Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# ESP8266_WLAN_speaker\nUsing an ESP8266 to playback an audio stream over WiFi using 7-bit (8-bit dithered) PWM.\n\nTo playback a mp3 file, simply call e.g.:\n\navconv -i gong.mp3 -f s32be -acodec pcm_u8 -ac 1 -ar 33000 tcp://192.168.1.100:5522\n\nWhere the IP is the IP of your esp8266 and gong.mp3 the path to the music file.\n\nYoutube demo video:\nhttps://www.youtube.com/watch?v=Ai2RrCrgZ1c\n\nSchematics can be found in this thread in the FHEM forum (german language):\nhttps://forum.fhem.de/index.php?topic=71087.0\n\nAt the moment there are several options for the amplification:\n* NPN-Transistor like BC107 (see here: https://forum.fhem.de/index.php?action=dlattach;topic=71087.0;attach=78026)\n* [PAM8302A (click for circuit diagram)](Documentation/CircuitDiagramWithPAM8302A.png)\n* active computer-speaker\n\n## How to on Windows\n\navconv is a cmd tool available for linux and Windows. If your on Windows go to the Libav site and download the latest windows build:\nhttp://builds.libav.org/windows/nightly-gpl/ (avconv is included there).\n\nTo \"Install\" you just have to unpack that archive somewhere. Now Navigate your command prompt into that unpacked folder and further down into\n\"usr\" -> \"bin\". Now you can use the command from the section above to play your file:\n\navconv -i gong.mp3 -f s32be -acodec pcm_u8 -ac 1 -ar 33000 tcp://192.168.1.100:5522\n\nWhere the IP is the IP of your esp8266 and gong.mp3 the path to the music file.\n\n## Can't build sketch\n\nWhen you open the sketch Arduino might ask you to create the appropiate folder to use that sketch. You have to manually copy the StatusLedModes.h nexto wherever your sketch is located now, or it won't build\n",
    "readme_length": 1656
  },
  {
    "name": "AutoAnalogAudio",
    "full_name": "TMRh20/AutoAnalogAudio",
    "description": "Create a wide range of sampling and audio related applications with simple API for onboard DAC (or PWM), ADC, DMA & Timers on Arduino devices (AVR & SAM)",
    "stars": 137,
    "forks": 35,
    "language": "C++",
    "url": "https://github.com/TMRh20/AutoAnalogAudio",
    "topics": [],
    "created_at": "2016-11-05T14:53:25Z",
    "updated_at": "2025-10-21T09:30:23Z",
    "homepage": "http://tmrh20.github.io/AutoAnalogAudio",
    "license": "N/A",
    "readme": "\n**AutoAnalogAudio for Arduino by TMRh20 2016**\n\nAutomated analog reads and analog output (streaming) using Arduino DAC(or PWM), ADC and Timers\n\n**New** Now also supports AVR devices (Uno,Nano,Mega,etc) with pseudo DAC using PWM\n\nPlays and records analog (wav/pcm audio) data using onboard DAC and ADC.\nTimers are adjusted automatically based on the rate of data delivery, to ensure smooth playback.\nDMA transfers used to maximize efficiency.\n\nDocumentation: http://TMRh20.github.io/AutoAnalogAudio/\nAlso see: http://nRF24.github.io/RF24Audio/\n\n    AutoAnalogAudio streaming via DAC & ADC by TMRh20\n    Copyright (C) 2016  TMRh20 - tmrh20@gmail.com, github.com/TMRh20\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "readme_length": 1321
  },
  {
    "name": "PWM",
    "full_name": "tpurtell/PWM",
    "description": null,
    "stars": 124,
    "forks": 45,
    "language": "C#",
    "url": "https://github.com/tpurtell/PWM",
    "topics": [],
    "created_at": "2015-07-20T01:15:44Z",
    "updated_at": "2025-09-11T03:14:21Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# WARNING: USE AT YOUR OWN RISK\nAdjusting the value from the default may affect the lifetime of the lamp in your screen.  You might end up looking into the abyss.  You have been WARNED!\n\n# PWM\nThis is a utility to allow for adjust the PWM driver frequency delivered from Intel Graphics embedded GPU to the embedded LCD panel.  It shows you the existing value and lets you pick a new one.\n\nIt was inspired by this \nhttps://wiki.archlinux.org/index.php/Backlight#Backlight_PWM_modulation_frequency_.28Intel_i915_only.29\nand this\nhttp://devbraindom.blogspot.com/2013/03/eliminate-led-screen-flicker-with-intel.html\nBut its for Windows x64 instead.  \n\nI think the value it reads back and writes is the frequency in Hz.  The actual output was not checked via high speed camera or anything like that, but it seems to have the same behavior as sending those intel_reg_write commands from linux with the computed values.  The driver actually has two configuration DWORDs,  the default for me for these two values in decimal is 2 and 200.  This utility just keeps the first value as read back from the driver and changes only the second value.  It's possible the first value is duty cycle or mode related, or something completely different.  You could experiment using the source code if you aren't afraid to explode.\n\n# LICENSE\nPublic domain\n",
    "readme_length": 1334
  },
  {
    "name": "fan-control-rock5b",
    "full_name": "pymumu/fan-control-rock5b",
    "description": "PWM fan speed control for rock5B; ROCK5Bé£Žæ‰‡é€Ÿåº¦æŽ§åˆ¶è½¯ä»¶ã€‚",
    "stars": 118,
    "forks": 33,
    "language": "C",
    "url": "https://github.com/pymumu/fan-control-rock5b",
    "topics": [
      "fan",
      "pwm",
      "rock5b"
    ],
    "created_at": "2022-10-20T14:29:44Z",
    "updated_at": "2025-10-20T09:37:11Z",
    "homepage": "",
    "license": "MIT License",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "ESP32-ESP32S2-AnalogWrite",
    "full_name": "Dlloydev/ESP32-ESP32S2-AnalogWrite",
    "description": "ESP32 PWM, Servo, Easing and Tone. Smart GPIO pin management and advanced control features.",
    "stars": 118,
    "forks": 23,
    "language": "C++",
    "url": "https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite",
    "topics": [
      "analogwrite",
      "esp32",
      "esp32-c3",
      "esp32-s2",
      "esp32-s3",
      "ledc",
      "note",
      "pwm",
      "servo",
      "tone"
    ],
    "created_at": "2021-04-14T03:49:36Z",
    "updated_at": "2025-11-19T21:21:28Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# ESP32 PWM, Servo, Easing and Tone Library\n\n[![arduino-library-badge](https://www.ardu-badge.com/badge/ESP32%20ESP32S2%20AnalogWrite.svg?)](https://www.ardu-badge.com/ESP32%20ESP32S2%20AnalogWrite)  <a href=\"https://registry.platformio.org/libraries/dlloydev/ESP32 ESP32S2 AnalogWrite\"><img src=\"https://badges.registry.platformio.org/packages/dlloydev/library/ESP32 ESP32S2 AnalogWrite.svg\" alt=\"PlatformIO Registry\" /></a>\n\n<details>\n\n<summary><h3>Comparison to Servo Library for Arduino</h3></summary>\n\n\n- Both libraries use the same header filename: `Servo.h`\n- Methods in both libraries have identical names.\n- With the Servo Library for Arduino, each servo is instantiated, whereas only one instance is used with the ESP32 ESP32S2 AnalogWrite library to control up to 16 servos. Therefore, the `write()` method in the ESP32 ESP32S2 AnalogWrite library has a pin parameter to select the attached servo.\n\n#### Comparison Table\n\n- Superscript values represent the number of available overload functions .\n- With the ESP32 ESP32S2 AnalogWrite library, both `Servo.h` and `pwmWrite.h` have access to all methods. Choose **one** header only that best suits your application. Note that `Servo.h` uses a Servo class that translates method names to match the Servo Library for Arduino. Each header gives full access to the libraries features.\n\n| Library: | [Servo Library for Arduino](https://github.com/arduino-libraries/Servo) | [ESP32 ESP32S2 AnalogWrite](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite) | [ESP32 ESP32S2 AnalogWrite](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite) |\n| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Header   | Servo.h                                                      | Servo.h                                                      | pwmWrite.h                                                   |\n| Includes | ServoTimers.h                                                | pwmWrite.h                                                   | driver/ledc.h                                                |\n| Methods  | attach()<sup> 2</sup>                                        | attach()<sup> 10</sup>                                       | attachServo()<sup> 10</sup>                                  |\n|          | write()                                                      | write()<sup> 2</sup>                                         | writeServo()<sup> 2</sup>                                    |\n|          | writeMicroseconds()                                          | writeMicroseconds()                                          | n/a                                                          |\n|          | read()                                                       | read()                                                       | read()                                                       |\n|          | attached()                                                   | attached()                                                   | attached()                                                   |\n|          | detach()                                                     | detach()                                                     | detach()                                                     |\n|          | attachedPin()                                                | attachedPin()                                                | attachedPin()                                                |\n|          | readMicroseconds()                                           | readMicroseconds()                                           | readMicroseconds()                                           |\n|          |                                                              | attachPwm()<sup> 2</sup>                                     | attach()<sup> 2</sup>                                        |\n|          |                                                              | attachInvert()<sup> 2</sup>                                  | attachInvert()<sup> 2</sup>                                  |\n|          |                                                              | writePwm()<sup> 4</sup>                                      | write()<sup> 4</sup>                                         |\n|          |                                                              | detached()                                                   | detached()                                                   |\n|          |                                                              | firstFreeCh()                                                | firstFreeCh()                                                |\n|          |                                                              | pause()                                                      | pause()                                                      |\n|          |                                                              | resume()                                                     | resume()                                                     |\n|          |                                                              | printDebug()                                                 | printDebug()                                                 |\n|          |                                                              | setFrequency()                                               | setFrequency()                                               |\n|          |                                                              | setResolution()                                              | setResolution()                                              |\n|          |                                                              | tone()                                                       | tone()                                                       |\n|          |                                                              | note()                                                       | note()                                                       |\n\n</details>\n\n![image](https://github.com/Dlloydev/jtag2updi/assets/63488701/8217e847-b427-4b2b-9f39-f941578af63d)\n\n### Description\n\nThis library uses the ESP32 Arduino framework's [ledc](https://github.com/espressif/arduino-esp32/blob/master/cores/esp32/esp32-hal-ledc.c) functions and provides up to 16 channels for servos, pwm, leds, buzzers etc. Includes smart GPIO pin management where any pin will not be automatically attached if previously accessed by other code. Includes advanced control methods like timer pause/resume, phase delay using hpoint, inverted pwm and tunable servo easing.\n\nServo Easing is fully integrated into the servo write and attach functions. Only 2 parameters give complete control over the speed and the easing characteristic of the servo. The method used for easing is a [Normalized Tunable Sigmoid](https://www.desmos.com/calculator/ejkcwglzd1) ([reference](https://dhemery.github.io/DHE-Modules/technical/sigmoid/)).\n\n\n\n#### Arduino core for the ESP32, ESP32-S2, ESP32-S3 and ESP32-C3\n\nRecommend using the [latest release](https://github.com/espressif/arduino-esp32), however this library works with release 2.0.7 or newer.\n\n<details>\n\n<summary><h3>Servo Easing</h3></summary>\n\nJust 2 easing parameters (speed and easing constant) for unlimited control ...\n\n```c++\nmyservo.write(servoPin1, pos1, speed1, 0.0);  // move 90 deg at 70 deg/s, linear\nmyservo.write(servoPin2, pos2, speed2, 0.6);  // mpve 180 deg at 140 deg/s, avg sigmoid\nmyservo.write(servoPin3, pos3, speed3, 0.8);  // move 90 deg at 180 deg/s, steep sigmoid\n```\n\n#### ![ServoEasing](https://user-images.githubusercontent.com/63488701/227943891-87cb7555-fe56-4064-a83a-38b99ad58e1d.gif)\n\n#### Speed Control:\n\nThe maximum speed in degrees/sec is derived from the servo's datasheet. For this [SG90 Micro Servo](https://robojax.com/learn/arduino/robojax-servo-sg90_datasheet.pdf) we have  Operating speed: 0.1 s/60 degree. In this case, the maximum value for the speed parameter is 600 deg/sec. When a new servo position value is set, the operating time in milliseconds = degrees to move / speed * 1000.\n\n#### Easing Control:\n\nThe easing constant ke controls how the servo moves to the set position by varying the speed. Its effect from linear (ke = 0.0) to maximum steep curve (ke = 0.99).\n\n#### Position Feedback: \n\nThe calculated position of the servo is the returned value \"ye\" of the writeServo function. The easing position ye is normalized (0.0-1.0) but can slightly over/undershoot this range. The servo has reached its programmed position when ye = 1.0 if the new setting is larger than previous and also when ye = 0.0 if the new position setting is smaller than previous.\n\n#### servoWrite:\n\nAfter a new servo position is programmed, repeatedly call the servoWrite function with the same parameters until the servo completes its motion (returned value ye = 1.0 or 0.0). The servo responds according to ke and speed. Servo position is incremented after each call. \n\n</details>\n\n<details>\n<summary><h3>Examples</h3></summary>\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/351231798778266200)  [Note Explorer â™© â™ª â™« â™¬](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Note_Explorer/Note_Explorer.ino)   Plays all 96 ledc notes that are available, non blocking\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/351175246893548120)  [Note_Player](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Note_Player/Note_Player.ino)   Playing Notes based on sliding pot position, 4th octave, non blocking\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/349336125753524820)  [Pwm_3phase_40kHz](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Pwm_3phase_40kHz/Pwm_3phase_40kHz.ino)   ESP32 3 Phase PWM Outputs (40kHz, 10-bit)\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/334722465700774482)  [Pwm_ESP32_3phase_10kHz](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Pwm_ESP32_3phase_10kHz/Pwm_ESP32_3phase_10kHz.ino)   ESP32 3 Phase PWM Outputs (10kHz, 10-bit)\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/334856585002091092)  [Pwm_ESP32_C3_3phase_10kHz](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Pwm_ESP32_3phase_10kHz/Pwm_ESP32_3phase_10kHz.ino)   ESP32 C3 3 Phase PWM Outputs (10kHz, 10-bit)\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/334765722024542804)  [Pwm_ESP32_S2_3phase_10kHz](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Pwm_ESP32_S2_3phase_10kHz/Pwm_ESP32_S2_3phase_10kHz.ino)   ESP32 S2 3 Phase PWM Outputs (10kHz, 10-bit)\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/349978851105833554)  [Pwm_Fade_Servo](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Pwm_Fade_Servo/Pwm_Fade_Servo.ino)   ESP32 fading 14 pairs of LEDs and controlling 2 servo motors\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/349232255258853970)  [Pwm_Fade16](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Pwm_Fade16/Pwm_Fade16.ino)   ESP32 fading 16 pairs of LEDs\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/349322326995632722)  [Pwm_Sync2_300kHz](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Pwm_Sync2_300kHz/Pwm_Sync2_300kHz.ino)   2 synchronized PWM outputs using the same timer (channel pair)\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/364791981216008193)  [Servo_Easing_Interrupt](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Servo_Easing_Interrupt/Servo_Easing_Interrupt.ino)   Servo Easing with position feedback and Interrupt control\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/361237697368753153)  [Servo_Easing_Position](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Servo_Easing_Position/Servo_Easing_Position.ino)   3 servos with easing and position feedback control\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/350033311963284051)  [Servo Knob](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Servo_Knob/Servo_Knob.ino)   Controls servo position by using a potentiometer \n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/355852275661848577)  [Servo_Knob_Six](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Servo_Knob_Six/Servo_Knob_Six.ino)   Potentiometer control of 6 servos on an ESP32-C3\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/350037178957431378)  [Servo Sweep](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Servo_Sweep/Servo_Sweep.ino)   Sweep a servo motor from 0-180 degrees and back\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/351967394028061269)  [Servo_Sweep_Inverted](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Servo_Sweep_Inverted/Servo_Sweep_Inverted.ino)   Using inverted PWM mode to sweep a servo motor\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/351978833396630095)  [Servo_Sweep_Speed](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Servo_Sweep_Speed/Servo_Sweep_Speed.ino)  Independent speed control of 2 servos\n\n[![Wokwi_badge](https://user-images.githubusercontent.com/63488701/212449119-a8510897-c860-4545-8c1a-794169547ba1.svg)](https://wokwi.com/projects/352178590336932865)  [Tone_Player](https://github.com/Dlloydev/ESP32-ESP32S2-AnalogWrite/blob/main/examples/Tone_Player/Tone_Player.ino)   Playing Tones based on sliding pot position\n\n</details>\n\n<details>\n\n<summary><h3>PWM Channel Configuration</h3></summary>\n\n| Board       | PWM Pins                  | PWM, Duty and Phase Channels | Frequency and Resolution Channels |\n| ----------- | ------------------------- | ---------------------------- | --------------------------------- |\n| ESP32       | 0-19, 21-23, 25-27, 32-39 | 16                           | 8                                 |\n| ESP32â€‘S2/S3 | 0-21, 26, 33-45           | 8                            | 4                                 |\n| ESP32â€‘C3    | 0- 10, 18-21              | 6                            | 3                                 |\n\nFrequency and resolution values are shared by each channel pair thats on the same timer. When any channel gets configured, the next lower or higher channel gets updated with the same frequency and resolution values as appropriate.\n\n| PWM Channel | Speed Mode | Timer | Frequency | Resolution | Duty | Phase |\n| :---------: | ---------- | ----- | --------- | ---------- | ---- | ----- |\n|      0      | 0          | 0     | 1         | 1          | 1    | 1     |\n|      1      | 0          | 0     | 1         | 1          | 2    | 2     |\n|      2      | 0          | 1     | 2         | 2          | 3    | 3     |\n|      3      | 0          | 1     | 2         | 2          | 4    | 4     |\n|      4      | 0          | 2     | 3         | 3          | 5    | 5     |\n|      5      | 0          | 2     | 3         | 3          | 6    | 6     |\n|      6      | 0          | 3     | 4         | 4          | 7    | 7     |\n|      7      | 0          | 3     | 4         | 4          | 8    | 8     |\n|      8      | 1          | 0     | 5         | 5          | 9    | 9     |\n|      9      | 1          | 0     | 5         | 5          | 10   | 10    |\n|     10      | 1          | 1     | 6         | 6          | 11   | 11    |\n|     11      | 1          | 1     | 6         | 6          | 12   | 12    |\n|     12      | 1          | 2     | 7         | 7          | 13   | 13    |\n|     13      | 1          | 2     | 7         | 7          | 14   | 14    |\n|     14      | 1          | 3     | 8         | 8          | 15   | 15    |\n|     15      | 1          | 3     | 8         | 8          | 16   | 16    |\n\n</details>\n\n## Reference (Servo.h)\n\n### Include and Instantiate\n\n```c++\n#include <Servo.h>\nServo myservo = Servo();\n```\n\n<details>\n\n<summary><h3>write()</h3></summary>\n\n##### Description:\n\nThis function accepts a value of type *float* that's processed to an unsigned duty value that takes full advantage of the servo channel's set resolution. If using a standard positional servo, this will set the angle of the shaft in degrees with range 0-180.  If using a continuous rotation servo, this will set the speed where the limits 0 and 180 are full speed in each direction and where the mid range (90) is no movement.\n\n| Entered Value *(float)*  | Coerced Value *(float)*  | Units        |\n| :----------------------- | :----------------------- | :----------- |\n| < 0                      | 0                        | degrees      |\n| 0-180                    | 0-180                    | degrees      |\n| > 180 AND < 500          | 180                      | degrees      |\n| â‰¥ 500 AND < servoMinUs   | servoMinUs               | microseconds |\n| servoMinUs to servoMaxUs | servoMinUs to servoMaxUs | microseconds |\n| > servoMaxUs             | servoMaxUs               | microseconds |\n\n**Timer Width (resolution)**\n\nWhen using this function, the timer width (resolution) will be14 bit if the target architecture is ESP32C3. For ESP32/S2/S3, the maximum bit width will be 20, which allows setting any width from14 to 20.\n\n**Servo Frequency**\n\nThe allowed range for servo frequency is 40 to 900 Hz. Any saved or entered frequency that's out of this range, will be set and saved as 50Hz.\n\n**Channel Pairing**\n\nThe frequency and resolution values are shared by each channel pair. When any channel gets configured, the next lower or higher channel on the same timer gets updated with the same frequency and resolution values as appropriate.\n\n**Attaching to free Channel**\n\nThis process is automatic - the servo pin will be attached to the next free channel. If you need to assign the servo pin(s) to specific channels or to set the minimum or maximum microsecond values, then call the `attach()`method first.\n\n##### Syntax\n\n```c++\nmyservo.write(pin, value)\nmyservo.write(pin, value, speed, ke)\n```\n\n##### Parameters\n\n- **pin**  The pin number which (if necessary) will be attached to the next free channel *(int)*\n- **value**  This value is converted to the pwm duty. See above table for range and units *(double)\n- **speed**  This value has units degrees/second (double). For example, if `speed` is set to 100 deg/s and the servo position value is changed from 0 to 180 deg, then the servo will take 1.8 sec (1800 ms) to complete its travel. Its motion (response) will be determined by `ke`,\n- **ke**  Servo easing constant for a [Normalized Tunable Sigmoid](https://www.desmos.com/calculator/ejkcwglzd1). A `ke` value of 0.0 represents a linear response. As you increase `ke`, this increases the steepness of a sigmoid response. When `ke` is 1.0, normal \"instantaneous\" servo response is enabled and the speed parameter is ignored.\n\n##### Returns\n\n- If the servo easing constant `ke` is 1.0 (default) then the pwm duty value *(uint32_t)* is returned.\n- If  `ke` is less than 1.0, then a normalized double value (0.0 to 1.0) is returned. This represents the programmed servo position from start to stop as it moves over time. When the returned value reaches 0.5, this represents both 50% travel and 50% time duration, no matter what easing constant is set.\n\n</details>\n\n<details>\n\n<summary><h3>writeMicroseconds()</h3></summary>\n\n##### Description\n\nThis function calls the write() function above.\n\n**Syntax**\n\n```c++\nmyservo.writeMicroseconds()\n```\n\n</details>\n\n<details>\n\n<summary><h3>read()</h3></summary>\n\n##### Description\n\nRead the current angle of the servo in degrees. The returned value is *float* type which provides improved resolution and takes advantage of the high resolution offered by the timer.\n\n**Syntax**\n\n```c++\nmyservo.read(pin)\n```\n\n##### Parameters\n\n- **pin**  The pin number (int)\n\n##### Returns\n\n- The angle of the servo, from 0 to 180 degrees *(float)*\n\n</details>\n\n<details>\n\n<summary><h3>readMicroseconds()</h3></summary>\n\n##### Description\n\nReads the timer channel's duty value in microseconds. The minimum limit is 544 Î¼s representing 0 degrees shaft rotation and the  maximum limit is 2400 Î¼s representing 180 degrees shaft rotation. The returned value is *float* type which provides improved resolution and takes advantage of the high resolution offered by the timer.\n\n**Syntax**\n\n```c++\nmyservo.readMicroseconds(pin)\n```\n\n##### Parameters\n\n- **pin**  The pin number (int)\n\n##### Returns\n\n- The channel's duty value converted to microseconds *(float)*\n\n</details>\n\n<details>\n\n<summary><h3>attach()</h3></summary>\n\n##### Description\n\nThis function allows auto-attaching a pin to the first available channel if only the pin is specified. To have the pin assigned to a specific channel, use both the pin and channel (ch) parameters. Additionally, there are parameters available for setting the servo timer values for minimum and maximum microseconds. \n\n**Syntax**\n\n```c++\nmyservo.attach(pin)                                       // auto attach to 1st free channel\nmyservo.attach(pin, invert)                               // as above with invert\nmyservo.attach(pin, ch)                                   // attach to specified channel\nmyservo.attach(pin, ch, invert)                           // as above with invert\nmyservo.attach(pin, minUs, maxUs)                         // auto attach to free ch, servo limits\nmyservo.attach(pin, ch, minUs, maxUs)                     // attach to specified ch, servo limits\nmyservo.attach(pin, ch, minUs, maxUs, invert)             // as above with invert\nmyservo.attach(pin, minUs, maxUs, speed, ke)              // attach to free ch, speed, easing const\nmyservo.attach(pin, ch, minUs, maxUs, speed, ke)          // as above but attaches to specified ch\nmyservo.attach(pin, ch, minUs, maxUs, speed, ke, invert)  // as above with invert\n```\n\n##### Parameters\n\n- **pin**  The pin number *(int)*\n\n- **ch**  This optional parameter is used to attach the pin to a specific channel *(int)*)\n\n- **minUs**  Minimum timer width in microseconds *(int)*\n\n- **maxUs**  Maximum timer width in microseconds *(int)*\n\n- **speed**  This servo easing parameter has units degrees/second (double). For example, if `speed` is set to 100 deg/s and the servo position value is changed from 0 to 180 deg, then the servo will take 1.8 sec (1800 ms) to complete its travel. Its motion (response) will be determined by `ke`,\n\n- **ke**  Servo easing constant for a [Normalized Tunable Sigmoid](https://www.desmos.com/calculator/ejkcwglzd1). A `ke` value of 0.0 represents a linear response. As you increase `ke`, this increases the steepness of a sigmoid response. When `ke` is 1.0, normal \"instantaneous\" servo response is enabled and the speed parameter is ignored.\n\n- **invert**  Inverts the PWM output. Allows using a simpler driver for higher voltage servo control. Only one NPN transistor or N-Channel MOSFET needed. No additional latency added as found with software inversion because the inverted pulse remains at the start of the refresh period rather than being flipped to the end of the refresh period  *(bool)*.\n\n  [Servo_Sweep_Inverted](https://wokwi.com/projects/351967394028061269)\n\n  ![image](https://user-images.githubusercontent.com/63488701/236273265-0cdf2dca-78b8-4afd-8924-1f263c7cde80.png)\n\n##### Returns\n\n- If not a valid pin, 254 *(uint8_t)*\n- free channels exist, 253 *(uint8_t)*\n- If attached, the channel number (0-15) *(uint8_t)*\n- If not attached, 255 *(uint8_t)*\n\n</details>\n\n<details>\n\n<summary><h3>attachPwm()</h3></summary>\n\n##### Description\n\nThis function allows auto-attaching a pin to the first available channel if only the pin is specified. To have the pin assigned to a specific channel, use both the pin and channel (ch) parameters. \n\n**Syntax**\n\n```c++\nmyservo.attach(pin)       // auto attach to 1st free channel\nmyservo.attach(pin, ch)   // attach to specified channel\n```\n\n##### Parameters\n\n- **pin**  The pin number *(int)*\n\n##### Returns\n\n- If not a valid pin, 254 *(uint8_t)*\n- free channels exist, 253 *(uint8_t)*\n- If attached, the channel number (0-15) *(uint8_t)*\n- If not attached, 255 *(uint8_t)*\n\n</details>\n\n<details>\n\n<summary><h3>attached()</h3></summary>\n\n##### Description\n\nThis function checks the pin status and if attached, returns the channel number. \n\n**Syntax**\n\n```c++\nmyservo.attached(pin)\n```\n\n##### Parameters\n\n- **pin**  The pin number *(int)*\n\n##### Returns\n\n- If not a valid pin, 254 *(uint8_t)*\n- free channels exist, 253 *(uint8_t)*\n- If attached, the channel number (0-15) *(uint8_t)*\n- If not attached, 255 *(uint8_t)*\n\n</details>\n\n<details>\n\n<summary><h3>attachInvert()</h3></summary>\n\n##### Description\n\nThis function allows auto-attaching a pin to the first available channel if only the pin is specified. To have the pin assigned to a specific channel, use both the pin and channel (ch) parameters. The pwm output will be inverted. The duty value represents the low period.\n\n**Syntax**\n\n```c++\nmyservo.attachInvert(pin);      // attach pin to next free channel with inverted pwm\nmyservo.attachInvert(pin, ch);  // attach to specified ch with inverted pwm\n```\n\n##### Parameters\n\n- **pin**  The pin number *(int)*\n- **ch**  This optional parameter is used to attach the pin to a specific channel *(int)*\n\n##### Returns\n\n- If not a valid pin, 254 *(uint8_t)*\n- free channels exist, 253 *(uint8_t)*\n- If attached, the channel number (0-15) *(uint8_t)*\n- If not attached, 255 *(uint8_t)*\n\n</details>\n\n<details>\n\n\n<summary><h3>attachedPin()</h3></summary>\n\n##### Description\n\nThis function returns the pin that's attached to the specified channel.\n\n**Syntax**\n\n```c++\nmyservo.attachedPin(ch)\n```\n\n##### Parameters\n\n- **pin**  The pin number *(int)*\n\n##### Returns\n\n- If attached, the pin number *(uint8_t)*\n- If the channel is free, 255 *(uint8_t)*\n\n</details>\n\n\n<details>\n\n<summary><h3>writePwm()</h3></summary>\n\n##### Description\n\nThis function writes the duty and optionally the frequency, resolution and phase parameters. If necessary, the pin will be automatically attached to the first available pwm channel. To avoid conflicts with other code, the pin will not be attached if previously accessed.\n\n##### Syntax\n\n```c++\nmyservo.writePwm(pin, duty)\nmyservo.writePwm(pin, duty, frequency)\nmyservo.writePwm(pin, duty, frequency, resolution)\nmyservo.writePwm(pin, duty, frequency, resolution, phase)\n```\n\n##### Parameters\n\n- **pin**  The pin number which (if necessary) will be attached to the next free channel *(int)*\n- **duty**  This sets the pwm duty. The range is 0 to (2**resolution) - 1 *(uint32_t)*\n- **frequency**  The pwm timer frequency (Hz). The frequency and resolution limits are interdependent *(uint32_t)*. For more details, see [Supported Range of Frequency and Duty Resolutions](https://docs.espressif.com/projects/esp-idf/en/latest/esp32/api-reference/peripherals/ledc.html#ledc-api-supported-range-frequency-duty-resolution).\n- **resolution**  The bit resolution of the pwm duty *(uint8_t)*\n- **phase**  This is also referred to as the **hpoint** value, which is the timer/counter value that the pwm output turns on. The useable range is the same as for the duty parameter. This can be used to phase shift the output or for synchronization. When the phase parameter is used, the pwm output will initiate in a paused state to allow synchronization *(uint32_t)*\n\n##### Returns\n\nThe set frequency *(float)*\n\n</details>\n\n<details>\n\n<summary><h3>detachPin()</h3></summary>\n\n##### Description\n\nThis function removes control of the pin from the specified PWM channel.  Also, the channel defaults are applied.\n\n**Syntax**\n\n```c++\nmyservo.detachPin(pin)\n```\n\n##### Parameters\n\n- **pin**  The pin number *(int)*\n\n##### Returns\n\n- nothing\n\n</details>\n\n<details>\n\n<summary><h3>pause()</h3></summary>\n\n##### Description\n\nThis function is used internally by the write() function when the phase parameter is used to allow synchronization of multiple pwm signals. \n\nIf this function is manually called, any channel(s) that get configured will have their PWM output paused.  Then calling `resume()` will start all newly configured channels at the same time. Note that this approach limits the maximum pwm frequency to about 10kHz or some pulses or glitches might occur during channel configuration.\n\n**Syntax**\n\n```c++\nmyservo.pause()\n```\n\n##### Parameters\n\n- none.\n\n##### Returns\n\n- nothing\n\n</details>\n\n<details>\n\n<summary><h3>resume()</h3></summary>\n\n##### Description\n\nThis function is used to start the pwm outputs of all channels to synchronize (align) the signals. Note that there will be a consistent delay between the startup of each timer which can be corrected by using the `write()` function's phase parameter.\n\n**Syntax**\n\n```c++\nmyservo.resume()\n```\n\n##### Parameters\n\n- none.\n\n##### Returns\n\n- nothing\n\n</details>\n\n<details>\n\n<summary><h3>setFrequency()</h3></summary>\n\n##### Description\n\nSets the PWM frequency on any PWM pin.\n\n**Syntax**\n\n```c++\nmyservo.setFrequency(pin, frequency)\n```\n\n##### Parameters\n\n- **pin**  The pin number  *(int)* If the pin is detached (free) and there's a free channel available, the pin will be attached to the first free channel that's found *(int)*\n- **frequency**  The frequency in Hz. The default is 1000 Hz *(uint32_t)*\n\n##### Returns\n\n- The frequency set by the timer hardware *(float)*\n\n</details>\n\n<details>\n\n<summary><h3>setResolution()</h3></summary>\n\n##### Description\n\nSets the PWM resolution for any PWM pin.\n\n**Syntax**\n\n```c++\nmyservo.setResolution(pin, resolution)\n```\n\n##### Parameters\n\n- **pin**  The pin number  *(int)* If the pin is detached (free) and there's a free channel available, the pin will be attached to the first free channel that's found *(int)*\n- **resolution**  The PWM resolution can be set from 1-bit to 16-bit, default is 8-bit *(uint8_t)*\n\n##### Returns\n\n- The set resolution reported by the pin channel *(uint8_t)*\n\n</details>\n\n<details>\n\n<summary><h3>tone()</h3></summary>\n\n##### Description:\n\nThis function generates a square wave of the specified frequency (and 50% duty  cycle and 8-bit resolution) on a pin. There will be no output (no tone) if the duration isn't specified or equals 0. The duration in milliseconds has range 0-65535 where 0 is off and 65535 is always on. The last parameter (interval)  specifies the pause time before the next call to tone becomes ready. The pin can be connected to a piezo buzzer or other speaker to play tones.\n\n**Channel Pairing**\n\nThe frequency and resolution values are shared by each channel pair. When the tone pin is attached, the next lower or higher channel on the same timer gets updated with the same frequency and resolution values as appropriate.\n\n**Attaching to free Channel**\n\nThis process is automatic - the tone pin will be attached to the next free channel. If you need to assign the tone pin to a specific channel, then call the `attach()`method first.\n\n##### Syntax\n\n```c++\nmyservo.tone(pin, frequency, duration)\nmyservo.tone(pin, frequency, duration, interval)\n```\n\n##### Parameters\n\n- **pin**  The pin number which (if necessary) will be attached to the next free channel *(int)*\n- **frequency**  The tone frequency (Hz) with range 1-65535 *(uint16_t)*.\n- **duration**  The duration in milliseconds with range 0-65535 *(uint16_t)*, where 0 is off (default) and 65535 is always on.\n- **interval**  This optional parameter specifies the pause time in milliseconds before the next call to tone becomes ready. *(uint16_t)*, range 0-65535, default = 0.\n\n##### Returns\n\n- nothing\n\n</details>\n\n<details>\n\n<summary><h3>note()</h3></summary>\n\n##### Description:\n\nThis function generates a square wave of the specified frequency (and 50% duty  cycle and 8-bit resolution) on a pin. There will be no output (no tone) if the duration isn't specified or equals 0. The duration in milliseconds has range 0-65535 where 0 is off and 65535 is always on. The last parameter (interval)  specifies the pause time before the next call to note becomes ready. The pin can be connected to a piezo buzzer or other speaker to play notes.\n\n**Channel Pairing**\n\nThe frequency and resolution values are shared by each channel pair. When the note pin is attached, the next lower or higher channel on the same timer gets updated with the same frequency and resolution values as appropriate.\n\n**Attaching to free Channel**\n\nThis process is automatic - the note pin will be attached to the next free channel. If you need to assign the tone pin to a specific channel, then call the `attach()`method first.\n\n##### Syntax\n\n```c++\npwm.note(pin, note, octave, duration, interval)\n```\n\n##### Parameters\n\n- **pin**  The pin number which (if necessary) will be attached to the next free channel *(int)*\n- **note**  The type is defined in [esp32-hal-ledc.h](https://github.com/espressif/arduino-esp32/blob/master/cores/esp32/esp32-hal-ledc.h) *(note_t)*.\n- **octave**  There are 8 octaves available, 1 to 8 *(uint8_t)* \n- **duration**  The duration in milliseconds with range 0-65535 *(uint16_t)*, where 0 is off (default) and 65535 is always on.\n- **interval**  This parameter specifies the pause time in milliseconds before the next call to tone becomes ready. *(uint16_t)*, range 0-65535, default = 0.\n\n##### Returns\n\n- nothing\n\n</details>\n\n<details>\n\n<summary><h3>printDebug()</h3></summary>\n\n##### Description\n\nThis function prints the available PWM pins to choose from and a formatted output showing the PWM pins that are in use (attached) and the channels that are unassigned (255).\n\n**Syntax**\n\n```c++\nmyservo.printDebug()\n```\n\n##### Parameters (optional)\n\n- none\n\n##### Returns\n\n- serial report on serial monitor\n\n![image](https://user-images.githubusercontent.com/63488701/229374511-de75b97d-f91f-44d0-b103-0ca858d16727.png)\n\n</details>\n\n```\nThis Library is licensed under the MIT License\n```\n",
    "readme_length": 35447
  },
  {
    "name": "ChIP-seq-analysis",
    "full_name": "crazyhottommy/ChIP-seq-analysis",
    "description": "ChIP-seq analysis notes from Ming Tang",
    "stars": 832,
    "forks": 309,
    "language": "Python",
    "url": "https://github.com/crazyhottommy/ChIP-seq-analysis",
    "topics": [
      "chip-seq",
      "histone-modifications",
      "transcription-factor-binding"
    ],
    "created_at": "2015-06-12T21:42:10Z",
    "updated_at": "2025-11-27T09:39:18Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# ChIP-seq-analysis\n\n### Snakemake pipelines\n\nI developed a Snakemake based ChIP-seq pipeline: [pyflow-ChIPseq](https://github.com/crazyhottommy/pyflow-ChIPseq).\nand ATACseq pipeline: [pyflow-ATACseq](https://github.com/crazyhottommy/pyflow-ATACseq)\n\n### Resources for ChIP-seq \n1. [ENCODE: Encyclopedia of DNA Elements](https://www.encodeproject.org/)  [ENCODExplorer](https://www.bioconductor.org/packages/release/bioc/html/ENCODExplorer.html): A compilation of metadata from ENCODE. A bioc package to access the meta data of ENCODE and download the raw files.\n2. [ENCODE Factorbook](https://www.encodeproject.org/)  \n3. [ChromNet ChIP-seq interactions](http://chromnet.cs.washington.edu/#/?search=&threshold=0.5)  \n    paper: [Learning the human chromatin network using all ENCODE ChIP-seq datasets](http://biorxiv.org/content/early/2015/08/04/023911)  \n4. [The International Human Epigenome Consortium (IHEC) epigenome data portal](http://epigenomesportal.ca/ihec/index.html?as=1)\n5. [GEO](http://www.ncbi.nlm.nih.gov/gds/?term=). Sequences are in .sra format, need to use sratools to dump into fastq.\n6. [European Nucleotide Archive](http://www.ebi.ac.uk/ena). Sequences are available in fastq format.\n7. [Data bases and software from Sheirly Liu's lab at Harvard](http://liulab.dfci.harvard.edu/WEBSITE/software.htm)\n8. [Blueprint epigenome](http://dcc.blueprint-epigenome.eu/#/home)\n9. [A collection of tools and papers for nucelosome positioning and TF ChIP-seq](http://generegulation.info/)\n10. [review paper:Deciphering ENCODE](http://www.cell.com/trends/genetics/fulltext/S0168-9525(16)00017-2)\n11. [EpiFactors](http://epifactors.autosome.ru/) is a database for epigenetic factors, corresponding genes and products.\n12. [biostar handbook](https://read.biostarhandbook.com/). My [ChIP-seq chapter](https://read.biostarhandbook.com/chip-seq/chip-seq-analysis.html) is out April 2017!\n13. [ReMap 2018](http://tagc.univ-mrs.fr/remap/) An integrative ChIP-seq analysis of regulatory regions. The ReMap atlas consits of 80 million peaks from 485 transcription factors (TFs), transcription coactivators (TCAs) and chromatin-remodeling factors (CRFs) from public data sets. The atlas is available to browse or download either for a given TF or cell line, or for the entire dataset.\n14. GTRDGene Transcription Regulation Database https://gtrd.biouml.org/#!\n\n### Papers on ChIP-seq\n1. [ChIP-seq guidelines and practices of the ENCODE and modENCODE consortia](http://www.ncbi.nlm.nih.gov/pubmed/22955991) \n2. [Practical Guidelines for the Comprehensive Analysis of ChIP-seq Data](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003326)    \n3. [Systematic evaluation of factors influencing ChIP-seq fidelity](http://www.nature.com/nmeth/journal/v9/n6/full/nmeth.1985.html)\n4. [ChIPâ€“seq: advantages and challenges of a maturing technology](http://www.nature.com/nrg/journal/v10/n10/abs/nrg2641.html)\n5. [ChIPâ€“seq and beyond: new and improved methodologies to detect and characterize proteinâ€“DNA interactions](http://www.nature.com/nrg/journal/v13/n12/abs/nrg3306.html) \n6. [Beyond library size: a field guide to NGS normalization](http://biorxiv.org/content/early/2014/06/19/006403)\n7. [ENCODE paper portol](http://www.nature.com/encode/threads)  \n8. [Enhancer discovery and characterization](http://www.nature.com/encode/threads/enhancer-discovery-and-characterization) \n9. 2016 review [Recent advances in ChIP-seq analysis: from quality management to whole-genome annotation](http://bib.oxfordjournals.org/content/early/2016/03/15/bib.bbw023.full)\n10. [bioinformatics paper:Features that define the best ChIP-seq peak calling algorithms](http://bib.oxfordjournals.org/content/early/2016/05/10/bib.bbw035.short?rss=1) compares different peak callers for TFs and histones.\n11. [Systematic comparison of monoclonal versus polyclonal antibodies for mapping histone modifications by ChIP-seq](http://biorxiv.org/content/early/2016/05/19/054387) The binding patterns for H3K27ac differed substantially between polyclonal and monoclonal antibodies. However, this was most likely due to the distinct immunogen used rather than the clonality of the antibody. Altogether, we found that monoclonal antibodies as a class perform as well as polyclonal antibodies. Accordingly, we recommend the use of monoclonal antibodies in ChIP-seq experiments.\n12. A nice small review: [Unraveling the 3D genome: genomics tools for multiscale exploration](http://www.cell.com/trends/genetics/pdf/S0168-9525(15)00063-3.pdf)\n13. Three very interesting papers, [Developmental biology: Panoramic views of the early epigenome](http://www.nature.com/nature/journal/v537/n7621/full/nature19468.html)\n14. [ChIP off the old block: Beyond chromatin immunoprecipitation](https://www.sciencemag.org/features/2018/12/chip-old-block-beyond-chromatin-immunoprecipitation). A nice review of the past and future of ChIPseq.\n15. [Histone Modifications: Insights into Their Influence on Gene Expression](https://www.sciencedirect.com/science/article/pii/S0092867418310481)\n    **Protocols**  \n1. [A computational pipeline for comparative ChIP-seq analyses](http://www.ncbi.nlm.nih.gov/pubmed/22179591)    \n2. [Identifying ChIP-seq enrichment using MACS](http://www.nature.com/nprot/journal/v7/n9/full/nprot.2012.101.html)  \n3. [Spatial clustering for identification of ChIP-enriched regions (SICER) to map regions of histone methylation patterns in embryonic stem cells](http://www.ncbi.nlm.nih.gov/pubmed/24743992)\n4. [ENCODE tutorials](http://www.genome.gov/27553900) \n5. [A User's Guide to the Encyclopedia of DNA Elements (ENCODE)](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001046)  \n6. [A toolbox of immunoprecipitation-grade monoclonal antibodies to human transcription factors](https://www.nature.com/articles/nmeth.4632) The data portal https://proteincapture.org/\n\n### Quality Control\nData downloaded from GEO usually are raw fastq files. One needs to do quality control (QC) on them.\n\n* [fastqc](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/)  \n* [multiqc](http://multiqc.info/) Aggregate results from bioinformatics analyses across many samples into a single report. Could be very useful to summarize the QC report. \n\n\n### Peak calling  \n\nBe careful with the peaks you get:  \n[Active promoters give rise to false positive â€˜Phantom Peaksâ€™ in ChIP-seq experiments](http://nar.oxfordjournals.org/content/early/2015/06/27/nar.gkv637.long)    \n\nIt is good to have controls for your ChIP-seq experiments. A DNA input control (no antibody is applied) is prefered.\nThe IgG control is also fine, but because so little DNA is there, you might get many duplicated reads due to PCR artifact.\n\n**For cancer cells, an input control can be used to correct for copy-number bias.**\n\n* [tools used by IHEC consortium](http://ihec-epigenomes.org/research/tools/)\n\n[A quote from Tao Liu:](https://groups.google.com/forum/#!searchin/macs-announcement/h3k27ac/macs-announcement/9_LB5EsjS_Y/nwgsPN8lR-kJ) who develped MACS1/2\n\n>I remember in a PloS One paper last year by Elizabeth G. Wilbanks et al.,  authors pointed out the best way to sort results in MACS is by -10*log10(pvalue) then fold enrichment. I agree with them. You don't have to worry about FDR too much if your input data are far more than ChIP data. MACS1.4 calculates FDR by swapping samples, so if your input signal has some strong bias somewhere in the genome, your FDR result would be bad. Bad FDR may mean something but it's just secondary.\n\n1. The most popular peak caller by Tao Liu: [MACS2](https://github.com/taoliu/MACS/). Now `--broad` flag supports broad peaks calling as well.\n\n2. [TF ChIP-seq peak calling using the Irreproducibility Discovery Rate (IDR) framework](https://github.com/nboley/idr) and many [Software Tools Used to Create the ENCODE Resource](https://genome.ucsc.edu/ENCODE/encodeTools.html)    \n3. [SICER](http://home.gwu.edu/~wpeng/Software.htm) for broad histone modification ChIP-seq\n4. [HOMER](http://homer.salk.edu/homer/ngs/peaks.html) can also used to call Transcription factor ChIP-seq peaks and histone \n    modification ChIP-seq peaks.\n5. [MUSIC](https://github.com/gersteinlab/MUSIC)\n6. [permseq](https://github.com/keleslab/permseq)  R package for mapping protein-DNA interactions in highly repetitive regions of the genomes with prior-enhanced read mapping. [Paper](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4618727/pdf/pcbi.1004491.pdf) on PLos Comp.\n7. [Ritornello](http://www.biorxiv.org/content/early/2015/12/11/034090): High fidelity control-free chip-seq peak calling. No input is required!\n8. Tumor samples are heterogeneous containing different cell types. [MixChIP: a probabilistic method for cell type specific protein-DNA binding analysis](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0834-3) \n9. [Detecting broad domains and narrow peaks in ChIP-seq data with hiddenDomains](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-0991-z) [tool](http://hiddendomains.sourceforge.net/)\n10. [BroadPeak: a novel algorithm for identifying broad peaks in diffuse ChIP-seq datasets](http://bioinformatics.oxfordjournals.org/content/29/4/492)\n11. [epic: diffuse domain ChIP-Seq caller based on SICER]( https://github.com/endrebak/epic). It is a re-writen of SICER for faster processing using more CPUs. (Will try it for broad peak for sure).\n    [epic2](https://github.com/biocore-ntnu/epic2) paper is out https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btz232/5421513?redirectedFrom=fulltext\n12. [Cistrome](http://cistrome.org/Cistrome/Cistrome_Project.html): The best place for wet lab scientist to check the binding sites. Developed by Shierly Liu lab in Harvard.\n13. [ChIP-Atlas](http://chip-atlas.org/) is an integrative and comprehensive database for visualizing and making use of public ChIP-seq data. ChIP-Atlas covers almost all public ChIP-seq data submitted to the SRA (Sequence Read Archives) in NCBI, DDBJ, or ENA, and is based on over 78,000 experiments.\n14. [A map of direct TF-DNA interactions in the human genome](https://unibind.uio.no/) UniBind is a comprehensive map of direct interactions between transcription factor (TFs) and DNA. High confidence TF binding site predictions were obtained from uniform processing of thousands of ChIP-seq data sets using the ChIP-eat software.\n15. [Accounting for GC-content bias reduces systematic errors and batch effects in ChIP-Seq peak callers](http://biorxiv.org/content/early/2016/12/01/090704) tool in [github](https://github.com/tengmx/gcapc)\n16. [SUPERmerge](https://github.com/Bohdan-Khomtchouk/SUPERmerge):ChIP-seq coverage island analysis algorithm for broad histone marks\n17. [PeakRanger](http://ranger.sourceforge.net/manual1.18.html) heard that it is good for broad peaks of H3K9me3 and H3K27me3.\n\n\n**Different parameters using the same program can produce drastic different sets of peaks especially for histone modifications with variable enrichment length and gaps between peaks. One needs to make a valid argument for parameters he uses**  \n\nAn example of different parameters for homer `findPeaks`:  \n![](./images/variablePeaks.png)\n\n### Tutorial\n\n* [tutorial by Simon van Heeringen at bioinfosummer](https://github.com/simonvh/bioinfosummer)\n\n### Binding does not infer functionality  \n\n* [A significant proportion of transcription-factor binding sites may be nonfunctional](http://judgestarling.tumblr.com/post/64874995999/hypotheses-about-the-functionality-of) A post from Judge Starling  \n\n* Several papers have shown that changes of adjacent TF binding poorly correlates with gene expression change:\n[Extensive Divergence of Transcription Factor Binding in Drosophila Embryos with Highly Conserved Gene Expression](http://www.plosgenetics.org/article/info%3Adoi%2F10.1371%2Fjournal.pgen.1003748)  \n[Transcription Factors Bind Thousands of Active and Inactive Regions in theDrosophila Blastoderm](http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.0060027)    \n\n[The Functional Consequences of Variation in Transcription Factor Binding](http://arxiv.org/abs/1310.5166)  \n>\" On average, 14.7% of genes bound by a factor were differentially expressed following the knockdown of that factor, suggesting that most interactions between TF and chromatin do not result in measurable changes in gene expression levels of putative target genes. \"\n\n* paper [A large portion of the ChIP-seq signal does not correspond to true binding](http://www.ncbi.nlm.nih.gov/pubmed/26388941?dopt=Abstract&utm_source=dlvr.it&utm_medium=twitter)\n* [BIDCHIPS: Bias-Decomposition of ChIP-seq Signals](http://www.perkinslab.ca/Software.html)  \n\t**mappability, GC-content and chromatin accessibility** affect ChIP-seq read counts. \n* [ChIP bias as a function of cross-linking time](http://link.springer.com/article/10.1007%2Fs10577-015-9509-1)   \n\n>We analyzed the dependence of the ChIP signal on the duration of formaldehyde cross-linking time for two proteins: DNA topoisomerase 1 (Top1) that is functionally associated with the double helix in vivo, especially with active chromatin, and green fluorescent protein (GFP) that has no known bona fide interactions with DNA. With short time of formaldehyde fixation, only Top1 immunoprecipation efficiently recovered DNA from active promoters, whereas prolonged fixation augmented non-specific recovery of GFP dramatizing the need to optimize ChIP protocols to minimize the time of cross-linking, especially for abundant nuclear proteins. Thus, ChIP is a powerful approach to study the localization of protein on the genome when care is taken to manage potential artifacts.\n\n\n### Gene set enrichment analysis for ChIP-seq peaks  \n\n[The Gene Ontology Handbook](http://link.springer.com/protocol/10.1007%2F978-1-4939-3743-1_13) Read it for basics for GO.\n\n1. [Broad Enrich](http://broad-enrich.med.umich.edu/)  \n2. [ChIP Enrich](http://chip-enrich.med.umich.edu/)  \n3. [GREAT](http://bejerano.stanford.edu/great/public/html/) predicts functions of cis-regulatory regions.  \n4. [ENCODE ChIP-seq significance tool](http://encodeqt.simple-encode.org/). Given a list of genes, co-regulating TFs will be identified.  \n5. [cscan](http://159.149.160.51/cscan/) similar to the ENCODE significance tool.  \n6. [CompGO: an R package for comparing and visualizing Gene Ontology enrichment differences between DNA binding experiments](http://www.biomedcentral.com/1471-2105/16/275)  \n7. [interactive and collaborative HTML5 gene list enrichment analysis tool](http://amp.pharm.mssm.edu/Enrichr/)\n8. [GeNets](http://www.broadinstitute.org/genets#computations) from Broad. Looks very promising.\n9. [Bioconductor EnrichmentBrowser](https://www.bioconductor.org/packages/3.3/bioc/html/EnrichmentBrowser.html)\n10. [clusterProfiler](http://bioconductor.org/packages/release/bioc/vignettes/clusterProfiler/inst/doc/clusterProfiler.html) by Guangchuan Yu, the author of `ChIPseeker`.\n11. [fgsea bioconductor package](http://bioconductor.org/packages/devel/bioc/html/fgsea.html) Fast Gene Set Entrichment Analysis.\n12. [paper: A Comparison of Gene Set Analysis Methods in Terms of Sensitivity, Prioritization and Specificity](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079217#pone-0079217-t001)\n13. [UniBind Enrichment Analysis](https://unibind.uio.no/enrichment/) predicts which sets of TFBSs from the UniBind database are enriched in a set of given genomic regions. Enrichment computations are performed using the LOLA tool.\n14. [BEHST](https://www.biorxiv.org/content/10.1101/168427v1) from Hoffman group: genomic set enrichment analysis enhanced through integration of chromatin long-range interactions\n15. [ChEA3: transcription factor enrichment analysis by orthogonal omics integration](https://amp.pharm.mssm.edu/chea3)\n\n\n### Chromatin state Segmentation  \n1. [ChromHMM](http://compbio.mit.edu/ChromHMM/)  from Manolis Kellis in MIT.\n  >In ChromHMM the raw reads are assigned to non-overlapping bins of 200 bps and a sample-specific threshold is used to transform the count data to binary values\n\n2. [Segway](https://www.pmgenomics.ca/hoffmanlab/proj/segway/) from Hoffman lab. Base pair resolution. Takes longer time to run.  \n3. [epicseg](https://github.com/lamortenera/epicseg) published 2015 in genome biology. Similiar speed with ChromHMM.   \n4. [Spectacle: fast chromatin state annotation using spectral learning](https://github.com/jiminsong/Spectacle). Also published 2015 in genome biology.  \n5. [chromstaR](http://biorxiv.org/content/early/2016/02/04/038612): Tracking combinatorial chromatin state dynamics in space and time\n6. [epilogos](http://epilogos.broadinstitute.org/) visualization and analysis of chromatin state model data.\n7. [Accurate promoter and enhancer identification in 127 ENCODE and Roadmap Epigenomics cell types and tissues by GenoSTAN](http://biorxiv.org/content/early/2016/02/24/041020)\n8. [StatePaintR](https://github.com/Simon-Coetzee/StatePaintR) StateHub-StatePaintR: rules-based chromatin state annotations.\n9. [IDEAS(https://github.com/yuzhang123/IDEAS/): an integrative and discriminative epigenome annotation system  http://sites.stat.psu.edu/~yzz2/IDEAS/\n\n### deep learning in ChIP-seq\n* [Coda](https://github.com/kundajelab/coda) uses convolutional neural networks to learn a mapping from noisy to high-quality ChIP-seq data. These trained networks can then be used to remove noise and improve the quality of new ChIP-seq data. From Ashul lab.\n* [DeepChrome](https://github.com/QData/DeepChrome) is a unified CNN framework that automatically learns combinatorial interactions among histone modification marks to predict the gene expression. (Is it really better than a simple linear model?)\n* [deep learning in biology](https://github.com/hussius/deeplearning-biology)\n* gReLU is a Python library to train, interpret, and apply deep learning models to DNA sequences. [Code documentation is available here.](https://github.com/Genentech/gReLU)\n* [tangerMEME](https://github.com/jmschrei/tangermeme) is an extension of the MEME suite concept to biological sequence analysis when you have a collection of sequences and a predictive model.\n* [Dissecting the cis-regulatory syntax of transcription initiation with deep learning](https://www.biorxiv.org/content/10.1101/2024.05.28.596138v1)\n* Transfer learning reveals sequence determinants of the quantitative response to transcription factor dosage https://www.biorxiv.org/content/10.1101/2024.05.28.596078v1\n  \n### Peak annotation \n\n1. Homer [`annotatePeak`](http://homer.salk.edu/homer/ngs/annotation.html) \n2. Bioconductor package [ChIPseeker](http://bioconductor.org/packages/release/bioc/html/ChIPseeker.html) by [Guangchuan Yu](http://ygc.name/)   \n   See an important post by him on 0 or 1 based [coordinates](http://ygc.name/2015/08/07/parsing-bed-coordinates/).\n \n >Most of the software for ChIP annotation doesn't considered this issue when annotating peak (0-based) to transcript (1-based). To my knowledge, only HOMER consider this issue. After I figure this out, I have updated ChIPseeker (version >= 1.4.3) to fix the issue.\n \n3. Bioconductor package [ChIPpeakAnno](http://bioconductor.org/packages/release/bioc/html/ChIPpeakAnno.html). \n\n4. [annotatr](https://github.com/rcavalcante/annotatr/) Annotation of Genomic Regions to Genomic Annotations.\n\n5. [geneXtendeR](https://bioconductor.org/packages/release/bioc/html/geneXtendeR.html) computes optimal gene extensions tailored to the broadness of the specific epigenetic mark (e.g., H3K9me1, H3K27me3), as determined by a user-supplied ChIP-seq peak input file. As such, geneXtender maximizes the signal-to-noise ratio of locating genes closest to and directly under peaks\n\n* [DNAshapeR predicts DNA shape features in an ultra-fast, high-throughput manner from genomic sequencing data](http://tsupeichiu.github.io/DNAshapeR/)  \n\n### Differential peak detection  \nLook at a [post](http://andre-rendeiro.me/2015/04/03/chipseq_diffbind_analysis/) and [here](http://crazyhottommy.blogspot.com/2013/10/compare-chip-seq-data-for-different.html) describing different tools. \nA review paper [A comprehensive comparison of tools for differential ChIP-seq analysis](http://bib.oxfordjournals.org/content/early/2016/01/12/bib.bbv110.short?rss=1)  \n\n![](./images/choose_diff_tool.png)\n\n\n* [ATAC-seq normalization method can significantly affect differential accessibility analysis and interpretation](https://epigeneticsandchromatin.biomedcentral.com/articles/10.1186/s13072-020-00342-y)\n* [Comparison of differential accessibility analysis strategies for ATAC-seq data](https://www.nature.com/articles/s41598-020-66998-4) https://github.com/Zhang-lab/BeCorrect to correct batch effect from the bedgraph files.\n\n1. [MultiGPS](http://mahonylab.org/software/multigps/)  \n\n2. [PePr](https://github.com/shawnzhangyx/PePr). It can also call peaks.  \n\n3. [histoneHMM](http://histonehmm.molgen.mpg.de/)  \n\n4. [diffreps](https://github.com/shenlab-sinai/diffreps) for histone.  developed by Shen Li's lab in Mount Sinai who also develped [ngs.plot](https://github.com/shenlab-sinai/ngsplot).  \n\n5. [diffbind bioconductor package](http://bioconductor.org/packages/release/bioc/html/DiffBind.html). Internally uses RNA-seq tools: EdgR or DESeq.  Most likely, I will use this tool.  \n\n6. [ChIPComp](http://web1.sph.emory.edu/users/hwu30/software/ChIPComp.html). Very little tutorial. Now it is on bioconductor.\n\n7. [csaw bioconductor package](http://bioconductor.org/packages/release/bioc/html/csaw.html). Tutorial [here](https://www.bioconductor.org/help/course-materials/2015/BioC2015/csaw_lab.html)  \n\n8. [chromDiff](http://compbio.mit.edu/ChromDiff/Download.html). Also from from Manolis Kellis in MIT. Similar with ChromHMM, documentation is not that detailed. Will have a try on this.  \n9. [MACS2 can detect differential peaks as well](https://github.com/taoliu/MACS/wiki/Call-differential-binding-events)  \n10. paper [Identifying differential transcription factor binding in ChIP-seq](http://journal.frontiersin.org/article/10.3389/fgene.2015.00169/full)  \n\n\n### Motif enrichment\n1. [HOMER](http://homer.salk.edu/homer/ngs/peakMotifs.html). It has really detailed documentation. It can also be used to call peaks. \n\nFor TF ChIP-seq, one can usually find the summit of the peak (macs14 will report the summit), and extend the summit to both sides to 100bp-500bp. One can then use those 100bp-500 bp small regions to do motif analysis. Usually, oen should find the motif for the ChIPed TF in the ChIP-seq experiment if it is a DNA binding protein.\n\nIt is trickier to do motif analysis using histone modification ChIP-seq. For example, the average peak size of H3K27ac is 2~3 kb. If one wants to find TF binding motifs from H3K27ac ChIP-seq data, it is good to narrow down the region a bit. MEME and many other motif finding tools require that the DNA sequence length to be small (~500bp). One way is to use `findPeaks` in homer turning on `-nfr`(nucleosome free region) flag, and then do motif analysis in those regions.\n\nsuggestions for finding motifs from histone modification ChIP-seq data from HOMER page:\n>Since you are looking at a region, you do not necessarily want to center the peak on the specific position with the highest tag density, which may be at the edge of the region.  Besides, in the case of histone modifications at enhancers, the highest signal will usually be found on nucleosomes surrounding the center of the enhancer, which is where the functional sequences and transcription factor binding sites reside.  Consider H3K4me marks surrounding distal PU.1 transcription factor peaks.  Typically, adding the -center >option moves peaks further away from the functional sequence in these scenarios.\n\nOther strategy similar to `-nfr` was developed in this paper: [Dissecting neural differentiation regulatory networks through epigenetic footprinting](http://www.ncbi.nlm.nih.gov/pubmed/25533951). In the method part of the paper, the authors computed a depletion score within the peaks, and use the footprinted regions to do motif analysis. (Thanks [kadir](https://twitter.com/canerakdemir) for pointing out the paper)\n\nhttp://homer.ucsd.edu/homer/ngs/peakMotifs.html  \n\n>Region Size (\"-size <#>\", \"-size <#>,<#>\", \"-size given\", default: 200)\nThe size of the region used for motif finding is important.  If analyzing ChIP-Seq peaks from a transcription factor, Chuck would recommend 50 bp for establishing the primary motif bound by a given transcription factor and 200 bp for finding both primary and \"co-enriched\" motifs for a transcription factor.  When looking at histone marked regions, **500-1000 bp is probably a good idea (i.e. H3K4me or H3/H4 acetylated regions)**.  In theory, HOMER can work with very large regions (i.e. 10kb), but with the larger the regions comes more sequence and longer execution time.  These regions will be based off the center of the peaks.  If you prefer an offset, you can specify \"-size -300,100\" to search a region of size 400 that is centered 100 bp upstream of the peak center (useful if doing motif finding on putative TSS regions).  If you have variable length regions, use the option \"-size given\" and HOMER will use the exact regions that were used as input.\n\nI just found [PARE](http://spundhir.github.io/PARE/). PARE is a computational method to Predict Active Regulatory Elements, specifically enhancers and promoters. H3K27ac and H3K4me can be used to define active enhancers.\n\n2. [MEME suite](http://meme.ebi.edu.au/meme/index.html). It is probably the most popular motif finding tool in the papers.  [protocol:Motif-based analysis of large nucleotide data sets using MEME-ChIP](http://www.nature.com/nprot/journal/v9/n6/full/nprot.2014.083.html)  \n3. [MEME R package](https://snystrom.github.io/memes-manual/)\n4. [JASPAR database](http://jaspar.binf.ku.dk/  )\n5. [pScan-ChIP](http://159.149.160.51/pscan_chip_dev/)  \n6. [MotifMap](http://motifmap.ics.uci.edu/#MotifSearch)  \n7. [RAST](http://rsat01.biologie.ens.fr/rsa-tools/index.html) Regulatory Sequence Analysis Tools.  \n8. [ENCODE TF motif database](http://compbio.mit.edu/encode-motifs/)  \n9. [oPOSSUM](http://opossum.cisreg.ca/oPOSSUM3/) is a web-based system for the detection of over-represented conserved transcription factor binding sites and binding site combinations in sets of genes or sequences.  \n10.  my post [how to get a genome-wide motif bed file](http://crazyhottommy.blogspot.com/2014/02/how-to-get-genome-wide-motif-bed-file.html) \n11.  Many other tools [here](http://omictools.com/motif-discovery-c84-p1.html)\n12. [A review of ensemble methods for de novo motif discovery in ChIP-Seq data](http://bib.oxfordjournals.org/content/early/2015/04/17/bib.bbv022.abstract)  \n13. [melina2](http://melina2.hgc.jp/public/index.html). If you only have one sequence and want to know what TFs might bind\n    there, this is a very useful tool.\n12. [STEME](https://pypi.python.org/pypi/STEME/). A python library for motif analysis. STEME started life as an approximation to the Expectation-Maximisation algorithm for the type of model used in motif finders such as MEME. **STEMEâ€™s EM approximation runs an order of magnitude more quickly than the MEME implementation for typical parameter settings**. STEME has now developed into a fully-fledged motif finder in its own right.  \n13. [CENTIPEDE: Transcription factor footprinting and binding site prediction](http://centipede.uchicago.edu/). [Tutorial](https://github.com/slowkow/CENTIPEDE.tutorial)  \n14. [msCentipede: Modeling Heterogeneity across Genomic Sites and Replicates Improves Accuracy in the Inference of Transcription Factor Binding](http://rajanil.github.io/msCentipede/)  \n15. [DiffLogo: A comparative visualisation of sequence motifs](http://bioconductor.org/packages/release/bioc/html/DiffLogo.html) \n16. [Weeder (version: 2.0)](http://159.149.160.51/modtools/)\n17. [MCAST: scanning for cis-regulatory motif clusters](http://bioinformatics.oxfordjournals.org/content/early/2016/01/14/bioinformatics.btv750.short?rss=1) Part of MEME suite.\n18. [Sequence-based Discovery of Regulons](http://iregulon.aertslab.org/) iRegulon detects the TF, the targets and the motifs/tracks from a set of genes.\n19. [Regulatory genomic toolbox](http://www.regulatory-genomics.org/motif-analysis/introduction/)\n20. [Parse TF motifs from public databases, read into R, and scan using 'rtfbs'](https://github.com/Danko-Lab/rtfbs_db)\n21. [Romulus: Robust multi-state identification of transcription factor binding sites from DNase-seq data](https://github.com/ajank/Romulus): Romulus is a computational method to accurately identify individual transcription factor binding sites from genome sequence information and cell-type--specific experimental data, such as DNase-seq. It combines the strengths of its predecessors, CENTIPEDE and Wellington, while keeping the number of free parameters in the model robustly low. The method is unique in allowing for multiple binding states for a single transcription factor, differing in their cut profile and overall number of DNase I cuts.\n22. [moca](https://github.com/saketkc/moca): Tool for motif conservation analysis.\n23. [gimmemotifs](https://github.com/simonvh/gimmemotifs) Suite of motif tools, including a motif prediction pipeline for ChIP-seq experiments. looks very useful, will take a look!\n24. [YAMDA](https://github.com/daquang/YAMDA): thousandfold speedup of EM-based motif discovery using deep learning libraries and GPU\n25. [motif clustering](https://www.vierstra.org/resources/motif_clustering)\n26. [RSAT matrix-clustering: dynamic exploration and redundancy reduction of transcription factor binding motif collections](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5737723/)\n\n\n### Super-enhancer identification   \n\nThe fancy \"supper-enhancer\" term was first introduced by [Richard Young](http://younglab.wi.mit.edu/publications.htm) in Whitehead Institute. Basically, super-enhancers are enhancers that span large genomic regions(~12.5kb). The concept of super-enhancer is not new. One of the most famous example is the Locus Control Region (LCR) that controls the globin gene expression, and this has been known for decades.  \n\nA review in Nature Genetics [What are super-enhancers?](http://www.nature.com/ng/journal/v47/n1/full/ng.3167.html)  \n\n[paper: Genetic dissection of the Î±-globin super-enhancer in vivo](http://www.nature.com/ng/journal/v48/n8/full/ng.3605.html) \n\n> By generating a series of mouse models, deleting each of the five regulatory elements of the Î±-globin super-enhancer individually and in informative combinations, we demonstrate that each constituent enhancer seems to act independently and in an additive fashion with respect to hematological phenotype, gene expression, chromatin structure and chromosome conformation, without clear evidence of synergistic or higher-order effects.\n\n[paper: Hierarchy within the mammary STAT5-driven Wap super-enhancer](http://www.nature.com/ng/journal/v48/n8/full/ng.3606.html)  \n[paper: Enhancers and super-enhancers have an equivalent regulatory role in embryonic stem cells through regulation of single or multiple genes](http://m.genome.cshlp.org/content/early/2016/11/28/gr.210930.116.abstract)\n\nFrom the [HOMER page](http://homer.salk.edu/homer/ngs/peaks.html)\n**How finding super enhancers works:**\n\n>Super enhancer discovery in HOMER emulates the original strategy used by the Young lab.  First, peaks are found just like any other ChIP-Seq data set.  Then, peaks found within a given distance are 'stitched' together into larger regions (by default this is set at 12.5 kb).  The super enhancer signal of each of these regions is then determined by the total normalized number reads minus the number of normalized reads in the input.  These regions are then sorted by their score, normalized to the highest score and the number of putative enhancer regions, and then super enhancers are identified as regions past the point where the slope is greater than 1.  \n\nExample of a super enhancer plot:\n![](./images/super-enhancer-plot.png)\n\n>In the plot above, all of the peaks past 0.95 or so would be considered \"super enhancers\", while the one's below would be \"typical\" enhancers.  If the slope threshold of 1 seems arbitrary to you, well... it is!  This part is probably the 'weakest link' in the super enhancer definition.  However, the concept is still very useful.  Please keep in mind that most enhancers probably fall on a continuum between typical and super enhancer status, so don't bother fighting over the precise number of super enhancers in a given sample and instead look for useful trends in the data.\n\n**Using ROSE from Young lab**   \n[ROSE: RANK ORDERING OF SUPER-ENHANCERS](http://younglab.wi.mit.edu/super_enhancer_code.html)  \n\n**[imPROSE](https://github.com/asntech/improse) - Integrated Methods for Prediction of Super-Enhancers\n\n**[CREAM](https://github.com/bhklab/CREAM) (Clustering of Functional Regions Analysis Method) is a new method for identification of clusters of functional regions (COREs) within chromosomes.** published in Genome Research by Mathieu Lupien group. paper: Identifying clusters of cis-regulatory elements underpinning TAD structures and lineage-specific regulatory networks.\n\n\n### Bedgraph, bigwig manipulation tools\n[WiggleTools](https://github.com/Ensembl/WiggleTools)  \n[bigwig tool](https://github.com/CRG-Barcelona/bwtool/wiki)  \n[bigwig-python](https://github.com/brentp/bw-python)  \n[samtools](http://www.htslib.org/)    \n[bedtools](http://bedtools.readthedocs.org/en/latest/) my all-time favorite tool from Araon Quinlan' lab. Great documentation! \n[pyBedGraph](https://www.biorxiv.org/content/10.1101/709683v1): a Python package for fast operations on 1-dimensional genomic signal tracks.\n[pyBigwig](https://github.com/deeptools/pyBigWig)\n[Hosting bigWig for UCSC visualization](http://crazyhottommy.blogspot.com/2014/02/hosting-bigwig-by-dropbox-for-ucsc.html)  \n[My first play with GRO-seq data, from sam to bedgraph for visualization](http://crazyhottommy.blogspot.com/2013/10/my-first-play-with-gro-seq-data-from.html)  \n[convert bam file to bigwig file and visualize in UCSC genome browser in a Box (GBiB)](http://crazyhottommy.blogspot.com/2014/10/convert-bam-file-to-bigwig-file-and.html). \n[megadept](https://github.com/ChristopherWilks/megadepth) is pretty fast, can access bigWig files from the web, works on macOS, Linux & Windows, plus is also available via \n@Bioconductor http://www.bioconductor.org/packages/release/bioc/html/megadepth.html which makes easy to use it in #rstats. For example, for quantifying expression of custom regions from recount3 data\n\n[Bigtools: a high-performance BigWig and BigBed library in rust](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btae350/7688332?login=false)\n\n\n\n### Peaks overlapping significance test\n[The genomic association tester (GAT)](https://github.com/AndreasHeger/gat)  \n[poverlap](https://github.com/brentp/poverlap) from Brent Pedersen. Now he is working with Aaron Quinlan at university of Utah.  \n[Genometric Correlation (GenometriCorr): an R package for spatial correlation of genome-wide interval datasets](http://genometricorr.sourceforge.net/)  \n[Location overlap analysis for enrichment of genomic ranges](http://bioconductor.org/packages/release/bioc/html/LOLA.html) bioconductor package.   \n[regioneR](http://bioconductor.org/packages/release/bioc/html/regioneR.html) Association analysis of genomic regions based on permutation tests\n[similaRpeak](http://bioconductor.org/packages/devel/bioc/html/similaRpeak.html): Metrics to estimate a level of similarity between two ChIP-Seq profiles\n\n### RNA-seq data integration\n[Beta](http://cistrome.org/BETA/) from Shirley Liu's lab in Harvard.  Tao Liu's previous lab.  \n\n\n### Heatmap, mata-plot \n\nMany papers draw meta-plot and heatmap on certain genomic regions (2kb around TSS, genebody etc) using ChIP-seq data. \n\nSee an example from the ngs.plot:  \n![](./images/meta-heatmap.png)\n\n\n**Tools**  \n\n1. [deeptools](https://github.com/fidelram/deepTools).It can do many others and have good documentation.\nIt can also generate the heatmaps, but I personally use [ngs.plot](https://github.com/shenlab-sinai/ngsplot) which is esy to use. (developed in Mount Sinai).  \n\n2. you can also draw heatmaps using R. just count (using either Homer or bedtools) the ChIP-seq reads in each bin and draw with heatmap.2 function. \n[here](http://crazyhottommy.blogspot.com/2013/08/how-to-make-heatmap-based-on-chip-seq.html) and [here](http://crazyhottommy.blogspot.com/2013/04/how-to-make-tss-plot-using-rna-seq-and.html). Those are my pretty old blog posts, I now have a much better idea on how to make those graphs from scratch.\n\n3. You can also use bioconductor [Genomation](http://www.bioconductor.org/packages/release/bioc/vignettes/genomation/inst/doc/GenomationManual-knitr.html). It is very versatile.\n4. [ChAsE](http://www.epigenomes.ca/tools.html)\n5. [Metaseq](https://pythonhosted.org/metaseq/example_session.html)\n6. [EnrichedHeatmaps](https://github.com/jokergoo/EnrichedHeatmap) from Zuguang Gu based on his own package `ComplexHeatmaps`. This is now my default go-to because of the flexiability of the package and the great user support. Thx!\n7. [A biostar post discussing the tools: Visualizations of ChIP-Seq data using Heatmaps](https://www.biostars.org/p/180314/)\n8. [A bioconductor package to produce metagene plots](http://bioconductor.org/packages/release/bioc/html/metagene.html)\n9. [Fluff is a Python package that contains several scripts to produce pretty, publication-quality figures for next-generation sequencing experiments](https://github.com/simonvh/fluff) I just found it 09/01/2016. looks promising especially for identifying the dynamic change.\n\n**One cavet is that the meta-plot (on the left) is an average view of ChIP-seq\ntag enrichment and may not reflect the real biological meaning for individual cases.**  \n\nSee a post from Lior Patcher [How to average genome-wide data](https://liorpachter.wordpress.com/2015/07/13/how-to-average-genome-wide-data/)  \n\nI replied the post:\n>for ChIP-seq, in addition to the average plot, a heatmap that with each region in each row should make it more clear to compare (although not quantitatively). a box-plot (or a histogram) is better in this case . I am really uncomfortable averaging the signal, as a single value (mean) is not a good description of the distribution.\n\nBy Meromit Singer:  \n>thanks for the paper ref! Indeed, an additional important issue with averaging is that one could be looking at the aggregation of several (possibly very distinct) clusters. Another thing we should all keep in mind if we choose to make such plots..\n\nA paper from Genome Research [Ubiquitous heterogeneity and asymmetry of the chromatin environment at regulatory elements](http://m.genome.cshlp.org/content/22/9/1735.full)\n\n### Enhancer databases\n* [FANTOM project](http://fantom.gsc.riken.jp/5/)  CAGE for promoters and enhancers.\n* [DENdb: database of integrated human enhancers](http://www.cbrc.kaust.edu.sa/dendb/)  \n* [VISTA enhancer browser](http://enhancer.lbl.gov/)  \n* [Super-enhancer database](http://www.bio-bigdata.com/SEA/)\n* [Genome-wide identification and characterization of HOT regions in the human genome](http://biorxiv.org/content/early/2016/01/07/036152.abstract)  \n* [EnhancerAtlas: a resource for enhancer annotation and analysis in 105 human cell/tissue types](http://www.enhanceratlas.org/)\n* [review: Computational Tools for Stem Cell Biology](http://www.sciencedirect.com/science/article/pii/S0167779916300567)\n* [Integrative analysis of 10,000 epigenomic maps across 800 samples for regulatory genomics and disease dissection](https://www.biorxiv.org/content/10.1101/810291v2) from Manolis Kellis group.\n* [Index and biological spectrum of accessible DNA elements in the human genome](https://www.biorxiv.org/content/10.1101/822510v1) DHS sites from John A Stamatoyannopoulos group.\n\n### Interesting Enhancer papers\n* [Multiplex enhancer-reporter assays uncover unsophisticated TP53 enhancer logic](http://genome.cshlp.org/content/26/7/882)\n* \n\n### Enhancer target prediction \n\n* [DoRothEA: collection of human and mouse regulons](https://github.com/saezlab/dorothea) DoRothEA is a gene regulatory network containing signed transcription factor (TF) - target gene interactions. DoRothEA regulons, the collection of a TF and its transcriptional targets, were curated and collected from different types of evidence for both human and mouse. A confidence level was assigned to each TF-target interaction based on the number of supporting evidence.\n* [Assessing Computational Methods for Transcription Factor Target Gene Identification Based on ChIP-seq Data](http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003342#pcbi.1003342.s019) \n* [Protein binding and methylation on looping chromatin accurately predict distal regulatory interactions](http://biorxiv.org/content/early/2015/07/09/022293)\n* [i-cisTarget](http://gbiomed.kuleuven.be/apps/lcb/i-cisTarget/)\n* [protocol iRegulon and i-cisTarget: Reconstructing Regulatory Networks Using Motif and Track Enrichment](http://www.ncbi.nlm.nih.gov/pubmed/26678384?dopt=Abstract&utm_source=dlvr.it&utm_medium=twitter)  \n* [Model-based Analysis of Regulation of Gene Expression: MARGE](http://cistrome.org/MARGE/) from Shirley Liu's lab. MARGE is a robust methodology that leverages a comprehensive library of genome-wide H3K27ac ChIP-seq profiles to predict key regulated genes and cis-regulatory regions in human or mouse. \n* [PrESSto: Promoter Enhancer Slider Selector Tool](http://pressto.binf.ku.dk/)\n* [TargetFinder](https://github.com/shwhalen/targetfinder). paper: [Enhancerâ€“promoter interactions are encoded by complex genomic signatures on looping chromatin](http://www.nature.com/ng/journal/v48/n5/full/ng.3539.html)\n* [C3D](https://github.com/mlupien/C3D) Cross Cell-type Correlation in DNaseI hypersensitivity. calculates correlations between open regions of chromatin based on DNase I hypersensitivity signals. Regions with high correlations are candidates for 3D interactions. It also performs association tests on each candidate and adjusts p-values. \n* [ABC](https://www.biorxiv.org/content/10.1101/529990v1) Activity-by-Contact model of enhancer specificity from thousands of CRISPR perturbations. Blog post https://jesseengreitz.wordpress.com/2019/02/10/preprint-activity-by-contact-model/\n* [A curated benchmark of enhancer-gene interactions for evaluating enhancer-target gene prediction methods](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1924-8) \"We use BENGI to test several published computational methods for linking enhancers with genes, including signal correlation and the TargetFinder and PEP supervised learning methods. We find that while TargetFinder is the best-performing method, it is only modestly better than a baseline distance method for most benchmark datasets when trained and tested with the same cell type and that TargetFinder often does not outperform the distance method when applied across cell types.\"\n\n\n### Allele-specific analysis  \n* [WASP: allele-specific software for robust molecular quantitative trait locus discovery](http://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.3582.html)\n* [ABC -- (Allele-specific Binding from ChIP-Seq)](https://github.com/mlupien/ABC/)\n* [SNPsplit: Allele-specific splitting of alignments between genomes with known SNP genotypes](http://f1000research.com/articles/5-1479/v2)\n* [BaalChIP](http://bioconductor.org/packages/devel/bioc/html/BaalChIP.html): Bayesian analysis of allele-specific transcription factor binding in cancer genomes. bioconductor pacckage, seems to be very useful.\n\n### SNPs affect on TF binding\n* [RegulomeDB](http://www.regulomedb.org/)  Use RegulomeDB to identify DNA features and regulatory elements in non-coding regions of the human genome by entering dbSNP id, chromosome regions or single Nucleotides.  \n* [motifbreakR](http://bioconductor.org/packages/devel/bioc/html/motifbreakR.html) A Package For Predicting The Disruptiveness Of Single Nucleotide Polymorphisms On Transcription Factor Binding Sites.\n* [GERV: A Statistical Method for Generative Evaluation of Regulatory Variants for Transcription Factor Binding](http://www.ncbi.nlm.nih.gov/pubmed/26476779?dopt=Abstract&utm_source=dlvr.it&utm_medium=twitter) From the same group as above.\n* [PRIME: Predicted Regulatory Impact of a Mutation in an Enhancer](https://github.com/aertslab/primescore) \n* [paper: Which Genetics Variants in DNase-Seq Footprints Are More Likely to Alter Binding?](http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1005875) [website](http://genome.grid.wayne.edu/centisnps/)\n* [paper: Large-scale identification of sequence variants influencing human transcription factor occupancy in vivo](http://www.nature.com/ng/journal/v47/n12/abs/ng.3432.html) \n* [A Sicence paper:Survey of variation in human transcription factors reveals prevalent DNA binding changes](http://science.sciencemag.org/content/351/6280/1450)\n* paper [Estimating the functional impact of INDELs in transcription factor binding sites: a genome-wide landscape](http://biorxiv.org/content/early/2016/06/07/057604)\n* [paper: Mutational Biases Drive Elevated Rates of Substitution at Regulatory Sites across Cancer Types](http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1006207)\n* [paper: Recurrent promoter mutations in melanoma are defined by an extended context-specific mutational signature](http://biorxiv.org/content/early/2016/08/12/069351)\n* [sasquatch](https://github.com/rschwess/sasquatch) Predicting the impact of regulatory SNPs from cell and tissue specific DNase-footprints\n\n### co-occurring TFs\n\n* In-silico Search for co-occuring transcription factors: [INSECT](http://bioinformatics.ibioba-mpsp-conicet.gov.ar:84/INSECT/index.html) \n* [INSECT 2](http://bioinformatics.ibioba-mpsp-conicet.gov.ar/INSECT2/)\n* CO-factors associated with Uniquely-bound GEnomic Regions:[COUGER](http://couger.oit.duke.edu/)  \n* \n\n### Conservation of the peak underlying DNA sequences\n* [bioconductor annotation package phastCons100way.UCSC.hg19](https://bioconductor.org/packages/release/data/annotation/html/phastCons100way.UCSC.hg19.html) see this [post](http://bioinfoblog.it/2015/12/are-fitness-genes-more-conserved-my-first-30-minutes-attempt/) how to use it.\n* \n\n\n### Integration of different data sets\n\n[methylPipe and compEpiTools: a suite of R packages for the integrative analysis of epigenomics data](http://www.ncbi.nlm.nih.gov/pubmed/26415965?dopt=Abstract&utm_source=dlvr.it&utm_medium=twitter)  \n\n[Copy number information from targeted sequencing using off-target reads](https://bioconductor.org/packages/release/bioc/html/CopywriteR.html) bioconductor CopywriteR package.   \n\n[3CPET](http://www.bioconductor.org/packages/release/bioc/html/R3CPET.html): Finding Co-factor Complexes in Chia-PET experiment using a Hierarchical Dirichlet Process\n\n## New single/few cell epigenomics\n\n* [GeF-seq: A Simple Procedure for Base Pair Resolution ChIP-seq](https://www.ncbi.nlm.nih.gov/m/pubmed/30109604/)\n\n* [Ultra-low input CUT&RUN (uliCUT&RUN) enables interrogation of TF binding from low cell numbers](https://www.biorxiv.org/content/early/2018/03/21/286351)\n\n* [We describe Cleavage Under Targets and Release Using Nuclease (CUT&RUN), a chromatin profiling strategy in which antibody-targeted controlled cleavage by micrococcal nuclease releases specific protein-DNA complexes into the supernatant for paired-end DNA sequencing](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5310842/) another cut&run method. maybe useful for scChIP-seq?\n\n* [Single-cell ChIP-seq reveals cell subpopulations defined by chromatin state](https://www.nature.com/articles/nbt.3383)\n\n* [Calling Cards enable multiplexed identification of the genomic targets of DNA-binding proteins](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3083092/) this is potentially can work with single cells.\n\n* [Ultra-parallel ChIP-seq by barcoding of intact nuclei](https://www.biorxiv.org/content/early/2018/03/05/276469) as low as 1000 cells.\n\n* [single-cell chromatin overall omic-scale landscape sequencing (scCOOL-seq) to generate a genome-wide map of DNA methylation and chromatin accessibility at single-cell resolution](https://www.nature.com/articles/s41556-018-0123-2)\n\n* [High-Throughput ChIPmentation: freely scalable, single day ChIPseq data generation from very low cell-numbers](https://www.biorxiv.org/content/early/2018/09/27/426957)\n\n* [CUT&Tag for efficient epigenomic profiling of small samples and single cells](https://www.biorxiv.org/content/10.1101/568915v1)\n\n* [CUT&Tag Data Processing and Analysis Tutorial](https://yezhengstat.github.io/CUTTag_tutorial/) protocols.io link https://www.protocols.io/view/cut-amp-tag-data-processing-and-analysis-tutorial-bjk2kkye\n\n* [Simultaneous quantification of protein-DNA contacts and transcriptomes in single cells](https://www.biorxiv.org/content/10.1101/529388v1) scDamID&T.\n\n* [Self-reporting transposons enable simultaneous readout of gene expression and transcription factor binding in single cells](https://www.biorxiv.org/content/10.1101/538553v2) piggyBac transposase.\n\n* [Mapping Histone Modifications in Low Cell Number and Single Cells Using Antibody-guided Chromatin Tagmentation (ACT-seq)](https://www.biorxiv.org/content/10.1101/571208v2) by Keji Zhao group.\n\n* [Single-cell chromatin immunocleavage sequencing (scChIC-seq) to profile histone modification](https://www.nature.com/articles/s41592-019-0361-7) by Keji Zhao group.\n\n* [CoBATCH for high-throughput single-cell epigenomic profiling](https://www.biorxiv.org/content/10.1101/590661v1) Protein A in fusion to Tn5 transposase is enriched through specific antibodies to genomic regions and Tn5 generates indexed chromatin fragments ready for the library preparation and sequencing.\n\n* [High-throughput single-cell ChIP-seq identifies heterogeneity of chromatin states in breast cancer](https://www.nature.com/articles/s41588-019-0424-9)\n\n### ChIP-exo\n\n* [Characterizing protein-DNA binding event subtypes in ChIP-exo data](https://www.biorxiv.org/content/early/2018/02/16/266536)\n* [paper: Simplified ChIP-exo assays](https://www.nature.com/articles/s41467-018-05265-7)\n\n### ATAC-seq\n\n>Some may notice that the peaks produced look both like peaks produced from the TF ChIP-seq pipeline as well as the histone ChIP-seq pipeline. This is intentional, as ATAC-seq data looks both like TF data (narrow peaks of signal) as well as histone data (broader regions of openness).\n\n* paper [From reads to insight: a hitchhikerâ€™s guide to ATAC-seq data analysis](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1929-3)\n* [ATACseqQC ](http://bioconductor.org/packages/release/bioc/html/ATACseqQC.html) a bioconductor package for quality control of ATAC-seq data.\n* [RASQUAL](https://github.com/dg13/rasqual) (Robust Allele Specific QUAntification and quality controL) maps QTLs for sequenced based cellular traits by combining population and allele-specific signals. [paper: Fine-mapping cellular QTLs with RASQUAL and ATAC-seq](http://www.nature.com/ng/journal/vaop/ncurrent/full/ng.3467.html) \n* [ATAC-seq Forum](https://sites.google.com/site/atacseqpublic/home?pli=1)  \n* [Single-cell ATAC-Seq](http://cole-trapnell-lab.github.io/projects/sc-atac/) \n* [A rapid and robust method for single cell chromatin accessibility profiling](https://www.biorxiv.org/content/early/2018/04/27/309831)\n* [Global Prediction of Chromatin Accessibility Using RNA-seq from Small Number of Cells](http://www.biorxiv.org/content/early/2016/01/01/035816)  from RNA-seq to DNA accessibility. [tool on github](https://github.com/WeiqiangZhou/BIRD)  \n* [NucleoATAC](https://github.com/GreenleafLab/NucleoATAC):Python package for calling nucleosomes using ATAC-Seq data \n* [chromVAR: Inferring transcription factor variation from single-cell epigenomic data](http://biorxiv.org/content/early/2017/02/21/110346) scATAC-seq\n* [ENCODE ATAC-seq guidelines](https://www.encodeproject.org/data-standards/atac-seq/)\n* [Brockman](https://carldeboer.github.io/brockman.html) is a suite of command line tools and R functions to convert genomics data into DNA k-mer words representing the regions associated with a chromatin mark, and then analyzing these k-mer sets to see how samples differ from each other. This approach is primarily intended for single cell genomics data, and was tested most extensively on single cell ATAC-seq data\n* [Reproducible inference of transcription factor footprints in ATAC-seq and DNase-seq datasets via protocol-specific bias modeling](https://www.biorxiv.org/content/early/2018/03/19/284364)\n* [msCentipede](http://rajanil.github.io/msCentipede/) is an algorithm for accurately inferring transcription factor binding sites using chromatin accessibility data (Dnase-seq, ATAC-seq) and is written in Python2.x and Cython.\n* The Differential ATAC-seq Toolkit [(DAStk)](https://biof-git.colorado.edu/dowelllab/DAStk) is a set of scripts to aid analyzing differential ATAC-Seq data.\n* [Identification of Transcription Factor Binding Sites using ATAC-seq](https://www.biorxiv.org/content/early/2018/07/17/362863)  We propose HINT-ATAC, a footprinting method that addresses ATAC- seq specific protocol artifacts\n* [HMMRATAC](https://github.com/LiuLabUB/HMMRATAC)splits a single ATAC-seq dataset into nucleosome-free and nucleosome-enriched signals, learns the unique chromatin structure around accessible regions, and then predicts accessible regions across the entire genome. We show that HMMRATAC outperforms the popular peak-calling algorithms on published human and mouse ATAC-seq datasets.\n\n### DNase-seq\n* [pyDNase](https://github.com/jpiper/pyDNase) - a library for analyzing DNase-seq data. [paper: Wellington-bootstrap: differential DNase-seq footprinting identifies cell-type determining transcription factors](http://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-015-2081-4)  \n* [paper: Analysis of computational footprinting methods for DNase sequencing experiments](http://www.nature.com/nmeth/journal/vaop/ncurrent/full/nmeth.3772.html) [tool](http://www.regulatory-genomics.org/hint/introduction/) \n* [paper: A practical guide for DNase-seq data analysis: from data management to common applications](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bby057/5053117)\n* Two nature prime: [Genome-wide footprinting: ready for prime time?](http://www.nature.com/nmeth/journal/v13/n3/full/nmeth.3766.html) [Genomic footprinting](http://www.nature.com/nmeth/journal/v13/n3/full/nmeth.3768.html)\n* [PING](http://bioconductor.org/packages/release/bioc/html/PING.html) biocondcutor package: Probabilistic inference for Nucleosome Positioning with MNase-based or Sonicated Short-read Data\n* [Basset](https://github.com/davek44/Basset) Convolutional neural network analysis for predicting DNA sequence activity]\n* [Analysis of optimized DNase-seq reveals intrinsic bias in transcription factor footprint identification](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4018771/)\n\n### Chromatin Interaction data (ChIA-PET, Hi-C)\n* [ChIA-PET2](https://github.com/GuipengLi/ChIA-PET2) a versatile and flexible pipeline for analysing different variants of ChIA-PET data\n* [TopDom : An efficient and Deterministic Method for identifying Topological Domains in Genomes](http://zhoulab.usc.edu/TopDom/)\n* [DBPnet: Inferring cooperation of DNA binding proteins in 3D genome](http://wanglab.ucsd.edu/star/DBPnet/index.html) \n* [Systematic identification of cooperation between DNA binding proteins in 3D space](http://www.nature.com/ncomms/2016/160727/ncomms12249/full/ncomms12249.html)\n* [DiffHiC](https://www.bioconductor.org/packages/release/bioc/html/diffHic.html) package maintained by Aaron Lun, who is the author of [csaw](https://bioconductor.org/packages/release/bioc/html/csaw.html) and [InteractionSet]( https://github.com/LTLA/InteractionSet) as well.\n* [protocol:Practical Analysis of Genome Contact Interaction Experiments](http://link.springer.com/protocol/10.1007/978-1-4939-3578-9_9)\n* [4D genome: a general repository for chromatin interaction data](http://4dgenome.research.chop.edu/)\n* [CCSI](http://songyanglab.sysu.edu.cn/ccsi/search.php): a database providing chromatinâ€“chromatin spatial interaction information. only hg38 for human and mm10 for mouse.\n* [LOGIQA](http://www.ngs-qc.org/logiqa/) is a database hosting local and global quality scores assessed over long-range interaction assays (e.g. Hi-C).Based on the concept applied by the NGS-QC Generator over ChIP-seq and related datasets, LOGIQA infers quality indicators by the comparison of multiple sequence reads random sampling assays.\n* [Computational Identification of Genomic Features That Influence 3D Chromatin Domain Formation](http://journals.plos.org/ploscompbiol/article?id=info:doi/10.1371/journal.pcbi.1004908)\n* [Feng Yue's lab](http://promoter.bx.psu.edu/hi-c/view.php) in PSU developed tools for Hi-C, 4C \n* [QuIN: A Web Server for Querying and Visualizing Chromatin Interaction Networks](https://quin.jax.org/)\n* [paper: Three-dimensional disorganization of the cancer genome occurs coincident with long-range genetic and epigenetic alterations](http://genome.cshlp.org/content/26/6/719)\n* [Exploring long-range genome interactions using the WashU Epigenome Browse](http://www.nature.com/nmeth/journal/v10/n5/full/nmeth.2440.html)\n* [MAPPING OF LONG-RANGE CHROMATIN INTERACTIONS BY PROXIMITY LIGATION ASSISTED CHIP-SEQ](http://biorxiv.org/content/early/2016/09/09/074294)\n* [HiChIP: Efficient and sensitive analysis of protein-directed genome architecture](http://biorxiv.org/content/early/2016/09/08/073619) HiChIP improves the yield of conformation-informative reads by over 10-fold and lowers input requirement over 100-fold relative to ChIA-PE\n* [A Compendium of Chromatin Contact Maps Reveals Spatially Active Regions in the Human Genome](http://www.cell.com/cell-reports/fulltext/S2211-1247(16)31481-4?elsca1=etoc&amp;elsca2=email&amp;elsca3=2211-1247_20161115_17_8_&amp;elsca4=Cell%20Press%7CWebinar) paper from Bing Ren's group. 21 tissue-specific TADs.\n\n### Caleb's take on HiChIP analysis\nFrom Caleb, the author of hichipper https://twitter.com/CalebLareau/status/1098312702651523077 thx!\n\nIn HiChIP data analyses, there are two primary problems that we are trying to solve. A) Which anchors (i.e. genomic loci) should be used as a feature set and B) which loops (i.e. interactions between pairs of loci) are important in the data. 2/n\n\nDepending on what you are hoping to use your data for, there are a variety of ways to think about anchors and loops. Two uses of HiChIP that come to mind are \"which gene is this enhancer talking to\" and \"which loops are differential between my celltype/condition of interest\" 3/n\n\nWhen Martin and I wrote hichipper, we envisioned the second question being more used (i.e. building out a framework for differential loop calling), so we wanted a pre-processing pipeline that was as inclusive of potential loops as possible that could be subsetted downstream 4/n\n\nTo these ends, we reported an improved version of anchor detection from HiChIP data by modeling the restriction enzyme cut bias explicitly, which helped identify high-quality anchors from the data itself 5/n\n\n(we achieve this by re-parametrizing MACS2 peak calling by essentially fitting a loess curve to the data in the previous picture) 6/n\n\nUnfortunately, based on user feedback, this modified background winds up with a very, very conservative peak calling if the library preparations are sub-par. Thus, the safest way to approach HiChIP data analyses is often to use a pre-defined anchor set 7/n\n\nThese can be from either a complementary ATAC-seq or ChIP-seq dataset for the conditions that you are interested in. From what I've seen, you can supply a bed file to hichipper or other tools directly. Hichipper does some other modifications by default to this bed file FYI 8/n\n\nIn terms of the second problem of identifying loops, hichipper didn't make any revolutionary progress. We recommend some level of CPM-based filtering + mango FDR calculation (implemented in hichipper) for identifying single-library significant loops. 9/n\n\nWhere I've personally done the most is getting multiple libraries from multiple conditions and using some sort of between-replicate logic to filter to a reasonable (~10,000-20,000) number of loops ( see e.g. https://github.com/caleblareau/k562-hichip â€¦) 10/n\n\n\nOther tools (that I admittedly have not tried) use a variety of statistical techniques to (probably more intelligently from what I can tell) merge anchors or filter loops for analyses. A brief run down of those that I'm aware of (not exhaustive)-- 11/n\n\nMAPS (https://www.biorxiv.org/content/biorxiv/early/2018/09/08/411835.full.pdf â€¦) uses a measure of reproducibility with ChIP-seq to define a normalization and significance basis for loop calling. Given HiChIP-specific restriction enzyme bias, this seems sensible 12/n\n\nFitHiChIP (https://www.biorxiv.org/content/early/2018/10/29/376194.full.pdf â€¦) provides automatic merging of nearby anchors to solve the \"hairball\" problem, which is clearly shown Fig. 1. When I compared hichipper to FitHiC, the bias regression seemed to perform well, but I ran into memory issues which high... 13/n\n\nresolution (i.e. ~2.5kb) HiChIP data, which the authors have apparently solved in FitHiChIP. 14/n\n\nAdditionally, there is CID, which uses a density-based method to further collapse anchors to solve the \"hairball\" problem. 15/n\n\nThere are certainly other tools out there, but from my experience, any of these four (hichipper, MAPS, FitHiChIP, and CID) will probably give you something sensible (again acknowledging that I myself haven't actually run these other 3 tools) 16/n\n\nAnd if you're still reading this, I'll be a bit more specific about how I view hichipper pros/cons from both my own use and others in the community: hichipper provides the most \"vanilla\" functionality to given sensible yet exhaustive anchors and loops. 17/n\n\nI prefer it this way because I find that for each data set, I have to apply variable downstream threshold and cutoffs because the assay is so variable depending on which experimentalist performs the protocol and the biological question often varies so much 18/n\n\nThis may be a negative for individuals new to bioinformatics or HiChIP data but seemingly a positive for someone more experienced in working with related data. It's not obvious to me which other tools may be more applicable to a novice 19/n\n\nHope this helps paint a picture-- do let me know what you find if you compare tools! I think that it would be useful for the community.  20/20\n\n\n",
    "readme_length": 62906
  },
  {
    "name": "MACS",
    "full_name": "macs3-project/MACS",
    "description": "MACS -- Model-based Analysis of ChIP-Seq",
    "stars": 760,
    "forks": 269,
    "language": "Python",
    "url": "https://github.com/macs3-project/MACS",
    "topics": [
      "atac-seq",
      "chip-seq",
      "dnase-seq",
      "macs",
      "peak-caller",
      "poisson-equation",
      "python"
    ],
    "created_at": "2011-03-02T19:40:25Z",
    "updated_at": "2025-11-24T16:10:28Z",
    "homepage": "https://macs3-project.github.io/MACS/",
    "license": "BSD 3-Clause \"New\" or \"Revised\" License",
    "readme": "# MACS: Model-based Analysis for ChIP-Seq\n\n![Status](https://img.shields.io/pypi/status/macs3.svg) ![License](https://img.shields.io/github/license/macs3-project/MACS) ![Programming languages](https://img.shields.io/github/languages/top/macs3-project/MACS) [![CI x64](https://github.com/macs3-project/MACS/actions/workflows/build-and-test-MACS3-x64.yml/badge.svg)](https://github.com/macs3-project/MACS/actions/workflows/build-and-test-MACS3-x64.yml) [![CI non x64](https://github.com/macs3-project/MACS/actions/workflows/build-and-test-MACS3-non-x64.yml/badge.svg)](https://github.com/macs3-project/MACS/actions/workflows/build-and-test-MACS3-non-x64.yml) [![CI Mac OS](https://github.com/macs3-project/MACS/actions/workflows/build-and-test-MACS3-macos.yml/badge.svg)](https://github.com/macs3-project/MACS/actions/workflows/build-and-test-MACS3-macos.yml)\n\n[![PyPI\ndownload](https://img.shields.io/pypi/dm/macs3?label=pypi%20downloads)](https://pypistats.org/packages/macs3)\n\nLatest Release:\n* Github: [![Github Release](https://img.shields.io/github/v/release/macs3-project/MACS)](https://github.com/macs3-project/MACS/releases)\n* PyPI: [![PyPI Release](https://img.shields.io/pypi/v/macs3.svg)](https://pypi.org/project/MACS3/)\n* Bioconda:[![Bioconda Badge](https://anaconda.org/bioconda/macs3/badges/version.svg)](https://anaconda.org/bioconda/macs3)\n* Debian Med: [![Debian Stable](https://img.shields.io/debian/v/macs/stable?label=debian%20stable)](https://packages.debian.org/stable/macs)[![Debian Unstable](https://img.shields.io/debian/v/macs/sid?label=debian%20sid)](https://packages.debian.org/sid/macs)\n\n\n## Introduction\n\nWith the improvement of sequencing techniques, chromatin\nimmunoprecipitation followed by high throughput sequencing (ChIP-Seq)\nis getting popular to study genome-wide protein-DNA interactions. To\naddress the lack of powerful ChIP-Seq analysis method, we presented\nthe **M**odel-based **A**nalysis of **C**hIP-**S**eq (MACS), for\nidentifying transcript factor binding sites. MACS captures the\ninfluence of genome complexity to evaluate the significance of\nenriched ChIP regions and MACS improves the spatial resolution of\nbinding sites through combining the information of both sequencing tag\nposition and orientation. MACS can be easily used for ChIP-Seq data\nalone, or with a control sample with the increase of\nspecificity. Moreover, as a general peak-caller, MACS can also be\napplied to any \"DNA enrichment assays\" if the question to be asked is\nsimply: *where we can find significant reads coverage than the random\nbackground*.\n\nPlease find MACS3 documentations through [MACS3\nwebsite](https://macs3-project.github.io/MACS/).\n\n## Contribute\n\nPlease read our [CODE OF CONDUCT](CODE_OF_CONDUCT.md) and [How to\ncontribute](CONTRIBUTING.md) documents. If you have any questions,\nsuggestion/ideas, or just want to have conversions with developers and\nother users in the community, we recommend using the [MACS\nDiscussions](https://github.com/macs3-project/MACS/discussions)\ninstead of posting to our\n[Issues](https://github.com/macs3-project/MACS/issues) page.\n\n## Ackowledgement\n\nMACS3 project is sponsored by [![CZI's Essential Open Source Software for Science](https://chanzuckerberg.github.io/open-science/badges/CZI-EOSS.svg)](https://czi.co/EOSS). And we particularly want to thank the user community for their supports, feedbacks and contributions over the years.\n\n## Citation\n\n2008: [Model-based Analysis of ChIP-Seq\n(MACS)](https://genomebiology.biomedcentral.com/articles/10.1186/gb-2008-9-9-r137)\n\n",
    "readme_length": 3543
  },
  {
    "name": "snakepipes",
    "full_name": "maxplanck-ie/snakepipes",
    "description": "Customizable workflows based on snakemake and python for the analysis of NGS data",
    "stars": 397,
    "forks": 89,
    "language": "Python",
    "url": "https://github.com/maxplanck-ie/snakepipes",
    "topics": [
      "atac-seq",
      "bisulfite-sequencing",
      "chip-seq",
      "hi-c",
      "ngs",
      "rna-seq",
      "snakemake",
      "workflow"
    ],
    "created_at": "2016-03-23T17:23:31Z",
    "updated_at": "2025-11-14T14:24:18Z",
    "homepage": "http://snakepipes.readthedocs.io",
    "license": "N/A",
    "readme": "[![linux](https://github.com/maxplanck-ie/snakepipes/actions/workflows/linux.yml/badge.svg)](https://github.com/maxplanck-ie/snakepipes/actions/workflows/linux.yml)\n[![osx](https://github.com/maxplanck-ie/snakepipes/actions/workflows/osx.yml/badge.svg)](https://github.com/maxplanck-ie/snakepipes/actions/workflows/osx.yml)\n[![pytest](https://github.com/maxplanck-ie/snakepipes/actions/workflows/pytest.yml/badge.svg)](https://github.com/maxplanck-ie/snakepipes/actions/workflows/pytest.yml)\n[![readthedocs](https://readthedocs.org/projects/snakepipes/badge/?version=latest)](https://snakepipes.readthedocs.io/en/latest/)\n[![citation](https://zenodo.org/badge/54579435.svg)](https://zenodo.org/badge/latestdoi/54579435)\n\n# SnakePipes\n\nsnakePipes are flexible and powerful workflows built using [snakemake](https://github.com/snakemake/snakemake) that simplify the analysis of NGS data.\n![snakePipes](docs/content/images/snakePipes_small.png)\n\n## Workflows\n\n- DNAmapping*  \n- ChIPseq*  \n- mRNAseq*  \n- ncRNAseq*  \n- ATACseq*  \n- scRNAseq  \n- HiC  \n- makePairs*  \n- Whole Genome Bisulfite Seq/WGBS  \n\n(* also available in allele-specific mode)\n\n## Installation\n\n[Conda](https://docs.conda.io/en/latest/#) is a pre-requisite for snakePipes. So make sure this is [installed](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) before.\n\nAfterwards you can create a snakePipes environment containing the installation by running:\n\n > conda create -n snakepipes -c mpi-ie -c bioconda -c conda-forge snakePipes\n\nIn case you'd like a development version, you can install snakePipes directly from github using pip:\n\n > git clone git@github.com:maxplanck-ie/snakepipes.git  \n > cd snakepipes\n > pip install .\n\nMake sure the environment you are installing this version into has python version 3.11 or later.\n\nAfter the installation some configurations have to be set, for which we refer to the documentation.\n\n## Documentation\n\nFor detailed documentation on setup and usage, please visit the [documentation](https://snakepipes.readthedocs.io/en/latest/).\n\n## Citation\n\nIf you adopt/run snakePipes for your analysis, please cite it as follows :\n\nBhardwaj, Vivek, Steffen Heyne, Katarzyna Sikora, Leily Rabbani, Michael Rauer, Fabian Kilpert, Andreas S. Richter, Devon P. Ryan, and Thomas Manke. 2019. â€œsnakePipes: Facilitating Flexible, Scalable and Integrative Epigenomic Analysis.â€ Bioinformatics , May. [doi:10.1093/bioinformatics/btz436](https://doi.org/10.1093/bioinformatics/btz436).\n\n## Note\n\nSnakePipes are under active development. We appreciate your help in improving it further. Please use issues to the GitHub repository for feature requests or bug reports.\n",
    "readme_length": 2677
  },
  {
    "name": "reg-gen",
    "full_name": "CostaLab/reg-gen",
    "description": "Regulatory Genomics Toolbox: Python library and set of tools for the integrative analysis of high throughput regulatory genomics data.",
    "stars": 112,
    "forks": 28,
    "language": "Python",
    "url": "https://github.com/CostaLab/reg-gen",
    "topics": [
      "atac-seq",
      "bioinformatics",
      "chip-seq",
      "differential-peak-calling",
      "dnase-seq",
      "footprinting",
      "genomics",
      "motif",
      "motif-analysis",
      "ngs"
    ],
    "created_at": "2015-06-17T11:19:33Z",
    "updated_at": "2025-11-24T14:57:14Z",
    "homepage": "https://reg-gen.readthedocs.io/",
    "license": "Other",
    "readme": "[![Stars](https://img.shields.io/github/stars/CostaLab/reg-gen?logo=GitHub&color=yellow)](https://github.com/CostaLab/reg-gen/stargazers)\n[![PyPI](https://img.shields.io/pypi/v/rgt?logo=PyPI)](https://pypi.org/project/RGT/)\n[![PyPIDownloads](https://static.pepy.tech/badge/rgt)](https://static.pepy.tech/badge/rgt)\n[![Docs](https://readthedocs.org/projects/reg-gen/badge/?version=latest)](https://reg-gen.readthedocs.io)\n\n# RGT - Regulatory Genomics Toolbox\n\nRGT is an open source Python 3.6+ library for analysis of regulatory genomics. RGT is programmed in an oriented object fashion and its core classes provide functionality for handling regulatory genomics data.\n\nThe toolbox is made of a core library and several tools:\n\n* [HINT](https://reg-gen.readthedocs.io/en/latest/hint/introduction.html): ATAC-seq/DNase-seq footprinting method\n* [THOR](https://reg-gen.readthedocs.io/en/latest/thor/introduction.html):\nChIP-Seq differential peak caller\n* [Motif Analysis](https://reg-gen.readthedocs.io/en/latest/motif_analysis/introduction.html): TBFS match and enrichment\n* [RGT-Viz](https://reg-gen.readthedocs.io/en/latest/rgt-viz/introduction.html): Visualization tool\n* [TDF](https://reg-gen.readthedocs.io/en/latest/tdf/introduction.html): DNA/RNA triplex domain finder\n\nSee https://reg-gen.readthedocs.io for documentation and tutorials.\n\n# Installation with conda\n\nWe recommend using conda to manage the python environment to avoid issues.\n\nYou can install conda from [here](https://docs.conda.io/en/latest/miniconda.html)\n\nOnce you successfully installed conda, first create a specific environment:\n\n```shell\nconda create -n rgt python=3.9\n```\n\nThen activate your environment and install the full RGT suite with all other dependencies:\n```shell\nconda activate rgt\npip install RGT\n```\n\nDetailed installation instructions and basic problem solving can be found on our [website](https://reg-gen.readthedocs.io/en/latest/rgt/installation.html).\n\nPlease also consider citing our main paper if you used any sub-tools from RGT:\n```\n@article{li2023rgt,\n  title={RGT: a toolbox for the integrative analysis of high throughput regulatory genomics data},\n  author={Li, Zhijian and Kuo, Chao-Chung and Ticconi, Fabio and Shaigan, Mina and Gehrmann, Julia and Gusmao, Eduardo Gade and Allhoff, Manuel and Manolov, Martin and Zenke, Martin and Costa, Ivan G},\n  journal={BMC bioinformatics},\n  volume={24},\n  number={1},\n  pages={1--12},\n  year={2023},\n  publisher={BioMed Central}\n}\n```\n\n\n",
    "readme_length": 2483
  },
  {
    "name": "enrichment_analysis",
    "full_name": "epigen/enrichment_analysis",
    "description": "A Snakemake workflow and MrBiomics module for performing genomic region set and gene set enrichment analyses using LOLA, GREAT, GSEApy, pycisTarget and RcisTarget.",
    "stars": 71,
    "forks": 3,
    "language": "Python",
    "url": "https://github.com/epigen/enrichment_analysis",
    "topics": [
      "atac-seq",
      "bioinformatics",
      "biomedical-data-science",
      "chip-seq",
      "enrichment-analysis",
      "gene-set-enrichment",
      "gene-sets",
      "genomic-regions",
      "motif-enrichment-analysis",
      "pipeline",
      "rna-seq",
      "snakemake",
      "visualization",
      "workflow"
    ],
    "created_at": "2021-06-16T14:38:00Z",
    "updated_at": "2025-11-20T13:58:16Z",
    "homepage": "https://epigen.github.io/enrichment_analysis/",
    "license": "MIT License",
    "readme": "[![MrBiomics](https://img.shields.io/badge/MrBiomics-red)](https://github.com/epigen/MrBiomics/)\n[![DOI](https://zenodo.org/badge/377527671.svg)](https://zenodo.org/badge/latestdoi/377527671)\n[![](https://tokei.rs/b1/github/epigen/enrichment_analysis?category=code)]() \n[![](https://tokei.rs/b1/github/epigen/enrichment_analysis?category=files)]()\n[![GitHub license](https://img.shields.io/github/license/epigen/enrichment_analysis)](https://github.com/epigen/enrichment_analysis/blob/master/LICENSE)\n![GitHub Release](https://img.shields.io/github/v/release/epigen/enrichment_analysis)\n[![Snakemake](https://img.shields.io/badge/Snakemake->=8.20.1-green)](https://snakemake.readthedocs.io/en/stable/)\n\n# Genomic Region & Gene Set Enrichment Analysis & Visualization Workflow for Human and Mouse Genomes.\n\nA [Snakemake 8](https://snakemake.readthedocs.io/en/stable/) workflow for enrichment analysis and visualization of **human (hg19 or hg38) or mouse (mm9 or mm10)** based genomic region sets and (ranked) gene sets. Together with the respective background region/gene sets, the enrichment within the configured databases is determined using LOLA, GREAT, GSEApy (over-representation analysis (ORA) & preranked GSEA), pycisTarget, RcisTarget and results saved as CSV files. Additionally, the most significant results are plotted for each region/gene set, database queried, and analysis performed. Finally, the results within the same \"group\" (e.g.,  stemming from the same analysis) are aggregated per database and analysis in summary CSV files and visualized using hierarchically clustered heatmaps and bubble plots. For collaboration, communication and documentation of results, methods and workflow information a detailed self-contained HTML report can be generated.\n\n> [!NOTE]  \n> This workflow adheres to the module specifications of [MrBiomics](https://github.com/epigen/MrBiomics), an effort to augment research by modularizing (biomedical) data science. For more details, instructions, and modules check out the project's repository.\n>\n> â­ï¸ **Star and share modules you find valuable** ðŸ“¤ - help others discover them, and guide our focus for future work!\n\n> [!IMPORTANT]  \n> **If you use this workflow in a publication, please don't forget to give credit to the authors by citing it using this DOI [10.5281/zenodo.7810621](https://doi.org/10.5281/zenodo.7810621).**\n\n![Workflow Rulegraph](./workflow/dags/rulegraph.svg)\n\n# ðŸ–‹ï¸ Authors\n- [Stephan Reichl](https://github.com/sreichl)\n- [Daria Romanovskaia](https://github.com/dariarom94)\n- [Rob ter Horst](https://github.com/rubbert)\n- [Christoph Bock](https://github.com/chrbock)\n\n# ðŸ’¿ Software\nThis project wouldn't be possible without the following software and their dependencies:\n\n| Software       | Reference (DOI)                                   |\n| :------------: | :-----------------------------------------------: |\n| Enrichr        | https://doi.org/10.1002/cpz1.90                   |\n| ggplot2        | https://ggplot2.tidyverse.org/                    |\n| GREAT          | https://doi.org/10.1371/journal.pcbi.1010378      |\n| GSEA           | https://doi.org/10.1073/pnas.0506580102           |\n| GSEApy         | https://doi.org/10.1093/bioinformatics/btac757    |\n| LOLA           | https://doi.org/10.1093/bioinformatics/btv612     |\n| pandas         | https://doi.org/10.5281/zenodo.3509134            |\n| pheatmap       | https://cran.r-project.org/package=pheatmap       |\n| pycisTarget    | https://doi.org/10.1038/s41592-023-01938-4        |\n| RcisTarget     | https://doi.org/10.1038/nmeth.4463                |\n| rGREAT         | https://doi.org/10.1093/bioinformatics/btac745    |\n| Snakemake      | https://doi.org/10.12688/f1000research.29032.2    |\n\n# ðŸ”¬ Methods\nThis is a template for the Methods section of a scientific publication and is intended to serve as a starting point. Only retain paragraphs relevant to your analysis. References [ref] to the respective publications are curated in the software table above. Versions (ver) have to be read out from the respective conda environment specifications (`workflow/envs/*.yaml file`) or post-execution in the result directory (`enrichment_analysis/envs/*.yaml`). Parameters that have to be adapted depending on the data or workflow configurations are denoted in squared brackets e.g., [X].\n\nThe outlined analyses were performed using the programming languages R (ver) [ref] and Python (ver) [ref] unless stated otherwise. All approaches statistically correct their results using expressed/accessible background genomic region/gene sets from the respective analyses that yielded the query region/gene sets.\n\n**Genomic region set enrichment analyses**\n\n**LOLA.** Genomic region set enrichment analysis was performed using LOLA (ver) [ref], which uses Fisherâ€™s exact test. The following databases were queried [lola_databases].\n\n**GREAT.** Genomic region set enrichment analysis was performed using GREAT [ref] implemented with rGREAT (ver) [ref]. The following databases were queried [local_databases].\n\n**pycisTarget.** Genomic region set TFBS (Transcription Factor Binding Site) motif enrichment analysis was performed using pycisTarget (ver) [ref]. The following databases were queried [pycisTarget_databases].\n\nFurthermore, genomic regions (query- and background-sets) were mapped to genes using GREAT (without background) and then analyzed as gene sets as described below for a complementary and extended perspective.\n\n**Gene set enrichment analyses (GSEA)**\n\n**Over-representation analysis (ORA).** Gene set ORA was performed using Enrichr [ref], which uses Fisherâ€™s exact test (i.e., hypergeometric test), implemented with GSEApy's (ver) [ref] function _enrich_. The following databases were queried [local_databases].\n\n**Preranked GSEA.** Preranked GSEA was performed using GSEA [ref], implemented with GSEApy's (ver) [ref] function _prerank_. The following databases were queried [local_databases].\n\n**RcisTarget.** Gene set TFBS (Transcription Factor Binding Site) motif enrichment analysis was performed using RcisTarget (ver) [ref]. The following databases were queried [RcisTarget_databases].\n\n**Aggregation**\nThe results of all queries belonging to the same analysis [group] were aggregated by method and database. Additionally, we filtered the results by retaining only the union of terms that were statistically significant (i.e. [adj_pvalue]<=[adjp_th]) in at least one query.\n\n**Visualization**\nAll analysis results were visualized in the same way.\n\nFor each query, method and database combination an enrichment dot plot was used to visualize the most important results.  The top [top_n] terms were ranked (along the y-axis) by the mean rank of statistical significance ([p_value]), effect-size ([effect_size]), and overlap ([overlap]) with the goal to make the results more balanced and interpretable. The significance (adjusted p-value) is denoted by the dot color, effect-size by the x-axis position, and overlap by the dot size.\n\nThe aggregated results per analysis [group], method and database combination were visualized using hierarchically clustered heatmaps and bubble plots. The union of the top [top_terms_n] most significant terms per query were determined and their effect-size and significance were visualized as hierarchically clustered heatmaps, and statistical significance ([adj_pvalue] < [adjp_th]) was denoted by \\*. Furthermore, a hierarchically clustered bubble plot encoding both effect-size (color) and statistical significance (size) is provided, with statistical significance denoted by \\*. All summary visualizationsâ€™ values were capped by [adjp_cap]/[or_cap]/[nes_cap] to avoid shifts in the coloring scheme caused by outliers.\n\n**The analysis and visualizations described here were performed using a publicly available Snakemake (ver) [ref] workflow [[10.5281/zenodo.7810621](https://doi.org/10.5281/zenodo.7810621)].**\n\n\n# ðŸš€ Features\nThe five tools LOLA, GREAT, pycisTarget, RcisTarget and GSEApy (over-representation analysis (ORA) & preranked GSEA) are used for various enrichment analyses. Databases to be queried can be configured (see `./config/config.yaml`). All approaches statistically correct their results using the provided background region/gene sets.\n- enrichment analysis methods\n    - **region set** (`\\*.bed`)\n        - [LOLA](http://bioconductor.org/packages/release/bioc/html/LOLA.html): Genomic Locus Overlap Enrichment Analysis is run locally using configured databases (`lola_databases`) taken from [LOLA Region Databases](https://databio.org/regiondb) or custom created using these [instructions](https://databio.org/regiondb#:~:text=Build%20your%20own%20custom%20database)\n        - [GREAT](https://doi.org/10.1371/journal.pcbi.1010378) using [rGREAT](http://bioconductor.org/packages/release/bioc/html/rGREAT.html): Genomic Regions Enrichment of Annotations Tool runs locally using configured databases (`local_databases`), additional resources are downloaded automatically during the analysis.\n        - [pycisTarget](https://pycistarget.readthedocs.io/en/latest/): Motif enrichment analysis in region sets to identify high confidence transcription factor (TF) cistromes is run locally using configured databases (`pycistarget_parameters:databases`) from the [cisTarget resources](https://resources.aertslab.org/cistarget/).\n    - **gene set** (`\\*.txt`) over-representation analysis (ORA_GSEApy)\n        - [GSEApy](https://gseapy.readthedocs.io/en/latest/) enrich() function performs Fisherâ€™s exact test (i.e., hypergeoemtric test) and is run locally using configured databases (`local_databases`).\n        - [RcisTarget](https://www.bioconductor.org/packages/release/bioc/html/RcisTarget.html): Motif enrichment analysis in gene sets to identify high confidence transcription factor (TF) cistromes is run locally using configured databases (`Rcistarget_parameters:databases`) from the [cisTarget resources](https://resources.aertslab.org/cistarget/).\n    - **region-based gene set** (`\\*.bed`) over-representation analysis (ORA_GSEApy) & TFBS motif enrichment analysis (RcisTarget)\n        - region-gene associations for each query and background region set are obtained using (r)GREAT, without accounting for background for improved performance and more genes. Correction for background is anyway included in the gene-based analyses downstream.\n        - they are used for a complementary ORA using GSEApy and TFBS motif enrichment analysis using RcisTarget.\n        - thereby an additional enrichment perspective for region sets can be gained through association to genes by querying the same and/or more databases, that are not supported/provided by region-based tools.\n    - **preranked gene set** (`\\*.csv`) enrichment analysis (preranked_GSEApy)\n        - [GSEApy](https://gseapy.readthedocs.io/en/latest/) prerank() function performs [preranked GSEA](https://doi.org/10.1073/pnas.0506580102) and is run locally using configured databases (`local_databases`).\n        - Note: only entries with the largest absolute score are kept and +/- infinity values are set to max/min, respectively.\n- **databases** have to be provided by the user\n    - databases (`local_databases`) for [rGREAT](http://bioconductor.org/packages/release/bioc/html/rGREAT.html) and [GSEApy](https://gseapy.readthedocs.io/en/latest/)\n      - local GMT (`\\*.gmt`) files e.g., from [MSigDB](http://www.gsea-msigdb.org/gsea/msigdb) or [Enrichr](https://maayanlab.cloud/Enrichr/#libraries)\n      - local (custom) JSON (`\\*.json`) files e.g., `{ \"MyDB_Term1\": [\"geneA\",\"geneB\",\"geneC\"],\"MyDB_Term2\": [\"geneX\",\"geneY\",\"geneZ\"]}`\n      - always use gene symbols e.g., STAT1\n      - the local databases are (converted,) copied and saved as GMT files in /resources.\n    - LOLA databases for [LOLA](http://bioconductor.org/packages/release/bioc/html/LOLA.html)\n      - downloaded from [LOLA Region Databases](https://databio.org/regiondb)\n      - custom databases created using these [instructions]\n      - pre-cached databases as .RData files are supported by simpleCache\n    - cisTarget databases for [pycisTarget](https://pycistarget.readthedocs.io/en/latest/) and [RcisTarget](https://www.bioconductor.org/packages/release/bioc/html/RcisTarget.html)\n      - downloaded from the [cisTarget resources](https://resources.aertslab.org/cistarget/)\n      - custom databases using these [instructions](https://github.com/aertslab/create_cisTarget_databases)\n- **group aggregation** of results per method and database\n    - results of all queries belonging to the same group are aggregated per method (e.g., ORA_GSEApy) and database (e.g., GO_Biological_Process_2021) by concatenation and saved as a long-format table (CSV).\n    - a filtered version taking the union of all statistically significant (i.e., adjusted p-value <`{adjp_th}`) terms per query is also saved as a long-format table (CSV).\n- **visualization**\n    - region/gene set specific enrichment dot plots are generated for each query, method and database combination\n        - the top `{top_n}` terms are ranked (along the y-axis) by the mean rank of statistical significance (`{p_value}`), effect-size (`{efect_size}` e.g., log2(odds ratio) or normalized enrichemnt scores), and overlap (`{overlap}` e.g., coverage or support) with the goal to make the ranking more balanced and interpretable\n        - significance (adjusted p-value) is presented by the dot color\n        - effect-size is presented by the x-axis position\n        - overlap is presented by the dot size\n    - group summary/overview\n        - the union of the top `{top_terms_n}` most significant terms per query, method, and database within a group is determined. \n        - their effect-size (effect) and statistical significance (adjp) are visualized as hierarchically clustered heatmaps, with statistical significance denoted by `\\*` (PDF).\n        - a hierarchically clustered bubble plot encoding both effect-size (color) and significance (size) is provided, with statistical significance denoted by `\\*` (PNG).\n        - all summary visualizations are configured to cap the values (`{adjp_cap}`/`{or_cap}`/`{nes_cap}`) to avoid shifts in the coloring scheme caused by outliers.\n- **results** (`{result_path}/enrichment_analysis`)\n    - the result directory contains a folder for each region/gene set `{query}` and `{group}`\n    - `{query}/{method}/{database}/` containing:\n        - result table (CSV): `{query}\\_{database}.csv`\n        - enrichment dot plot (PNG): `{query}\\_{database}.{png}`\n    - `{group}/{method}/{database}/` containing\n        - aggregated result table (CSV): `{group}\\_{database}\\_all.csv`\n        - filtered aggregated result table (CSV): `{group}\\_{database}\\_sig.csv`\n        - hierarchically clustered heatmaps visualizing statistical significance and effect-sizes of the top `{top_terms_n}` terms (PDF): `{group}\\_{database}\\_{adjp|effect}\\_heatmap.pdf`\n        - hierarchically clustered bubble plot visualizing statistical significance and effect-sizes simultaneously (PNG):  `{group}\\_{database}\\_summary.{png}`\n\nNote:\n- Despite usage of the correct parameter, **rGREAT** was not using the provided cores during testing. Nevertheless, it is still provided as parameter.\n- **(r)GREAT** performs [two statistical test](https://great-help.atlassian.net/wiki/spaces/GREAT/pages/655456/Statistics) (binomial and hypergeometric), results of both are reported and should be considered. Which results are used for the visualization can be configured in `column_names:GREAT`.\n- **pycisTarget** for region set enrichment does not use the provided background regions. In case this is desired (e.g., conensus regions or TF ChIP-seq data) follow the [instructions for custom cisTarget databases](https://github.com/aertslab/create_cisTarget_databases) using your own regions and use them as database.\n\n# ðŸ› ï¸ Usage\nHere are some tips for the usage of this workflow:\n1. Download all relevant databases (see [Resources](#resources)).\n2. Configure the analysis using the configuration YAML and an annotation file (see [Configuration](#configuration))\n3. Run the analysis on every query gene/region set of interest (e.g., results of differential analyses) with the respective background genes/regions (e.g., all expressed genes or consensus regions).\n4. generate the [Snakemake Report](https://snakemake.readthedocs.io/en/stable/snakefiles/reporting.html)\n5. look through the overview plots of your dedicated groups and queried databases in the report\n6. dig deeper by looking at the \n    - aggregated result table underlying the summary/overview plot\n    - enrichment plots for the individual query sets\n7. investigate interesting hits further by looking into the individual query result tables.\n\n# âš™ï¸ Configuration\nDetailed specifications can be found here [./config/README.md](./config/README.md)\n\n# ðŸ§¬ How to convert feature lists to BED files\nThis enrichment analysis workflow requires **genomic regions** to be in the standard **`BED`** format (`chromosome`, `start`, `end`, `name`). However, upstream differential analysis workflows (e.g., using `limma`) often produce a simple text file containing only a list of significant feature IDs (e.g., `peak_1024`, `peak_5531`). To use these results, you must first convert this list of IDs into a valid `BED` file by mapping each ID to its genomic coordinates.\n\nWe provide a Python helper script, [`features_to_bed.py`](./helpers/features_to_bed.py), and a Snakemake rule to automate this crucial step. Simply adapt them to your setup. Both take two files as input:\n1.  Your list of significant feature IDs (e.g., `features.txt`).\n2.  An annotation file that contains the genomic coordinates for *all* features in your experiment (e.g., `consensus_annotation.csv`).\n\nBoth implementations merge these files, finds the coordinates for your significant features, and saves them in the required `BED` format. In the Snakemake rule `input` and `output` paths can be replaced with wildcards to fit your analysis.\n\n```python\nrule convert_features2bed:\n    input:\n        consensus_annotation = \"path/to/consensus_annotation.csv\",\n        features_txt = \"path/to/dea_limma/features.txt\",\n    output:\n        features_bed = \"path/to/features.bed\",\n    params:\n        region_col = \"peak_id\",\n        chr_col = \"gencode_chr\",\n        start_col = \"gencode_start\",\n        end_col = \"gencode_end\",\n    run:\n        # load files as pandas df\n        consensus_df = pd.read_csv(input.consensus_annotation)\n        features_df = pd.read_csv(input.features_txt, header=None, names=[params.region_col])\n        # map using params\n        merged_df = pd.merge(features_df, consensus_df, on=params.region_col, how=\"inner\")\n        # Select and order the columns required for the BED file format.\n        bed_df = merged_df[[params.chr_col, params.start_col, params.end_col, params.region_col]]\n        # save in BED format\n        bed_df.to_csv(output.features_bed, sep=\"\\t\", header=False, index=False)\n```\n\n# ðŸ“– Examples\nExplore detailed examples showcasing module usage in our comprehensive end-to-end [MrBiomics Recipes](https://github.com/epigen/MrBiomics?tab=readme-ov-file#-recipes), including data, configuration, annotation and results:\n- [ATAC-seq Analysis Recipe](https://github.com/epigen/MrBiomics/wiki/ATAC%E2%80%90seq-Analysis-Recipe)\n- [RNA-seq Analysis Recipe](https://github.com/epigen/MrBiomics/wiki/RNA%E2%80%90seq-Analysis-Recipe)\n- [Integrative Analysis Recipe](https://github.com/epigen/MrBiomics/wiki/Integrative-Analysis-Recipe)\n- [scRNA-seq Analysis Recipe](https://github.com/epigen/MrBiomics/wiki/scRNA%E2%80%90seq-Analysis-Recipe)\n- [scCRISPR-seq Analysis Recipe](https://github.com/epigen/MrBiomics/wiki/scCRISPR%E2%80%90seq-Analysis-Recipe)\n\nFurthermore, we provide four example queries across all tools with four different databases:\n- three are region sets from a [LOLA Vignette](http://code.databio.org/LOLA/articles/usingLOLACore.html). Download the example data by following the instructions below.\n- one is a preranked gene-score set derived from the GDS289 [fgsea R package example data](https://github.com/ctlab/fgsea/blob/master/inst/extdata/GDS289.tsv) (`score=-log10(p-value) * sign(LFC)`).\n- the total runtime was ~23 minutes on an HPC with 1 core and 32GB RAM per job.\n- note: we are using a hg38 database for pycistarget, because the respective hg19 database is not compatible with the current pycisTarget version (https://github.com/aertslab/pycistarget/issues/37).\n\nFollow these steps to run the complete analysis:\n1. Download all necessary data (query and resources)\n    ```sh\n    # change working directory to the cloned worklfow/module enrichment_analysis\n    cd enrichment_analysis\n\n    # download and extract the region set test data\n    wget -c http://cloud.databio.org.s3.amazonaws.com/vignettes/lola_vignette_data_150505.tgz -O - | tar -xz -C test/data/\n\n    # create and enter resources folder\n    mkdir resources\n    cd resources\n\n    # download LOLACore databases and move to the correct location\n    wget http://big.databio.org/regiondb/LOLACoreCaches_180412.tgz\n    tar -xzvf LOLACoreCaches_180412.tgz\n    mv nm/t1/resources/regions/LOLACore/ .\n    rm -rf nm\n\n    # download a local database\n    wget https://data.broadinstitute.org/gsea-msigdb/msigdb/release/2023.2.Hs/c2.cgp.v2023.2.Hs.symbols.gmt\n\n    # download cisTarget resources\n    mkdir cistarget\n    cd cistarget\n    wget https://resources.aertslab.org/cistarget/databases/homo_sapiens/hg38/refseq_r80/mc_v10_clust/gene_based/hg38_500bp_up_100bp_down_full_tx_v10_clust.genes_vs_motifs.rankings.feather\n    wget https://resources.aertslab.org/cistarget/databases/homo_sapiens/hg38/screen/mc_v10_clust/region_based/hg38_screen_v10_clust.regions_vs_motifs.rankings.feather\n    wget https://resources.aertslab.org/cistarget/motif2tf/motifs-v10nr_clust-nr.hgnc-m0.001-o0.0.tbl\n\n    # change your working directory back to the root of the module\n    cd ../../\n    ```\n2. activate your conda Snakemake environment, run a dry-run (-n flag), run the workflow and generate the report using the provided configuration\n    ```sh\n    conda activate snakemake\n    # dry-run\n    snakemake -p --use-conda --configfile test/config/example_enrichment_analysis_config.yaml -n\n    # real run\n    snakemake -p --use-conda --configfile test/config/example_enrichment_analysis_config.yaml\n    # report\n    snakemake --report test/report.html --configfile test/config/example_enrichment_analysis_config.yaml\n    ```\n\n# ðŸ”— Links\n- [GitHub Repository](https://github.com/epigen/enrichment_analysis/)\n- [GitHub Page](https://epigen.github.io/enrichment_analysis/)\n- [Zenodo Repository](https://doi.org/10.5281/zenodo.7810621)\n- [Snakemake Workflow Catalog Entry](https://snakemake.github.io/snakemake-workflow-catalog?usage=epigen/enrichment_analysis)\n\n# ðŸ“š Resources\n- Helper scripts\n  - [Combine all text files within a specified folder into one JSON to be used as database.](./helpers/txts_to_json_database.py)\n  - [Generate a file listing CSV of the current folder as basis for the annotation file.](./helpers/feature_list_to_csv.sh)\n  - [Convert feature lists to BED files for genomic region enrichment analysis.](./helpers/features_to_bed.py)\n- Recommended compatible [MrBiomics](https://github.com/epigen/MrBiomics) modules for upstream processing and analyses:\n    - [ATAC-seq Processing](https://github.com/epigen/atacseq_pipeline) to quantify chromatin accessibility.\n    - [scRNA-seq Data Processing & Visualization](https://github.com/epigen/scrnaseq_processing_seurat) for processing (multimodal) single-cell transcriptome data.\n    - [<ins>Sp</ins>lit, F<ins>ilter</ins>, Norma<ins>lize</ins> and <ins>Integrate</ins> Sequencing Data](https://github.com/epigen/spilterlize_integrate/) after count quantification.\n    - [Differential Analysis with limma](https://github.com/epigen/dea_limma) to identify and visualize statistically significantly different features (e.g., genes or genomic regions) between sample groups.\n    - [Differential Analysis using Seurat](https://github.com/epigen/dea_seurat) to identify and visualize statistically significantly different features (e.g., genes or proteins) between groups.\n- Package for [simplifying enrichment](http://www.bioconductor.org/packages/release/bioc/html/simplifyEnrichment.html) results using the [ComplexHeatmap](https://bioconductor.org/packages/release/bioc/html/ComplexHeatmap.html) package.\n- Web versions of some of the used tools\n    - [LOLA](http://lolaweb.databio.org/)\n    - [GREAT](http://great.stanford.edu/public/html/index.php)\n    - [Enrichr](https://maayanlab.cloud/Enrichr/)\n- Databases & resources for region/gene sets\n    - [LOLA region databases](https://databio.org/regiondb)\n      - [custom LOLA database instructions](https://databio.org/regiondb#:~:text=Build%20your%20own%20custom%20database)\n    - [cisTarget resources](https://resources.aertslab.org/cistarget/)\n      - [custom cistarget database instructions](https://github.com/aertslab/create_cisTarget_databases)\n    - [Enrichr gene set databases](https://maayanlab.cloud/Enrichr/#libraries)\n    - [The Molecular Signatures Database (MSigDB)](https://www.gsea-msigdb.org/gsea/msigdb/)\n\n# ðŸ“‘ Publications\nThe following publications successfully used this module for their analyses.\n- [Datlinger, Pankevich, Arnold et al. (2025) Nature - Systematic discovery of CRISPR-boosted CAR T cell immunotherapies](https://www.nature.com/articles/s41586-025-09507-9)\n- [Traxler, Reichl et al. (2025) Cell Systems - Integrated time-series analysis and high-content CRISPR screening delineate the dynamics of macrophage immune regulation](https://doi.org/10.1016/j.cels.2025.101346)\n\n# â­ Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=epigen/enrichment_analysis&type=Date)](https://star-history.com/#epigen/enrichment_analysis&Date)\n",
    "readme_length": 25842
  },
  {
    "name": "PePr",
    "full_name": "shawnzhangyx/PePr",
    "description": "a peak-calling and differential analysis tool for replicated ChIP-Seq data",
    "stars": 37,
    "forks": 9,
    "language": "Python",
    "url": "https://github.com/shawnzhangyx/PePr",
    "topics": [],
    "created_at": "2014-06-18T18:15:48Z",
    "updated_at": "2025-11-17T08:39:42Z",
    "homepage": null,
    "license": "GNU General Public License v3.0",
    "readme": "\n# PePr v1.1.10 or newer \n\n### Introduction\nPePr is a ChIP-Seq Peak-calling and Prioritization pipeline that uses a sliding window approach and models read counts across replicates and between groups with a negative binomial distribution. PePr empirically estimates the optimal shift/fragment size and sliding window width, and estimates dispersion from the local genomic area. Regions with less variability across replicates are ranked more favorably than regions with greater variability. Optional post-processing steps are also made available to filter out peaks not exhibiting the expected shift size and/or to narrow the width of peaks.\n\n### Installation\n1. Make sure your python version is higher than 2.6. Version 3.X may not be fully supported yet.\n2. Install **pip** in your system if you don't have it. \n3. `pip install PePr` or `pip install PePr --user`(if you don't have administrator privilege). Optionally, you can download tarball (PePr-[version].tar.gz) from [github](https://github.com/shawnzhangyx/PePr/) and install using `pip install PePr-[version].tar.gz`\n4. If installation is successful, you could directly invoke the script by typing `PePr`. A help message will show up. \n\n### Supported File Formats\n* Single-end: BED, BAM, SAM. **For an example of BED format, please see https://genome.ucsc.edu/FAQ/FAQformat.html#format1. BED files need to have at least 6 columns (including chromsome name[1], start[2], end[3], and strand[6]. To estimate shift size (default), you need to have reads from both \"+\" and \"-\" strand\".)**\n* Paired-end: BAM, SAM. The files must be sorted by the read names. Users can use `samtools sort -n sample.bam sample.sorted_by_name` to sort the file. \n\n### Scripts to call PePr\nThe following scripts are available after the installation and can be called directly from bash console/terminal.\n* **PePr**: the main PePr function. Will run the entire pipline including the parameter estimation and statistical testing. *This does not include post-processing step.*\n* **PePr-preprocess**: PePr preprocess function. Will only run the parameter estimation step and output a parameter file for the user to use. Users could use/modify the parameter file and then run **PePr** main function with it. \n* **PePr-postprocess**: PePr postprocess function. This function will take PePr peaks and remove suspected false positives using the peak shapes. The ChIP and input files of the group where the peaks from are required for this function. For example, chip1 and input1 files will be needed for chip1_peaks. This function loads all of the reads into memory, so if you don't have enough memory on your machine, you don't have to give PePr all ChIP/input files. 2 to 3 files might suffice under certain circumstances. \n\n### Basic Usage Examples\n*Warning: These are working examples with minimal required parameters. For the best performance (or to avoid bad fitting) on your data, please read this manual carefully and choose the right parameters.*\n\n* For peak-calling, run: \n```\nPePr -c chip_rep1.bam,chip_rep2.bam -i input_rep1.bam,input_rep2.bam -f bam\n```\n* For differential binding analysis with input samples, run: \n```\nPePr -c chip1_rep1.bam,chip1_rep2.bam -i input1_rep1.bam,input1_rep2.bam --chip2 chip2_rep1.bam,chip2_rep2.bam --input2 input2_rep2.bam,input2_rep2.bam -f bam --diff\n```\n* For differential binding analysis without input samples, run: \n```\nPePr -c chip1_rep1.bam,chip1_rep2.bam --chip2 chip2_rep1.bam,chip2_rep2.bam -f bam --diff\n```\n* To use a parameter file, run: `PePr -p parameter_file.txt`. For how to write a parameter file, see the section `Parameter File` below. \n\n### Parameters\n| Parameter|Description|\n|:---|:---|\n|*-p/--parameter-file*|Use parameter file instead of command line options. Using a parameter file will ignore all other command line options. See the next section for parameter file configuration.|\n|*-i/--input1*|Group 1 input files. Multiple file names are separated by comma, e.g. input1.bam,input2.bam. you can also specify relative path to the file names, like folder1/input1.bam,folder2/input2.bam,folder3/input3.bam|\n|*-c/--chip1* |Group 1 ChIP files. |\n|*--input2*|Group 2 input files. Use in differential binding analysis.|\n|*--chip2*|Group 2 ChIP files. Use in differential binding analysis.|\n|*-n/--name*|Experiment name. It will be prefix to all output files from PePr. Default: \"NA\"|\n|*-f/--file-format*|Read file format. Currently support bed, sam, bam, sampe (sam paired-end), bampe (bam paired-end) |\n|*-s/--shiftsize*|Half the fragment size. The number of bases to shift forward and reverse strand reads toward each other. If not specified by user, PePr will empirically estimate this number from the data for each ChIP sample.|\n|*-w/--windowsize*| Sliding window size. If not specified by user, PePr will estimate this by calculating the average width of potential peaks. The lower and upper bound for PePr estimate is 100bp and 1000bp. User provided window size is not constrained, but we recommend to stay in this range (100-1000bp).|\n|*--diff*|Tell PePr to perform differential binding analysis.|\n|*--threshold*| p-value cutoff. Default:1e-5.|\n|*--peaktype*| sharp or broad. Default is broad. PePr treats broad peaks (like H3k27me3) and sharp peaks(like most transcriptions factors) slightly different. Specify this option if you know the feature of the peaks.|\n|*--normalization*|inter-group, intra-group, scale, or no. Default is intra-group for peak-calling and inter-group for differential binding analysis. PePr is using a modified TMM method to normalize for the difference in IP efficiencies between samples (see the supplementary methods of the paper). It is making an implicit assumption that there is substantial overlap of peaks in every sample. However, it is sometimes not true between groups (for example, between TF ChIP-seq and TF knockout). So for differential binding analysis, switch to intra-group normalization. *scale* is simply scaling the reads so the total library sizes are the same. *no* normalization will not do normalization. |\n|*--keep-max-dup*|maximum number of duplicated reads at *each single position* to keep. If not specified, will not remove any duplicate.|\n|*--num-processors*|Number of CPUs to run in parallel.|\n|*--input-directory*|where the data files are. The path specified here will be a prefix added to each of the files. The best practice is to always use absolute path in here.|\n|*--output-directory*|where you want the output files to be. PePr will add this path as a prefix to the output files. It is recommended to use the absolute path.|\n|*--version*|Will show the version number and exit.|\n\n### Parameter File\nThe parameter file is an easier way of running PePr by including the running parameters in one file. It is effectively the same as running from the command line. A basic example is provided below: \n```\n#filetype       filename\nchip1   chip_rep1.bed\nchip1   chip_rep2.bed\ninput1  input_rep1.bed\ninput1  input_rep2.bed\nfile-format     bed\npeaktype     broad\ndifftest     FALSE\nkeep-max-dup 2\nthreshold     1e-5\nname    test\n```\nPePr will also output a complete parameter file for you to keep a record of your running parameters and produce the same results. \n### Output Files\n* **NAME__PePr_peaks.bed**: A tab-delimited file containing chromosomal position of the peak, name, signal value, fold change, p-value and Benjamini-Hochberg FDR. Peak format is same as the [ENCODE BroadPeak](https://genome.ucsc.edu/FAQ/FAQformat.html#format13) format. \n* **NAME__PePr_[chip1/2]_peaks.bed**: this is the same as above, but only available when you run in differential binding mode. \"chip1_peaks\" are enriched in chip1, \"chip2_peaks* are enriched in chip2. \n* **NAME__PePr_parameters.txt**: A file containing the parameters to reproduce the results. \n* **NAME-Date-Time-SessionID-debug.log**: This file contains the detailed information about the running status. Useful debugging information contains: Chromosomes analyzed, shift size and window size estimation, number of candidate windows, etc.\n\n\n### Links\n* https://github.com/shawnzhangyx/PePr/ # Source code\n* https://renlab.sdsc.edu/omics/tags/PePr/ # PePr FAQ\n* https://pypi.python.org/pypi/pepr #PyPI package index\n\n### Questions?\nYou're also welcome to shoot me an e-mail at yanxiazh@umich.edu, I'll try replying to you as soon as possible. In the e-mail, please include **[1] a copy of your command/script to call PePr, [2] paramters.txt file, and [3] log file**. It will speed up the troubleshooting process. \n\n<!-- Questions can also be posted on https://renlab.sdsc.edu/omics or Github, you'll very likely find similar problems in there too (https://renlab.sdsc.edu/omics/tags/PePr/?tab=Summary).  -->\n\n\n### Cite PePr\nZhang Y, Lin YH, Johnson TD, Rozek LS, Sartor MA. [PePr: A peak-calling prioritization pipeline to identify consistent or differential peaks from replicated ChIP-Seq data.](http://www.ncbi.nlm.nih.gov/pubmed/24894502) Bioinformatics. 2014.\n",
    "readme_length": 8975
  },
  {
    "name": "atac-seq-pipeline",
    "full_name": "ENCODE-DCC/atac-seq-pipeline",
    "description": "ENCODE ATAC-seq pipeline",
    "stars": 436,
    "forks": 171,
    "language": "Python",
    "url": "https://github.com/ENCODE-DCC/atac-seq-pipeline",
    "topics": [],
    "created_at": "2017-08-23T18:05:49Z",
    "updated_at": "2025-11-27T17:05:55Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# ENCODE ATAC-seq pipeline\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.156534.svg)](https://doi.org/10.5281/zenodo.156534)[![CircleCI](https://circleci.com/gh/ENCODE-DCC/atac-seq-pipeline/tree/master.svg?style=svg)](https://circleci.com/gh/ENCODE-DCC/atac-seq-pipeline/tree/master)\n\n## Introduction\n\nThis pipeline is designed for automated end-to-end quality control and processing of ATAC-seq and DNase-seq data. The pipeline can be run on compute clusters with job submission engines as well as on stand alone machines. It inherently makes uses of parallelized/distributed computing. Pipeline installation is also easy as most dependencies are automatically installed. The pipeline can be run end-to-end, starting from raw FASTQ files all the way to peak calling and signal track generation using a single caper submit command. One can also start the pipeline from intermediate stages (for example, using alignment files as input). The pipeline supports both single-end and paired-end data as well as replicated or non-replicated datasets. The outputs produced by the pipeline include 1) formatted HTML reports that include quality control measures specifically designed for ATAC-seq and DNase-seq data, 2) analysis of reproducibility, 3) stringent and relaxed thresholding of peaks, 4) fold-enrichment and pvalue signal tracks. The pipeline also supports detailed error reporting and allows for easy resumption of interrupted runs. It has been tested on some human, mouse and yeast ATAC-seq datasets as well as on human and mouse DNase-seq datasets.\n\nThe ATAC-seq pipeline protocol specification is [here](https://docs.google.com/document/d/1f0Cm4vRyDQDu0bMehHD7P7KOMxTOP-HiNoIvL1VcBt8/edit?usp=sharing). Some parts of the ATAC-seq pipeline were developed in collaboration with Jason Buenrostro, Alicia Schep and Will Greenleaf at Stanford.\n\n## Issues with PE Fastqs downloaded from SRA\n\nRead names in PE Fastqs should be consistent across the files pair. Do not use `--readids` in `fastq-dump` so that reads in a pair have the same read name. Inconsitent read names (for example, `READNAME.1` in FQ1 and `READNAME.2` in FQ2) will result in an empty BAM error in a `filter` step.\n\n\n### Features\n\n* **Portability**: The pipeline run can be performed across different cloud platforms such as Google, AWS and DNAnexus, as well as on cluster engines such as SLURM, SGE and PBS.\n* **User-friendly HTML report**: In addition to the standard outputs, the pipeline generates an HTML report that consists of a tabular representation of quality metrics including alignment/peak statistics and FRiP along with many useful plots (IDR/TSS enrichment). An example of the [HTML report](https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR889WQX/example_output/qc.html). The [json file](https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR889WQX/example_output/qc.json) used in generating this report.\n* **Supported genomes**: Pipeline needs genome specific data such as aligner indices, chromosome sizes file and blacklist. We provide a genome database downloader/builder for hg38, hg19, mm10, mm9. You can also use this [builder](docs/build_genome_database.md) to build genome database from FASTA for your custom genome.\n\n## Installation\n\n1) Install Caper (Python Wrapper/CLI for [Cromwell](https://github.com/broadinstitute/cromwell)).\n\t```bash\n\t$ pip install caper\n\t```\n\n2) **IMPORTANT**: Read Caper's [README](https://github.com/ENCODE-DCC/caper/blob/master/README.md) carefully to choose a backend for your system. Follow the instruction in the configuration file.\n\t```bash\n\t# backend: local or your HPC type (e.g. slurm, sge, pbs, lsf). read Caper's README carefully.\n\t$ caper init [YOUR_BACKEND]\n\n\t# IMPORTANT: edit the conf file and follow commented instructions in there\n\t$ vi ~/.caper/default.conf\n\t```\n\n3) Git clone this pipeline.\n\t```bash\n\t$ cd\n\t$ git clone https://github.com/ENCODE-DCC/atac-seq-pipeline\n\t$ cd atac-seq-pipeline\n\t```\n\n4) Define test input JSON.\n\t```bash\n\tINPUT_JSON=\"https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ_subsampled.json\"\n\t```\n\n5) If you have Docker and want to run pipelines locally on your laptop. `--max-concurrent-tasks 1` is to limit number of concurrent tasks to test-run the pipeline on a laptop. Uncomment it if run it on a workstation/HPC.\n\t```bash\n\t# check if Docker works on your machine\n\t$ docker run ubuntu:latest echo hello\n\n\t# --max-concurrent-tasks 1 is for computers with limited resources\n\t$ caper run atac.wdl -i \"${INPUT_JSON}\" --docker --max-concurrent-tasks 1\n\t```\n\n6) Otherwise, install Singularity on your system. Please follow [this instruction](https://neuro.debian.net/install_pkg.html?p=singularity-container) to install Singularity on a Debian-based OS. Or ask your system administrator to install Singularity on your HPC.\n\t```bash\n\t# check if Singularity works on your machine\n\t$ singularity exec docker://ubuntu:latest echo hello\n\n\t# on your local machine (--max-concurrent-tasks 1 is for computers with limited resources)\n\t$ caper run atac.wdl -i \"${INPUT_JSON}\" --singularity --max-concurrent-tasks 1\n\n\t# on HPC, make sure that Caper's conf ~/.caper/default.conf is correctly configured to work with your HPC\n    # the following command will submit Caper as a leader job to SLURM with Singularity\n    $ caper hpc submit atac.wdl -i \"${INPUT_JSON}\" --singularity --leader-job-name ANY_GOOD_LEADER_JOB_NAME\n\n    # check job ID and status of your leader jobs\n    $ caper hpc list\n\n    # cancel the leader node to close all of its children jobs\n    # If you directly use cluster command like scancel or qdel then\n    # child jobs will not be terminated\n    $ caper hpc abort [JOB_ID]\n\t```\n\n7) (Optional Conda method) **WE DO NOT HELP USERS FIX CONDA DEPENDENCY ISSUES. IF CONDA METHOD FAILS THEN PLEASE USE SINGULARITY METHOD INSTEAD**. **DO NOT USE A SHARED CONDA. INSTALL YOUR OWN [MINICONDA3](https://docs.conda.io/en/latest/miniconda.html) AND USE IT.**\n\t```bash\n\t# check if you are not using a shared conda, if so then delete it or remove it from your PATH\n\t$ which conda\n\n\t# uninstall pipeline's old environments\n\t$ bash scripts/uninstall_conda_env.sh\n\n\t# install new envs, you need to run this for every pipeline version update.\n\t# it may be killed if you run this command line on a login node on HPC.\n\t# it's recommended to make an interactive node with enough resources and run it there.\n\t$ bash scripts/install_conda_env.sh\n\n\t# if installation fails please use Singularity method instead.\n\n\t# on your local machine (--max-concurrent-tasks 1 is for computers with limited resources)\n\t$ caper run atac.wdl -i \"${INPUT_JSON}\" --conda --max-concurrent-tasks 1\n\n\t# on HPC, make sure that Caper's conf ~/.caper/default.conf is correctly configured to work with your HPC\n    # the following command will submit Caper as a leader job to SLURM with Conda\n    $ caper hpc submit atac.wdl -i \"${INPUT_JSON}\" --conda --leader-job-name ANY_GOOD_LEADER_JOB_NAME\n\n    # check job ID and status of your leader jobs\n    $ caper hpc list\n\n    # cancel the leader node to close all of its children jobs\n    # If you directly use cluster command like scancel or qdel then\n    # child jobs will not be terminated\n    $ caper hpc abort [JOB_ID]\n\t```\n\n\n## Input JSON file specification\n\n> **IMPORTANT**: DO NOT BLINDLY USE A TEMPLATE/EXAMPLE INPUT JSON. READ THROUGH THE FOLLOWING GUIDE TO MAKE A CORRECT INPUT JSON FILE. ESPECIALLY FOR AUTODETECTING/DEFINING ADAPTERS.\n\nAn input JSON file specifies all the input parameters and files that are necessary for successfully running this pipeline. This includes a specification of the path to the genome reference files and the raw data fastq file. Please make sure to specify absolute paths rather than relative paths in your input JSON files.\n\n1) [Input JSON file specification (short)](docs/input_short.md)\n2) [Input JSON file specification (long)](docs/input.md)\n\n\n## Running and sharing on Truwl\n\nYou can run this pipeline on [truwl.com](https://truwl.com/). This provides a web interface that allows you to define inputs and parameters, run the job on GCP, and monitor progress. To run it you will need to create an account on the platform then request early access by emailing [info@truwl.com](mailto:info@truwl.com) to get the right permissions. You can see the example case from this repo at [https://truwl.com/workflows/instance/WF_e85df4.f10.8880/command](https://truwl.com/workflows/instance/WF_e85df4.f10.8880/command). The example job (or other jobs) can be forked to pre-populate the inputs for your own job.\n\nIf you do not run the pipeline on Truwl, you can still share your use-case/job on the platform by getting in touch at [info@truwl.com](mailto:info@truwl.com) and providing your inputs.json file.\n\n\n## Running on Terra/Anvil (using Dockstore)\n\nVisit our pipeline repo on [Dockstore](https://dockstore.org/workflows/github.com/ENCODE-DCC/atac-seq-pipeline). Click on `Terra` or `Anvil`. Follow Terra's instruction to create a workspace on Terra and add Terra's billing bot to your Google Cloud account.\n\nDownload this [test input JSON for Terra](https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ_subsampled.terra.json) and upload it to Terra's UI and then run analysis.\n\nIf you want to use your own input JSON file, then make sure that all files in the input JSON are on a Google Cloud Storage bucket (`gs://`). URLs will not work.\n\n\n## Running on DNAnexus (using Dockstore)\n\nSign up for a new account on [DNAnexus](https://platform.dnanexus.com/) and create a new project on either AWS or Azure. Visit our pipeline repo on [Dockstore](https://dockstore.org/workflows/github.com/ENCODE-DCC/atac-seq-pipeline). Click on `DNAnexus`. Choose a destination directory on your DNAnexus project. Click on `Submit` and visit DNAnexus. This will submit a conversion job so that you can check status of it on `Monitor` on DNAnexus UI.\n\nOnce conversion is done download one of the following input JSON files according to your chosen platform (AWS or Azure) for your DNAnexus project:\n- AWS: https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ_subsampled_dx.json\n- Azure: https://storage.googleapis.com/encode-pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ_subsampled_dx_azure.json\n\nYou cannot use these input JSON files directly. Go to the destination directory on DNAnexus and click on the converted workflow `atac`. You will see input file boxes in the left-hand side of the task graph. Expand it and define FASTQs (`fastq_repX_R1` and also `fastq_repX_R2` if it's paired-ended) and `genome_tsv` as in the downloaded input JSON file. Click on the `common` task box and define other non-file pipeline parameters. e.g. `auto_detect_adapters` and `paired_end`.\n\nWe have a separate project on DNANexus to provide example FASTQs and `genome_tsv` for `hg38` and `mm10`. We recommend to make copies of these directories on your own project.\n\n`genome_tsv`\n- AWS: https://platform.dnanexus.com/projects/BKpvFg00VBPV975PgJ6Q03v6/data/pipeline-genome-data/genome_tsv/v4\n- Azure: https://platform.dnanexus.com/projects/F6K911Q9xyfgJ36JFzv03Z5J/data/pipeline-genome-data/genome_tsv/v4\n\nExample FASTQs\n- AWS: https://platform.dnanexus.com/projects/BKpvFg00VBPV975PgJ6Q03v6/data/pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ/fastq_subsampled\n- Azure: https://platform.dnanexus.com/projects/F6K911Q9xyfgJ36JFzv03Z5J/data/pipeline-test-samples/encode-atac-seq-pipeline/ENCSR356KRQ/fastq_subsampled\n\n\n## Running on DNAnexus (using our pre-built workflows)\n\nSee [this](docs/tutorial_dx_web.md) for details.\n\n\n## How to organize outputs\n\nInstall [Croo](https://github.com/ENCODE-DCC/croo#installation). **You can skip this installation if you have installed pipeline's Conda environment and activated it**. Make sure that you have python3(> 3.4.1) installed on your system. Find a `metadata.json` on Caper's output directory.\n\n```bash\n$ pip install croo\n$ croo [METADATA_JSON_FILE]\n```\n\n## How to make a spreadsheet of QC metrics\n\nInstall [qc2tsv](https://github.com/ENCODE-DCC/qc2tsv#installation). Make sure that you have python3(> 3.4.1) installed on your system. \n\nOnce you have [organized output with Croo](#how-to-organize-outputs), you will be able to find pipeline's final output file `qc/qc.json` which has all QC metrics in it. Simply feed `qc2tsv` with multiple `qc.json` files. It can take various URIs like local path, `gs://` and `s3://`.\n\n```bash\n$ pip install qc2tsv\n$ qc2tsv /sample1/qc.json gs://sample2/qc.json s3://sample3/qc.json ... > spreadsheet.tsv\n```\n\nQC metrics for each experiment (`qc.json`) will be split into multiple rows (1 for overall experiment + 1 for each bio replicate) in a spreadsheet.\n",
    "readme_length": 12859
  },
  {
    "name": "TOBIAS",
    "full_name": "loosolab/TOBIAS",
    "description": "Transcription factor Occupancy prediction By Investigation of ATAC-seq Signal ",
    "stars": 234,
    "forks": 44,
    "language": "Python",
    "url": "https://github.com/loosolab/TOBIAS",
    "topics": [
      "atac-seq",
      "bioinformatics",
      "footprinting"
    ],
    "created_at": "2019-09-04T09:29:05Z",
    "updated_at": "2025-11-27T09:29:22Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "TOBIAS - Transcription factor Occupancy prediction By Investigation of ATAC-seq Signal \n=======================================\n\n\n[![PyPI Version](https://img.shields.io/pypi/v/tobias.svg?style=plastic)](https://pypi.org/project/tobias/)\n[![PyPI download month](https://img.shields.io/pypi/dm/tobias.svg?style=plastic)](https://pypi.python.org/pypi/tobias/)\n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=plastic)](http://bioconda.github.io/recipes/tobias/README.html)\n[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg?style=plastic)](https://GitHub.com/loosolab/TOBIAS/graphs/commit-activity)\n[![publication](https://img.shields.io/badge/Publication-NatComm-blue.svg?style=plastic)](https://doi.org/10.1038/s41467-020-18035-1)\n\nIntroduction \n------------\n\nATAC-seq (Assay for Transposase-Accessible Chromatin using high-throughput sequencing) is a sequencing assay for investigating genome-wide chromatin accessibility. The assay applies a Tn5 Transposase to insert sequencing adapters into accessible chromatin, enabling mapping of regulatory regions across the genome. Additionally, the local distribution of Tn5 insertions contains information about transcription factor binding due to the visible depletion of insertions around sites bound by protein - known as _footprints_. \n\n**TOBIAS** is a collection of command-line bioinformatics tools for performing footprinting analysis on ATAC-seq data, and includes:\n\n<img align=\"right\" width=150 src=\"/figures/tobias.png\">\n\n- Correction of Tn5 insertion bias\n- Calculation of footprint scores within regulatory regions\n- Estimation of bound/unbound transcription factor binding sites\n- Visualization of footprints within and across different conditions\n\nFor information on each tool, please see the [wiki](https://github.com/loosolab/TOBIAS/wiki/).\n\nAlthough TOBIAS was originally developed for bulk experiments, it can also be applied to single-cell resolution data. To do that, we recommend generating individual pseudobulk BAM files from scATAC-seq cell type clusters. This can be done using the [sc-framework](https://github.com/loosolab/SC-Framework), which also contains a [notebook](https://github.com/loosolab/SC-Framework/blob/main/atac_analysis/notebooks/0A_tobias.ipynb) specifically designed for scATAC-seq TOBIAS analyses. It is important to note that the quality of the single cells and the cell clustering is paramount for achieving a clean footprinting analysis. Therefore, we recommend using [PEAKQC](https://github.com/loosolab/PEAKQC) for cell quality assessment beforehand.\n\nInstallation\n------------\nTOBIAS is written as a python package and can be quickly installed via pip:\n```bash\n$ pip install tobias\n```\n\nTOBIAS is also available as a conda package on the Bioconda channel:\n```bash\n$ conda install tobias -c bioconda\n```\nPlease see the [installation](https://github.com/loosolab/TOBIAS/wiki/installation) page for more info.\n\nUsage\n------------\nAll tools are available through the command-line as ```TOBIAS <TOOLNAME>```, for example:\n``` \n$ TOBIAS ATACorrect\n__________________________________________________________________________________________\n\n                                   TOBIAS ~ ATACorrect\n__________________________________________________________________________________________\n\nATACorrect corrects the cutsite-signal from ATAC-seq with regard to the underlying\nsequence preference of Tn5 transposase.\n\nUsage:\nTOBIAS ATACorrect --bam <reads.bam> --genome <genome.fa> --peaks <peaks.bed>\n\nOutput files:\n- <outdir>/<prefix>_uncorrected.bw\n- <outdir>/<prefix>_bias.bw\n- <outdir>/<prefix>_expected.bw\n- <outdir>/<prefix>_corrected.bw\n- <outdir>/<prefix>_atacorrect.pdf\n\n(...)\n```\n\nOverview and command-line examples\n-------------\n\n* [ATACorrect](https://github.com/loosolab/TOBIAS/wiki/ATACorrect): Bias correction of ATAC-seq reads in open chromatin\n* [ScoreBigwig](https://github.com/loosolab/TOBIAS/wiki/ScoreBigwig): Calculate footprint scores from corrected cutsites\n* [BINDetect](https://github.com/loosolab/TOBIAS/wiki/BINDetect): Estimation of differentially bound motifs based on scores, sequence and motifs\n* [PlotAggregate](https://github.com/loosolab/TOBIAS/wiki/PlotAggregate): Plot aggregated ATAC-seq signals in combinations of .bed/.bw to visualize footprints\n* [PlotHeatmap](https://github.com/loosolab/TOBIAS/wiki/PlotHeatmap): Plot heatmaps and aggregates of ATAC-seq signals in combinations of .bed/.bw to visualize footprints\n* [PlotTracks](https://github.com/loosolab/TOBIAS/wiki/PlotTracks): Plot IGV-style genomic signals such as cutsites and footprints across a selection of regions\n* [FormatMotifs](https://github.com/loosolab/TOBIAS/wiki/FormatMotifs): A utility to convert and join/split across different motif-file formats\n* [ClusterMotifs](https://github.com/loosolab/TOBIAS/wiki/Additional) : Cluster motifs and create consensus motifs based on similarity\n* [CreateNetwork](https://github.com/loosolab/TOBIAS/wiki/CreateNetwork): Create TF-TF binding network from annotated TFBS\n* [FilterFragments](https://github.com/loosolab/TOBIAS/wiki/Additional): Filter fragments from a .bam-file using a .bed-file of regions\n* [Additional utility tools](https://github.com/loosolab/TOBIAS/wiki/Additional)\n\n\nPipelines\n----------------\nWhile each TOBIAS tool can be run independently, they are developed to be run as part of an analysis pipeline. We provide ready-made pipelines for performing bias-correction, footprinting, differential binding and visualization for multiple conditions automatically.\n\n**Snakemake pipeline**  \nWe provide a pre-set snakemake workflow which is found [here](https://github.com/loosolab/TOBIAS_snakemake).\n\n**Nextflow pipeline**  \nYou can also run the TOBIAS tool as a nextflow pipeline. The pre-set workflow can be found [here](https://github.molgen.mpg.de/loosolab/TOBIAS-nextflow).\n\n**Nextflow kubernetes/de.NBI cloud aware pipeline**  \nWe also provide the TOBIAS nextflow pipeline for a cloud computing environment. One version utilizes a [kubernetes framework](https://github.molgen.mpg.de/loosolab/TOBIAS-nextflow/tree/master/TOBIAS_MAPOKS), and a second version utilizing a webbased job scheduler, started automatically within a local TOBIAS run, making use of the de.NBI [cloud](https://github.molgen.mpg.de/loosolab/TOBIAS-nextflow/tree/master/TOBIAS_MACSEK).\n\nHelp \n--------\nIn case of any issues/questions/comments, please check out the [FAQ](https://github.com/loosolab/TOBIAS/wiki/FAQ). Otherwise, please write an issue [here](https://github.com/loosolab/TOBIAS/issues).\n\nHow to cite\n------------\n\nBentsen, M., Goymann, P., Schultheis, H. et al. ATAC-seq footprinting unravels kinetics of transcription factor binding during zygotic genome activation. Nat Commun 11, 4267 (2020). \n\nDOI: https://doi.org/10.1038/s41467-020-18035-1\n\nLicense\n------------\nThis project is licensed under the [MIT license](LICENSE). \n",
    "readme_length": 6946
  },
  {
    "name": "seq2science",
    "full_name": "vanheeringen-lab/seq2science",
    "description": "Automated and customizable preprocessing of Next-Generation Sequencing data, including full (sc)ATAC-seq, ChIP-seq, and (sc)RNA-seq workflows. Works equally easy with public as local data. ",
    "stars": 167,
    "forks": 28,
    "language": "Python",
    "url": "https://github.com/vanheeringen-lab/seq2science",
    "topics": [
      "atac-seq",
      "bioinformatics",
      "bioinformatics-pipeline",
      "chip-seq",
      "fastq",
      "ngs",
      "pipeline",
      "reproducible-research",
      "rna-seq",
      "snakemake",
      "sra",
      "workflows"
    ],
    "created_at": "2019-07-11T11:12:45Z",
    "updated_at": "2025-11-24T10:12:57Z",
    "homepage": "https://vanheeringen-lab.github.io/seq2science",
    "license": "MIT License",
    "readme": "# seq2science\n[![bioconda-badge](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat)](http://bioconda.github.io/recipes/seq2science/README.html)\n[![anaconda-version](https://anaconda.org/bioconda/seq2science/badges/version.svg)](https://anaconda.org/bioconda/seq2science/badges/version.svg)\n[![nr downloads](https://img.shields.io/conda/dn/bioconda/seq2science.svg?style=flat)](http://bioconda.github.io/recipes/seq2science/README.html)\n[![star this repo](https://img.shields.io/github/stars/vanheeringen-lab/seq2science?style=flat&color=brightgreen)](https://github.com/vanheeringen-lab/seq2science/stargazers)\n[![Test Status](http://ocimum.science.ru.nl/jenkins/buildStatus/icon?job=seq2science%2Fmaster&subject=tests)](http://ocimum.science.ru.nl/jenkins/job/seq2science/job/master/lastBuild/display/redirect/)\n[![docs](https://github.com/vanheeringen-lab/seq2science/workflows/docs/badge.svg)](https://vanheeringen-lab.github.io/seq2science)\n[![DOI](https://img.shields.io/badge/DOI-10.7717%2Fpeerj.16380-%2302A7FC)](https://doi.org/10.7717/peerj.16380)\n[![DOI](https://zenodo.org/badge/196379320.svg)](https://zenodo.org/badge/latestdoi/196379320)\n\nSeq2science is the attempt of the *van heeringen lab* to generate a collection of generic pipelines/workflows which can be used by complete beginners to bioinformatics and experienced bioinformaticians alike. Please take a look at our [docs](https://vanheeringen-lab.github.io/seq2science/) for help with installation, how to run it, and best practices.\n\n![alt text](https://vanheeringen-lab.github.io/seq2science/_static/seq2science.png \"seq2science\")\n\nOur supported workflows:\n* [Downloading of fastq](https://vanheeringen-lab.github.io/seq2science/content/workflows/download_fastq.html)\n* [Alignment](https://vanheeringen-lab.github.io/seq2science/content/workflows/alignment.html)\n* [ATAC-seq](https://vanheeringen-lab.github.io/seq2science/content/workflows/atac_seq.html)\n* [RNA-seq](https://vanheeringen-lab.github.io/seq2science/content/workflows/rna_seq.html)\n* [ChIP-seq](https://vanheeringen-lab.github.io/seq2science/content/workflows/chip_seq.html)\n* [scATAC-seq](https://vanheeringen-lab.github.io/seq2science/content/workflows/scatac_seq.html)\n* [scRNA-seq](https://vanheeringen-lab.github.io/seq2science/content/workflows/scrna_seq.html)\n\n## New users\nInformation for new users, such as how to install and configure can be found in our [getting started](https://vanheeringen-lab.github.io/seq2science/content/gettingstarted.html) page, and our [Frequently Asked Questions (FAQ)](https://vanheeringen-lab.github.io/seq2science/content/faq.html) section. \n\n## Feedback\nAnything not working as expected? Please reach out to us via our [issues](https://github.com/vanheeringen-lab/seq2science/issues) page and we'll try to help you as soon as possible!\n\n## Cite us\nvan der Sande M, FrÃ¶lich S, SchÃ¤fers T, Smits JGA, Snabel RR, Rinzema S, van Heeringen SJ. 2023. Seq2science: an end-to-end workflow for functional genomics analysis. PeerJ 11:e16380 https://doi.org/10.7717/peerj.16380\n",
    "readme_length": 3079
  },
  {
    "name": "atac_dnase_pipelines",
    "full_name": "kundajelab/atac_dnase_pipelines",
    "description": "ATAC-seq and DNase-seq processing pipeline",
    "stars": 166,
    "forks": 78,
    "language": "Python",
    "url": "https://github.com/kundajelab/atac_dnase_pipelines",
    "topics": [
      "atac-seq",
      "bioinformatics-pipeline",
      "dnase-seq"
    ],
    "created_at": "2016-02-28T19:04:42Z",
    "updated_at": "2025-07-23T19:33:24Z",
    "homepage": "",
    "license": "BSD 3-Clause \"New\" or \"Revised\" License",
    "readme": "This pipeline has been deprecated as of June 2018. Please update your pipelines to the WDL-based pipeline at [https://github.com/ENCODE-DCC/atac-seq-pipeline](https://github.com/ENCODE-DCC/atac-seq-pipeline)\n===================================================\n\nATAC-Seq / DNase-Seq Pipeline\n===================================================\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.156534.svg)](https://doi.org/10.5281/zenodo.156534)\n\nThis pipeline is designed for automated end-to-end quality control and processing of ATAC-seq or DNase-seq data. The pipeline can be run on compute clusters with job submission engines or stand alone machines. It inherently makes uses of parallelized/distributed computing. Pipeline installation is also easy as most dependencies are automatically installed. The pipeline can be run end-to-end i.e. starting from raw FASTQ files all the way to peak calling and signal track generation; or can be started from intermediate stages as well (e.g. alignment files). The pipeline supports single-end or paired-end ATAC-seq or DNase-seq data (with or without replicates). The pipeline produces pretty HTML reports that include quality control measures specifically designed for ATAC-seq and DNase-seq data, analysis of reproducibility, stringent and relaxed thresholding of peaks, fold-enrichment and pvalue signal tracks.  The pipeline also supports detailed error reporting and easy resuming of runs. The pipeline has been tested on human, mouse and yeast ATAC-seq data and human and mouse DNase-seq data.\n\nThe ATAC-seq pipeline specification is also the official pipeline specification of the Encyclopedia of DNA Elements (ENCODE) consortium. The ATAC-seq pipeline protocol definition is [here](https://docs.google.com/document/d/1f0Cm4vRyDQDu0bMehHD7P7KOMxTOP-HiNoIvL1VcBt8/edit?usp=sharing). Some parts of the ATAC-seq pipeline were developed in collaboration with Jason Buenrostro, Alicia Schep and Will Greenleaf at Stanford.\n\nThe DNase-seq pipeline specification is [here](https://docs.google.com/document/d/1e3cCormg0SnQW6zr7VYBWvXC1GiPc5GSy80qlKBPwlA/edit?usp=sharing). Note that this is NOT the same as the official ENCODE DNase-seq pipeline (which is based on John Stam lab's processing pipeline).\n\n* Go to [Genomic pipelines in Kundaje lab](https://kundajelab.github.io/bds_pipeline_modules)\n* Go to [Discussion channel](https://groups.google.com/forum/#!forum/klab_genomic_pipelines_discuss)\n* Jump to [Usage](#usage)\n* Jump to [Output directory structure and file naming](#output-directory-structure-and-file-naming)\n* Jump to [ENCODE accession guideline](#encode-accession-guideline)\n* Jump to [Troubleshooting](#troubleshooting)\n\n# Installation\n\nInstall software/database in a correct order according to your system. For example on Kundaje lab's clusters, you only need to install one software [Pipeline](#pipeline).\n\n* General computer\n  * [Java](#java)\n  * [Conda](#conda)\n  * [BigDataScript](#bigdatascript)\n  * [Pipeline](#pipeline)\n  * [Dependencies](#dependencies)\n  * [Genome data](#genome-data)\n\n* Kundaje lab's clusters\n  * [Pipeline](#pipeline)\n\n* Stanford NEW SCG cluster\n  * [Conda](#conda)\n  * [BigDataScript](#bigdatascript)\n  * [Pipeline](#pipeline)\n  * [Dependencies](#dependencies)\n\n* Stanford OLD SCG cluster\n  * [Conda](#conda)\n  * [BigDataScript](#bigdatascript)\n  * [Pipeline](#pipeline)\n  * [Dependencies](#dependencies)\n\n* Stanford Sherlock cluster\n  * [Conda](#conda)\n  * [BigDataScript](#bigdatascript)\n  * [Pipeline](#pipeline)\n  * [Dependencies](#dependencies)\n\n## Java\n\nInstall Java 8 (jdk >= 1.8 or jre >= 1.8) on your system. If you don't have super-user privileges on your system, locally install it and add it to your `$PATH`.\n\n* For Debian/Ubuntu (>14.10) based Linux:\n\n     ```\n     $ sudo apt-get install git openjdk-8-jre\n     ```\n\n* For Fedora/Red-Hat based Linux: \n \n     ```\n     $ sudo yum install git java-1.8.0-openjdk\n     ```\n\n* For Ubuntu 14.04 (trusty):\n\n     ```\n     $ sudo add-apt-repository ppa:webupd8team/java -y\n     $ sudo apt-get update\n     $ sudo apt-get install oracle-java8-installer\n     ```\n\n## Conda\n\n**REMOVE ANY ANACONDA OR OTHER VERSIONS OF CONDA FROM YOUR BASH STARTUP SCRIPT. WE CANNOT GUARANTEE THAT PIPELINE WORKS WITH OTHER VERSIONS OF CONDA. ALSO REMOVE R AND OTHER CONFLICTING MODULES FROM IT TOO**. Remove any other Anaconda from your `$PATH`. Check your loaded modules with `$ module list` and unload any Anaconda modules in your bash startup scripts (`$HOME/.bashrc` or `$HOME/.bash_profile`). Add `unset PYTHONPATH` to your bash start up scripts.\n\nInstall Miniconda3 [latest](https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh) on your system. **IMPORTANT** Make sure that the absolute path of the destination directory is short. Long path will cause an error in the depenecies installation step [issue #8](https://github.com/kundajelab/TF_chipseq_pipeline/issues/8).\n\n```\n$ wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n$ bash Miniconda3-latest-Linux-x86_64.sh\n```\n\nAnswer `yes` for the final question. If you choose `no`, you need to manually add Miniconda3 to your `$HOME/.bashrc`.\n\n```\nDo you wish the installer to prepend the Miniconda3 install location\nto PATH in your /your/home/.bashrc ? [yes|no]\n[no] >>> yes\n```\n\nOpen a new terminal after installation.\n\n\n## BigDataScript\n\nInstall BigDataScript v0.99999e ([forked](https://github.com/leepc12/BigDataScript)) on your system.\nThe original [BDS v0.99999e](https://github.com/pcingola/BigDataScript) does not work correctly with the pipeline\n(see [PR #142](https://github.com/pcingola/BigDataScript/pull/142) and [issue #131](https://github.com/pcingola/BigDataScript/issues/131)).\n\n```\n$ wget https://github.com/leepc12/BigDataScript/blob/master/distro/bds_Linux.tgz?raw=true -O bds_Linux.tgz\n$ mv bds_Linux.tgz $HOME && cd $HOME && tar zxvf bds_Linux.tgz\n```\n\nAdd `export PATH=$PATH:$HOME/.bds` to your `$HOME/.bashrc`. If Java memory occurs, add `export _JAVA_OPTIONS=\"-Xms256M -Xmx728M -XX:ParallelGCThreads=1\"` too.\n\n\n## Pipeline\n\nGet the latest version of the pipeline. **Don't forget to add `--recursive`**. ATAC-Seq pipeline uses modules from an external git repo (ataqc). ATAQC will not work correctly without `--recursive`.\n\n```\n$ git clone https://github.com/kundajelab/atac_dnase_pipelines --recursive\n```\n\n## Dependencies\n\nInstall software dependencies automatically. It will create two conda environments (bds_atac and bds_atac_py3) under your conda.\n\n```\n$ bash install_dependencies.sh\n```\n\nIf you don't use `install_dependencies.sh`, manually replace BDS's default `bds.config` with a correct one:\n\n```\n$ cp bds.config ./utils/bds_scr $HOME/.bds\n```\n\nIf `install_dependencies.sh` fails, run `./uninstall_dependencies.sh`, fix problems and then try `bash install_dependencies.sh` again.\n\n## Genome data\n\nInstall genome data for a specific genome `[GENOME]`. Currently `hg19`, `mm9`, `hg38` and `mm10` are available. Specify a directory `[DATA_DIR]` to download genome data. A species file generated on `[DATA_DIR]` will be automatically added to your `./default.env` so that the pipeline knows that you have installed genome data using `install_genome_data.sh`. If you want to install multiple genomes make sure that you use the same directory `[DATA_DIR]` for them. Each genome data will be installed on `[DATA_DIR]/[GENOME]`. If you use other BDS pipelines, it is recommended to use the same directory `[DATA_DIR]` to save disk space.\n\n**IMPORTANT**: `install_genome_data.sh` can take longer than an hour for downloading data and building index. **DO NOT** run the script on a login node, use `qlogin` for SGE and `srun --pty bash` for SLURM.\n\n```\n# DO NOT run this on a login node\n$ bash install_genome_data.sh [GENOME] [DATA_DIR]\n```\n\nIf you have super-user privileges on your system, it is recommended to install genome data on `/your/data/bds_pipeline_genome_data` and share them with others.\n\n```\n$ sudo su\n$ bash install_genome_data.sh [GENOME] /your/data/bds_pipeline_genome_data\n```\n\nYou can find a species file `[SPECIES_FILE]` on `/your/data/bds_pipeline_genome_data` for each pipeline type. Then others can use the genome data by adding `-species_file [SPECIES_FILE_PATH]` to the pipeline command line. Or they need to add `species_file = [SPECIES_FILE_PATH]` to the section `[default]` in their `./default.env`.\n\n# Installation for internet-free computers\n\nThe pipeline does not need internet connection but installers (`install_dependencies.sh` and `install_genome_data.sh`) do need it. So the workaround should be that first install dependencies and genome data on a computer that is connected to the internet and then move Conda and genome database directories to your internet-free one. Both computers should have **THE SAME LINUX VERSION**. \n\n* On your computer that has an internet access,\n  * Follow [the installation instruction for general computers](#installation)\n  * Move your Miniconda3 directory to `$HOME/miniconda3` on your internet-free computer.\n  * Move your genome database directory, which has `bds_atac_species.conf` and directories per species, to `$HOME/genome_data` on your internet-free computer. `$HOME/genome_data` on your internet-free computer should have `bds_atac_species.conf`.\n  * Move your BDS directory `$HOME/.bds` to `$HOME/.bds` on your internet-free computer.\n  * Move your pipeline directory `atac_dnase_pipelines/` to `$HOME/atac_dnase_pipelines/` on your internet-free computer.\n\n* On your internet-free computer,\n  * Add your `miniconda3/bin` and BDS binary to `$PATH` in your bash initialization script (`$HOME/.bashrc` or `$HOME/.bash_profile`).\n\n     ```\n     export PATH=\"$PATH:$HOME/miniconda3/bin\"\n     export PATH=\"$PATH:$HOME/.bds\"\n     ```\n\n  * Modify `[default]` section in `$HOME/atac_dnase_pipelines/default.env`.\n\n     ```\n     [default]\n     conda_bin_dir=$HOME/miniconda3/bin\n     species_file=$HOME/genome_data/bds_atac_species.conf\n     ```\n\n* Modify all paths in `$HOME/genome_data/bds_atac_species.conf` so that they correctly point to the right files.\n* Check BDS version.\n     ```\n     $ bds -version\n     Bds 0.99999e (build 2016-08-26 06:34), by Pablo Cingolani\n     ```\n* Make sure that your java rumtime version is >= 1.8.\n     ```\n     $ java -version\n     java version \"1.8.0_111\"\n     Java(TM) SE Runtime Environment (build 1.8.0_111-b14)\n     Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode)\n     ```\n\n# Usage\n\nWe recommend using BASH to run pipelines.\n\nFor general use, use the following command line: (for PE data set)\n```\n$ bds atac.bds -species [SPECIES; hg19, mm9, ... ] -nth [NUM_THREADS] -fastq1_1 [READ1] -fastq1_2 [READ2]\n```\n\nFor DNase-seq: (it's <b>NOT `-dnase`</b>!)\n```\n-dnase_seq\n```\n\nThe pipeline does not trim adapters by default. To automatically detect adapters,\n\n```\n-auto_detect_adapter\n```\n\nTo specify adapter for each fastq, add `-adapter[REPLICATE_ID]_[PAIR_ID]:[POOL_ID]` for paired end dataset and `-adapter[REPLICATE_ID]:[POOL_ID]` for single ended one.\nYou can skip `:[POOL_ID]` if you have single fastq per replicate (SE) or single pair of fastqs per replicate (PE).\n\n<b>IMPORTANT!</b> If your data set is <b>SINGLE ENDED</b> add the following to the command line, otherwise the pipeline works for PE by default.\n```\n-se \n```\nYou can also individually specify endedness for each replicate.\n```\n-se[REPLICATE_ID] \t# for exp. replicates, \n```\n```\n-se1 -pe2 -se3 ...\n```\nIf you want to just align your data (no peak calling or further steps like IDR).\n```\n-align\n```\nIf you don't want ATAQC, add the following to command line. \n```\n-no_ataqc \n```\nIf you have just one replicate (PE), specify fastqs with `-fastq[REP_ID]_[PAIR_ID]`.\n```\n-fastq1_1 [READ_PAIR1] -fastq1_2 [READ_PAIR2] \\\n```\nFor multiple replicates (PE), specify fastqs with `-fastq[REP_ID]_[PAIR_ID]`. Add `-fastq[]_[]` for each replicate and pair to the command line:replicates.\n```\n-fastq1_1 [READ_REP1_PAIR1] -fastq1_2 [READ_REP1_PAIR2] -fastq2_1 [READ_REP2_PAIR1] -fastq2_2 [READ_REP2_PAIR2] ...\n```\nFor multiple replicates (SE), specify fastqs with `-fastq[REP_ID]`:\n```\n-se -fastq1 [READ_REP1] -fastq2 [READ_REP2] ...\n```\nYou can also specify an adapter to be trimmed for each fastq. Example: `-adapter1_1 [ADAPTER1_1] -adapter1_2 [ADAPTER1_2] ...` for PE or `-adapter1 [ADAPTER1] -adapter2 [ADAPTER2] ...`.\n\nYou can start from bam files. There are two kinds of bam files (raw or deduped) and you need to explicitly choose between raw bam (bam) and deduped one (filt_bam) with `-input [BAM_TYPE]`. Don't forget to add `-se` if they are not paired end (PE). For raw bams,\n```\n-bam1 [RAW_BAM_REP1] -bam2 [RWA_BAM_REP1] ...\n```\nFor deduped (filtered) bams:\n```\n-filt_bam1 [FILT_BAM_REP1] -filt_bam2 [FILT_BAM_REP1] ...\n```\nFor tagaligns (non-tn5-shifted):\n```\n-tag1 [TAGALIGN_REP1] -tag2 [TAGALIGN_REP2] ...\n```\nYou can also mix up any data types.\n```\n-bam1 [RAW_BAM_REP1] -tag2 [TAGALIGN_REP2] ...\n```\nTo subsample beds (tagaligns) add the following to the command line. This is different from subsampling for cross-corr. analysis. Peaks will be called with subsampled tagaligns.\n```\n-subsample [NO_READS_TO_SUBSAMPLE]\n```\nTo change # of lines to subsample for cross-corr. analysis. This will not affect tasks downstream (peak calling and IDR).\n```\n-nreads [NO_READS_TO_SUBSAMPLE]\n```\nTo disable pseudo replicate generation, add the following. By default, peak calling and IDR will be done for true replicates and pseudo replicates, but if you have `-true_rep` in the command line, you will also get IDR on true replicates only.\n```\n-true_rep\n```\nIDR analysis is optional in the pipeline by default. If there are more than two replicates, IDR will be done for every pair of replicates. to enable IDR add the following:\n```\n-enable_idr\n```\nFor multimapping, (multimapping is disabled by default). Using this parameter implies `-mapq_thresh 30` (MapQ Thresh for the pipeline is fixed at 30).\n```\n-multimapping [MULTIMAPPING; use 4 for ENCODE samples]\n```\nTo force a set of parameters (`-smooth_win 73 -idr_thresh 0.05 -multimapping 4`) for ENCODE3.\n```\n-ENCODE3\n```\n\nYou can also define parameters in a configuration file. Key names in a configruation file are identical to parameter names on command line. \n```\n$ bds atac.bds [CONF_FILE]\n\n$ cat [CONF_FILE]\nspecies = [SPECIES; hg19, mm9, ...]\nnth   = [NUM_THREADS]\nfastq1_1= [READ1]\nfastq1_2= [READ2]\n...\n```\n\n## List of all parameters\n\nTo list all parameters and default values for them,\n```\n$ bds atac.bds\n```\n\n## Stopping / Resuming pipeline\n\nPress Ctrl + C on a terminal or send any kind of kill signals to it. Please note that this will delete all intermediate files and incomplete outputs for the running tasks. The pipeline automatically determines if each task has finished or not (by comparing timestamps of input/output files for each task). To run the pipeline from the point of failure, correct error first and then just run the pipeline with the same command that you started the pipeline with. There is no additional parameter for restarting the pipeline.\n\n## Running pipelines with a cluster engine\n\nOn servers with a cluster engine (such as Sun Grid Engine and SLURM), **DO NOT QSUB/SBATCH BDS COMMAND LINE**. Run BDS command directly on login nodes. BDS is a task manager and it will automatically submit(qsub/sbatch) and manage its sub tasks.\n\n**IMPORTANT!** Please read this section carefully if you run pipelines on Stanford SCG4 and Sherlock cluster.\n\nMost clusters have a policy to limit number of threads and memory per user on a login node. One BDS process, as a Java-based task manager, takes up to 1GB of memory and 50 threads even though it just submits/monitors subtasks. So if you want to run more than 50 pipelines in parallel, your cluster will kill BDS processes due to resource limit on a login node (check resource limit per user with `ulimit -a`). For example of 50 pipelines, 50 GB of memory and 2500 threads will be taken by 50 BDS processes. So the Workaround for this is to make an interactive node to keep all BDS processes alive. Such interactive node must have long walltime enough to wait for all pipelines in it to finish. Recommended resource setting is 0.5GB memory per pipeline.\n\nSGE example to make an interactive node for 100 pipelines: 1 cpu, 100GB memory, 3 days walltime.\n\n```\n$ qlogin -l h_rt=72:00:00 -l h_vmem=100G\n```\n\nSLURM example to make an interactive node for 100 pipelines: 1 cpus, 100GB memory, 3 days walltime.\n\n```\n$ srun -n 1 --mem 100G -t 3-0 -p [YOUR_PARTITON] --qos normal --pty /bin/bash -i -l \n```\n\nOnce you get an interactive node, repeat the following commands per sample to run a pipeline with using [`bds_scr`](#managing-multiple-pipelines). Add `-q_for_slurm_account` to the command line to use the parameter `-q` for SLURM account (`sbatch --acount`) instead of partition (`sbatch -p`).\n\n```\n$ cd [WORK_DIR]\n$ bds_scr [SCREEN_NAME] [LOG_FILE_PATH] atac.bds -system [CLUSTER_ENGINE: slurm or sge] -q [SGE_QUEUE_OR_SLURM_PARTITION] -nth [MAX_NUM_THREAD_PER_PIPELINE] ...\n$ sleep 2 # wait for 2 seconds for safety\n```\n\nThen you can monitor your pipelines with `screen -ls` and `tail -f [LOG_FILE_PATH]`. If you want to run more than 200 pipelines, you would want to make multiple interactive nodes and distribute your samples to them.\n\n## Parallelization\n\nFor completely serialized jobs, add `-no_par` to the command line. Individual tasks can still go multi-threaded.\n\n**IMPORTANT!** You can set up a limit for total number of threads with `-nth [MAX_TOTAL_NO_THREADS]`. Total number of threads used by a pipeline will not exceed this limit.\n\nDefault `-nth` for each cluster is defined on `./default.env` (e.g. 16 on SCG and 8 on Kundaje lab cluster)\n\nThe pipeline automatically distributes `[MAX_TOTAL_NO_THREADS]` threads for jobs according to corresponding input file sizes. For example of two fastqs (1GB and 2GB) with `-nth 6`, 2 and 4 threads are allocated for aligning 1GB and 2GB fastqs, respectively. The same policy applies to other multi-threaded tasks like deduping and peak calling.\n\nHowever, all multi-threaded tasks (like bwa, bowtie2, spp and macs2) still have their own max. memory (`-mem_APPNAME [MEM_APP]`) and walltime (`-wt_APPNAME [WALLTIME_APP]`) settings. Max. memory is **NOT PER CPU**. For example on Kundaje lab cluster (with SGE flag activated `bds -s sge bds_atac.bds ...`) or on SCG4, the actual shell command submitted by BDS for each task is like the following:\n\n```\nqsub -V -pe shm [NTH_ALLOCATED_FOR_APP] -h_vmem=[MEM_APP]/[NTH_ALLOCATED_FOR_APP] -h_rt=[WALLTIME_APP] -s_rt=[WALLTIME_APP] ...\n```\n\nThis ensures that total memory reserved for a cluster job equals to `[MEM_APP]`. The same policy applies to SLURM.\n\n## Specifying a cluster queue/partition\n\nYou can specifiy a queue `[QUEUE_NAME]` on Sun Grid Engine or partition/account on SLURM. But you cannot specify both account and partition at the same time for SLURM. You can skip `-q_for_slurm_account` on Stanford SCG cluster since the pipeline will automatically detect SCG servers and add it.\n```\nbds atac.bds -system sge -q [SGE_QUEUE_NAME] ...\nbds atac.bds -system slurm -q [SLURM_PARTITON_NAME] ... # Sherlock example\nbds atac.bds -system slurm -q_for_slurm_account -q [SLURM_ACCOUNT_NAME] ... # SCG example\n```\n\n\n## Managing multiple pipelines\n\n`./utils/bds_scr` is a BASH script to create a detached screen for a BDS script and redirect stdout/stderr to a log file `[LOG_FILE_NAME]`. If a log file already exists, stdout/stderr will be appended to it.\n\nMonitor the pipeline with `tail -f [LOG_FILE_NAME]`. The only difference between `bds_scr` and `bds` is that you have `[SCR_NAME] [LOG_FILE_NAME]` between `bds` command and its parameters (or a BDS script name).\n\nYou can skip `[LOG_FILE_NAME]` then a log file `[SCR_NAME].log` will be generated on the working directory.\n\nYou can also add any BDS parameters (like `-dryRun`, `-d` and `-s`). The following example is for running a pipeline on Sun Grid Engine.\n\n```\n$ bds_scr [SCR_NAME] [LOG_FILE_NAME] atac.bds ...\n$ bds_scr [SCR_NAME] atac.bds ...\n$ bds_scr [SCR_NAME] -s sge atac.bds ...\n```\n\nOnce the pipeline run is done, the screen will be automatically closed. To kill a pipeline manually while it's running, use `./utils/kill_scr` or `screen -X quit`:\n\n```\n$ screen -X -S [SCR_NAME] quit\n$ kill_scr [SCR_NAME]\n```\n\n## Java issues (memory and temporary directory)\n\nPicard tools is used for marking dupes in the reads and it's based on Java. If you see any Java heap space errors then increase memory limit for Java with `-mem_ataqc [MEM]` (default: `20G`) and `-mem_dedup [MEM]` (default: `12G`).\n\nIf your `/tmp` quickly fills up and you want to change temporary directory for all Java apps in the pipeline, then add the following line to your bash startup script (`$HOME/.bashrc`). Our pipeline takes in `$TMPDIR` (not `$TMP`) for all Java apps.\n\n```\nexport TMPDIR=/your/temp/dir/\n```\n\nAnother quick workaround for dealing with Java issues is not to use Picard tools in the pipeline. Add `-use_sambamba_markdup` to your command line and then you can use `sambamba markdup` instead of `picard markdup`.\n\n\n## How to customize genome data installer?\n\nPlease refer to the section `Installer for genome data` on [BDS pipeline programming](https://kundajelab.github.io/bds_pipeline_modules/programming.html).\n\n\n## Useful HTML reports\n\nThere are two kinds of HTML reports provided by the pipeline.\n\n* BigDataScript HTML report for debugging: Located at the working folder with name atac_[TIMESTAMP]_report.html. This report is automatically generated by BigDataScript and is useful for debugging since it shows summary, timeline, Stdout and Stderr for each job.\n\n* ATAC-Seq pipeline report for QC and result: The pipeline automatically generate a nice HTML report (Report.html) on its output directory (specified with -out_dir or just './out'). It summarizes files and directory structure, includes QC reports and show a workflow diagram and genome browser tracks for peaks and signals (bigwigs for pValue and fold change). Move your output directory to a web directory (for example, /var/www/somewhere) or make a softlink of it to a web directory. For genome browser tracks, specify your web directory root for your output  While keeping its structure. Make sure that you have bgzip and tabix installed on your system. Add the following to the command line:\n\n      -url_base http://your/url/to/output -title [PREFIX_FOR_YOUR_REPORT]\n\n\n# Output directory structure and file naming\n\nFor more details, refer to the file table section in an HTML report generated by the pipeline. Files marked as (E) are outputs to be uploaded during ENCODE accession.\n```\nout                               # root dir. of outputs\nâ”‚\nâ”œ *report.html                    #  HTML report\nâ”œ *tracks.json                    #  Tracks datahub (JSON) for WashU browser\nâ”œ ENCODE_summary.json             #  Metadata of all datafiles and QC results\nâ”‚\nâ”œ align                           #  mapped alignments\nâ”‚ â”œ rep1                          #   for true replicate 1 \nâ”‚ â”‚ â”œ *.trim.fastq.gz             #    adapter-trimmed fastq\nâ”‚ â”‚ â”œ *.bam                       #    raw bam\nâ”‚ â”‚ â”œ *.nodup.bam (E)             #    filtered and deduped bam\nâ”‚ â”‚ â”œ *.tagAlign.gz               #    tagAlign (bed6) generated from filtered bam\nâ”‚ â”‚ â”œ *.tn5.tagAlign.gz           #    TN5 shifted tagAlign for ATAC pipeline (not for DNase pipeline)\nâ”‚ â”‚ â”” *.*M.tagAlign.gz            #    subsampled tagAlign for cross-corr. analysis\nâ”‚ â”œ rep2                          #   for true repilicate 2\nâ”‚ ...\nâ”‚ â”œ pooled_rep                    #   for pooled replicate\nâ”‚ â”œ pseudo_reps                   #   for self pseudo replicates\nâ”‚ â”‚ â”œ rep1                        #    for replicate 1\nâ”‚ â”‚ â”‚ â”œ pr1                       #     for self pseudo replicate 1 of replicate 1\nâ”‚ â”‚ â”‚ â”œ pr2                       #     for self pseudo replicate 2 of replicate 1\nâ”‚ â”‚ â”œ rep2                        #    for repilicate 2\nâ”‚ â”‚ ...                           \nâ”‚ â”” pooled_pseudo_reps            #   for pooled pseudo replicates\nâ”‚   â”œ ppr1                        #    for pooled pseudo replicate 1 (rep1-pr1 + rep2-pr1 + ...)\nâ”‚   â”” ppr2                        #    for pooled pseudo replicate 2 (rep1-pr2 + rep2-pr2 + ...)\nâ”‚\nâ”œ peak                             #  peaks called\nâ”‚ â”” macs2                          #   peaks generated by MACS2\nâ”‚   â”œ rep1                         #    for replicate 1\nâ”‚   â”‚ â”œ *.narrowPeak.gz            #     narrowPeak (p-val threshold = 0.01)\nâ”‚   â”‚ â”œ *.filt.narrowPeak.gz (E)   #     blacklist filtered narrowPeak \nâ”‚   â”‚ â”œ *.narrowPeak.bb (E)        #     narrowPeak bigBed\nâ”‚   â”‚ â”œ *.narrowPeak.hammock.gz    #     narrowPeak track for WashU browser\nâ”‚   â”‚ â”œ *.pval0.1.narrowPeak.gz    #     narrowPeak (p-val threshold = 0.1)\nâ”‚   â”‚ â”” *.pval0.1.*K.narrowPeak.gz #     narrowPeak (p-val threshold = 0.1) with top *K peaks\nâ”‚   â”œ rep2                         #    for replicate 2\nâ”‚   ...\nâ”‚   â”œ pseudo_reps                          #   for self pseudo replicates\nâ”‚   â”œ pooled_pseudo_reps                   #   for pooled pseudo replicates\nâ”‚   â”œ overlap                              #   naive-overlapped peaks\nâ”‚   â”‚ â”œ *.naive_overlap.narrowPeak.gz      #     naive-overlapped peak\nâ”‚   â”‚ â”” *.naive_overlap.filt.narrowPeak.gz #     naive-overlapped peak after blacklist filtering\nâ”‚   â”” idr                           #   IDR thresholded peaks\nâ”‚     â”œ true_reps                   #    for replicate 1\nâ”‚     â”‚ â”œ *.narrowPeak.gz           #     IDR thresholded narrowPeak\nâ”‚     â”‚ â”œ *.filt.narrowPeak.gz (E)  #     IDR thresholded narrowPeak (blacklist filtered)\nâ”‚     â”‚ â”” *.12-col.bed.gz           #     IDR thresholded narrowPeak track for WashU browser\nâ”‚     â”œ pseudo_reps                 #    for self pseudo replicates\nâ”‚     â”‚ â”œ rep1                      #    for replicate 1\nâ”‚     â”‚ ...\nâ”‚     â”œ optimal_set                 #    optimal IDR thresholded peaks\nâ”‚     â”‚ â”” *.filt.narrowPeak.gz (E)  #     IDR thresholded narrowPeak (blacklist filtered)\nâ”‚     â”œ conservative_set            #    optimal IDR thresholded peaks\nâ”‚     â”‚ â”” *.filt.narrowPeak.gz (E)  #     IDR thresholded narrowPeak (blacklist filtered)\nâ”‚     â”œ pseudo_reps                 #    for self pseudo replicates\nâ”‚     â”” pooled_pseudo_reps          #    for pooled pseudo replicate\nâ”‚\nâ”‚   \nâ”‚ \nâ”œ qc                              #  QC logs\nâ”‚ â”œ *IDR_final.qc                 #   Final IDR QC\nâ”‚ â”œ rep1                          #   for true replicate 1\nâ”‚ â”‚ â”œ *.align.log                 #    Bowtie2 mapping stat log\nâ”‚ â”‚ â”œ *.dup.qc                    #    Picard (or sambamba) MarkDuplicate QC log\nâ”‚ â”‚ â”œ *.pbc.qc                    #    PBC QC\nâ”‚ â”‚ â”œ *.nodup.flagstat.qc         #    Flagstat QC for filtered bam\nâ”‚ â”‚ â”œ *M.cc.qc                    #    Cross-correlation analysis score for tagAlign\nâ”‚ â”‚ â”œ *M.cc.plot.pdf/png          #    Cross-correlation analysis plot for tagAlign\nâ”‚ â”‚ â”” *_qc.html/txt               #    ATAQC report\nâ”‚ ...\nâ”‚\nâ”œ signal                          #  signal tracks\nâ”‚ â”œ macs2                         #   signal tracks generated by MACS2\nâ”‚ â”‚ â”œ rep1                        #    for true replicate 1 \nâ”‚ â”‚ â”‚ â”œ *.pval.signal.bigwig (E)  #     signal track for p-val\nâ”‚ â”‚ â”‚ â”” *.fc.signal.bigwig   (E)  #     signal track for fold change\nâ”‚ ...\nâ”‚ â”” pooled_rep                    #   for pooled replicate\nâ”‚ \nâ”œ report                          # files for HTML report\nâ”” meta                            # text files containing md5sum of output files and other metadata\n```\n\n# ENCODE accession guideline\n\nFor each pipeline rune, `ENCODE_summary.json` file is generated under the output directory (`-out_dir`) for ENCODE accession (uploading pipeline outputs to the ENCODE portal). This JSON file includes all metadata and QC metrics required for ENCODE accession.\n\nFor ENCODE3, Please make sure that you run pipelines with `-ENCODE3` flag.\n\nParameters required for ENCODE accesssion:\n```\n# required\n        -ENCODE_accession <string>       : ENCODE experiment accession ID (or dataset).\n        -ENCODE_award <string>           : ENCODE award (e.g. /awards/U41HG007000/).\n        -ENCODE_lab <string>             : Lab (e.g. /labs/anshul-kundaje/)\n        -ENCODE_assembly <string>        : hg19, GRCh38, mm9, mm10.\n        -ENCODE_alias_prefix <string>    : Alias = Alias_prefix + filename\n# optional\n        -ENCODE_award_rfa <string>       : ENCODE award RFA (e.g. ENCODE3).\n        -ENCODE_assay_category <string>  : ENCODE assay category.\n        -ENCODE_assay_title <string>     : ENCODE assay title.\n```\n\nWe also provide an [ENCODE fastq downloader](https://github.com/kundajelab/ENCODE_downloader). It downloads fastqs matching award_rfa, assay_category and assay_title, and then automatically generate a shell script to run multiple pipelines. Such shell script also includes these ENCODE accession parameter set.\n\n## ENCODE accession spreadsheet (CSV) generation\n\n`./utils/parse_summary_ENCODE_accession_recursively.py` recursively finds `ENCODE_summary.json` files and parse them to generate one big CSV spreadsheet for ENCODE accession.\n\n```\n$ python ./utils/parse_summary_ENCODE_accession_recursively.py -h\n\nusage: ENCODE_summary.json parser for ENCODE accession [-h]\n                                                       [--out-file OUT_FILE]\n                                                       [--search-dir SEARCH_DIR]\n                                                       [--json-file JSON_FILE]\n                                                       [--sort-by-genome-and-exp]\n                                                       [--ignored-accession-ids-file IGNORED_ACCESSION_IDS_FILE]\n\nRecursively find ENCODE_summary.json, parse it and make a CSV for uploading to\nthe ENCODE portal. Use https://github.com/ENCODE-DCC/pyencoded-\ntools/blob/master/ENCODE_submit_files.py for uploading.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --out-file OUT_FILE   Output CSV filename)\n  --search-dir SEARCH_DIR\n                        Root directory to search for ENCODE_summary.json\n  --json-file JSON_FILE\n                        Specify json file name to be parsed\n  --sort-by-genome-and-exp\n                        Sort rows by genomes and ENCODE experiment accession\n                        ID\n  --ignored-accession-ids-file IGNORED_ACCESSION_IDS_FILE\n                        Accession IDs in this text file will be ignored. (1\n                        acc. ID per line)\n```\n\n## QC metrics spreadsheet (TSV) generation\n\n`./utils/parse_summary_qc_recursively.py` recursively finds `ENCODE_summary.json` files and parse them to generate one big TSV spreadsheet for QC metrics.\n\n```\n$ python parse_summary_qc_recursively.py -h\nusage: ENCODE_summary.json parser for QC [-h] [--out-file OUT_FILE]\n                                         [--search-dir SEARCH_DIR]\n                                         [--json-file JSON_FILE]\n\nRecursively find ENCODE_summary.json, parse it and make a TSV spreadsheet of\nQC metrics.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --out-file OUT_FILE   Output TSV filename)\n  --search-dir SEARCH_DIR\n                        Root directory to search for ENCODE_summary.json\n  --json-file JSON_FILE\n                        Specify json file name to be parsed\n```\n\n\n# Programming with BDS\n\n* [Using genomic pipeline modules in Kundaje lab](https://kundajelab.github.io/bds_pipeline_modules/programming.html)\n\n* [BigDataScript github repo](https://github.com/pcingola/BigDataScript)\n\n* [BigDataScript documentation](http://pcingola.github.io/BigDataScript/bigDataScript_manual.html)\n\n# Requirements \n\n* For python2 (python 2.x >= 2.7) and R-3.x, [requirements.txt](requirements.txt)\n\n* For python3, [requirements_py3.txt](requirements_py3.txt)\n\n# Troubleshooting\n\nSee more [troubleshootings](https://kundajelab.github.io/bds_pipeline_modules/troubleshooting.html)\n\n### samtools ncurses bug\n\nPrepend a directory for `libncurses.so.5` to `LD_LIBRARY_PATH`. See `install_dependencies.sh` for solution.\n```\nsamtools: symbol lookup error: /lib/x86_64-linux-gnu/libncurses.so.5: undefined symbol: _nc_putchar\n```\n\n### Error: could not find environment: bds_atac\n\nUnload any Anaconda Python modules. Remove locally installed Anaconda Python from your `$PATH`.\n\n### Error: could not find environment: bds_atac\n\nUnload any Anaconda Python modules. Remove locally installed Anaconda Python from your `$PATH`.\n\n# Alternate Cloud-based Implementations\n\n* The [Encyclopedia of DNA Elements (ENCODE) Project](https://www.encodeproject.org/pipelines/) is in the process of adopting this pipeline for uniform processing of ENCODE ATAC-seq data. The [official ENCODE implementation](https://github.com/ENCODE-DCC) by the ENCODE Data Coordination Center will be an exact mirror of our pipeline on [the DNAnexus cloud](https://www.dnanexus.com/) (i.e. results will be exactly reproducible). Note that using this service requires a user to pay for cloud compute time.\n\n* [Epinomics](http://www.epinomics.co/) provides an independent, *free*, cloud-based pipeline implementation that adheres to the analysis protocol specifications of our pipeline. This implementation can be accessed at [https://open.epigenomics.co/#/encode](https://open.epigenomics.co/#/encode).\n\n### Error: Java disk space error: Disk quota exceeded\n\nThis error happens when `${TMPDIR}` or `/tmp` is full so Java cannot write temporary files on it. You can specify Java temporary directory with the following paramter.\n```\n-java_tmp_dir [PATH]\n```\n\n# Contributors\n\n* Jin wook Lee - PhD Student, Mechanical Engineering Dept., Stanford University\n* Chuan Sheng Foo - PhD Student, Computer Science Dept., Stanford University\n* Daniel Kim - MD/PhD Student, Biomedical Informatics Program, Stanford University\n* Nathan Boley - Postdoc, Dept. of Genetics, Stanford University\n* Anshul Kundaje - Assistant Professor, Dept. of Genetics, Stanford University\n\nWe'd also like to acknowledge Jason Buenrostro, Alicia Schep and William Greenleaf who contributed prototype code for some parts of the ATAC-seq pipeline.\n",
    "readme_length": 34099
  },
  {
    "name": "ATAC-seq",
    "full_name": "harvardinformatics/ATAC-seq",
    "description": null,
    "stars": 144,
    "forks": 42,
    "language": "Python",
    "url": "https://github.com/harvardinformatics/ATAC-seq",
    "topics": [],
    "created_at": "2017-10-06T14:12:09Z",
    "updated_at": "2025-11-22T08:30:23Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "## Table of Contents\n[ATAC-seq overview](#overview)<br>\n[Experimental design](#design)<br>\n[Compute access / Odyssey](#odyssey)<br>\n[Sequence reads](#reads)<br>\n[Quality control](#qc)<br>\n[Alignment](#alignments)<br>\n[Peak calling](#peak)<br>\n[Next steps](#next)<br>\n[References](#references)<br>\n\n\n## ATAC-seq overview <a name=\"overview\"></a>\n\nATAC-seq (Assay for Transposase-Accessible Chromatin with high-throughput sequencing) is a method for determining chromatin accessibility across the genome.  It utilizes a hyperactive Tn5 transposase to insert sequencing adapters into open chromatin regions (Fig. 1).  High-throughput sequencing then yields reads that indicate these regions of increased accessibility.\n\n<figure>\n  <img src=\"overview.png\" alt=\"ATAC-seq\" width=\"600\">\n  <figcaption><strong>Figure 1.</strong>  ATAC-seq overview (<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4374986/\">Buenrostro <i>et al.</i>, 2015</a>).</figcaption>\n</figure>\n<br>\n\n## Experimental design <a name=\"design\"></a>\n\nThe developers of the ATAC-seq method have published a [detailed protocol](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4374986/) for the laboratory procedure (Buenrostro *et al.*, 2015).\n\nHere are a few additional things to consider when planning an ATAC-seq experiment:\n\n### 1. Replicates\n\nLike most high-throughput sequencing applications, ATAC-seq requires that biological replicates be run.  This ensures that any signals observed are due to biological effects and not idiosyncracies of one particular sample or its processing.  To begin with, two replicates per experimental group are sufficient.\n\n### 2. Controls\n\nWith ATAC-seq, control groups are not typically run, presumably due to the expense and the limited value obtained.  A control for a given sample would be genomic DNA from the sample that, instead of transposase treatment, is fragmented (e.g. by sonication), has adapters ligated, and is sequenced along with the ATAC sample.  Such a control could be useful to help define regions of the genome that are more challenging to sequence or to align reads unambiguously.\n\n### 3. PCR amplification\n\nIn preparing libraries for sequencing, the samples should be amplified using as few PCR cycles as possible.  This will help to reduce PCR duplicates, which are exact copies of DNA fragments that can interfere with the biological signal of interest.\n\n### 4. Sequencing depth\n\nSequencing depth will vary based on the size of the reference genome and the degree of open chromatin expected.  For studies of human samples, [Buenrostro *et al.* (2015)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4374986/) recommend more than 50 million mapped reads per sample.\n\n### 5. Sequencing mode\n\nFor ATAC-seq, we recommend **paired-end sequencing**, for several reasons.\n\n* More sequence data leads to better alignment results.  Many genomes contain numerous repetitive elements, and failing to align reads to certain genomic regions unambiguously renders those regions inaccessible to the assay.  Additional sequence data, such as with paired-end sequencing, helps to reduce these alignment ambiguities.\n\n* With ATAC-seq, we are interested in knowing the full span of the DNA fragments generated by the assay.  A DNA fragment generated by the ATAC is typically longer than a sequence read, so a read will define only one end of the fragment.  Therefore, with single-end sequencing, we would have to guess where the other end of the fragment is.  Since paired-end sequencing generates reads from both ends, the full span of the DNA fragment is known precisely.\n\n* PCR duplicates are identified more accurately.  As noted above, PCR duplicates are artifacts of the procedure, and they should be removed as part of the analysis pipeline (see below for more details).  However, computational programs that remove PCR duplicates (e.g. Picard's [MarkDuplicates](http://broadinstitute.github.io/picard/command-line-overview.html#MarkDuplicates)) typically identify duplicates based on comparing ends of aligned reads.  With single-end reads, there is only one position to compare, and so any reads whose 5' ends match will be considered duplicates.  Thus, many false positives may result, and perfectly good reads will be removed from further analysis.  On the other hand, after paired-end sequencing, both ends of the original DNA fragments are defined.  To be declared a duplicate, both ends of one fragment would need to match both ends of another fragment, which is far less likely to occur by chance.  Therefore, paired-end sequencing leads to fewer false positives.\n\n\n## Compute access / Odyssey <a name=\"odyssey\"></a>\n\nThis document assumes that you have an account on the [Odyssey computer cluster](https://www.rc.fas.harvard.edu/training/introduction-to-odyssey-online/) of Harvard University.  An account can be requested [here](https://portal.rc.fas.harvard.edu/request/account/new).\n\nPrograms, like those listed below (e.g. FastQC, Bowtie2, MACS2), are run on Odyssey by submitting jobs via the [SLURM management system](https://www.rc.fas.harvard.edu/resources/running-jobs/).\nThe jobs take the form of shell scripts, which are submitted with the [sbatch command](https://www.rc.fas.harvard.edu/resources/running-jobs/#Submitting_batch_jobs_using_the_sbatch_command).  The shell scripts request computational resources (time, memory, and number of cores) for a job; it is better to request more resources than expected, rather than risk having a job terminated prematurely for exceeding its limits.\n\n\n## Sequence reads <a name=\"reads\"></a>\n\nThe raw sequence files generated by the sequencing core are in [FASTQ format](https://en.wikipedia.org/wiki/FASTQ_format).  They are gzip-compressed, with '.gz' file extensions.  It is unnecessary, not to mention wasteful of time and disk space, to decompress the sequence files; all common bioinformatics tools can analyze compressed files.\n\nFor paired-end sequencing, there are two files per sample: `<sample>.R1.fastq.gz` and `<sample>.R2.fastq.gz`.\n\nSamples that were sequenced on multiple lanes may have separate files for each lane; these should be concatenated using the `cat` command:\n\n    cat  <sample>.lane1.R1.fastq.gz  <sample>.lane2.R1.fastq.gz  >  <sample>.R1.fastq.gz\n    cat  <sample>.lane1.R2.fastq.gz  <sample>.lane2.R2.fastq.gz  >  <sample>.R2.fastq.gz\n\nHowever, different replicates should not be concatenated, but instead should be processed separately.\n\n\n## Quality control <a name=\"qc\"></a>\n\n### FastQC\n\nIt is generally a good idea to generate some quality metrics for your raw sequence data.  One tool that is commonly used for this purpose is [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/).\n\nOn Odyssey, each sequence file would be analyzed like this:\n\n    module load fastqc\n    fastqc <sample>.R1.fastq.gz\n\n\nFastQC is efficient; it can process a file of 20 million reads in about 5 minutes with less than 250MB memory used.  The output from FastQC is an HTML file, which can be examined via a web browser.  The report lists various statistics about your reads, such as their count and lengths.  It also provides graphical representations of your data based on a number of categories, including quality scores, GC levels, PCR duplicates, and adapter content.\n\nThe FastQC report is there to alert you to potential issues with your data, but it is not the final determinant of the outcome of your ATAC-seq experiment.  Do not be overly concerned if your FastQC reports contain one or more red 'X' marks; this is not a reason to delete your data and start all over again.\n\n\n### Adapter removal\n\nFor reads derived from short DNA fragments, the 3' ends may contain portions of the Illumina sequencing adapter.  This adapter contamination may prevent the reads from aligning to the reference genome and adversely affect the downstream analysis.  If you suspect that your reads may be contaminated with adapters (either from the FastQC report [\"Overrepresented sequences\" or \"Adapter content\" sections], or from the size distribution of your sequencing libraries), you should run an adapter removal tool.  Here are two options:\n\n#### 1. Cutadapt\n\nOne of the most widely used adapter removal programs is [cutadapt](http://cutadapt.readthedocs.io/en/stable/guide.html).  Cutadapt searches input reads for a given adapter sequence.  When it finds the adapter, it removes the adapter and everything that follows it.  Reads that do not match the adapter remain unaltered.\n\nSome things to note when using cutadapt:\n\n* The adapter sequences need to be provided via the `-a` argument.  If you do not know which adapters were used for your samples, consult the sequencing core.\n\n* Cutadapt will attempt to match a minimal length of the provided adapter sequence.  The default value for this argument (`-O`) is 3bp.  The downside of using such a small value is the possibility of false positives (trimming reads' good sequences that happen to match part of the adapter).  On the other hand, increasing this parameter will result in more false negatives, since reads with adapter contamination may contain sequencing errors that prevent a match.\n\n#### 2. NGmerge\n\nAn alternative approach to adapter removal is provided by [NGmerge](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2579-2), which was developed in the Informatics Group.  Unlike cutadapt, NGmerge does not require that the adapter sequences be provided, nor does it require a parameter for the minimum length of adapter to match (in fact, it does not perform adapter matching at all).  However, it works only with paired-end sequencing, so those with single-end sequencing should stick with cutadapt.\n\nNGmerge is based on the principle that, with paired-end sequencing, adapter contamination occurs only when both reads fully overlap.  The program aligns each pair of reads, and in cases where they align with 3' overhangs, it clips the overhangs of both reads (Fig. 2).  Reads that align without 3' overhangs (or do not align at all) remain unaltered.\n\n<figure>\n  <img src=\"adapter_removal.png\" alt=\"Adapter removal\" width=\"300\">\n  <figcaption><strong>Figure 2.</strong>  The original DNA fragment contains sequencing adapters on both ends (gray boxes).  Because the fragment is short, the paired-end reads (R1, R2) extend into the sequencing adapters.  NGmerge aligns the reads, and clips the 3' overhangs.</figcaption>\n</figure>\n<br>\n<br>\n\nNGmerge is available on Odyssey in the ATAC-seq module:\n\n    module load ATAC-seq\n    NGmerge -a  -1 <sample>.R1.fastq.gz  -2 <sample>.R2.fastq.gz  -o <name>\n\nThe output files will be `<name>_1.fastq.gz` and `<name>_2.fastq.gz`.  Of the many arguments available with NGmerge, here are the most important ones for this application:\n\n| Argument   | Description                                  |\n|:----------:|----------------------------------------------|\n| `-a`       | Adapter-removal mode (**must** be specified) |\n| `-e <int>` | Minimum length of overlap, i.e. the minimum DNA fragment length (default 50bp) |\n| `-n <int>` | Number of cores on which to run              |\n| `-v`       | Verbose mode                                 |\n\n\nFor more information about the parameters and options of NGmerge, please consult the [UserGuide](https://github.com/harvardinformatics/NGmerge/blob/master/UserGuide.pdf) or [README](https://github.com/harvardinformatics/NGmerge/blob/master/README.md) that accompanies the [source code](https://github.com/harvardinformatics/NGmerge) on GitHub.\n\nFor input files of 20 million paired reads, NGmerge should run in less than one hour on a single core, with minimal memory usage.  Of course, the run-time will decrease with more cores (`-n`).\n\nOther than adapter removal, we do not recommend any trimming of the reads.  Such adjustments can complicate later steps, such as the identification of PCR duplicates.\n\n\n## Alignment <a name=\"alignments\"></a>\n\nThe next step is to align the reads to a reference genome.  There are many programs available to perform the alignment.  Two of the most popular are [BWA](http://bio-bwa.sourceforge.net/bwa.shtml) and [Bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml).  We will focus on Bowtie2 here.\n\n### Genome indexing\n\nIn order to align reads to a genome, the reference sequence must be indexed.  This is a time- and memory-intense procedure, but it needs to be done only once for a given genome.\n\nFor many model organisms, the genome and pre-built reference indexes are available from [iGenomes](https://support.illumina.com/sequencing/sequencing_software/igenome.html).  Otherwise, Bowtie2 indexes are made from a FASTA genome file using the program `bowtie2-build`:\n\n```\nmodule load bowtie2\nbowtie2-build  <genome.fa>  <genomeIndexName>\n```\n\n### Alignment\n\nOnce the indexes are built, the reads can be aligned using Bowtie2.  A brief look at the [manual](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml) reveals the large number of parameters and options available with Bowtie2.  Here are a few that may benefit alignment of an ATAC-seq dataset on Odyssey:\n\n<table>\n  <tr>\n    <th align=\"center\">Argument</th>\n    <th>Description</th>\n  </tr>\n  <tr>\n    <td align=\"center\"><code>-X &lt;int&gt;</code></td>\n    <td>Maximum DNA fragment length (default 500bp).  If you anticipate that you may have DNA fragments longer than the default value, you should increase this parameter accordingly; otherwise, alignments from such fragments will be considered not properly paired (see Fig. 3B below).</td>\n  </tr>\n  <tr>\n    <td nowrap align=\"center\"><code>--very-sensitive</code></td>\n    <td>Bowtie2 has a number of alignment and effort parameters that interact in complex (and sometimes unexpected) ways.  Preset collections of these parameters are provided for convenience; the default is <code>--sensitive</code>, but better alignment results are frequently achieved with <code>--very-sensitive</code>.</td>\n  </tr>\n  <tr>\n    <td align=\"center\"><code>-p &lt;int&gt;</code></td>\n    <td>Number of cores on which to run</td>\n  </tr>\n</table>\n\n\nThe output is a [SAM file](https://samtools.github.io/hts-specs/SAMv1.pdf), which contains alignment information for each input read.  The SAM can be compressed to a binary format (BAM) and sorted with [SAMtools](http://www.htslib.org/doc/samtools.html).  This is best accomplished by piping the output from Bowtie2 directly to `samtools view` and `samtools sort`, e.g.:\n\n```\nmodule load bowtie2   # if not already loaded\nmodule load samtools\nbowtie2  --very-sensitive  -x <genomeIndexName>  -1 <name>_1.fastq.gz  -2 <name>_2.fastq.gz \\\n  |  samtools view -u -  \\\n  |  samtools sort -  >  <BAM>\n```\n\nFor input files of 20 million paired reads, this command takes around five hours.  This can be decreased by increasing the number of cores in the Bowtie2 command.  For example, one could specify eight cores for Bowtie2 with `-p 8` and adjust the request in the SLURM script to `#SBATCH -n 10` (that is, eight cores for Bowtie2 and one each for SAMtools view and sort).  The memory usage of Bowtie2 depends primarily on the genome length; enough must be requested to load the genome indexes.\n\nBowtie2 also provides (via stderr) a summary of the mapping results, including counts of reads analyzed, properly paired alignments, and reads that aligned to multiple genomic locations.  By default, Bowtie2 will randomly choose one of multiple equivalent mapping locations for a read.\n\n\n### Alignment adjustments\n\n#### Mitochondrial reads\n\nIt is a well-known problem that ATAC-seq datasets usually contain a large percentage of reads that is derived from mitochondrial DNA (for example, see [this discussion](http://seqanswers.com/forums/showthread.php?t=35318)).  Some have gone as far as [using CRISPR to reduce mitochondrial contamination](https://www.nature.com/articles/s41598-017-02547-w).  The recently published [Omni-ATAC method](https://www.nature.com/articles/nmeth.4396) uses detergents to remove mitochondria and is likely to be more accessible for most researchers (but, [do **not** follow their computational workflow](https://www.biorxiv.org/content/early/2018/12/17/496521)).\n\nRegardless of your lab protocol, you will have some mitochondrial reads in your sequence data.  Since there are no ATAC-seq peaks of interest in the mitochondrial genome, these reads will only complicate the subsequent steps.  Therefore, we recommend that they be removed from further analysis, via one of the following methods:\n\n1. Remove the mitochondrial genome from the reference genome before aligning the reads.  In human/mouse genome builds, the mitochondrial genome is labeled 'chrM'.  That sequence can be deleted from the reference prior to building the genome indexes.  The downside of this approach is that the alignment numbers will look much worse; all of the mitochondrial reads will count as unaligned.\n\n2. Remove the mitochondrial reads after alignment.  A python script, creatively named removeChrom, is available in the ATAC-seq module to accomplish this.  For example, to remove all 'chrM' reads from a BAM file, one would run this:\n\n```\nmodule load ATAC-seq samtools   # if not already loaded\nsamtools view -h  <inBAM>  |  removeChrom - - chrM  |  samtools view -b -  >  <outBAM>\n```\n\n#### PCR duplicates\n\nPCR duplicates are exact copies of DNA fragments that arise during PCR.  Since they are artifacts of the library preparation procedure, they may interfere with the biological signal of interest.  Therefore, they should be removed as part of the analysis pipeline.\n\nOne commonly used program for removing PCR duplicates is Picard's [MarkDuplicates](http://broadinstitute.github.io/picard/command-line-overview.html#MarkDuplicates).  It is run as follows:\n\n    module load picard\n    java -jar $PICARD_TOOLS_HOME/picard.jar MarkDuplicates I=<inBAM> O=<outBAM> M=dups.txt REMOVE_DUPLICATES=true\n\nThe output file specified by `M=` lists counts of alignments analyzed and duplicates identified.\n\n#### Non-unique alignments\n\nIt is not uncommon for short sequence reads to align equally well to multiple locations in a reference genome, especially given the repetitive nature of genomes.  Some researchers choose to remove non-uniquely aligned reads, using the `-q` parameter of `samtools view`.  For reads with multiple alignments, Bowtie2 (or BWA) will report only one alignment (by default) and will assign it a low mapping quality (MAPQ) score, which is defined as -10 * log<sub>10</sub>Pr{mapping position is wrong}.  To eliminate alignments with MAPQ &lt; 10 (i.e., where Bowtie2 has determined Pr{mapping position is wrong} &gt; 0.1), one would run the following:\n\n```\nmodule load samtools   # if not already loaded\nsamtools view -b  -q 10  <inBAM>  >  <outBAM>\n```\n\n## Peak calling <a name=\"peak\"></a>\n\nModel-based Analysis of ChIP-Seq ([MACS2](https://github.com/taoliu/MACS)) is a program for detecting regions of genomic enrichment.  Though designed for ChIP-seq, it works just as well on ATAC-seq and other genome-wide enrichment assays that have narrow peaks.  The main program in MACS2 is `callpeak`, and its options are described below.  (Note that the latest version of MACS2 on Odyssey, v2.1.2_dev, is different from the most recent official [release](https://pypi.org/project/MACS2/) from March 2016.)\n\nAs input, MACS2 takes the alignment files produced in the previous steps.  However, it is important to remember that the read alignments indicate only a portion of the DNA fragments generated by the ATAC.  Therefore, one must consider how one wants MACS2 to interpret the alignments.\n\n### Alignments to analyze\n\nWith paired-end sequencing, the types of alignments that are produced fall into two basic categories: \"properly paired\" and \"singletons\" (Fig. 3).\n\n<figure>\n  <img src=\"alignments.png\" alt=\"Alignment types\" width=\"700\">\n  <figcaption><strong>Figure 3.</strong>  Paired-end alignments.  <strong>A:</strong> Reads that are properly paired align in opposite orientations on the same reference sequence (chromosome).  The reads may overlap to some extent (bottom).  <strong>B:</strong> A singleton read (R1) can be not properly paired for several reasons: if its mate (R2) is unaligned (upper left), aligns to a different chromosome (upper right), aligns in the incorrect orientation (middle cases), or aligns in the correct orientation but at an invalid distance (bottom).  In all cases except the upper left, the R2 read is also a singleton.</figcaption>\n</figure>\n<br>\n<br>\n\nAn important consideration when using MACS2 is deciding which types of alignments should be analyzed and how those alignments should be interpreted.  The analysis mode is set by the `-f` argument.  Here are the options with MACS2:\n\n1. Analyze only properly paired alignments, but ignore R2 reads and treat R1 reads as singletons.  This is the default option (`-f AUTO`).  MACS2 creates a model of the fragment lengths and extends the 3' ends of the R1 reads to the calculated average length.  An alternative is to skip this model building and instead extend each read to a specified length (e.g., `--nomodel --extsize 300` for 300bp fragments).  The value of the length parameter is usually determined from the average size during library preparation (the default value is 200bp if no value is specified).  However, neither of these approaches utilizes the value of paired-end sequencing, which defines both fragment ends.\n\n2. Analyze only properly paired alignments with `-f BAMPE`.  Here, the fragments are defined by the paired alignments' ends, and there is no modeling or artificial extension.  Singleton alignments are ignored.  This is the preferred option for using only properly paired alignments.\n\n3. Analyze all alignments.  For this approach, a python script, SAMtoBED, is available in the ATAC-seq module.  This script converts the read alignments to BED intervals, treating the properly paired alignments as such and extending the singleton alignments as specified.  There are four options for the singletons: ignore them, keep them as is, extend them to an arbitrary length (similar to the `--extsize` option of MACS2), or extend them to the average length calculated from the properly paired alignments.  Here is an example command, using the \"extend to average length\" option (`-x`):\n\n```\nmodule load ATAC-seq\nsamtools view -h  <BAM>  |  SAMtoBED  -i -  -o <BED>  -x  -v\n```\n\nThe output from SAMtoBED is a [BED file](https://genome.ucsc.edu/FAQ/FAQformat.html#format1) that should be analyzed by MACS2 with `-f BEDPE`.\n\n(Note that the BEDTools program [bamtobed](http://bedtools.readthedocs.io/en/latest/content/tools/bamtobed.html) cannot be used here, since its output is in a nonstandard BED format that MACS2 cannot analyze.)\n\nIn deciding among these analysis options, it may help to consider the counts produced by Bowtie2, which indicate how many alignments fall into each category.  For example, if most of the reads are aligned in proper pairs, it may be sufficient to use option #2.  On the other hand, option #3 is preferred if a substantial fraction of the reads consists of singletons.\n\n\n### Other arguments\n\nIn addition to the analysis mode explained above, MACS2 has a number of parameters and options to set.  Here are a few to consider:\n\n<table>\n  <tr>\n    <th align=\"center\">Argument</th>\n    <th>Description</th>\n  </tr>\n  <tr>\n    <td align=\"center\"><code>-n &lt;str&gt;</code></td>\n    <td>Name of the sample.  The output files will be named using the specified string as the prefix.</td>\n  </tr>\n  <tr>\n    <td align=\"center\"><code>-g &lt;int&gt;</code></td>\n    <td>Effective genome size, i.e. the size of the organismâ€™s genome that can be analyzed (not including Ns, repetitive sequences, etc.).  This will be less than the actual genome size.  Parameters are provided for some model organisms, and the default value is <code>hs</code> (for <i>Homo sapiens</i>), which corresponds to a value of 2.7e9.</td>\n  </tr>\n  <tr>\n    <td align=\"center\"><code>-q &lt;float&gt;</code></td>\n    <td>Minimum <i>q</i>-value (adjusted <i>p</i>-value, or false discovery rate [FDR]) for peak calling (default 0.05).  Reducing this threshold will decrease the number of peaks identified by MACS2 but increase the confidence in the called peaks.</td>\n  </tr>\n  <tr>\n    <td nowrap align=\"center\"><code>--keep-dup &lt;arg&gt;</code></td>\n    <td>How to handle PCR duplicates (default: <code>--keep-dup 1</code>, i.e. remove all potential duplicates).  If PCR duplicates have been removed by another program, such as Picard's MarkDuplicates, then specify <code>--keep-dup all</code>.</td>\n  </tr>\n  <tr>\n    <td nowrap align=\"center\"><code>--max-gap &lt;int&gt;</code></td>\n    <td>Maximum gap between significant sites to cluster them together (default 50bp). <strong>(v2.1.2_dev only)</strong></td>\n  </tr>\n  <tr>\n    <td nowrap align=\"center\"><code>--min-length &lt;int&gt;</code></td>\n    <td>Minimum length of a peak (default 100bp). <strong>(v2.1.2_dev only)</strong></td>\n  </tr>\n</table>\n\n\nNote that MACS2 is not multithreaded, so it runs on a single core only.\n\nThe full MACS2 command for an ATAC-seq dataset from *C. elegans*, using all alignments (converted to BED intervals), might look like this:\n\n    module load macs2\n    macs2 callpeak  -t <BED>  -f BEDPE  -n NAME  -g ce  --keep-dup all\n\nCalling peaks for 20 million fragments should require less than ten minutes and 1GB of memory.\n\n\n### Output files\n\nThere are three output files from a standard `macs2 callpeak` run.  For a run with `-n NAME`, the output files are NAME_peaks.xls, NAME_peaks.narrowPeak, and NAME_summits.bed.  The most useful file is NAME_peaks.narrowPeak, a plain-text BED file that lists the genomic coordinates of each peak called, along with various statistics (fold-change, *p*- and *q*-values, etc.).\n\n\n## Next steps <a name=\"next\"></a>\n\nOnce the peaks have been identified by MACS2 for a set of samples, there are several follow-up steps that can be taken, depending on the experimental design. \n\n### Visualization\n\nSome researchers find it useful to generate visualizations of the peaks in a genomic context.\n\nFor ATAC-seq in model organisms, the peak file (NAME_peaks.narrowPeak) can be uploaded directly to the [UCSC genome browser](https://genome.ucsc.edu/cgi-bin/hgCustom).  Note that a peak file without a header line should have the following added to the beginning of the file:\n\n    track type=narrowPeak\n\nAn alternative visualization tool is the [Integrative Genomics Viewer](http://software.broadinstitute.org/software/igv/) (IGV).  Peak files can be loaded directly (File â†’ Load from File).  Viewing BAM files with IGV requires that they be sorted (by coordinate) and indexed using [SAMtools](http://www.htslib.org/doc/samtools.html).  Note that the BAMs show the read alignments, but not the full fragment lengths as generated by the ATAC and analyzed by MACS2 (in `BAMPE` or `BEDPE` mode).\n\n\n### Comparing peak files\n\nDetermining genomic regions that are common or different to a set of peak files is best accomplished with [BEDTools](http://bedtools.readthedocs.io/en/latest/index.html), a suite of software tools that enables \"genome arithmetic.\"\n\nFor example, [bedtools intersect](http://bedtools.readthedocs.io/en/latest/content/tools/intersect.html) determines regions that are common to two peak files, such as replicates of the same experimental group.\n\nFinding differences between two peak files, such as control vs. experimental groups, is accomplished via [bedtools subtract](http://bedtools.readthedocs.io/en/latest/content/tools/subtract.html).\n\n\n### Annotation\n\nIt is helpful to know what genomic features are near the peaks called by MACS2.  One program that is commonly used to annotate peaks is [ChIPseeker](https://bioconductor.org/packages/release/bioc/html/ChIPseeker.html).  Like MACS2, ChIPseeker was originally designed to be used in the analysis of ChIP-seq, but it works just as well with ATAC-seq.\n\nChIPseeker requires that the genome of interest be annotated with locations of genes and other features.  The [ChIPseeker user guide](https://bioconductor.org/packages/release/bioc/vignettes/ChIPseeker/inst/doc/ChIPseeker.html) is extremely helpful in using this R/Bioconductor package.\n\n\n### Motif finding\n\n[HOMER](http://homer.ucsd.edu/homer/introduction/basics.html) is a suite of software designed for [motif discovery](http://homer.ucsd.edu/homer/ngs/peakMotifs.html).  It takes a peak file as input and checks for the enrichment of both known sequence motifs and de novo motifs.\n\n\n\n## References <a name=\"references\"></a>\n\nAndrews S. (2010).  FastQC: a quality control tool for high throughput sequence data.  Available online at: http://www.bioinformatics.babraham.ac.uk/projects/fastqc\n\nBuenrostro JD, Giresi PG, Zaba LC, Chang HY, Greenleaf WJ.  Transposition of native chromatin for fast and sensitive epigenomic profiling of open chromatin, DNA-binding proteins and nucleosome position.  Nat Methods. 2013 Dec;10(12):1213-8.\n\nBuenrostro JD, Wu B, Chang HY, Greenleaf WJ.  ATAC-seq: A Method for Assaying Chromatin Accessibility Genome-Wide.  Curr Protoc Mol Biol. 2015 Jan 5;109:21.29.1-9.\n\nCorces MR, Trevino AE, Hamilton EG, Greenside PG, Sinnott-Armstrong NA, Vesuna S, Satpathy AT, Rubin AJ, Montine KS, Wu B, Kathiria A, Cho SW, Mumbach MR, Carter AC, Kasowski M, Orloff LA, Risca VI, Kundaje A, Khavari PA, Montine TJ, Greenleaf WJ, Chang HY.  An improved ATAC-seq protocol reduces background and enables interrogation of frozen tissues.  Nat Methods. 2017 Oct;14(10):959-962.\n\nGaspar JM. Improved peak-calling with MACS2. bioRxiv. 2018 Dec 17. doi: http://dx.doi.org/10.1101/496521\n\nGaspar JM. NGmerge: merging paired-end reads via novel empirically-derived models of sequencing errors. BMC Bioinformatics. 2018 Dec 20;19(1):536.\n\nHeinz S, Benner C, Spann N, Bertolino E, Lin YC, Laslo P, Cheng JX, Murre C, Singh H, Glass CK.  Simple combinations of lineage-determining transcription factors prime cis-regulatory elements required for macrophage and B cell identities.  Mol Cell. 2010 May 28;38(4):576-89.\n\nLangmead B, Salzberg SL.  Fast gapped-read alignment with Bowtie 2.  Nat Methods. 2012 Mar 4;9(4):357-9.\n\nLi H, Handsaker B, Wysoker A, Fennell T, Ruan J, Homer N, Marth G, Abecasis G, Durbin R; 1000 Genome Project Data Processing Subgroup.  The Sequence Alignment/Map format and SAMtools.  Bioinformatics. 2009 Aug 15;25(16):2078-9.\n\nMartin M.  Cutadapt removes adapter sequences from high-throughput sequencing reads.  EMBnet.journal. 2011;17:10-2.\n\nMontefiori L, Hernandez L, Zhang Z, Gilad Y, Ober C, Crawford G, Nobrega M, Jo Sakabe N.  Reducing mitochondrial reads in ATAC-seq using CRISPR/Cas9.  Sci Rep. 2017 May 26;7(1):2451.\n\nQuinlan AR.  BEDTools: The Swiss-Army Tool for Genome Feature Analysis.  Curr Protoc Bioinformatics. 2014 Sep 8;47:11.12.1-34.\n\nYu G, Wang LG, He QY.  ChIPseeker: an R/Bioconductor package for ChIP peak annotation, comparison and visualization.  Bioinformatics. 2015 Jul 15;31(14):2382-3.\n",
    "readme_length": 30981
  },
  {
    "name": "NucleoATAC",
    "full_name": "GreenleafLab/NucleoATAC",
    "description": "nucleosome calling using ATAC-seq",
    "stars": 109,
    "forks": 32,
    "language": "Python",
    "url": "https://github.com/GreenleafLab/NucleoATAC",
    "topics": [
      "atac-seq",
      "bioinformatics",
      "nuclesosome"
    ],
    "created_at": "2015-03-10T01:30:24Z",
    "updated_at": "2025-11-30T14:04:23Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# NucleoATAC\n\n**This package is no longer being actively maintained; feel free to post issues that others in the community may respond to, but this package will likely not be updated further. Additionally, if anyone wants to maintain a fork of the package or has developed an alternative package for similar purposes, would be happy to link to that repo here.**\n\nPython package for calling nucleosomes using ATAC-seq data.\nAlso includes general scripts for working with paired-end ATAC-seq data (or potentially other paired-end data).\n\nPlease cite our paper at [Genome Research](http://genome.cshlp.org/content/25/11/1757) if you use this tool in your research.\n\nPlease use GitHub Issues to bring up any errors that occur with software rather than emailing authors.\n\nNote on Versions:  \n\n* version 0 represents code used for biorxiv manuscript\n* version 0.2.1 was used for Genome Research manuscript (See Supplemental Information as well)\n\nDocumentation  can be found at http://nucleoatac.readthedocs.org/en/latest/\n\nIf you want to easily read in NucleoATAC outputs into R for further processing or exploration, check out [NucleoATACR](https://github.com/GreenleafLab/NucleoATACR/)\n\nCurrently NucleoATAC only supports Python 2.7 (No Python 3). If anyone is interested in adding Python 3 support, pull requests welcome :smile:\n",
    "readme_length": 1326
  },
  {
    "name": "SCALE",
    "full_name": "jsxlei/SCALE",
    "description": "Single-cell ATAC-seq analysis via Latent feature Extraction",
    "stars": 103,
    "forks": 20,
    "language": "Python",
    "url": "https://github.com/jsxlei/SCALE",
    "topics": [],
    "created_at": "2018-10-19T07:22:53Z",
    "updated_at": "2025-09-20T16:55:13Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "[![Stars](https://img.shields.io/github/stars/jsxlei/SCALE?logo=GitHub&color=yellow)](https://github.com/jsxlei/scale/stargazers)\n[![PyPI](https://img.shields.io/pypi/v/scale.svg)](https://pypi.org/project/scale)\n[![Downloads](https://pepy.tech/badge/scale)](https://pepy.tech/project/scale-atac)\n# Single-Cell ATAC-seq analysis via Latent feature Extraction\n![](https://github.com/jsxlei/SCALE/wiki/png/model.png)\n\n## News \n2022.06.30 Introduce the highly_variable_genes from scanpy to filter peaks and support for input from multiomics data h5mu\n2021.04    A new online integration tool [SCALEX](https://github.com/jsxlei/SCALEX) on scRNA-seq and scATAC-seq is available!    \n2021.01.14 Update to compatible with [h5ad](https://anndata.readthedocs.io/en/latest/anndata.AnnData.html) file and [scanpy](https://scanpy.readthedocs.io/en/stable/index.html)\n\n## Installation  \n\nSCALE neural network is implemented in [Pytorch](https://pytorch.org/) framework.  \nRunning SCALE on CUDA is recommended if available.   \n\n#### install from PyPI\n\n    pip install scale\n\t\n#### install latest develop version from GitHub\n\n\tpip install git+https://github.com/jsxlei/SCALE.git\n\nor download and install\n\n\tgit clone git://github.com/jsxlei/SCALE.git\n\tcd SCALE\n\tpython setup.py install\n    \nInstallation only requires a few minutes.  \n\n## Quick Start\n\n#### Input\n* h5ad file\n* **count matrix file**:  \n\t* row is peak and column is barcode, in **txt** / **tsv** (sep=**\"\\t\"**) or **csv** (sep=**\",\"**) format\n* mtx **folder** contains **three files**:   \n\t* **count file**: count in **mtx** format, filename contains key word **\"count\"** / **\"matrix\"**    \n\t* **peak file**: 1-column of peaks **chr_start_end**, filename contains key word **\"peak\"**  \n\t* **barcode file**: 1-column of barcodes, filename contains key word **\"barcode\"**\n* h5mu file, e.g. filename.h5mu/atac\n\n#### Run \n\n    SCALE.py -d [input]\n\n#### Output\nOutput will be saved in the output folder including:\n* **model.pt**:  saved model to reproduce results cooperated with option --pretrain\n* **adata.h5ad**:  saved data including Leiden cluster assignment, latent feature matrix and UMAP results.\n* **umap.pdf**:  visualization of 2d UMAP embeddings of each cell\n\n#### Imputation  \nGet binary imputed data in adata.h5ad file using scanpy **adata.obsm['binary']** with option **--binary** (recommended for saving storage)\n\n    SCALE.py -d [input] --binary  \n    \nor get numerical imputed data in adata.h5ad file using scanpy **adata.obsm['imputed']** with option **--impute**\n\n    SCALE.py -d [input] --impute\n     \n#### Useful options  \n* [--outdir] or [-o]: save results in a specific folder\n* [--embed]: tSNE/UMAP, embed feature by tSNE or UMAP \n* [--min_peaks]: filter low quality cells by valid peaks number, default 100\n* [--min_cells]: filter low quality peaks by valid cells number, default 3 (previous default is 0.01), now replaced by [--n_feature] \n* [--n_feature]: filter peaks by selecting highly variable features, default 100,000; use [--n_feature] -1 to disable.\n* [--lr]: modify the initial learning rate, default is 0.002: \n* [--max_iter] or [-i]: max iteration number, default is 30000  \n* [--seed]: random seed for parameter initialization, default is 18 \n* [--binary]: binarize the imputation values\n* [-k]: if cluster number is known\n\t\n\n#### Help\nLook for more usage of SCALE\n\n\tSCALE.py --help \n\nUse functions in SCALE packages.\n\n\timport scale\n\tfrom scale import *\n\tfrom scale.plot import *\n\tfrom scale.utils import *\n\t\n#### Running time\n<p float=\"left\">\n  <img src=\"https://github.com/jsxlei/SCALE/wiki/png/runtime.png\" width=\"350\" />\n  <img src=\"https://github.com/jsxlei/SCALE/wiki/png/memory.png\" width=\"350\" /> \n</p>\n\n\n## Tutorial\n\n\n**[Tutorial Forebrain](https://github.com/jsxlei/SCALE/wiki/Forebrain)**   Run SCALE on dense matrix **Forebrain** dataset (k=8, 2088 cells)\n\n\n#### Data availability  \n* [Forebrain](http://zhanglab.net/SCALE_SOURCE_DATA/Forebrain.h5ad)\n* [Splenocyte](http://zhanglab.net/SCALE_SOURCE_DATA/Splenocyte.h5ad)\n* [mouse_atlas](http://zhanglab.net/SCALE_SOURCE_DATA/mouse_atlas.h5ad)\n* [InSilico](http://zhanglab.net/SCALE_SOURCE_DATA/InSilico.h5ad)\n* [Leukemia](http://zhanglab.net/SCALE_SOURCE_DATA/Leukemia.h5ad)\n* [GM12878vsHEK](http://zhanglab.net/SCALE_SOURCE_DATA/GM12878vsHEK.h5ad)\n* [GM12878vsHL](http://zhanglab.net/SCALE_SOURCE_DATA/GM12878vsHL.h5ad)\n* [Breast_Tumor](http://zhanglab.net/SCALE_SOURCE_DATA/Breast_Tumor.h5ad)\n\n\n## Reference\n[Lei Xiong, Kui Xu, Kang Tian, Yanqiu Shao, Lei Tang, Ge Gao, Michael Zhang, Tao Jiang & Qiangfeng Cliff Zhang. SCALE method for single-cell ATAC-seq analysis via latent feature extraction. Nature Communications, (2019).](https://www.nature.com/articles/s41467-019-12630-7)\n",
    "readme_length": 4730
  },
  {
    "name": "atacseq_pipeline",
    "full_name": "epigen/atacseq_pipeline",
    "description": "Ultimate ATAC-seq Data Processing, Quantification and Annotation Snakemake Workflow and MrBiomics Module.",
    "stars": 93,
    "forks": 2,
    "language": "Python",
    "url": "https://github.com/epigen/atacseq_pipeline",
    "topics": [
      "atac-seq",
      "bash",
      "bioinformatics",
      "biomedical-data-science",
      "ngs",
      "pipeline",
      "python",
      "snakemake",
      "workflow"
    ],
    "created_at": "2021-03-22T12:53:47Z",
    "updated_at": "2025-11-25T16:08:05Z",
    "homepage": "https://epigen.github.io/atacseq_pipeline/",
    "license": "MIT License",
    "readme": "[![MrBiomics](https://img.shields.io/badge/MrBiomics-red)](https://github.com/epigen/MrBiomics/)\n[![DOI](https://zenodo.org/badge/350342694.svg)](https://zenodo.org/doi/10.5281/zenodo.6323634)\n[![](https://tokei.rs/b1/github/epigen/atacseq_pipeline?category=code)]() \n[![](https://tokei.rs/b1/github/epigen/atacseq_pipeline?category=files)]()\n[![GitHub license](https://img.shields.io/github/license/epigen/atacseq_pipeline)](https://github.com/epigen/atacseq_pipeline/blob/master/LICENSE)\n![GitHub Release](https://img.shields.io/github/v/release/epigen/atacseq_pipeline)\n[![Snakemake](https://img.shields.io/badge/Snakemake->=8.20.1-green)](https://snakemake.readthedocs.io/en/stable/)\n\n# Ultimate ATAC-seq Data Processing, Quantification & Annotation Pipeline\nA [Snakemake 8](https://snakemake.readthedocs.io/en/stable/) workflow implementation of the [BSF's](https://www.biomedical-sequencing.org/) [ATAC-seq Data Processing Pipeline](https://github.com/berguner/atacseq_pipeline \"ATAC-seq Data Processing Pipeline\") extended by downstream quantification and annotation steps using Bash and Python.\n\n> [!NOTE]  \n> This workflow adheres to the module specifications of [MrBiomics](https://github.com/epigen/MrBiomics), an effort to augment research by modularizing (biomedical) data science. For more details, instructions, and modules check out the project's repository.\n>\n> â­ï¸ **Star and share modules you find valuable** ðŸ“¤ - help others discover them, and guide our future work!\n\n> [!IMPORTANT]  \n> **If you use this workflow in a publication, please don't forget to give credit to the authors by citing it using this DOI [10.5281/zenodo.6323634](https://doi.org/10.5281/zenodo.6323634).**\n\n![Workflow Rulegraph](./workflow/dags/rulegraph.svg)\n\n# ðŸ–‹ï¸ Authors\n- [Stephan Reichl](https://github.com/sreichl)\n- [Bekir ErgÃ¼ner](https://github.com/berguner)\n- [Daniele Barreca](https://github.com/DanieleBarreca)\n- [Lukas Folkman](https://github.com/lukas-folkman)\n- [Fangwen Zhao](https://github.com/fwzhao)\n- [Rob ter Horst](https://github.com/rubbert)\n- [Lina Dobnikar](https://github.com/ld401)\n- [Christoph Bock](https://github.com/chrbock)\n\n# ðŸ’¿ Software\nThis project wouldn't be possible without the following software and their dependencies:\n\n| Software       | Reference (DOI)                                   |\n| :------------: | :-----------------------------------------------: |\n| bedtools       | https://doi.org/10.1093/bioinformatics/btq033     |\n| Bowtie2        | https://doi.org/10.1038/nmeth.1923                |\n| deeptools      | https://doi.org/10.1093/nar/gkw257                |\n| ENCODE         | https://doi.org/10.1038/s41598-019-45839-z        |\n| fastp          | https://doi.org/10.1093/bioinformatics/bty560     |\n| HOMER          | https://doi.org/10.1016/j.molcel.2010.05.004      |\n| MACS2          | https://doi.org/10.1186/gb-2008-9-9-r137          |\n| MultiQC        | https://doi.org/10.1093/bioinformatics/btw354     |\n| pybedtools     | https://doi.org/10.1093/bioinformatics/btr539     |\n| pandas         | https://doi.org/10.5281/zenodo.3509134            |\n| samblaster     | https://doi.org/10.1093/bioinformatics/btu314     |\n| samtools       | https://doi.org/10.1093/bioinformatics/btp352     |\n| Snakemake      | https://doi.org/10.12688/f1000research.29032.2    |\n| UROPA          | https://doi.org/10.1038/s41598-017-02464-y        |\n\n# ðŸ”¬ Methods\nThis is a template for the Methods section of a scientific publication and is intended to serve as a starting point. Only retain paragraphs relevant to your analysis. References [ref] to the respective publications are curated in the software table above. Versions (ver) have to be read out from the respective conda environment specifications (`workflow/envs/*.yaml file`) or post-execution in the result directory (`atacseq_pipeline/envs/*.yaml`). Parameters that have to be adapted depending on the data or workflow configurations are denoted in squared brackets e.g., [X].\n\n**Processing.**\nSequencing adapters were removed using the software fastp (ver) [ref]. Bowtie2 (ver) [ref] was used for the alignment of the short reads (representing locations of transposition events) to the [GRCh38 (hg38)/GRCm38 (mm10)] assembly of the [human/mouse] genome using the â€œ--very-sensitiveâ€ parameter. PCR duplicates were marked using samblaster (ver) [ref]. Aligned BAM files were then sorted, filtered using ENCODE blacklisted regions [ref], samtools view flags [SAM_flag], and indexed using samtools (ver) [ref]. To detect the open chromatin regions, peak calling was performed using MACS2 (ver) [ref] using the â€œ--nomodelâ€, â€œ--keep-dup [macs2_keep_dup]â€ and â€œ--extsize 147â€ options on each sample. HOMER (ver) [ref] function findMotifs was used for motif enrichment analysis of the detected open chromatin regions. Quality control metrics were aggregated and reported using MultiQC (ver) [ref], [X] sample(s) needed to be removed.\n\n**Quantification.**\nA consensus region set, comprising of [X] genomic regions, was generated, by merging the identified peak summits, extended by [slop_extension]bp on both sides using the slop function from bedtools (ver) [ref] and pybedtools (ver) [ref], across all samples while again discarding peaks overlapping blacklisted features as defined by the ENCODE project [ref]. The consensus region set was used to quantify the chromatin accessibility in each sample by summing the number of reads overlapping each consensus region. The consensus region set, and sample-wise quantification of accessibility was performed using bedtools (ver) [ref] and pybedtools (ver) [ref]. Furthermore, the consensus region set was used to quantify the peak support per sample and each region was mapped to its closest TSS according to the HOMER annotation within proximal TSS up and downstream distances [proximal_size_up/down] yielding a gene/TSS-based quantification. Complementary, all promoter regions, defined by the same parameters, were quantified for each sample and aggregated to yield a gene/promoter-based quantification. Finally, all sample-wise enriched known motifs according to HOMER were aggregated.\n\n**Annotation.**\nConsensus regions were annotated using annotatePeaks function from HOMER (ver) [ref]. Additionally, we annotated all consensus regions using UROPA (ver) [ref] and genomic features from the [GENCODE vX] basic gene annotation as: â€œTSS proximalâ€ if the regionâ€™s midpoint was within [X] bp upstream or [X] bp downstream from a TSS, or if the region overlapped with a TSS; â€œgene bodyâ€ if the region overlapped with a gene; â€œdistalâ€ if the regionâ€™s midpoint was within [X] bp of a TSS; and â€œintergenicâ€ otherwise. For each region, only the closest feature was considered, and the annotations took precedence in the following order: TSS proximal, gene body, distal, and intergenic. Finally, bedtools was employed to quantify nucleotide counts and proportional content per consensus region.\n\nThe processing and quantification described here was performed using a publicly available Snakemake [ver] (ref) workflow [[10.5281/zenodo.6323634](https://doi.org/10.5281/zenodo.6323634)].\n\n# ðŸš€ Features\n- Processing (`results/`)\n    - Alignment of both single-end and paired-end reads in raw/unaligned/unmapped [uBAM](https://gatk.broadinstitute.org/hc/en-us/articles/360035532132-uBAM-Unmapped-BAM-Format) format with Bowtie2.\n      - Filtering using `samtools view` can be configured using [SAM Flags](https://broadinstitute.github.io/picard/explain-flags.html) (`SAM_flag`).\n    - Peak calling with `MACS2`.\n      - Duplicate handling can be configured using `macs2_keep_dup`.\n      - Even though the peak support of a region in a certain sample is 0, does not mean that there are no reads counted in the count matrix, it just states that there was no peak called.\n      - The peak support can be >1 for certain samples in case of a consensus region spanning more than one peak within the respective sample.\n    - Peak annotation and motif analysis HOMER.\n    - Quantification of TSS coverage.\n- Reporting (`report/`)\n    - MultiQC report generation using MultiQC, extended with an in-house developed plugin [atacseq_report](./workflow/scripts/multiqc_atacseq).\n    - Sample annotation is visualized as a hierarchically-clustered QC heatmap with matching metadata annotation, exported both as a `PNG` and an interactive `HTML` with metadata as tooltips (`sample_annotation.{png|html}`).\n- Quantification (`counts/`)\n    - Consensus region set generation across all called peaks (`consensus_regions.bed`).\n    - Read count quantification of the consensus regions across samples, yielding a count matrix with dimensions consensus regions X samples (`consensus_counts.csv`).\n    - Peak support quantification of the consensus regions across samples, yielding a count matrix with dimensions consensus regions X samples (`support_counts.csv`).\n    - Consensus regions mapped to closest gene TSS according to HOMER (Distance to TSS) within proximal TSS up and downstream distances (`TSS_regions.bed`, `TSS_counts.csv`, `TSS_annotation.csv`).\n    - Read count quantification of promoter regions based on provided proximal TSS up and downstream distances (`promoter_regions.bed`, `promoter_counts.csv`, `promoter_annotation.csv`).\n      - [Pseudoautosomal regions in human](https://www.ensembl.org/info/genome/genebuild/human_PARS.html) chromosome `Y` are skipped.\n    - Aggregation of all sample-wise HOMER known motif enrichment results into one CSV in long-format (`HOMER_knownMotifs.csv`).\n- Annotation (`counts/`)\n    - Sample annotation file based on `MultiQC` general stats and provided annotations for downstream analysis (`sample_annotation.csv`).\n    - Consensus region set annotation using (`consensus_annotation.csv`)\n      - `UROPA` with regulatory build and gencode as references, configurable here: `workflow/resources/UROPA/*.txt`.\n      - `HOMER` with `annotatePeaks.pl`. NB: We have empirically found, that some human sex genes, e.g., the well established protein coding genes UTY and STS, are not annotated.\n      - `bedtools` for nucleotide counts/content (e.g., % of GC).\n\n\n\n> [!IMPORTANT]\n> **Duplicate Handling Strategy: PCR vs. Biological Signal**\n> \n> ATAC-seq libraries often contain **PCR duplicates** (artifacts from amplification) and **biological duplicates** (independent transposition events at the same locus). Differentiating them is critical: PCR duplicates inflate false positives, while removing biological duplicates reduces signal dynamic range.\n> \n> You can control duplicate handling at two distinct stages in `config/config.yaml`, and should do so with intention:\n> \n> 1. **Alignment Filtering (`SAM_flag`):** Controls whether duplicates are physically removed from the output BAM files.\n>    * **Recommendation:** Keep duplicates in the `BAM` (`SAM_flag: 2316`). This preserves library complexity information (useful for QC) and allows downstream tools to model redundancy themselves.\n>    * **Alternative:** Hard-filter duplicates (`SAM_flag: 3340`) in case of low-complexity/noisy libraries or to reduce file size.\n> \n> 2. **Peak Calling (`macs2_keep_dup`):** Controls how `MACS2` counts duplicates when identifying open chromatin regions.\n>    * **`auto` (Default/Recommended):** `MACS2` calculates the maximum expected duplicates based on sequencing depth and genome size, retaining biological signal while removing excessive PCR artifacts.\n>    * **`1`:** Conservative approach. Counts only one read per genomic location. Removes all potential PCR bias but may underestimate signal in highly open regions (e.g., promoters).\n>    * **`all`:** Uses every read. Only use this if you have very low duplication rates or have already hard-filtered PCR duplicates in the alignment step.\n> \n> **The decision depends** also on your downstream analysis steps e.g., rigorous filtering (e.g., using `edgeR::filterByExpr`) and/or accounting for PCR bias by normalization conditional on genomic region length and GC content (e.g., [CQN](https://academic.oup.com/biostatistics/article/13/2/204/1746212)) and goals (e.g., differential accessibility analysis).\n> **We recommend** reading this **ChIP-seq** tutorial's section on [\"Removing redundancy\"](https://hbctraining.github.io/Intro-to-ChIPseq/lessons/05_peak_calling_macs.html), while keeping in mind critical assay differences.\n\n# ðŸ› ï¸ Usage\nThese steps are the recommended usage for this workflow:\n\n0. Configure the workflow by pointing to the relevant resources, e.g., downloaded from Zenodo for [hg38 or mm10 (see instructions below)](#resources).\n1. Perform only the processing, by setting the pass_qc annotation for all samples to 0.\n2. Use the generated MultiQC report (result_path/ataceq_pipeline/report/multiqc_report.html) to judge the quality of each sample (see tips in the next section on [Quality Control](#quality-control)).\n3. Fill out the mandatory quality control column (pass_qc) in the annotation file accordingly (everything >0 will be included in the downstream steps).\n4. Finally, execute the remaining downstream quantification and annotation steps by running the workflow. Thereby only the samples that passed quality control will be included in the consensus region set generation (i.e., the feature space) and all downstream steps.\n\n> [!NOTE]\n> Although inputs and parameters may be identical, **MACS2 peak calling can yield slightly varying results** (Â± a few peaks) due to stochastic elements in its algorithm (e.g., duplicate handling). This minor variability in peak calls sohuld have no impact on downstream analyses or the overall robustness of results.\n\nThis workflow is written with Snakemake and its usage is described in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog?usage=epigen/atacseq_pipeline).\n\n# âš™ï¸ Configuration\nDetailed specifications can be found here [./config/README.md](./config/README.md)\n\n# ðŸ“– Examples\nExplore a detailed example showcasing module usage and downstream analysis in our comprehensive end-to-end [MrBiomics Recipe](https://github.com/epigen/MrBiomics?tab=readme-ov-file#-recipes) for [ATAC-seq Analysis](https://github.com/epigen/MrBiomics/wiki/ATAC%E2%80%90seq-Analysis-Recipe), including data, configuration, annotation and results.\n\n# ðŸ” Quality Control\nBelow are some guidelines for the manual quality control of each sample using the generated `MultiQC` report and visualized (interactive) sample annotation, but keep in mind that every experiment/dataset is different. Thresholds are general suggestions and may vary based on experiment type, organism, and library prep.\n\n1. Reads Mapped ~ $30\\cdot 10^{6}$ ($>20\\cdot 10^{6}$ at least)\n2. % Aligned >90%\n3. % Mitochondrial <10%\n4. Peaks (depend on reads)\n    - FriP (Fraction of reads in Peaks) ~ >20% (can be misleading as 80-90% are also not good)\n    - Regulatory regions >10% (as it is roughly 10% of the genome)\n    - TSS (Transcription Start Site) normalized coverage ideally > 4 (at least >2)\n    - % Duplications â€œnot excessiveâ€\n5. Inspect [Genome Browser Tracks](https://github.com/epigen/genome_tracks/) using UCSC Genome Browser (online) or IGV (local)\n    - Compare all samples to the best, based on above's QC metrics.\n    - Check cell type / experiment-specific markers or sex chromosome (`X`/`Y`) for accessibility as positive controls.\n    - Check e.g., developmental regions for accessibility as negative controls.\n6. [Unsupervised Analysis](https://github.com/epigen/unsupervised_analysis) (e.g., PCA or UMAP)\n    - Identify outliers/drivers of variation, especially in the control samples and within replicates.\n\n> [!IMPORTANT]  \n> Sometimes reads map to `Y` in females, because `X` and `Y` chromosomes both have [pseudoautosomal regions (PARs)](https://www.ensembl.org/info/genome/genebuild/human_PARS.html) that are common between the two chromosomes.\n  \nMy personal QC value scheme to inform downstream analyses (e.g., unsupervised analysis)\n- 0 = did not pass\n- 2 options\n  - for every metric that is not ok subtract 0.25 from 1, which means it requires 4 â€œstrikesâ€ for a sample to be removed due to low quality.\n  - alternative\n      - 0.5 = passed with reservations (e.g., metrics and genome browser tracks were not optimal, but still good enough)\n      - 0.75 = not ideal (e.g., at least metrics or IGV tracks were not optimal)\n- 1 = passed (perfect)\n\nFinally, a previous PhD student in our lab, [AndrÃ© Rendeiro](https://orcid.org/0000-0001-9362-5373), wrote about [\"ATAC-seq sample quality, wet lab troubleshooting and advice\"](https://github.com/epigen/open_pipelines/blob/master/pipelines/atacseq.md#sample-quality-wet-lab-troubleshooting-and-advice).\n\n# ðŸ”— Links\n- [GitHub Repository](https://github.com/epigen/atacseq_pipeline/)\n- [GitHub Page](https://epigen.github.io/atacseq_pipeline/)\n- [Zenodo Repository](https://doi.org/10.5281/zenodo.6323634)\n- [Snakemake Workflow Catalog Entry](https://snakemake.github.io/snakemake-workflow-catalog?usage=epigen/atacseq_pipeline)\n\n# ðŸ“š Resources\n- Data Resources: To ensure the reproducibility of results and to make the workflow accessible we provide all required reference data for the analysis of ATAC-seq samples for [human GRCh38 (hg38)](https://doi.org/10.5281/zenodo.6344173) and [mouse GRCm38 (mm10)](https://doi.org/10.5281/zenodo.6344321) genomes on Zendodo.\n\n  **Command line**\n  ```console\n  # download Zenodo records using zenodo_get\n\n  # install zenodo_get v1.3.4\n  conda install -c conda-forge zenodo_get=1.3.4\n\n  # human GRCh38 (hg38)\n  zenodo_get --record 6344173 --output-dir=resources/atacseq_pipeline/hg38/\n  cd resources/atacseq_pipeline/hg38\n  unzip indices_for_Bowtie2.zip && rm indices_for_Bowtie2.zip\n\n  # mouse GRCm38 (mm10)\n  zenodo_get --record 6344322 --output-dir=resources/atacseq_pipeline/mm10/\n  cd resources/atacseq_pipeline/mm10\n  unzip indices_for_Bowtie2.zip && rm indices_for_Bowtie2.zip\n  ```\n  **Snakemake rule** for workflows\n  ```python\n  #### Get resources from Zenodo as custom Snakemake rule ####\n  # Downloads Bowtie2 indices for hg38 from Zenodo record 6344173 and unpacks them.\n    rule MyATAC_get_resources:\n        output:\n            \"resources/MyATAC/atacseq_pipeline/hg38/gencode.v38.basic.annotation.gtf\",\n            resource_dir = directory(\"resources/MyATAC/atacseq_pipeline/hg38/\"),\n        params:\n            zenodo_record = \"6344173\",\n            zip_filename = \"indices_for_Bowtie2.zip\"\n        conda:\n            \"../envs/zenodo_get.yaml\"\n        shell:\n            \"\"\"\n            # Download the specific record to the target directory\n            zenodo_get --record {params.zenodo_record} --output-dir={output.resource_dir}\n    \n            # Change directory, unzip the specific file, and remove the zip archive\n            # Using && ensures commands run sequentially and stop if one fails\n            cd {output.resource_dir} && \\\n            unzip {params.zip_filename} && \\\n            rm {params.zip_filename}\n            \"\"\"\n  ```\n- [How to convert feature lists to BED files](https://github.com/epigen/enrichment_analysis?tab=readme-ov-file#-how-to-convert-feature-lists-to-bed-files)\n- Recommended compatible [MrBiomics](https://github.com/epigen/MrBiomics) modules for\n  - upstream analysis:\n      - [Fetch Public Sequencing Data and Metadata Using iSeq](https://github.com/epigen/fetch_ngs/) to retrieve and prepare public ATAC-ses data for downstream processing.\n  - downstream analysis (in that order):\n      - [Genome Browser Track Visualization](https://github.com/epigen/genome_tracks/) for quality control and visual inspection/analysis of genomic regions/genes of interest or top hits.\n      - [<ins>Sp</ins>lit, F<ins>ilter</ins>, Norma<ins>lize</ins> and <ins>Integrate</ins> Sequencing Data](https://github.com/epigen/spilterlize_integrate/) after count quantification.\n      - [Unsupervised Analysis](https://github.com/epigen/unsupervised_analysis) to understand and visualize similarities and variations between cells/samples, including dimensionality reduction and cluster analysis. Useful for all tabular data including single-cell and bulk sequencing data.\n      - [Differential Analysis with limma](https://github.com/epigen/dea_limma) to identify and visualize statistically significantly different features (e.g., genes or genomic regions) between sample groups.\n      - [Enrichment Analysis](https://github.com/epigen/enrichment_analysis) for biomedical interpretation of (differential) analysis results using prior knowledge.\n    - [Introduction to ChIP-seq using high performance computing](https://hbctraining.github.io/Intro-to-ChIPseq/)\n\n# ðŸ“‘ Publications\nThe following publications successfully used this module for their analyses.\n- [Haladik et al. (2025) Cell Reports Medicine - Image-based drug screening combined with molecular profiling identifies signatures and drivers of therapy resistance in pediatric AML](https://www.cell.com/cell-reports-medicine/fulltext/S2666-3791(25)00377-5)\n- [Traxler, Reichl et al. (2025) Cell Systems - Integrated time-series analysis and high-content CRISPR screening delineate the dynamics of macrophage immune regulation](https://doi.org/10.1016/j.cels.2025.101346)\n- [Troester et al. (2025) Nature Communications - Transcriptional and epigenetic rewiring by the NUP98::KDM5A fusion oncoprotein directly activates CDK12](https://doi.org/10.1038/s41467-025-59930-9)\n- [Casteels et al. (2022) Cell Reports - SMNDC1 links chromatin remodeling and splicing to regulate pancreatic hormone expression](https://doi.org/10.1016/j.celrep.2022.111288)\n\n# â­ Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=epigen/atacseq_pipeline&type=Date)](https://star-history.com/#epigen/atacseq_pipeline&Date)\n",
    "readme_length": 21701
  },
  {
    "name": "pyflow-ATACseq",
    "full_name": "crazyhottommy/pyflow-ATACseq",
    "description": "ATAC-seq snakemake pipeline",
    "stars": 88,
    "forks": 30,
    "language": "Python",
    "url": "https://github.com/crazyhottommy/pyflow-ATACseq",
    "topics": [
      "atac-seq",
      "snakemake"
    ],
    "created_at": "2017-11-07T21:04:41Z",
    "updated_at": "2025-10-07T07:21:05Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "### ATACseq pipeline \n\n### Understand the method\n\n![](./ATAC.jpg)\n\nsource: http://www.the-scientist.com/?articles.view/articleNo/44772/title/Reveling-in-the-Revealed/  \n\nhttps://www.biostars.org/p/209592/\n\n### references\n\n[Ashul lab protocol]( https://docs.google.com/document/d/1f0Cm4vRyDQDu0bMehHD7P7KOMxTOP-HiNoIvL1VcBt8/edit#)\n\nataqv for QC: https://github.com/ParkerLab/ataqv\n\n### Peak calling\n\nPeaks of read enrichment were mapped using macs2. For ATAC-Seq libraries, reads mapping to chrM were excluded prior to peak calling:\n\n```bash\nsamtools view -h -F1024 $bam | grep -v -P '\\tchrM\\t' | samtools view -b - > $tmpBam\nmacs2 callpeak --keep-dup all -t $tmpBam -n ${bname} \n\n```\n\n* -f 3: only include alignments marked with the SAM flag 3, which means \"properly paired and mapped\"  \n* -F 4: exclude aligned reads with flag 4: the read itself did not map  \n* -F 8: exclude aligned reads with flag 8: their mates did not map  \n* -F 256: exclude alignments with flag 256, which means that bwa mapped the read to multiple places in the reference genome, and this alignment is not the best  \n* -F 1024: exclude alignments marked with SAM flag 1024, which indicates that the read is an optical or PCR duplicate (this flag would be set by Picard)  \n* -F 2048: exclude alignments marked with SAM flag 2048, indicating chimeric alignments, where bwa decided that parts of the read mapped to different regions in the  \ngenome. These records are the individual aligned segments of the read. They usually indicate structural variation. We're not going to base peak calls on them.\n\n\n### motif foot-print\n\nhttps://sites.google.com/site/atacseqpublic/atac-seq-analysis-methods/offsetmethods  \n\n> for peak calling, The paper mentioned \"all reads aligning to + strand were offset by +4bp, all reads aligning to the - strand are offset -5 bp\". \n\n> The offsets are only really important when doing TF footprinting using CENTIPEDE. The simplest approach for me was to convert a *bam to *bed (bedtools bamtobed), \n> apply the offset using awk (or whichever method you prefer) and if its useful, convert back to a bam (bedtools bedtobam).\n\nconvert bam to bed\n\n```bash\nbedtools bamtobed -i bowtie_dup_rm.bam > my.bed\n```\nShift the forward reads 4bp and reverse reads 5bp:\n\n```bash\nawk 'BEGIN {OFS = \"\\t\"} ; {if ($6 == \"+\") print $1, $2 + 4, $3 + 4, $4, $5, $6; else print $1, $2 - 5, $3 - 5, $4, $5, $6}' my.bed > my_shifted.bed\n```\n\n\n### work flow of the pipeline\n\nplease cite [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1043588.svg)](https://doi.org/10.5281/zenodo.1043588)\n\n![](./rule_diagram.png)\n\n\n### Dependencies\n\n* [snakemake](https://bitbucket.org/snakemake/snakemake). snakemake is python3\n* [bowtie2](bowtie-bio.sourceforge.net/bowtie2/)\n* [atactk](http://atactk.readthedocs.io/en/latest/) `trim_adapter`\n* [fastqc](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)\n* `bamCoverage` v2.3.3 from [deeptools](https://github.com/fidelram/deepTools) for making RPKM normalized and input subtracted bigwig files\n* [bowtie1](http://bowtie-bio.sourceforge.net/index.shtml) for aligning short reads (< 50bp)\n* [samblaster](https://github.com/GregoryFaust/samblaster) v0.1.22 to remove duplicates and downsampling.\n* [samtools](http://www.htslib.org/) v1.3.1\n* [multiQC](http://multiqc.info/)\n* [phantompeakqual](https://github.com/kundajelab/phantompeakqualtools)\n\n### How to distribute workflows\n\nread [doc](https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html)\n\n```bash\nssh shark.mdanderson.org\n\n# start a screen session\nscreen\n\n# make a folder, name it yourself, I named it workdir\nmkdir /rsch2/genomic_med/krai/workdir/\n\ncd /rsch2/genomic_med/krai/workdir/\n\ngit clone https://gitlab.com/tangming2005/snakemake_ATACseq_pipeline\n\ncd snakemake_ATACseq_pipeline\n\n## edit the config.yaml file as needed, e.g. set mouse or human for ref genome, p value cut off for peak calling\nnano config.yaml\n\n## skip this if on Shark, samir has py351 set up for you. see below STEPS\nconda create -n snakemake python=3 snakemake\nsource activate snakemake\n```\n\n## STEPS \n\n### on Shark\n\nthere is a python3 environment set up. just do\n\n```bash\nsource activate py351\n```\n\n\n### create the sample.json file  by feeding a fastq folder. this folder should be a folder containing all the samples.\n\nplease use the **full path** for the folder that contains your fastq folders.\n\n`python3 sample2json.py --fastq_dir /path/to/the/fastq/`\n\ne.g.\n\n```bash\nKR_PM374/\nâ”œâ”€â”€ Sample_ATAC-COAD-hCRC-R1-ATAC--NC-ELO-tEDD1aE31Dg\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-hCRC-R1-ATAC--NC-ELO-tEDD1aE31Dg_S4_L001_R1_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-hCRC-R1-ATAC--NC-ELO-tEDD1aE31Dg_S4_L001_R2_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-hCRC-R1-ATAC--NC-ELO-tEDD1aE31Dg_S4_L002_R1_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-hCRC-R1-ATAC--NC-ELO-tEDD1aE31Dg_S4_L002_R2_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-hCRC-R1-ATAC--NC-ELO-tEDD1aE31Dg_S4_L003_R1_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-hCRC-R1-ATAC--NC-ELO-tEDD1aE31Dg_S4_L003_R2_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-hCRC-R1-ATAC--NC-ELO-tEDD1aE31Dg_S4_L004_R1_001.fastq.gz\nâ”‚Â Â  â””â”€â”€ ATAC-COAD-hCRC-R1-ATAC--NC-ELO-tEDD1aE31Dg_S4_L004_R2_001.fastq.gz\nâ”œâ”€â”€ Sample_ATAC-COAD-m280-R1-ATAC--NC-ELO-t6313a3D02g\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m280-R1-ATAC--NC-ELO-t6313a3D02g_S1_L001_R1_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m280-R1-ATAC--NC-ELO-t6313a3D02g_S1_L001_R2_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m280-R1-ATAC--NC-ELO-t6313a3D02g_S1_L002_R1_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m280-R1-ATAC--NC-ELO-t6313a3D02g_S1_L002_R2_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m280-R1-ATAC--NC-ELO-t6313a3D02g_S1_L003_R1_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m280-R1-ATAC--NC-ELO-t6313a3D02g_S1_L003_R2_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m280-R1-ATAC--NC-ELO-t6313a3D02g_S1_L004_R1_001.fastq.gz\nâ”‚Â Â  â””â”€â”€ ATAC-COAD-m280-R1-ATAC--NC-ELO-t6313a3D02g_S1_L004_R2_001.fastq.gz\nâ”œâ”€â”€ Sample_ATAC-COAD-m320-R1-ATAC--NC-ELO-t7AECa1D99g\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m320-R1-ATAC--NC-ELO-t7AECa1D99g_S2_L001_R1_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m320-R1-ATAC--NC-ELO-t7AECa1D99g_S2_L001_R2_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m320-R1-ATAC--NC-ELO-t7AECa1D99g_S2_L002_R1_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m320-R1-ATAC--NC-ELO-t7AECa1D99g_S2_L002_R2_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m320-R1-ATAC--NC-ELO-t7AECa1D99g_S2_L003_R1_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m320-R1-ATAC--NC-ELO-t7AECa1D99g_S2_L003_R2_001.fastq.gz\nâ”‚Â Â  â”œâ”€â”€ ATAC-COAD-m320-R1-ATAC--NC-ELO-t7AECa1D99g_S2_L004_R1_001.fastq.gz\nâ”‚Â Â  â””â”€â”€ ATAC-COAD-m320-R1-ATAC--NC-ELO-t7AECa1D99g_S2_L004_R2_001.fastq.gz\nâ””â”€â”€ Sample_ATAC-COAD-mNOD-R1-ATAC--NC-ELO-t076AaFFEDg\n    â”œâ”€â”€ ATAC-COAD-mNOD-R1-ATAC--NC-ELO-t076AaFFEDg_S3_L001_R1_001.fastq.gz\n    â”œâ”€â”€ ATAC-COAD-mNOD-R1-ATAC--NC-ELO-t076AaFFEDg_S3_L001_R2_001.fastq.gz\n    â”œâ”€â”€ ATAC-COAD-mNOD-R1-ATAC--NC-ELO-t076AaFFEDg_S3_L002_R1_001.fastq.gz\n    â”œâ”€â”€ ATAC-COAD-mNOD-R1-ATAC--NC-ELO-t076AaFFEDg_S3_L002_R2_001.fastq.gz\n    â”œâ”€â”€ ATAC-COAD-mNOD-R1-ATAC--NC-ELO-t076AaFFEDg_S3_L003_R1_001.fastq.gz\n    â”œâ”€â”€ ATAC-COAD-mNOD-R1-ATAC--NC-ELO-t076AaFFEDg_S3_L003_R2_001.fastq.gz\n    â”œâ”€â”€ ATAC-COAD-mNOD-R1-ATAC--NC-ELO-t076AaFFEDg_S3_L004_R1_001.fastq.gz\n    â””â”€â”€ ATAC-COAD-mNOD-R1-ATAC--NC-ELO-t076AaFFEDg_S3_L004_R2_001.fastq.gz\n\n```\n\n### naming conventions of the files\n\nregular expression is used to capture the file name.\n\ninside the `sample2json.py`:\n\n```python\nm = re.search(r\"-([A-Z]{4})-([A-Z0-9a-z]{4})-[0-9A-Z]{2}-.+_(L[0-9]{3})_(R[12])_[0-9]{3}.fastq.gz\", file)\nproject = m.group(1)\nsample = m.group(2)\nlane = m.group(3)\n# R1 or R2 for forward and reverse read\nreads = m.group(4)  \n\n```\n\ncheck the file information in the json file:\n\n```\nless -S samples.json \n```\n\n### dry run to test \n\n```bash\n## dry run\nsnakemake -np\n```\n\nif no errors, preceed below.\n\n### Using [DRMAA](https://www.drmaa.org/)\n\n[job control through drmaa](http://drmaa-python.readthedocs.io/en/latest/tutorials.html#controlling-a-job)\n\nDRMAA is only supported on `Shark`.\n\n```bash\nmodule load drmma\n./pyflow-drmaa-ATACseq.sh\n```\n\nUsing `drmaa` can `control + c` to stop the current run.\n\nDependent jobs are submitted one by one, if some jobs failed, the pipeline will stop. Good for initital testing.\n\n### submit all jobs to the cluster\n\n```bash\n./pyflow-ATACseq.sh \n```\n\nAll jobs will be submitted to the cluster on queue.  This is useful if you know your jobs will succeed for most of them and the jobs are on queue to gain priority.\n\n### job control\n\nTo kill all of your pending jobs you can use the command:\n\n```bash\nbkill ` bjobs -u krai |grep PEND |cut -f1 -d\" \"`\n```\n\n```\n      bjobs -pl\n       Display detailed information of all pending jobs of the invoker.\n\n      bjobs -ps\n       Display only pending and suspended jobs.\n\n      bjobs -u all -a\n       Display all jobs of all users.\n\n      bjobs -d -q short -m apple -u mtang1\n       Display all the recently finished jobs submitted by john to the\n       queue short, and executed on the host apple.\n```\n\n### rerun some of the jobs\n\n```bash\n\n# specify the name of the rule, all files that associated with that rule will be rerun. e.g. rerun macs2 calling peaks rule,\n./pyflow-ChIPseq -R call_peaks_macs2\n\n## rerun one sample, just specify the name of the target file\n\n./pyflow-ATACseq -R 04aln/m280.sorted.bam\n\n##rerun only align rule\n./pyflow-ATACseq -f align\n\n## check snakemake -h\n## -R -f -F --until are useufl\n```\n\n### checking results after run finish\n\n```bash\n\nsnakemake --summary | sort -k1,1 | less -S\n\n# or detailed summary will give you the commands used to generated the output and what input is used\nsnakemake --detailed-summary | sort -k1,1 > snakemake_run_summary.txt\n```\n\n\n### clean the folders\n\nI use echo to see what will be removed first, then you can remove all later.\n\n```\nfind . -maxdepth 1 -type d -name \"[0-9]*\" | xargs echo rm -rf\n```\n\n\n### Snakemake does not trigger re-runs if I add additional input files. What can I do?\n\nSnakemake has a kind of â€œlazyâ€ policy about added input files if their modification date is older than that of the output files. One reason is that information what to do cannot be inferred just from the input and output files. You need additional information about the last run to be stored. Since behaviour would be inconsistent between cases where that information is available and where it is not, this functionality has been encoded as an extra switch. To trigger updates for jobs with changed input files, you can use the command line argument â€“list-input-changes in the following way:\n\n```bash\nsnakemake -n -R `snakemake --list-input-changes`\n\n```\n\n### How do I trigger re-runs for rules with updated code or parameters?\n\n```bash\nsnakemake -n -R `snakemake --list-params-changes`\n```\n\nand\n\n```bash\n$ snakemake -n -R `snakemake --list-code-changes`\n```\n\n\n\n### Peak calling \n\nby [Tao Liu](https://github.com/taoliu/MACS/issues/145):\n\n>If you followed original protocol for ATAC-Seq, you should get Paired-End reads. If so, I would suggest you just use \"--format BAMPE\" to let MACS2 pileup the whole fragments in general. But if you want to focus on looking for where the 'cutting sites' are, then '--nomodel --shift -100 --extsize 200' should work.\n",
    "readme_length": 10985
  },
  {
    "name": "ATAC-pipe",
    "full_name": "QuKunLab/ATAC-pipe",
    "description": "Analysis pipeline for ATAC-seq Data",
    "stars": 62,
    "forks": 27,
    "language": "Python",
    "url": "https://github.com/QuKunLab/ATAC-pipe",
    "topics": [],
    "created_at": "2017-12-18T12:37:11Z",
    "updated_at": "2025-09-02T12:47:42Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# ATAC-pipe\nAnalysis pipeline for ATAC-seq Data  \n\n### Please see Manual_for_ATAC-pipe.pdf\n",
    "readme_length": 91
  },
  {
    "name": "pepatac",
    "full_name": "databio/pepatac",
    "description": "A modular, containerized pipeline for ATAC-seq data processing",
    "stars": 61,
    "forks": 16,
    "language": "Python",
    "url": "https://github.com/databio/pepatac",
    "topics": [],
    "created_at": "2016-05-12T21:29:13Z",
    "updated_at": "2025-11-20T21:21:44Z",
    "homepage": "http://pepatac.databio.org",
    "license": "BSD 2-Clause \"Simplified\" License",
    "readme": "<img src=\"docs/img/pepatac_logo_white.svg\" alt=\"pepatac logo\" height=\"70\" align=\"left\"/>\n\n<br></br>\n\n---\n\n[![PEP compatible](http://pepkit.github.io/img/PEP-compatible-green.svg)](http://pepkit.github.io)\n\nPEPATAC is a pipeline for ATAC-seq data. For more information see: http://pepatac.databio.org/\n\n## Docs\n\nDevelop docs with:\n\n```\nmkdocs serve -f mkdocs.yml\n```\n\nBuild and deploy with:\n\n```\nmkdocs build -f mkdocs.yml -d $CODEBASE/code.databio.org/pepatac/\n```\n\n## Contributing\n\nPull requests welcome. Active development should occur in a development or feature branch.\n\n## Contributors\n\n* Jason Smith, jasonsmith@virginia.edu\n* Nathan Sheffield, nathan@code.databio.org\n* Jin Xu, jinxu9@stanford.edu\n* Ryan Corces, rcorces@stanford.edu\n* Vince Reuter, vreuter@protonmail.com\n* Donald Campbell, https://github.com/donaldcampbelljr\n* Others... (add your name)\n\n## Citing\n\nIf you find PEPATAC useful in your research, please cite: \n\nJason P. Smith, M. Ryan Corces, Jin Xu, Vincent P. Reuter, Howard Y. Chang, and Nathan C. Sheffield. <b>PEPATAC: An optimized ATAC-seq pipeline with serial alignments.</b> <i>bioRxiv</i> (2021). <a href=\"https://doi.org/10.1101/2020.10.21.347054\">DOI: 10.1101/2020.10.21.347054</a>\n\n",
    "readme_length": 1218
  },
  {
    "name": "genome_tracks",
    "full_name": "epigen/genome_tracks",
    "description": "A Snakemake workflow and MrBiomics module for easy visualization of genome browser tracks of aligned BAM files (e.g., RNA-seq, ATAC-seq, scRNA-seq, ...) powered by the wrapper gtracks for the package pyGenomeTracks, and IGV-reports.",
    "stars": 53,
    "forks": 1,
    "language": "Python",
    "url": "https://github.com/epigen/genome_tracks",
    "topics": [
      "atac-seq",
      "bioinformatics",
      "biomedical-data-science",
      "genome-browser",
      "genome-track",
      "genomic-regions",
      "pipeline",
      "python",
      "rna-seq",
      "scrna-seq",
      "snakemake",
      "visualization",
      "workflow"
    ],
    "created_at": "2021-12-15T09:40:20Z",
    "updated_at": "2025-10-08T07:44:27Z",
    "homepage": "https://epigen.github.io/genome_tracks/",
    "license": "MIT License",
    "readme": "[![MrBiomics](https://img.shields.io/badge/MrBiomics-red)](https://github.com/epigen/MrBiomics/)\n[![DOI](https://zenodo.org/badge/438573546.svg)](https://zenodo.org/doi/10.5281/zenodo.10849097)\n[![](https://tokei.rs/b1/github/epigen/genome_tracks?category=code)]() \n[![](https://tokei.rs/b1/github/epigen/genome_tracks?category=files)]()\n[![GitHub license](https://img.shields.io/github/license/epigen/genome_tracks)](https://github.com/epigen/genome_tracks/blob/master/LICENSE)\n![GitHub Release](https://img.shields.io/github/v/release/epigen/genome_tracks)\n[![Snakemake](https://img.shields.io/badge/Snakemake->=8.20.1-green)](https://snakemake.readthedocs.io/en/stable/)\n\n# Genome Browser Track Visualization Workflow \nA [Snakemake 8](https://snakemake.readthedocs.io/en/stable/) workflow for easy visualization of genome browser tracks of aligned/mapped BAM files (e.g., RNA-seq, ATAC-seq, scRNA-seq, ...) powered by the wrapper [gtracks](https://gitlab.com/salk-tm/gtracks) for the package [pyGenomeTracks](https://github.com/deeptools/pyGenomeTracks) and [IGV-reports](https://github.com/igvteam/igv-reports).\n\n> [!NOTE]  \n> This workflow adheres to the module specifications of [MrBiomics](https://github.com/epigen/MrBiomics), an effort to augment research by modularizing (biomedical) data science. For more details, instructions, and modules check out the project's repository.\n>\n> â­ï¸ **Star and share modules you find valuable** ðŸ“¤ - help others discover them, and guide our future work!\n\n> [!IMPORTANT]  \n> **If you use this workflow in a publication, please don't forget to give credit to the authors by citing it using this DOI [10.5281/zenodo.10849097](https://zenodo.org/doi/10.5281/zenodo.10849097).**\n\n![Workflow Rulegraph](./workflow/dags/rulegraph.svg)\n\n# ðŸ–‹ï¸ Authors\n- [Stephan Reichl](https://github.com/sreichl)\n- [Christoph Bock](https://github.com/chrbock)\n\n# ðŸ’¿ Software\nThis project wouldn't be possible without the following software and their dependencies:\n\n| Software | Reference (DOI) |\n| :---: | :---: |\n| deeptools | https://doi.org/10.1093/nar/gkw257 |\n| gtracks | https://gitlab.com/salk-tm/gtracks |\n| igv-reports | https://github.com/igvteam/igv-reports |\n| pygenometracks | https://doi.org/10.1093/bioinformatics/btaa692 |\n| samtools | https://doi.org/10.1093/bioinformatics/btp352 |\n| sinto | https://github.com/timoast/sinto |\n\n# ðŸ”¬ Methods\nThis is a template for the Methods section of a scientific publication and is intended to serve as a starting point. Only retain paragraphs relevant to your analysis. References [ref] to the respective publications are curated in the software table above. Versions (ver) have to be read out from the respective conda environment specifications (`workflow/envs/*.yaml file`) or post-execution in the result directory (`genome_tracks/envs/*.yaml`). Parameters that have to be adapted depending on the data or workflow configurations are denoted in squared brackets e.g., [X].\n\n__(optional) Single-cell preprocessing.__ Each single-cell BAM file was split into [group]-wise BAM files according to it's cell barcode metadata using filterbarcodes from the command line tool sinto (ver) [ref].\n\n__Processing.__ Aligned (filtered, and indexed) BAM files were merged by [group] using samtools (ver) [ref]. Each merged BAM file's coverage was determined for dowmnstream analysis and visualization using bamCoverage from the command line tool deepTools (ver) [ref] and saved in the bigWig format. Finally, we extracted coordinates, extended start and end by [base_buffer] bases, and number of isoforms of all relevant genes/genomic regions [gene_list] from the 12 column BED file genome [genome] annotation [genome_bed].\n\n__Visualization.__ Visualizations for each relevant gene/genomic region and [category] were generated by using the generated bigWig coverage files and vertically stacking genome browser tracks with their annotation at the [x_axis] and each track scaled by [y_max] reads. The plotting was performed using the python wrapper gtracks (ver) [ref] for the package pyGenomeTracks (ver) [ref]. Additionally, an interactive self-contained IGV-report containing all merged samples and gene/genomic regions of interest was generated using igv-reports (ver) [ref]. Finally, a UCSC genome browser track hub was created for online sharing and inspection using [UCSC Genome Browser](https://genome.ucsc.edu/). Both the plotted tracks and the UCSC genome browser tracks were color coded according to [group].\n\n**The processing and visualizations described here were performed using a publicly available Snakemake [ver] (ref) workflow [[10.5281/zenodo.10849097](https://zenodo.org/doi/10.5281/zenodo.10849097)].**\n\n# ðŸš€ Features\nThe workflow performs the following steps to produce the outlined results (`genome_tracks/`).\n\n- Processing\n  0. (optional) Single-cell BAM files are split into groups according to their metadata file using [sinto::filterbarcodes](https://timoast.github.io/sinto/basic_usage.html#filter-cell-barcodes-from-bam-file) and saved in folders named by the md5 hash of the input BAM file path (`sc_bams/{md5hash}/{group}.bam`). Downstream they are processed and visualized the same as bulk samples.\n  1. BAM files of the same group are merged and indexed using [samtools::merge](https://www.htslib.org/doc/samtools-merge.html). (`merged_bams/{group}.bam`)\n  2. A bigWig file per merged BAM file is generated using [deepTools::bamCoverage](https://deeptools.readthedocs.io/en/develop/content/tools/bamCoverage.html). (`bigWigs/{group}.bw`)\n  3. Annotations per gene are retrieved from the 12-column BED file (not necessary for genomic regions).\n      - coordinates from the 12-column BED file are extracted and extended at start/end by the parameter base_buffer.\n      - the number of isoforms i.e. number of lines in the BED file is determined (__only for genes, for genomic regions it is hardcoded to 1__) to plot below the tracks.\n      - genes that were not found in the provided genome BED file are reported (`genes_not_found.csv`).\n  4. A BED file of all genes and genomic regions is generated for the IGV-report (`genes.bed`).\n- Visualization\n  - One plot including all groups visualizing the coverage (i.e., bigWigs) per gene/genomic region with the determined gene-parameters i.e, coordinates and gene-rows is generated (`tracks/{gene|region}.{svg|pdf|png}`). The track height (`ymax`) and colors (`track_colors`) are configurable within the `gene_list` or config file, respectively. Default color is black.\n  - An interactive self-contained IGV-report from merged BAM files with all genes/regions is generated for inspection for sharing (`igv-report.html`).\n  - A UCSC genome browser track hub for all bigWigs is set up (`bigWigs/`), color coded according to configuration. See detailed instructions for usage and sharing below.\n\n# ðŸ› ï¸ Usage\nHere are some tips for the usage of this workflow:\n- Start with the 1-5 most interesting genes (e.g., marker genes for cell types as quality control or the most differentially expressed between conditions) and few/relevant samples for a test run.\n- Set y-max to auto (i.e., 0 in the `gene_list` CSV table) for the first run to get a feeling for the magnitudes in your samples/groups and adapt to the highest peaks afterward to make the tracks comparable within a gene/region of interest.\n- Splitting single-cell BAM files, merging BAM files and generating bigWig files take the longest, but are performed only once (multithreaded 4x configured threads). The plot generation for different genes/genomic regions afterward is very fast. Therefore, it is recommended to get the workflow going with a few samples and then increase the number of samples and genes/regions afterward.\n- For simple quality control (QC) of all/individual samples just provide a unique sample name in the `group` column of the annnotation file. Then no samples will be merged into groups, but only renamed (useful before sharing).\n\nThis workflow is written with Snakemake and its usage is described in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog?usage=epigen/genome_tracks).\n\n# âš™ï¸ Configuration\nDetailed specifications can be found here [./config/README.md](./config/README.md)\n\n# ðŸ“– Examples\nExplore detailed examples showcasing module usage in our comprehensive end-to-end [MrBiomics Recipes](https://github.com/epigen/MrBiomics?tab=readme-ov-file#-recipes), including data, configuration, annotation and results:\n- [ATAC-seq Analysis Recipe](https://github.com/epigen/MrBiomics/wiki/ATAC%E2%80%90seq-Analysis-Recipe)\n- [RNA-seq Analysis Recipe](https://github.com/epigen/MrBiomics/wiki/RNA%E2%80%90seq-Analysis-Recipe)\n- [scRNA-seq Analysis Recipe](https://github.com/epigen/MrBiomics/wiki/scRNA%E2%80%90seq-Analysis-Recipe)\n- [scCRISPR-seq Analysis Recipe](https://github.com/epigen/MrBiomics/wiki/scCRISPR%E2%80%90seq-Analysis-Recipe)\n\nRuntime examples for different data modalities:\n- 78 ATAC-seq samples in 31 groups took 22 minutes with max 4 cores and 4GB memory.\n- 64 RNA-seq samples in 31 groups took 16 minutes with max 4 cores and 4GB memory.\n- 2 10x genomics 5' scRNA-seq samples/reactions each ~10k cells split in 2 small subset groups took 31 minutes with max 4 cores and 8GB memory.\n\n# ðŸ§¬ Genome Browser Tracks\nThe `bigWigs` directory contains the read coverage per sample/group in bigWig format (`{group}.bw`) for visual inspection of each sample e.g., during QC or group e.g., comparison of conditions. Below are instructions for two different approaches (online/local).\n\n## UCSC Genome Browser Track Hub (online)\n0. Requirement: web server.\n1. Copy (or symlink) the `bigWigs` directory to an externally __accessible__ location on your web server (=`web_server_location`).\n2. Create a UCSC Genome Browser hyperlink\n    - the general formula is: ucsc_url + genome + web_server_location + bigWigs/hub.txt\n    - concretely: `http://genome-euro.ucsc.edu/cgi-bin/hgTracks?db=` + {genome} + `&hubUrl=` + {web_server_location} + `bigWigs/hub.txt`\n    - mm10 example: [http://genome-euro.ucsc.edu/cgi-bin/hgTracks?db=mm10&hubUrl=https://medical-epigenomics.org/data/genome_tracks/mm10test/hub/hub.txt](http://genome-euro.ucsc.edu/cgi-bin/hgTracks?db=mm10&hubUrl=https://medical-epigenomics.org/data/genome_tracks/mm10test/hub/hub.txt)\n    - hg38 example: [http://genome-euro.ucsc.edu/cgi-bin/hgTracks?db=hg38&hubUrl=https://medical-epigenomics.org/data/genome_tracks/hg38test/hub/hub.txt](http://genome-euro.ucsc.edu/cgi-bin/hgTracks?db=hg38&hubUrl=https://medical-epigenomics.org/data/genome_tracks/hg38test/hub/hub.txt)\n3. Share the link with the world e.g., collaborators or upon publication of your data.\n\n**A new feature (2024-08-30)** allows users to download all visible data in the current region directly from our tracks display. This facilitates reproducibility when writing reports or publications as data can update and change over time. This feature can be found in the blue bar menu by going to **Downloads > Download Current Track Data**. The resulting pop-up dialogue box (see screenshot below) can configure the exact tracks to download from all visible tracks, as well as the file name and the output format (JSON, csv, tsv).\n![UCSC_download](https://github.com/user-attachments/assets/174ad904-52a2-458f-a1e7-387a0ff95007)\n\n## IGV: Integrative Genomics Viewer (local/offline)\n0. Requirement: [IGV Desktop application](https://igv.org/doc/desktop/).\n1. Open IGV.\n2. Select genome.\n3. Drag and drop all/selected bigWig files from the `bigWigs` directory directly into the IGV application.\n\n# ðŸ”— Links\n- [GitHub Repository](https://github.com/epigen/genome_tracks/)\n- [GitHub Page](https://epigen.github.io/genome_tracks/)\n- [Zenodo Repository](https://zenodo.org/doi/10.5281/zenodo.10849097)\n- [Snakemake Workflow Catalog Entry](https://snakemake.github.io/snakemake-workflow-catalog?usage=epigen/genome_tracks)\n\n# ðŸ“š Resources\n- Recommended compatible [MrBiomics](https://github.com/epigen/MrBiomics) modules for upstream processing:\n    - [ATAC-seq Data Processing & Quantification Pipeline](https://github.com/epigen/atacseq_pipeline) for processing, quantification and annotation of ATAC-seq samples.\n- [UCSC Genome Browser annotation track database](https://genome.ucsc.edu/cgi-bin/hgTables)\n    - recommended source for the required 12 column BED file annotation of the respective genome.\n    - e.g., for mm10 from UCSC as gzip https://genome.ucsc.edu/cgi-bin/hgTables assembly:mm10 -> track:NCBI RefSeq -> table:refFlat; output format: BED\n\n# ðŸ“‘ Publications\nThe following publications successfully used this module for their analyses.\n- [Traxler, Reichl et al. (2025) Cell Systems - Integrated time-series analysis and high-content CRISPR screening delineate the dynamics of macrophage immune regulation](https://doi.org/10.1016/j.cels.2025.101346)\n- ...\n\n# â­ Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=epigen/genome_tracks&type=Date)](https://star-history.com/#epigen/genome_tracks&Date)\n",
    "readme_length": 13014
  },
  {
    "name": "AMULET",
    "full_name": "UcarLab/AMULET",
    "description": "A count based method for detecting doublets from single nucleus ATAC-seq (snATAC-seq) data.",
    "stars": 31,
    "forks": 7,
    "language": "Python",
    "url": "https://github.com/UcarLab/AMULET",
    "topics": [],
    "created_at": "2020-11-02T19:43:31Z",
    "updated_at": "2025-02-17T03:33:05Z",
    "homepage": "https://ucarlab.github.io/AMULET/",
    "license": "GNU General Public License v3.0",
    "readme": "# AMULET: ATAC-seq MULtiplet Estimation Tool #\nA count based method for detecting multiplets from single nucleus ATAC-seq (snATAC-seq) data.\n\n**Publication:** <https://doi.org/10.1186/s13059-021-02469-x>\n\nNote: The jar files for running using BAM files can be found in the releases: https://github.com/UcarLab/AMULET/releases\n\n# License #\nAMULET is provided under the GPLv3 license (see LICENSE). \n\nNote: Some AMULET code has been (or can be) released under the MIT license (i.e., any code that is not associated with R).\n\n# System Requirements #\nPython 3 with the following libraries:\n- numpy\n- pandas\n- scipy\n- statsmodels\n\n`pip3 install numpy pandas scipy statsmodels`\n\nIf using BAM files:\nJava SE 8 (build 231) or higher.\n\n# Installation #\nIf the system requirements are satisfied:\n\n1. Download the latest release.\n2. Unzip the files to your preferred output directory.\n3. Enable the shell script to be executed using `chmod` (e.g., `chmod +x AMULET.sh`)\n\nThe shell file can be run from there. Java is portable and the jar should not need to be recompiled for most platforms.\n\n# Running AMULET #\nRunning AMULET consists of two parts: 1) Identifying all loci with >2 uniquely aligning reads for each cell, and 2) detecting multiplets that have more loci with >2 reads than expected. The bash shell script combines both of these steps, but they can be run independently as well. The latter (multiplet detection) may be useful for running multiplet detection using q-values different from the default.\n\n## Bash shell script ##\n\nTo run using the bash shell script, provide the following arguments:\n1. Path to the bam file (e.g., possorted.bam from CellRanger)\n2. Path to a barcode to cell_id map in CSV format (e.g., singlecell.csv from CellRanger)\n3. Path to the list of chromosomes to use for the analysis (e.g., human_chromosomes_noxy.txt)\n4. Path to known repetitive elements. (e.g., restrictionlist_repeats_segdups_rmsk_hg19.bed depending on genome)\n5. Path to an existing output directory to write output files.\n6. Path of this script's directory.\n\nFragment file example:\n\n`AMULET.sh /path/to/fragments.tsv.gz /path/to/singlecell.csv /path/to/human_autosomes.txt /path/to/repeatfilter.bed /path/to/output/ /path/to/shellscript/`\n\nBAM file example:\n\n`AMULET.sh /path/to/possorted.bam /path/to/singlecell.csv /path/to/human_autosomes.txt /path/to/repeatfilter.bed /path/to/output/ /path/to/shellscript/`\n\nNote: If you know the input bam file is coordinate sorted and you get an error message saying it's not, please use the `--forcesorted` option. There is a problem with the SAMReader library not recognizing this flag correctly in the header.\n\nExample:\n\n`AMULET.sh --forcesorted /path/to/possorted.bam /path/to/singlecell.csv /path/to/human_autosomes.txt /path/to/repeatfilter.bed /path/to/output/ /path/to/shellscript/`\n\n### Advanced: Optional Arguments ###\nThe above can be used for common CellRanger outputs. Use the following options to adjust for differences in CellRanger outputs or for different inputs in general:\n\n`--expectedoverlap` Expected number of reads overlapping. (Default: 2)\n\n`--maxinsertsize` The maximum insert size (in bp) between read pairs. (Default: 900)\n\n`--startbases` The amount of bases add to the start position. (BAM Default: 4, Frag Default: 0)\n\n`--endbases` The amount of bases to add to the end position. (BAM Default: -5, Frag Default: 0)\n\n\nBAM Input Only:\n\n`--bambc` The bam file attribute to use to extract the barcode from reads (Default: \"CB\")\n\n`--forcesorted` Forces the input bam file to be treated as sorted.\n\n`--bcidx` The column index (counting from 0) of the CSV file barcode to cell id map for the barcode. (Default: 0)\n\n`--cellidx` The column index (counting from 0) of the CSV file barcode to cell id map for the cell id. (Default: 0)\n\n`--iscellidx` The column index (counting from 0) of the CSV file barcode to cell id map for determining if the barcode corresponds to a cell. (Default: 9)\n\n`--mapqthresh` Threshold for filtering low map quality reads (<= comparison). (Default: 30)\n\n`--maxinsertsize` The maximum insert size (in bp) between read pairs. (Default: 900)\n\nExamples:\n\n`AMULET.sh --bambc CB --bcidx 1 /path/to/possorted.bam /path/to/singlecell.csv /path/to/human_autosomes.txt /path/to/repeatfilter.bed /path/to/output/ /path/to/shellscript/`\n\n`AMULET.sh --bcidx 1 --cellidx 2 --iscellidx 3 /path/to/possorted.bam /path/to/singlecell.csv /path/to/human_autosomes.txt /path/to/repeatfilter.bed /path/to/output/ /path/to/shellscript/`\n\n\n## Running overlap counter and multiplet detection independently ##\n\n### Fragment File Overlap Counter (Detect overlaps from .tsv/.txt/.tsv.gz/.txt.gz using Python) ###\n\nTo run the fragment overlap counter, use `python3 FragmentFileOverlapCounter.py` and provide the following input arguments:\n\n`FRAGMENTFILE` Path to the fragment file (e.g., fragments.tsv.gz from CellRanger)\n\n`BCMAP` Path to a barcode to cell_id map in CSV format (e.g., singlecell.csv from CellRanger)\n\n`CHRLIST` Path to the list of chromosomes to use for the analysis (e.g., human_autosomes.txt)\n\n`OUTDIR` Path to an existing output directory to write output files.\n\nExample:\n\n`python3 FragmentFileOverlapCounter.py BAMFILE BCMAP CHRLIST OUTDIR`\n\nOptional Arguments:\n\n`--expectedoverlap`    Expected number of reads overlapping. (Default: 2)\n\n`--maxinsertsize` The maximum insert size (in bp) between read pairs. (Default: 900)\n\n`--startbases` The amount of bases add to the start position. (Default: 0)\n\n`--endbases` The amount of bases to add to the end position (can be negative). (Default: 0)\n\n\n### BAM Overlap Counter (Detect overlaps from BAM using Java) ###\n\nTo run the overlap counter jar file, use `java -jar snATACOverlapCounter.jar` and provide the following input arguments:\n\n`BAMFILE` Path to the bam file (e.g., possorted.bam from CellRanger)\n\n`BCMAP` Path to a barcode to cell_id map in CSV format (e.g., singlecell.csv from CellRanger)\n\n`CHRLIST` Path to the list of chromosomes to use for the analysis (e.g., human_autosomes.txt)\n\n`OUTDIR` Path to an existing output directory to write output files.\n\nExample:\n\n`java -jar snATACOverlapCounter.jar BAMFILE BCMAP CHRLIST OUTDIR`\n\nOptional Arguments:\n\n`--expectedoverlap`    Expected number of reads overlapping. (Default: 2)\n\n`--bambc` The bam file attribute to use to extract the barcode from reads (Default: \"CB\")\n\n`--forcesorted` Forces the input bam file to be treated as sorted.\n\n`--bcidx` The column index (counting from 0) of the CSV file barcode to cell id map for the barcode. (Default: 0)\n\n`--cellidx` The column index (counting from 0) of the CSV file barcode to cell id map for the cell id. (Default: 8)\n\n`--iscellidx` The column index (counting from 0) of the CSV file barcode to cell id map for determining if the barcode corresponds to a cell. (Default: 9)\n\n`--mapqthresh` Threshold for filtering low map quality reads (<= comparison). (Default: 30)\n\n`--maxinsertsize` The maximum insert size (in bp) between read pairs. (Default: 900)\n\n`--startbases` The amount of bases add to the start position. (Default: 4)\n\n`--endbases` The amount of bases to add to the end position (can be negative). (Default: -5)\n\n\nExamples:\n\n`java -jar snATACOverlapCounter.jar --bambc CB BAMFILE BCMAP CHRLIST OUTDIR`\n\n`java -jar snATACOverlapCounter.jar --bambc CB --cellidx 1 BAMFILE BCMAP CHRLIST OUTDIR`\n\n### Multiplet Detection (Python) ###\n\nThe multiplet detection python file requires 3 input arguments. \n\n`OVERLAPS` Path to the Overlaps.txt output from overlap counter.\n\n`OVERLAPSUMMARY` Path to the OverlapSummary.txt output from overlap counter.\n\n`OUTDIR` Path to an existing output directory to write output files.\n\nOptional Arguments:\n\n`--rfilter` A bed file of known problematic/repetitive regions to exclude from multiplet detection.\n\n`--q` The q-value threshold for detecting multiplets.\n\n`--qrep` The q-value threshold for inferring repetitive regions to exclude from multiplet detection.\n\nExample:\n\n`python3 AMULET.py --rfilter /path/to/repeats.bed OVERLAPS OVERLAPSUMMARY OUTDIR`\n\n# Interpreting Results #\n\n## Overlap Counter ##\n\nThe overlap counter script produces four output files describing the run and the data: Overlaps.txt, OverlapSummary.txt, StatSummary.txt, and RunTime.txt\n\n### Overlaps.txt ##\n\nA tab delimited file with the following columns:\n\n1. **chr** - Chromosome of the genomic region with an overlap >2.\n2. **start** - The start position of the overlap >2.\n3. **end** - The end position of the overlap >2.\n4. **Cell Id** - The cell id (e.g., _cell_0, _cell_1, etc. from CellRanger).\n5. **Min Overlap Count** - The minimum number of reads overlapping within this region.\n6. **Max Overlap Count** - The maximum number of reads overlapping within this region.\n7. **Mean Mapping Quality** - The average mapping quality for all reads within this region.\n8. **Max Mapping Quality** - The max mapping quality for all reads within this region.\n9. **Starts** - The start positions of each read within this region for this cell id.\n10. **Ends** - The end positions of each read within this region for this cell id.\n\t\t\n### OverlapSummary.txt ###\n\nA tab delimited file with the following columns:\n\n1. **Cell Id** - The cell id (e.g., _cell_0, _cell_1, etc. from CellRanger).\n2. **Number of Valid Read Pairs** - The total number of valid read pairs for the cell (i.e., all forward strand reads with mates).\n3. **Number of Overlaps** - The total number of overlaps observed in this cell (before merging).\n4. **Barcode** - The read barcode associated with the cell id.\n5. **Total Number of Reads** - The total number of reads (forward and reverse strange reads counted independently).\n\n### StatSummary.txt ###\n\nA file containing the following information:\n\n1. **Total Reads** - The total number of reads in the bam file.\n2. **Duplicate Reads** - The total number of reads flagged as a duplicate.\n3. **Low Mapping Quality Reads** - The total number of reads with mapping quality <= 30.\n4. **Valid Read Pairs** - The total number of read pairs that pass the following:\n    * ReadPairedFlag = True\n    * ReadUnmappedFlag = False\n    * MateUnmappedFlag = False\n    * SecondaryorSupplementary = False\n    * DuplicateReadFlag = False\n    * ReferencerIndex != MateReferenceIndex (i.e., read pairs map to the same chromosome)\n    * Insert size is positive and < 900bp (~6 nucleosomes)\n    * Mapping quality > 30\n    * The read's chromosome is in the chromosome list provided.\n5. **Mean Read Length** - The average length of forward strand reads.\n6. **Mean Insert Size** - The average length of end to end insert sizes.\n\n### RunTime.txt ###\n\nShows the runtime (in seconds) for identifying all instances of read overlaps >2.\n\n## Multiplet Detection ##\n\nThe multiplet detection python script produces three output files: MultipletProbabilities, MultipletCellIds_xx.txt, and MultipletBarcodes_xx.txt (xx  corresponding to the q-value threshold used to call multiplets).\n\n### MultipletProbabilities.txt ###\n\nA tab delimited file with the following columns:\n\n1. **cell_id** - The cell id (e.g., _cell_0, _cell_1, etc. from CellRanger)\n2. **barcode** - The cell barcode.\n3. **p-value** - The Poisson probability obtained from the cumulative distribution function.\n4. **q-value** - The FDR corrected p-value for multiple hypothesis testing.\n\n### MultipletCellIds_xx.txt ###\n\nFiles with the MultipletCellIds prefix correspond to multiplet cell ids with a q-value threshold specified by xx (i.e., 0.xx). For example 01 implies a q-value threshold of 0.01.\n\n### MultipletBarcodes_xx.txt ###\n\nFiles with the MultipletBarcodes prefix correspond to multiplet cell barcodes with a q-value threshold specified by xx (i.e., 0.xx). For example 01 implies a q-value threshold of 0.01.\n\n",
    "readme_length": 11711
  },
  {
    "name": "circdna",
    "full_name": "nf-core/circdna",
    "description": "Pipeline for the identification of extrachromosomal circular DNA (ecDNA) from Circle-seq, WGS, and ATAC-seq data that were generated from cancer and other eukaryotic cells.",
    "stars": 31,
    "forks": 16,
    "language": "Python",
    "url": "https://github.com/nf-core/circdna",
    "topics": [
      "ampliconarchitect",
      "ampliconsuite",
      "circle-seq",
      "circular",
      "dna",
      "eccdna",
      "ecdna",
      "extrachromosomal-circular-dna",
      "genomics",
      "nextflow",
      "nf-core",
      "pipeline",
      "workflow"
    ],
    "created_at": "2021-11-02T09:45:33Z",
    "updated_at": "2025-08-17T20:16:52Z",
    "homepage": "https://nf-co.re/circdna",
    "license": "MIT License",
    "readme": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-circdna_logo_dark.png\">\n    <img alt=\"nf-core/circdna\" src=\"docs/images/nf-core-circdna_logo_light.png\">\n  </picture>\n</h1>\n[![GitHub Actions CI Status](https://github.com/nf-core/circdna/workflows/nf-core%20CI/badge.svg)](https://github.com/nf-core/circdna/actions?query=workflow%3A%22nf-core+CI%22)\n[![GitHub Actions Linting Status](https://github.com/nf-core/circdna/workflows/nf-core%20linting/badge.svg)](https://github.com/nf-core/circdna/actions?query=workflow%3A%22nf-core+linting%22)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/circdna/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.8085422?labelColor=000000)](https://doi.org/10.5281/zenodo.8085422)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg)](https://sylabs.io/docs/)\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/nf-core/circdna)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23circdna-4A154B?logo=slack)](https://nfcore.slack.com/channels/circdna) [![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?logo=twitter)](https://twitter.com/nf_core) [![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/circdna** is a bioinformatics best-practice analysis pipeline for the identification of extrachromosomal circular DNAs (ecDNAs) in eukaryotic cells. The pipeline is able to process WGS, ATAC-seq data or Circle-Seq data generated from short-read sequencing technologies. Depending on the input data and selected analysis branch, nf-core/circdna is able to identify various types of ecDNAs. This includes the detection of smaller ecDNAs, often referred to as eccDNAs or microDNAs, as well as larger ecDNAs that exhibit amplification. These analyses are facilitated through the use of prominent software tools that are widely recognized in the field of ecDNA or circular DNA research.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources.The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/circdna/results).\n\n## Pipeline summary\n\n1. Merge re-sequenced FastQ files ([`cat`](http://www.linfo.org/cat.html))\n2. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n3. Adapter and quality trimming ([`Trim Galore!`](https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/))\n4. Map reads using BWA-MEM ([`BWA`](https://github.com/lh3/bwa))\n5. Sort and index alignments ([`SAMtools`](https://sourceforge.net/projects/samtools/files/samtools/))\n6. Choice of multiple ecDNA identification routes\n   1. [`Circle-Map ReadExtractor`](https://github.com/iprada/Circle-Map) -> [`Circle-Map Realign`](https://github.com/iprada/Circle-Map)\n   1. [`Circle-Map ReadExtractor`](https://github.com/iprada/Circle-Map) -> [`Circle-Map Repeats`](https://github.com/iprada/Circle-Map)\n   1. [`CIRCexplorer2`](https://circexplorer2.readthedocs.io/en/latest/)\n   1. [`Samblaster`](https://github.com/GregoryFaust/samblaster) -> [`Circle_finder`](https://github.com/pk7zuva/Circle_finder) **Does not use filtered BAM file, specificied with --keep_duplicates false**\n   1. Identification of circular amplicons [`AmpliconArchitect`](https://github.com/jluebeck/AmpliconArchitect)\n   1. De Novo Assembly of ecDNAs [`Unicycler`](https://github.com/rrwick/Unicycler) -> [`Minimap2`](https://github.com/lh3/minimap2)\n7. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Functionality Overview\n\nA graphical view of the pipeline and its diverse branches can be seen below.\n\n<p align=\"center\">\n<img src=\"docs/images/circdna_pipeline_metromap.png\" alt=\"nf-core/circdna metromap\" width=\"70%\">\n</p>\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n`FASTQ` input data:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\n`BAM` input data:\n\n```csv\nsample,bam\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.bam\n```\n\nEach row represents a pair of fastq files (paired end) or a single bam file generated from paired-end reads.\n\nNow, you can run the pipeline using:\n\n```bash\n   nextflow run nf-core/circdna --input samplesheet.csv --outdir <OUTDIR> --genome GRCh38 -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --circle_identifier <CIRCLE_IDENTIFIER> --input_format <\"FASTQ\"/\"BAM\">\n```\n\n### Test AmpliconSuite-Pipeline with a test data-set\n\nTo test the correct installation of the pipeline and the use of AmpliconArchitect inside the [AmpliconSuite-Pipeline](https://github.com/AmpliconSuite/AmpliconSuite-pipeline), a small WGS data set is uploaded to [AWS](https://aws.amazon.com/) and can be downloaded and used with the parameter `-profile test_AA_local`. You just need to specify your local paths to the `aa_data_repo` and the `mosek_license_dir`. See [AmpliconSuite-Pipeline](https://github.com/AmpliconSuite/AmpliconSuite-pipeline) for information about the data repository and the Mosek license. To note, the Mosek license file needs to be named `mosek.lic` inside the `mosek_license_dir`.\n\nYou can test the pipeline using:\n\n```bash\n   nextflow run nf-core/circdna -profile test_AA_local,<docker/singularity/podman/shifter/charliecloud/conda/institute> --outdir <OUTDIR> --aa_data_repo <path/to/aa_data_repo/> --mosek_license_dir <path/to/mosek_license_directory/>\n```\n\n## Available ecDNA identifiers\n\nPlease specify the parameter `circle_identifier` depending on the pipeline branch used for circular DNA identifaction. Please note that some branches/software are only tested with specific NGS data sets.\n\n### Identification of putative ecDNA junctions with ATAC-seq or Circle-seq data\n\n> `circle_finder` uses [Circle_finder](https://github.com/pk7zuva/Circle_finder) > `circexplorer2` uses [CIRCexplorer2](https://circexplorer2.readthedocs.io/en/latest/) > `circle_map_realign` uses [Circle-Map Realign](https://github.com/iprada/Circle-Map) > `circle_map_repeats` uses [Circle-Map Repeats](https://github.com/iprada/Circle-Map) for the identification of repetetive ecDNA\n\n### Identification of amplified ecDNAs with WGS data\n\n> `ampliconarchitect` uses [AmpliconArchitect](https://github.com/jluebeck/AmpliconArchitect) inside the [AmpliconSuite-Pipeline](https://github.com/AmpliconSuite/AmpliconSuite-pipeline)\n\n### De novo assembly of ecDNAs with Circle-seq data\n\n> `unicycler` uses [Unicycler](https://github.com/rrwick/Unicycler) for de novo assembly of ecDNAs and [Minimap2](https://github.com/lh3/minimap2) for accurate mapping of the identified circular sequences.\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/circdna/usage) and the [parameter documentation](https://nf-co.re/circdna/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/circdna/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/circdna/output).\n\n## Credits\n\nnf-core/circdna was originally written by [Daniel Schreyer](https://github.com/DSchreyer), University of Glasgow, Institute of Cancer Sciences, Peter Bailey Lab.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- SÃ©bastian Guizard: Review and Discussion of Pipeline\n- Alex Peltzer: Code Review\n- Phil Ewels: Help in setting up the pipeline repository and directing the pipeline development\n- nf-core community: Answering all nextflow and nf-core related questions\n- Peter Bailey: Discussion of Software and Pipeline Architecture\n\nThis pipeline has been developed by Daniel Schreyer as part of the PRECODE project. PRECODE received funding from the European Unionâ€™s Horizon 2020 Research and Innovation Program under the Marie SkÅ‚odowska-Curie grant agreement No 861196.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#circdna` channel](https://nfcore.slack.com/channels/circdna) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/circdna for your analysis, please cite it using the following doi: [10.5281/zenodo.6685250](https://doi.org/10.5281/zenodo.6685250)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
    "readme_length": 11830
  },
  {
    "name": "msCentipede",
    "full_name": "rajanil/msCentipede",
    "description": "A hierarchical multiscale model for inferring transcription factor binding from chromatin accessibility data.",
    "stars": 26,
    "forks": 6,
    "language": "Python",
    "url": "https://github.com/rajanil/msCentipede",
    "topics": [],
    "created_at": "2013-11-03T06:53:38Z",
    "updated_at": "2024-12-29T07:41:12Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# msCentipede\n\n**msCentipede** is an algorithm for accurately inferring transcription factor binding sites using chromatin\naccessibility data (Dnase-seq, ATAC-seq) and is written in Python2.x and Cython. \nThe [hierarchical multiscale model underlying msCentipede](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0138030) identifies factor-bound genomic sites\nby using patterns in DNA cleavage resulting from the action of nucleases in open chromatin regions \n(regions typically bound by transcription factors). msCentipede, \na generalization of the [CENTIPEDE](http://centipede.uchicago.edu) model, accounts for \nheterogeneity in the DNA cleavage patterns around sites bound by transcription factors.\n\nThis code repository contains set of scripts to load the data and run the algorithm. The current document summarizes \nhow to download and setup this software package and provides instructions on how to run the software\non a test dataset of motif instances and some publicly available DNase-seq data.\n\n## Dependencies\n\nmsCentipede depends on \n+ [Numpy](http://www.numpy.org/)\n+ [Scipy](http://www.scipy.org/)\n+ [Cython](http://cython.org/)\n+ [Cvxopt](http://www.cvxopt.org/)\n+ [Pysam](https://github.com/pysam-developers/pysam)\n\nA number of python distributions already have the first two modules packaged in them. It is also\nstraightforward to install all these dependencies \n (1) using package managers for MACOSX and several Linux distributions,\n (2) from platform-specific binary packages, and\n (3) directly from source\n\n## Getting the source code\n\nTo obtain the source code from github, let us assume you want to clone this repo into a\ndirectory named `proj`:\n\n    mkdir ~/proj\n    cd ~/proj\n    git clone https://github.com/rajanil/msCentipede\n\nTo retrieve the latest code updates, you can do the following:\n\n    cd ~/proj/msCentipede\n    git fetch\n    git merge origin/master\n\nSince the algorithm is written in Cython, msCentipede will have to be compiled into a shared object in the following way, before it can be executed.\n\n    python setup.py build_ext --inplace\n\nThis step will generate a number of warning messages that can be ignored. If there are no error messages, the compilation step should produce two files `mscentipede.c` and `mscentipede.so`.\n\n## Executing the code\n\nThe script you will need to execute is `call_binding.py`. To see command-line \noptions that need to be passed to the script, you can do the following:\n\n    $ python call_binding.py\n\n    runs msCentipede, to infer transcription factor binding, given a set of motif\n    instances and chromatin accessibility data\n\n    positional arguments:\n      motif_file            name of a gzipped text file containing positional\n                            information and other attributes for motif instances\n                            of a transcription factor. columns of the file should\n                            be as follows. Chromosome Start End Strand PWM_Score\n                            [Attribute_1 Attribute_2 ...]. additional attributes\n                            are optional.\n      bam_files             whitespace separated list of bam files from a\n                            chromatin accessibility assay\n\n    optional arguments:\n      -h, --help            show this help message and exit\n      --task {learn,infer}  specify whether to learn model parameters or infer\n                            factor binding (default: learn)\n      --protocol {ATAC_seq,DNase_seq}\n                            specifies the chromatin accessibility protocol\n                            (default:DNase_seq)\n      --model {msCentipede,msCentipede_flexbg,msCentipede_flexbgmean}\n                            models differ in how they capture background rate of\n                            enzyme cleavage (default:msCentipede)\n      --restarts RESTARTS   number of re-runs of the algorithm (default: 1)\n      --mintol MINTOL       convergence criterion for change in per-site marginal\n                            likelihood (default: 1e-6)\n      --model_file MODEL_FILE\n                            file name to store the model parameters\n      --posterior_file POSTERIOR_FILE\n                            file name to store the posterior odds ratio, and\n                            likelihood ratios for each model component, at each\n                            motif.\n      --log_file LOG_FILE   file name to store some statistics of the EM algorithm\n      --window WINDOW       size of window around the motif instance, where\n                            chromatin accessibility profile is used for inferring\n                            transcription factor binding. (default: 128)\n      --batch BATCH         maximum number of motif instances used for learning\n                            model parameters. this is also the number of motif\n                            instances on which inference is performed at a time.\n                            (default: 10000)\n      --bam_file_genomicdna BAM_FILE_GENOMICDNA\n                            bam file from a chromatin accessibility assay on\n                            genomic DNA\n      --seed SEED           set seed for random initialization of parameters\n\nWe will now describe in detail how to use this software using an example dataset of CTCF motif instances on chromosome 10 in hg19 coordinates is provided in `test/Ctcf_chr10_motifs.txt.gz`. DNase-seq data for the GM12878 cell line (bam and bai files) can be downloaded from ENCODE to `test/` . In the following instructions, we assume the data files are named [Gm12878_Rep1.bam](http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseGm12878AlnRep1.bam) and [Gm12878_Rep2.bam](http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseGm12878AlnRep2.bam).\n\nThe software is designed to run in two separate steps. In the first step, optimal values for the model parameters are estimated using a subset of all motif instances. In the second step, posterior probability of factor binding is inferred for all motif instances. Since accurate estimates of model parameters can be obtained using 5000-10000 motif instances, this strategy enables efficient inference for those transcription factors that have orders of magnitude more motif instances genomewide. If more motif instances are available in the file than the value of the flag `--batch`, then `batch` number of motif instances that have the highest PWM score are used in learning model parameters.\n\n### Key Inputs\n\nThe key inputs that need to be passed to this script are \n+   a path to the file containing the list of motif instances\n+   the bam files (sorted and indexed) containing sequencing reads from a chromatin accessibility assay (DNase-seq or ATAC-seq). \n\n    *Note: these inputs are positional arguments and the files must be specified in the correct order (as shown above).* \n\nThe gzipped file of motif instances should have the following format.\n\n    Chr   Start     Stop      Strand  PwmScore\n    chr10 3944439   3944456   +       15.21570492\n    chr10 15627426  15627443  -       20.39377594\n\nIn the above format, positions are 0-based. *Start* corresponds to the first base of the core motif for *+* strand motif instances and the last base of the core motif for *-* strand motif instances.\n\nWhen multiple library / sample replicates are available, the bam files for the replicates can be provided as separate files, separated by whitespace. Bam files containing single-end reads and paired-end reads can be mixed since msCentipede currently does not model the fragment size distribution. However, bam files from different protocols or drastically different read lengths are best not mixed since protocol or read length differences could mask biologically meaningful heterogeneity that is relevant in identifying transcription factor binding sites. If the data were generated using an ATAC-seq protocol, the location of transpositions can be automatically identified from the read mapping positions by passing the flag `--protocol=ATAC_seq`.\n\n### Learning model parameters\n\nThe model parameters can be learned by passing the following flags.\n\n    python call_binding.py --task learn test/Ctcf_chr10_motifs.txt.gz test/Gm12878_Rep1.bam test/Gm12878_Rep2.bam\n\nThis will run msCentipede with all other default values and output a log file `test/Ctcf_chr10_motifs_msCentipede_log.txt` and a file `test/Ctcf_chr10_motifs_msCentipede_model_parameters.pkl` in which the model parameter objects are stored. This is a standard Python pickle file that can be viewed using the `cPickle` module.\n\n### Inferring factor binding\n\nThe posterior log odds of binding for a set of motif instances can be computed by passing the following flags.\n\n    python call_binding.py --task infer test/Ctcf_chr10_motifs.txt.gz test/Gm12878_Rep1.bam test/Gm12878_Rep2.bam\n\nThis will run msCentipede with all other default values and output a file `test/Ctcf_chr10_motifs_msCentipede_binding_posterior.txt.gz`.\n\n### Plotting the factor-specific cleavage profile\n\nIn order to check whether the algorithm could successfully learn a chromatin accessibility profile for a given set of motifs, we have provided a script to plot the shape of the inferred cleavage profile at factor bound sites.\n\n    $ python plot_accessibility_profile.py\n    \n    positional arguments\n     motif_file            name of a gzipped text file containing positional\n                            information and other attributes for motif instances\n                            of a transcription factor. columns of the file should\n                            be as follows. Chromosome Start End Strand PWM_Score\n                            [Attribute_1 Attribute_2 ...]. additional attributes\n                            are optional.\n    \n    optional arguments\n      --protocol {ATAC_seq,DNase_seq}\n                            specifies the chromatin accessibility protocol\n                            (default:DNase_seq)\n      --model {msCentipede,msCentipede_flexbg,msCentipede_flexbgmean}\n                            a different background cleavage profile will be shown\n                            depending on the model (default:msCentipede)\n\n### Optional parameters\n\nInstead of the default file names, you can specify the file name to which the run log, model parameters and binding posterior odds will be written, using the flags `--log_file`, `--model_file` and `--posterior_file`, respectively. For example, these flags allow using a previously learned model on a new set of motif sites for the same transcription factors. \n\nThe differences between the three models *msCentipede* , *msCentipede_flexbgmean* , and *msCentipede_flexbg* are specified in detail in the associated [publication](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0138030). If the model flag is specified to be *msCentipede_flexbgmean* or *msCentipede_flexbg*, then a path to a bam file containing chromatin accessibility data from genomic DNA must be passed, using the flag `--bam_file_genomicdna`.\n\n",
    "readme_length": 11082
  },
  {
    "name": "Deopen",
    "full_name": "kimmo1019/Deopen",
    "description": "A hybrid deep convolutional neural network for predicting chromatin accessibility",
    "stars": 25,
    "forks": 8,
    "language": "Python",
    "url": "https://github.com/kimmo1019/Deopen",
    "topics": [],
    "created_at": "2017-06-08T06:44:23Z",
    "updated_at": "2025-09-09T03:22:31Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# Deopen\nDeopen is a hybrid deep learning based framework to automatically learn the regulatory code of DNA sequences and predict chromatin accessibility.\n\n# Requirements\n- h5py\n- hickle\n- Scikit-learn=0.18.2\n- Theano=0.8.0\n- Lasagne=0.2.dev1\n- nolearn=0.6.0\n\n# Installation\nDownload Deopen by\n```shell\ngit clone https://github.com/kimmo1019/Deopen\n```\nInstallation has been tested in a Linux/MacOS platform with Python2.7.\n\n# Instructions\n\n \n\nPreprocessing data for model training\n```shell\npython Gen_data.py <options> -pos <positive_bed_file> -neg <negative_bed_file> -out <outputfile>\n```\n```\nArguments:\n  positive_bed_file: positive samples (bed format)\n  e.g. chr1\t9995\t10995\t\n       chr3\t564753\t565753\n       chr7\t565935\t566935\n       \n  negative_bed_file: negative samples (bed format)\n  e.g. chr1\t121471114\t121472114\t\n       chr2\t26268350\t26269350\n       chr5\t100783702\t100784702\n  \n  outputfile: preprocessed data for model training (hkl format)\n \nOptions:\n  -l <int> length of sequence (default: 1000)\n```\nRun Deopen classification model\n```shell\nTHEANO_FLAGS='device=gpu,floatX=float32' python Deopen_classification.py -in <inputfile> -out <outputfile>\n```\n```\n Arguments:  \n  inputfile: preprocessed data for model training (hkl format)  \n  outputfile: prediction outcome to be saved (hkl format)\n```\n Run Deopen regression model\n```shell\nTHEANO_FLAGS='device=gpu,floatX=float32' python Deopen_regression.py -in <inputfile> -reads <readsfile> -out <outputfile>\n```\n```\n Arguments:  \n  inputfile: preprocessed file containing different features (hkl format)  \n  readsfile: reads count for each sample (hkl format)  \n  outputfile: trained model to be saved (hkl format)\n```\n# Citation\nLiu, Qiao, et al. \"Chromatin accessibility prediction via a hybrid deep convolutional neural network.\" Bioinformatics 34.5 (2017): 732-738.\n\n```\n@article{liu2017chromatin,\n  title={Chromatin accessibility prediction via a hybrid deep convolutional neural network},\n  author={Liu, Qiao and Xia, Fei and Yin, Qijin and Jiang, Rui},\n  journal={Bioinformatics},\n  volume={34},\n  number={5},\n  pages={732--738},\n  year={2017},\n  publisher={Oxford University Press}\n}\n```\n\n# License\nThis project is licensed under the MIT License - see the LICENSE.md file for details\n",
    "readme_length": 2257
  },
  {
    "name": "hic-data-analysis-bootcamp",
    "full_name": "hms-dbmi/hic-data-analysis-bootcamp",
    "description": "Workshop on measuring, analyzing, and visualizing the 3D genome with Hi-C data.",
    "stars": 207,
    "forks": 49,
    "language": "Jupyter Notebook",
    "url": "https://github.com/hms-dbmi/hic-data-analysis-bootcamp",
    "topics": [
      "4d-nucleome-network",
      "cooler",
      "data-visualization",
      "hi-c",
      "hidivelab",
      "higlass",
      "hipiler"
    ],
    "created_at": "2018-05-02T18:36:59Z",
    "updated_at": "2025-11-28T03:39:13Z",
    "homepage": "https://hms-dbmi.github.io/hic-data-analysis-bootcamp/",
    "license": "MIT License",
    "readme": "# Hi-C Data Analysis Bootcamp\n\n> A tutorial on measuring, analyzing, and visualizing the 3D genome with Hi-C provided by Harvard, MIT, and UMassMed.\n\n![Funky Colormaps](/teaser.jpg?raw=true \"Some funky colormaps\")\n\n**ðŸ“¢ Slides, code, and data is available for you to rerun the analyses!** \n\n## Introduction\n\n4D Nucleome Data Coordination and Integration Center and the Center for 3D Structure and Physics of the Genome hosted a Hi-C data analysis bootcamp at Harvard Medical School on May, 8th 2018. This repo contains the material for this bootcamp. Below, you can find more information on how to walk through the hands-on sessions offline.\n\n## Files in this repository\n\n* **Tutorial Part 1 (Hi-C Protocol)**: [Slides PDF](./HiC-Protocol.pdf) | [PPTX](./HiC-Protocol.pptx)\n* **Tutorial Part 2 (From fastqs to contact matrices)**: [Slides HTML](https://hms-dbmi.github.io/hic-data-analysis-bootcamp/)\n* **Tutorial Part 3 (From contact matrices to biology)**: [Slides PDF](./From-Contact-Matrix-to-Biology.pdf) | [PPTX](./From-Contact-Matrix-to-Biology.pptx)\n* **Tutorial Part 4 (Hi-C Data Visualization - HiGlass)**: [Slides HTML](https://hms-dbmi.github.io/hic-data-analysis-bootcamp/#24)\n* **Tutorial Part 5 (Hi-C Data Visualization - HiPiler)**: [Slides PDF](./HiPiler-Exploring-HiC-Features-Through-Visual-Decomposition.pdf) | [HTML](https://speakerdeck.com/flekschas/hi-c-data-visualization-with-hipiler)\n\n## Presenters\n\n* Johan Gibcus, Research Instructor, Universy Massachusetts Medical School\n* [Nezar Abdennur](http://nvictus.me/), PhD student, MIT\n* [Soo Lee](https://compbio.hms.harvard.edu/people/soohyun-lee), Senior Bioinformatics Scientist, Harvard Medical School\n* [Peter Kerpedjiev](http://emptypipes.org/about), Postdoctoral Research Fellow, Harvard Medical School\n* [Fritz Lekschas](https://lekschas.de/) PhD Student, Harvard University\n* [Leonid Mirny](http://mirnylab.mit.edu/) Professor, MIT\n\n## Organizers\n\n* [Burak Alver](https://compbio.hms.harvard.edu/people/burak-alver-0), Scientific Project Manager, Harvard Medical School\n* [Nils Gehlenborg](http://gehlenborglab.org/), Assistant Professor, Harvard Medical School\n* [Peter Park](https://compbio.hms.harvard.edu/), Professor, Harvard Medical School\n\n## Motivation and Objectives\n\nDue in large part to the explanatory power of chromosome organization in gene regulation, its association with disease and disorder as well as the unanswered questions regarding the mechanisms behind its maintenance and function, the 3D structure and function of the genome are becoming increasingly target of scientific scrutiny. With efforts such as the 4D Nucleome Project and ENCODE 4 already beginning to generate large amounts of data, the ability to analyze and visualize it will be a valuable asset to any computational biologist tasked with interpretation of experimental results.\n\nThe objectives of this tutorial are\n* To introduce the theoretical concepts related to 3D genome data analysis\n* To familiarize participants with the data types, analysis pipeline, and common tools for analysis and visualization of 3D genome data\n* To provide a hands on experience in data analysis by walking through some common use cases of existing tools for data analysis and visualization.\n\nAfter the workshop participants should be able to obtain, process, analyze, and visualize 3D genome data on their own as well as to understand some of the logic, motivation and pitfalls associated with common operations such as matrix balancing and multi-resolution visualization.\n\nThe subject matter and practical exercises presented in this tutorial will be accessible to a broad audience. Prior experience with next generation sequencing and the data it produces will be helpful for understanding the subsequent processing steps used to derive contact maps as well as some of the artifacts that can arise during data processing.\n\nThe material will be most useful to computational biologists and biologists working on genomics-related topics.\n\n## Student Requirements\n\n * A server will be set up for students with all the required software.\n * Windows users, please install [Putty (for ssh)](https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html).\n\n## Agenda\n\n**09:00 - 09:10** Introduction and Overview (Peter Park and Burak Alver, Harvard)\n\n**09:10 - 10:30** Hi-C Protocol (Johan Gibcus, UMass)\n\n**10:30 - 10:45** _Break_\n\n**10:45 - 12:15** From fastqs to contact matrices (Soohyun Lee, Harvard)\n\n**12:15 - 13:00** _Lunch_\n\n**13:00 - 14:00** From contact matrices to biology (Nezar Abdennur, MIT)\n\n**14:00 - 15:00** Hi-C Data Visualization - HiGlass (Peter Kerpedjiev, Harvard)\n\n**15:00 - 15:15** _Break_\n\n**15:15 - 16:00** Hi-C Data Visualization - HiPiler (Fritz Lekschas, Harvard)\n\n**16:00 - 17:00** Keynote Speaker - Leonid Mirny, MIT\n\n\n## Instructor Bios\n\n### Johan Gibcus\n\nJohan Gibcus is a Research Instructor at the University of Massachussetts Medical School. He has not only used but also refined the Hi-C protocol to answer important biological questions about chromosome organization and replication. Web: [http://www.dekkerlab.org/](http://www.dekkerlab.org/)\n\n### Soo Lee\n\nSoo Lee is a Senior Bioinformatics Scientist in the Department of Biomedical Informatics at Harvard Medical School. She is creating cloud-based pipelines for Hi-C and other genomic data and developing infrastructure for automation of such pipelines as part of the 4D Nucleome Data Coordination and Integration Center. Web: [compbio.hms.harvard.edu/people/soohyun-lee](https://compbio.hms.harvard.edu/people/soohyun-lee)\n\n### Nezar Abdennur\n\nNezar Abdennur is a PhD candidate in Computational and Systems Biology at MIT. His research focuses on the determinants of 3D genome organization and the development of tools for dealing with large Hi-C datasets. Twitter: [@nv1ctus](https://twitter.com/nv1ctus) Web: [nvictus.me](http://nvictus.me)\n\n### Peter Kerpedjiev\n\nPeter Kerpedjiev is a postdoctoral researcher working on creating tools (such as [HiGlass](http://higlass.io)) for visualizing large genomic data sets. Twitter: [@pkerpedjiev](https://twitter.com/pkerpedjiev) Web: [emptypipes.org](http://emptypipes.org)\n\n### Fritz Lekschas\n\nFritz is a PhD student working on biomedical information visualization with focus on large multiscale genomic data sets. He created tools like [HiPiler](http://hipiler.lekschas.de) or [Scalable Insets](http://scalable-insets.lekschas.de) Twitter: [@flekschas](https://twitter.com/flekschas) Web: [lekschas.de](https://lekschas.de)\n\n### Leonid Mirny\n\nLeonid Mirny is a professor at MIT's Institute for Medical Engineering & Science. His lab studies the three dimensional organization of chromosomes using a combination of computational analysis and simulation. Twitter: [@leonidmirny](https://twitter.com/leonidmirny) Web: [mirnylab.mit.edu](http://mirnylab.mit.edu/)\n\n## Pointers for Offline Walk-through\n\nDuring the bootcamp, users were given access to linux servers where\n- docker was installed,\n- conda was installed,\n- a conda enivronment was set up with a number of dependencies\ninstalled, including juypter notebook,\n- higlass-manager was installed,\n- and sample data was downloaded.\n\nYou can set up a similar environment and walk through the hands-on\nsessions of the bootcamp by following the instructions below. Allow 30G of storage for all files used in the tutorial.\n\n### From fastqs to contact matrices\n\n1. Install [docker](https://docs.docker.com/install/), if you have not already done so. ([Docker](https://docs.docker.com/) is a lighter alternative to virtual machines.)\n2. Pull the docker image: `docker pull duplexa/4dn-hic:v42`. This docker image contains a number of software that have been pre-installed for HiC data processing.\n3. Download the sample data for this session under your home directory to \"~/data/\" (or edit the commands on the slides accordingly, if you prefer a different directory).\n```\nmkdir data\ncd data/\n# input fastq files\nwget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/input_R1.fastq.gz\nwget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/input_R2.fastq.gz\ngunzip input_R1.fastq.gz\ngunzip input_R2.fastq.gz\n# bwa genome index\nwget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/hg38.bwaIndex.tgz\ntar -xzf hg38.bwaIndex.tgz\nrm hg38.bwaIndex.tgz\n# chromsizes\nwget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/hg38.mainonly.chrom.size\n# prebaked output files\nwget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/prebaked.tgz\ntar -xzf prebaked.tgz\nrm prebaked.tgz\n# move back a directory\ncd ..\n```\n\nNow, you should be able to follow slides 1 through 23 of [the tutorial](https://hms-dbmi.github.io/hic-data-analysis-bootcamp/). When you are finished, exit the docker container with `Ctrl-d` before proceeding to the next part.\n\n### Working in a cluster without docker\n\nIf you are working in a High Performance Compute Cluster, you may not\nbe allowed the install Docker. Instead, you can find the recipe for\nthe docker image used above\n[here](https://github.com/4dn-dcic/docker-4dn-hic/tree/v42). The exact configuration of the docker image can be seen in the\n[dockerfile](https://github.com/4dn-dcic/docker-4dn-hic/blob/v42/Dockerfile). You\ncan get information on the  bioinformatics software installed inside the docker image in the [download.sh](https://github.com/4dn-dcic/docker-4dn-hic/blob/v42/downloads.sh)\nfile.\n\n### From contact matrices to biology\n\n1. Install [conda](https://conda.io/miniconda.html), if you have not\n   already done so. Conda is an open source package management tool that allows you to create separate environments.\n2. Clone this repo and set up the environment.\n    ```\n    git clone https://github.com/hms-dbmi/hic-data-analysis-bootcamp\n    cd hic-data-analysis-bootcamp\n    git pull\n    #you may need some of the following in case you have an issue creating an environment\n    #conda update --all -y\n    #sudo yum install -y hg\n    #conda install gcc\n    conda env create -n bootcamp -f environment.yml\n    ```\n3. Download the sample data for this session into the pre-existing \"notebooks/data\" directory (or edit the commands on the slides accordingly, if you prefer a different directory.\n    ```\n    # from the hic-data-analysis-bootcamp directory we just made\n    cd notebooks/data\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/NIPBL.1000.mcool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/NIPBL.10000.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/NIPBL.20000.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/NIPBL.40000.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/NIPBL.100000.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/TAM.1000.mcool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/TAM.10000.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/TAM.20000.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/TAM.40000.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/TAM.100000.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/UNTR.1000.mcool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/UNTR.10000.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/UNTR.20000.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/UNTR.40000.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/UNTR.100000.cool\n    \n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/CtcfCtrl.mm9__VS__InputCtrl.mm9.narrowPeak_with_motif.txt.gz\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/GSM1551552_HIC003_merged_nodups.txt.subset.gz\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/NIPBL_R1.nodups.pairs.gz\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/NIPBL_R1.nodups.pairs.gz.px2\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/Rao2014-GM12878-MboI-allreps-filtered.1000kb.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/Rao2014-GM12878-MboI-allreps-filtered.5kb.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/UNTR_R1.nodups.pairs.gz\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/UNTR_R1.nodups.pairs.gz.px2\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/b37.chrom.sizes.reduced\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/ctcf-sites.paired.300kb_flank10kb.tsv\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/hg19.chrom.sizes.reduced\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/mm9.chrom.sizes.reduced\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/mm9.fa\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/ranked_TSS.tsv\n    ```\n4. Go back to the \"notebooks\" directory and activate the environment to run the jupyter notebook.\n    ```\n    cd ..\n    source activate bootcamp\n    jupyter notebook\n    ```\nIf you're running it on your local machine, the notebook will open at http://localhost:8888. You may have to input the token displayed when starting up the Jupyter. Follow the steps in the notebooks starting with the top one, named \"00_intro_cooler-cli\".\n\n### HiGlass\n1. Install and start docker on your machine.\n    ```\n    docker pull gehlenborglab/higlass:v0.2.63  # higlass\n    pip install higlass-manage --upgrade\n    ```\n2. Download the sample data.\n    ```\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/Schwarzer-et-al-2017-NIPBL.multi.cool\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/Schwarzer-et-al-2017-RNAseq-minus.bw\n    wget https://s3.amazonaws.com/4dn-dcic-public/hic-data-analysis-bootcamp/Schwarzer-et-al-2017-UNTR.multi.cool\n    ```\n\nNow, you should be able to follow slides 24 through 59 of [the tutorial](https://hms-dbmi.github.io/hic-data-analysis-bootcamp/#24).\n\n\n## Resources\n\n### Software\n\n* [bwa](https://github.com/lh3/bwa) and [SAM spec](https://samtools.github.io/hts-specs/SAMv1.pdf)\n* [pairsamtools](https://github.com/mirnylab/pairsamtools)\n* [pairix](https://github.com/4dn-dcic/pairix)\n* [cooler](https://github.com/mirnylab/cooler) and [docs](http://cooler.readthedocs.io/en/latest/)\n* [HiGlass](http://higlass.io), [source code](https://github.com/hms-dbmi/higlass/), and [docs](https://hms-dbmi.github.io/higlass-docs/)\n* [HiPiler](http://hipiler.higlass.io), [source code](https://github.com/flekschas/hipiler), [docs](https://github.com/flekschas/hipiler/wiki), [project page](http://hipiler.lekschas.de)\n\n### Package and environment management\n\n* [conda](https://conda.io/miniconda.html)\n* [bioconda](https://bioconda.github.io/)\n\n\n\n### Papers\n\n* Imakaev, Maxim, et al. \"Iterative correction of Hi-C data reveals hallmarks of chromosome organization.\" Nature methods 9.10 (2012): 999-1003. doi:[10.1038/nmeth.2148](https://doi.org/10.1038/nmeth.2148)\n* Lajoie, Bryan R., Job Dekker, and Noam Kaplan. \"The Hitchhikerâ€™s guide to Hi-C analysis: practical guidelines.\" Methods 72 (2015): 65-75. doi:[10.1016/j.ymeth.2014.10.031](https://doi.org/10.1016/j.ymeth.2014.10.031)\n* Kerpedjiev, Peter, et al. \"HiGlass: Web-based Visual Comparison And Exploration Of Genome Interaction Maps\" bioRxiv. doi:[10.1101/121889](https://doi.org/10.1101/121889)\n* Lekschas, Fritz et al. \"HiPiler: Visual Exploration Of Large Genome Interaction Matrices With Interactive Small Multiples\" IEEE Transactions on Visualization and Computer Graphics, 24(1), 522-531. [doi:10.1109/TVCG.2017.2745978](https://doi.org/10.1109/TVCG.2017.2745978)\n* Belaghzal H, et al. \"Hi-C 2.0: An optimized Hi-C procedure for high-resolution genome-wide mapping of chromosome conformation.\" Methods. 2017 [https://doi.org/10.1016/j.ymeth.2017.04.004](https://doi.org/10.1016/j.ymeth.2017.04.004)\n* Golloshi R, et al. \"Iteratively improving Hi-C experiments one step at a time.\" Methods. 2018 [https://doi.org/10.1016/j.ymeth.2018.04.033](https://doi.org/10.1016/j.ymeth.2018.04.033)\n* Oddes, Sivan, et al. \"Three invariant Hi-C interaction patterns: applications to genome assembly\". bioRxiv 306076. [https://doi.org/10.1101/306076](https://doi.org/10.1101/306076)\n",
    "readme_length": 16681
  },
  {
    "name": "Higashi",
    "full_name": "ma-compbio/Higashi",
    "description": "single-cell Hi-C, scHi-C, Hi-C, 3D genome, nuclear organization, hypergraph",
    "stars": 92,
    "forks": 15,
    "language": "Jupyter Notebook",
    "url": "https://github.com/ma-compbio/Higashi",
    "topics": [
      "3d-genome",
      "hypergraph",
      "machine-learning",
      "single-cell"
    ],
    "created_at": "2020-12-11T22:11:20Z",
    "updated_at": "2025-11-14T15:53:30Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "\n# Higashi: Multiscale and integrative scHi-C analysis\n<img src=\"https://github.com/ma-compbio/Higashi/blob/main/figs/logo2.png\" align=\"right\"\n     alt=\"logo\" width=\"290\">\n\nhttps://doi.org/10.1038/s41587-021-01034-y\n\nAs a computational framework for scHi-C analysis, Higashi has the following features:\n\n-  Higashi represents the scHi-C dataset as a **hypergraph**\n     - Each cell and each genomic bin are represented as the cell node and the genomic bin node.\n     - Each non-zero entry in the single-cell contact map is modeled as a hyperedge. \n     - The read count for each chromatin interaction is used as the attribute of the hyperedge. \n- Higashi uses a **hypergraph neural network** to unveil high-order interaction patterns within this constructed hypergraph.\n- Higashi can produce the **embeddings** for the scHi-C for downstream analysis.\n-  Higashi can **impute single-cell Hi-C contact maps**, enabling detailed characterization of 3D genome features such as **TAD-like domain boundaries** and **A/B compartment scores** at single-cell resolution.\n\n--------------------------\n\n![figs/Overview.png](https://github.com/ma-compbio/Higashi/blob/main/figs/short_overview.png)\n\n# Installation\n\nWe now have Fast-Higashi on conda.\n`conda install -c ruochiz fasthigashi`\n\nThe conda support for Higashi is still an on-going effort. Currently, you can install it by:\n\n```{bash}\ngit clone https://github.com/ma-compbio/Higashi/\ncd Higashi\npython setup.py install\n```\n\nIt is recommended to have pytorch installed (with CUDA support when applicable) after installing higashi / fast-higashi.\n\n# Documentation\nPlease see [the wiki](https://github.com/ma-compbio/Higashi/wiki) for extensive documentation and example tutorials.\n\nHigashi is constantly being updated, see [change log](https://github.com/ma-compbio/Higashi/wiki/Change-Log) for the updating history\n\n# Tutorial\n- Higashi on [4DN sci-Hi-C (Kim et al.)](https://github.com/ma-compbio/Higashi/blob/main/tutorials/4DN_sci-Hi-C_Kim%20et%20al.ipynb)\n- Higashi on [Ramani et al.](https://github.com/ma-compbio/Higashi/blob/main/tutorials/Ramani%20et%20al.ipynb)\n- Fast-Higashi on [Lee et al.](https://github.com/ma-compbio/Fast-Higashi/blob/main/PFC%20tutorial.ipynb)\n\n# Cite\n\nCite our paper by\n\n```\n@article {Zhang2020multiscale,\n\tauthor = {Zhang, Ruochi and Zhou, Tianming and Ma, Jian},\n\ttitle = {Multiscale and integrative single-cell Hi-C analysis with Higashi},\n\tyear={2021},\n\tpublisher = {Nature Publishing Group},\n\tjournal = {Nature biotechnology}\n}\n```\n\n![figs/Overview.png](https://github.com/ma-compbio/Higashi/blob/main/figs/higashi_title.png)\n\n# See also\n\nFast-Higashi for more efficient and robust scHi-C embeddings\nhttps://www.cell.com/cell-systems/fulltext/S2405-4712(22)00395-7\n\nhttps://github.com/ma-compbio/Fast-Higashi\n\n# Contact\n\nPlease contact ruochiz@andrew.cmu.edu or raise an issue in the github repo with any questions about installation or usage. \n",
    "readme_length": 2929
  },
  {
    "name": "3d-genome-processing-tutorial",
    "full_name": "hms-dbmi/3d-genome-processing-tutorial",
    "description": "A 3D genome data processing tutorial for ISMB/ECCB 2017",
    "stars": 52,
    "forks": 10,
    "language": "Jupyter Notebook",
    "url": "https://github.com/hms-dbmi/3d-genome-processing-tutorial",
    "topics": [
      "4d-nucleome-network",
      "hi-c",
      "hidivelab",
      "higlass",
      "tutorial",
      "visualization"
    ],
    "created_at": "2017-07-06T16:12:49Z",
    "updated_at": "2025-11-14T09:02:24Z",
    "homepage": "https://hms-dbmi.github.io/3d-genome-processing-tutorial/",
    "license": "MIT License",
    "readme": "# ISMB Tutorial: 3D Genome Data Processing, Analysis, and Visualization Tutorial\n\n## Files in this repository\n\n* **Tutorial Part 1 (Introduction to HiC)**: ISMB-Tutorial-Part-1-Nezar.pptx\n* **Tutorial Part 2 (Data Processing and Visualization)**: [Introduction](https://github.com/hms-dbmi/3d-genome-processing-tutorial/blob/master/ISMB-Tutorial-Visualization-Intro.pdf) and ISMB-Tutorial-Part-2-Soo-Nils-Peter.html or [https://hms-dbmi.github.io/3d-genome-processing-tutorial/](https://hms-dbmi.github.io/3d-genome-processing-tutorial/)\n* **Tutorial Part 3 (Data Analysis for Nuclear Compartmentalization)**: ISMB-Tutorial-Part-3-Ma.pdf\n\n## Organizers & Presenters\n\n* [Nezar Abdennur](http://nvictus.me/), PhD student, MIT\n* [Soo Lee](https://compbio.hms.harvard.edu/people/soohyun-lee), Senior Bioinformatics Scientist, Harvard Medical School\n* [Nils Gehlenborg](http://gehlenborglab.org/), Assistant Professor, Harvard Medical School\n* [Peter Kerpedjiev](http://emptypipes.org/about), Postdoctoral Research Fellow, Harvard Medical School\n* [Jian Ma](http://www.cs.cmu.edu/~jianma/), Associate Professor, Carnegie Mellon University\n\n## Motivation\n\nDue in large part to the explanatory power of chromosome organization in gene regulation, its association with disease and disorder as well as the unanswered questions regarding the mechanisms behind its maintenance and function, the 3D structure and function of the genome are becoming increasingly target of scientific scrutiny. With efforts such as the 4D Nucleome Project and ENCODE 4 already beginning to generate large amounts of data, the ability to analyze and visualize it will be a valuable asset to any computational biologist tasked with interpretation of experimental results.\n\n## Objectives\n\nThe objectives of this tutorial are\n\n* To introduce the theoretical concepts related to 3D genome data analysis\n* To familiarize participants with the data types, analysis pipeline, and common tools for analysis and visualization of 3D genome data\n* To provide a hands on experience in data analysis by walking through some common use cases of existing tools for data analysis and visualization.\n\n## Goal\n\nAfter the workshop participants should be able to obtain, process, analyze, and visualize 3D genome data on their own as well as to understand some of the logic, motivation and pitfalls associated with common operations such as matrix balancing and multi-resolution visualization.\n\n## Intended audience and level\n\nThe subject matter and practical exercises presented in this tutorial will be accessible to a broad audience. Prior experience with next generation sequencing and the data it produces will be helpful for understanding the subsequent processing steps used to derive contact maps as well as some of the artifacts that can arise during data processing.\n\nThe material will be most useful to computational biologists and biologists working on genomics-related topics. \nTentative Schedu\n\n\n## Student Requirements:\n\n* Install Docker\n  * https://www.docker.com/community-edition\n* Install Miniconda\n  * https://conda.io/miniconda.html  \n* Windows users\n  * Putty (for ssh)\n  * https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html\n  \n## Agenda\n\n**10:00 - 10:25 - Introduction and Overview**\n\n* Who are we? [5 minutes]\n* What weâ€™ll cover [5 minutes]\n* AWS accounts and ssh in [10 minutes]\n\n**10:25 - 11:30 Hi-C Analysis**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Intro to Hi-C [Nezar]**\n\n* 15 minutes:\n  * Introduction\n    * Protocol\n    * What Hi-C sees\n    * Levels of genome organization inferred from patterns\n      * Checkering: compartments\n      * Enriched squares without checkering: TADs\n      * Corner peaks: loops\n    * How Hi-C is processed\n    * How features are assessed\n  * Hi-C processing steps (informational)\n    * Mapping\n    * Filtering\n    * Creating a list of contacts\n    * Binning\n    * Normalization\n    * Feature analysis\n      * scaling\n      * compartments\n      * TADs\n    * QC\n\n* 45min\n  * Practical - processing pipeline [Soo]: \n    * Mapping\n      * bwa mem -SP5M index fastq1 fastq2 | samtools view -bhS - > output.bam\n      * samtools view output.bam | head -n 5\n      * output: bam file\n    * Filtering / sorting / Creating a list of contacts\n      * pairsamtools\n      * outputs: pairs, bam\n      * Pairs / pairix (indexes the pairs file)\n    * Binning\n      * Cooler / cool\n    * Normalization\n      * Cooler / cool\n    * map a small Hi-C dataset using distiller (https://github.com/mirnylab/distiller) and generate contact matrices using cooler (https://github.com/mirnylab/cooler)\n  * Practical - Feature analysis [Nezar]: \n    * Jupyter notebook cooler walkthrough\n    * Cis vs trans and scaling (contact probability vs genomic distance)\n    * Compartment profile, saddle plots\n    * Insulation, TADs\n    * Pileups\n\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**11:30 - 11:45 Coffee Break**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**11:45 - 12:55 Visualization**\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Existing tools for contact matrix exploration**\n\n* 20 minutes [Nils]: \n  * 3D genome browser\t\n  * WashU epigenome browser\n  * Juicebox\n  * HiGlass \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Using HiGlass (http://higlass.io) to display contact maps [Peter]**\n\n* 30 minutes: \n  * Overview of common operations such as adding tracks, removing tracks, adding views, removing view, linking views by zoom and location\n  * Practical: \n    * Create an interactive version of a figure\n* 20 minutes: Installing HiGlass\n  * Overview of the HiGlass architecture and description of the infrastructure used to run it\n  * Practical: \n    * Create a local HiGlass instance\n    * Convert a contact map to multi-resolution format and import it\n    * Convert a bigWig file to hitile format and import it\n    * Open both files in the client and navigate to an interesting location\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**12:55 - 1:30 - Data Analysis for Nuclear Compartmentalization [Jian Ma]**\n\n* Introduction\n* DamID analysis\n* Repli-seq analysis\n* Data from emerging technologies\n\n\n## Instructor Bios\n\n### Nezar Abdennur\n\nNezar is a PhD candidate in Computational and Systems Biology at MIT. His research focuses on the determinants of 3D genome organization and the development of tools for dealing with large Hi-C datasets. http://nvictus.me\n\n### Soo Lee\n\nSoo Lee is a Senior Bioinformatics Scientist in the Department of Biomedical Informatics at Harvard Medical School. She is creating cloud-based pipelines for Hi-C and other genomic data and developing infrastructure for automation of such pipelines as part of the 4D Nucleome Data Coordination and Integration Center. https://compbio.hms.harvard.edu/people/soohyun-lee\n\n### Nils Gehlenborg\n\nNils is an Assistant Professor in the Department of Biomedical Informatics at Harvard Medical School. His research group is developing tools to visualize 3D genome conformation data as well as heterogeneous data from large-scale cancer genomics studies. http://gehlenborglab.org \n\n### Peter Kerpedjiev\n\nPeter is a postdoctoral researcher working on creating tools (such as HiGlass) for visualizing large genomic data sets. Web site: http://emptypipes.org/about\n\n### Jian Ma\n\nJian Ma is an Associate Professor in the School of Computer Science at Carnegie Mellon University. The research in his group focuses on developing algorithms to better understand genome structure and function. http://www.cs.cmu.edu/~jianma/\n\n\n## Resources\n\nSoftware\n\n* [bwa](https://github.com/lh3/bwa) and [SAM spec](https://samtools.github.io/hts-specs/SAMv1.pdf)\n* [pairsamtools](https://github.com/mirnylab/pairsamtools)\n* [pairix](https://github.com/4dn-dcic/pairix)\n* [cooler](https://github.com/mirnylab/cooler) and [docs](http://cooler.readthedocs.io/en/latest/)\n* [HiGlass](http://cooler.readthedocs.io/en/latest/) and [wiki](https://github.com/hms-dbmi/higlass/wiki)\n* [HiPiler](https://github.com/flekschas/hipiler)\n\n\nBioinformatics workflow managers\n\n* [snakemake](https://snakemake.readthedocs.io/en/stable/)\n* [nextflow](https://www.nextflow.io/)\n\n\nPackage and environment management\n\n* [conda](https://conda.io/miniconda.html)\n* [bioconda](https://bioconda.github.io/)\n\n\nHi-C methods papers\n\n* Imakaev, Maxim, et al. \"Iterative correction of Hi-C data reveals hallmarks of chromosome organization.\" Nature methods 9.10 (2012): 999-1003. doi:[10.1038/nmeth.2148](https://doi.org/10.1038/nmeth.2148)\n* Lajoie, Bryan R., Job Dekker, and Noam Kaplan. \"The Hitchhikerâ€™s guide to Hi-C analysis: practical guidelines.\" Methods 72 (2015): 65-75. doi:[10.1016/j.ymeth.2014.10.031](https://doi.org/10.1016/j.ymeth.2014.10.031)\n",
    "readme_length": 8641
  },
  {
    "name": "TADkit",
    "full_name": "3DGenomes/TADkit",
    "description": "3D Genome Browser",
    "stars": 32,
    "forks": 11,
    "language": "JavaScript",
    "url": "https://github.com/3DGenomes/TADkit",
    "topics": [
      "3d-genome-browser",
      "3d-models",
      "chromatin",
      "genome",
      "javascript"
    ],
    "created_at": "2014-08-17T13:41:07Z",
    "updated_at": "2025-06-30T23:23:10Z",
    "homepage": "https://3dgenomes.github.io/TADkit/",
    "license": "MIT License",
    "readme": "![TADkit Logo](https://raw.githubusercontent.com/3DGenomes/TADkit/master/resources/logo/tadkit-logo-title.png)\n\n![Release](https://img.shields.io/github/release/3DGenomes/TADkit.svg)\n![License](https://img.shields.io/github/license/3DGenomes/TADkit.svg)\n\n<br />\n![AngularJS](https://img.shields.io/badge/AngularJS-1.6.10-red.svg)\n![Three.js](https://img.shields.io/badge/Three.js-v0.97.0-orange.svg)\n\n\n### **Unfortunately we have ceased the development and maintenance of TADkit**\n\n\nTADkit is a HTML5 and JavaScript-based 3D genome browser. It makes use of D3.js for rendering the 1D and 2D tracks and WebGl by Three.js for rendering the 3D track.\n\nTADkit is currently developed at the [MarciusLab](http://www.marciuslab.org) with the contributions of Mike Goodstadt, David Castillo and many maembers of our Lab.\n\n##Description\nTADkit creates interactive 3D representations of chromatin conformations modeled from 3C-based interaction matrices. \nThe user can overlay 1D and 2D tracks of genomic data to these 3D views to directly evaluate the relationship \nbetween the 3D structure of the genome and its biological function.\n\n##Documentation\n###Instalation\nTo install TADkit in your server or locally, you need to download the repository and move the \"tadkit\" folder \nin your desired directoy. Next, point your browser to the index.html file in that directory. If you want to install it\nlocally you'll need to use Chrome with the flag \"--allow-file-access-from-files\" to run JavaScript locally (additional information for other Internet browsers [here](https://github.com/mrdoob/three.js/wiki/How-to-run-things-locally).\n\n###How to use it\nVisualization of 3D models and overlaying of data for analysis in TADkit is achieved in three simple steps. (1) The user may import a [TADbit](http://3Dgenomes.org/tadbit/) JSON file by using the drop-in area or the file menu in the TADkit main page. (2) The user is presented with an overview of the data. Each 3D model cluster is shown as an ensemble of grey strands, with its centroid model highlighted. After selecting a cluster, the browser view opens (see figure below) with a 3D scene of the centroid of the selected cluster, along with â€œclassicâ€ genomic tracks. (3) Finally, the user may add additional genomic tracks to the browser by inputting them using BigWig or BedGraph formats.\n\n![Browser View](https://raw.githubusercontent.com/3DGenomes/TADkit/master/resources/screenshots/tadkit_scene.png)\n\nThe browser view is composed by: \n* a. Top toolbar with the model title, internal navigation links, data input link and the current genomic position.\n* b. 3D rendering of chromatin as contiguous spheres or a chromatin â€œfiberâ€. \n* c. Gene information located in the viewpoint of the 3D model.\n* d. Detailed information for the selected locus.\n* e. Stacked â€œclassicalâ€ tracks showing: \n  * The genes found in the region covered by the current model (automatically fetched using the Ensembl REST API).\n  * A linear depiction of the 3D proximity from the selected viewpoint to the rest of the model.\n  * A graph of the restraints imposed during modeling (blue for repulsive and red for attacting restraints). \n  * Additional user-imported tracks.\n\n",
    "readme_length": 3192
  },
  {
    "name": "MATCHA",
    "full_name": "ma-compbio/MATCHA",
    "description": "multiway chromatin interaction, 3D genome, single-nucleus, hypergraph representation learning",
    "stars": 29,
    "forks": 6,
    "language": "Python",
    "url": "https://github.com/ma-compbio/MATCHA",
    "topics": [
      "3d-genome",
      "hypergraph",
      "hypergraph-learning",
      "machine-learning",
      "multiway-interactions"
    ],
    "created_at": "2019-11-07T18:53:09Z",
    "updated_at": "2024-07-19T03:55:13Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# MATCHA: Probing multi-way chromatin interaction with hypergraph representation learning\n\nThis is the implementation of the algorithm MATCHA for analyzing multi-way chromatin interaction data via hypergraph representation learning.\n\n## Requirements\nThe main part of the alogrithm (`process.py, generate_kmers.py, main.py`) requires\n\n\n- h5py\n- numpy\n- pytorch\n- pybloom_live (https://github.com/joseph-fox/python-bloomfilter)\n- scikit-learn\n- tqdm\n\n\nThe visualization part of the algorihtm (`denoise_contact.py`) requires\n\n- seaborn\n- matplotlib\n\n## Configure the parameters\n\nAll the input parameters are stored in the config.JSON file. \nPlease fill in this file before running the program.\nNote that, some scripts only use part of these parameters, so these parameters can be filled in before running those specific script.\n\n| params       | description                  | example                   | used in    |\n|--------------|------------------------------|---------------------------|------------|\n| cluster_path | the path of the cluster file | \"./4DNFIBEVVTN5.clusters\" | process.py |\n|mcool_path | the path of the mcool file | \"./4DNFIUOOYQC3.mcool\" | process.py|\n|resolution | the resolution to consider (bin size) | 1000000 | process.py|\n|chrom_list | list of the chromosomes to consider | [\"chr1\", \"chr2\"] | process.py, main.py|\n|chrom_size | the path of the chromatin size file | \"./hg38.chrom.sizes.txt\" | process.py|\n|temp_dir | the directory of the temp files to store | \"../Temp\" | all|\n|max_cluster_size| the maximum cluster size to consider | 25| process.py, generate_kmers.py |\n|min_distance | minimum pairwise genomic distance constraint for multi-way interactions (in unit of the number of bins) |0| generate_kmers.py, main.py, denoise_contact.py|\n|k-mer_size| list of the size of the k-mers to considier | [2,3,4,5] | generate_kmers.py, main.py, \n|min_freq_cutoff | only consider k-mers with occurrence frequency >= | 2 | generate_kmers.py|\n|quantile_cutoff_for_positive | the quantile cutoff of hyperedges to be considered as positive samples. For instance, 0.6 represents the hyperedges with occurrence frequency in the top 40% (>= 0.6) would be used as positive samples. The cut-off is applied to different sized hyperedges separately| 0.6 | main.py | \n|quantile_cutoff_for_unlabel | the quantile cutoff of hyperedges to be considered as non-negative samples (positive + samples that cannot be confidently classified as either positive or negative samples) | 0.4 | main.py | \n|embed_dim | embedding dimensions for the bins | 64| main.py|\n\n\n## Usage\n1. `cd Code`\n2. Run `python process.py`, which will parse the input cluster file, mcool file and the chromosome size files. There will be 3 key output files:\n   1. `bin2node.npy, node2bin.npy` within the `temp_dir` above. As the name indicates, it's a dictionary that maps the genomic bin to the node id and vice verse. The genomic bin has the format of `chr1:2000000`\n   2. `node2chrom.npy`. It maps the node id to the chromosome.\n   3. All these dictionaries can be loaded through `np.load(FILEPATH, allow_picke=True).item()`\n3. Run `python generate_kmers.py`, which will further transfer the parsed cluster file into a list of k-mers (hyperedges) with the corresponding occurrence frequencies. The output files are\n   1. `all_<k-mer size>_counter.npy`: the generated k-mers\n   2. `all_<k-mer size>_freq_counter.npy`: the occurrence frequency corresponds to the generated k-mers\n4. Run `python main.py`, which will train the model based on the generated dataset. The output includes:\n   1. `model2load` within the `temp_dir` above. The model can be loaded by `model = torch.load(FILEPATH)`. The model can return predictions through `model(x)`. Note that the `x` should be a pytorch tensor of dtype `torch.long`\n   2. `embeddings.npy` lies in the root dir. It's the embedding vectors for the genomic bins. The shape of the vectors are `(num of genomic bins, embed_dim chosen above)`. The mapping relationship between the genomic bin and its index in this vector can be retrived in the dictionary `node2bin.npy, bin2node.npy` mentioned above.\n5. To generate the denoised contact matrix, run `python denoise_contact.py` There will be output figures named as `chr1_origin.png` and `chr1_denoise.png`, etc... produced in the root dir. There will also be an mcool file named as `denoised.mcool` in the root dir, which contains the denoised intra-chromosomal contact matrix at the given resolution.\n\n6. To predict the probabilities of forming multi-way chromatin interactions for a custom list of genome coordinate, run `python predict_multiway.py -i INPUT_FILE -o OUTPUT_FILE`. The `INPUT_FILE` should be a text file where each line is a tab separated list of genome coordinates. For example:\n```text\nchr1:1000000<tab>chr2:20000<tab>chr3:40000\nchr1:1000000<tab>chr2:20000<tab>chr3:40000<tab>chr1:12345\n```\nThe output file will be a list of the probability scores stored in the `OUTPUT_FILE`\n\n## Cite\n\nIf you want to cite our paper\n\n```\n@article{zhang2020matcha,\n  title={MATCHA: Probing Multi-way Chromatin Interaction with Hypergraph Representation Learning},\n  author={Zhang, Ruochi and Ma, Jian},\n  journal={Cell Systems},\n  volume={10},\n  number={5},\n  pages={397--407},\n  year={2020},\n  publisher={Elsevier}\n}\n```\n",
    "readme_length": 5290
  },
  {
    "name": "genomedisco",
    "full_name": "kundajelab/genomedisco",
    "description": "Software for comparing contact maps from HiC, CaptureC and other 3D genome data.",
    "stars": 26,
    "forks": 9,
    "language": "Jupyter Notebook",
    "url": "https://github.com/kundajelab/genomedisco",
    "topics": [
      "3c",
      "3d-genome",
      "bioinformatics",
      "bioinformatics-pipeline",
      "capture-c",
      "contact-matrix",
      "hi-c",
      "hic"
    ],
    "created_at": "2017-02-05T05:31:08Z",
    "updated_at": "2025-11-17T07:39:10Z",
    "homepage": "",
    "license": "N/A",
    "readme": "# GenomeDISCO\n\n\n\n`GenomeDISCO` (**DI**fferences between **S**moothed **CO**ntact maps) is a package for comparing contact maps of 3D genome structures, obtained from experiments such as Hi-C, Capture-C, ChIA-PET, HiChip, etc. It uses random walks on the contact map graph for smoothing before comparing the contact maps, resulting in a concordance score that can be used for quality control of biological replicates.\n\nRead the full paper here: \n*GenomeDISCO: A concordance score for chromosome conformation capture experiments using random walks on contact map graphs.* Oana Ursu, Nathan Boley, Maryna Taranova, Y. X. Rachel Wang, Galip Gurkan Yardimci, William Stafford Noble, Anshul Kundaje. Bioinformatics, 2018. https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/bty164/4938489?redirectedFrom=fulltext\n\n\nInstallation\n===\n\n1. Install [Anaconda](https://www.continuum.io/downloads). GenomeDISCO is compatible with Python 2.\n2. Obtain and install GenomeDISCO with the following commands:\n```\ngit clone http://github.com/kundajelab/genomedisco\npip install --editable genomedisco/\n```\n\nQuick start\n====\n\nSay you want to compare 2 contact maps. For this example, we will use a subset of datasets from Rao et al., 2014. \n\nFirst, configure the files used in the example: (this will create all input files necessary for the example on which we will run GenomeDISCO)\n\n```\ngenomedisco/examples/configure_example.sh\n```\n\nThen run the concordance analysis:\n\n```\ncd genomedisco\ngenomedisco run_all --metadata_samples examples/metadata.samples --metadata_pairs examples/metadata.pairs --bins examples/Bins.w50000.bed.gz --outdir examples/output \n```\n\nFor detailed explanations of all inputs to GenomeDISCO, see the [\"Inputs\" section below](#inputs)\n\nTo run reproducibility analysis in batches (more than one comparison), all you need to do is modify the `--metadata_samples` and `--metadata_pairs` to add the additional samples and sample pairs respectively that you wish to compare. For details, see [\"Analyzing multiple dataset pairs\"](#analyzing-multiple-dataset-pairs)\n\nRunning other methods for measuring concordance and QC of Hi-C data\n====\n\nTo run other available methods for computing the reproducibility of Hi-C data, refer to the repository http://github.com/kundajelab/3DChromatin_ReplicateQC and follow the instructions there.\n\nThe reproducibility methods supported in 3DChromatin_ReplicateQC are:\n- GenomeDISCO (http://github.com/kundajelab/genomedisco)\n- HiCRep (http://github.com/qunhualilab/hicrep) \n- HiC-Spector (http://github.com/gersteinlab/HiC-spector) \n- QuASAR-Rep (part of the hifive suite at http://github.com/bxlab/hifive) \n\nNote: given that both GenomeDISCO and 3DChromatin_ReplicateQC use the same underlying base code, they share the parameter options below, resulting in shared README sections for these.\n\nInputs\n=============\n\nBefore running GenomeDISCO, make sure to have the following files:\n\n- **contact map** For each of your samples, you need a file containing the counts assigned to each pair of bins in your contact map, and should have the format `chr1 bin1 chr2 bin2 value`. Note: GenomeDISCO assumes that this file contains the contacts for all chromosomes, and will split it into individual files for each chromosome.\n\n- **bins** This file contains the full set of genomic regions associated with your contact maps, in the format `chr start end name` where name is the name of the bin as used in the contact map files above. GenomeDISCO supports both fixed-size bins and variable-sized bins (e.g. obtained by partitioning the genome into restriction fragments). \n\nGenomeDISCO takes the following inputs:\n\n- `--metadata_samples` Information about the samples being compared. Tab-delimited file, with columns \"samplename\", \"samplefile\". Note: each samplename should be unique. Each samplefile listed here should follow the format \"chr1 bin1 chr2 bin2 value\n\n- `--metadata_pairs` Each row is a pair of sample names to be compared, in the format \"samplename1 samplename2\". Important: sample names used here need to correspond to the first column of the --metadata_samples file.\n\n- `--bins` A (gzipped) bed file of the all bins used in the analysis. It should have 4 columns: \"chr start end name\", where the name of the bin corresponds to the bins used in the contact maps.\n\n- `--re_fragments` Add this flag if the bins are not uniform bins in the genome (e.g. if they are restriction-fragment-based).By default, the code assumes the bins are of uniform length.\n\n- `--parameters_file` File with parameters for reproducibility and QC analysis. For details see [\"Parameters file\"](#parameters-file)\n\n- `--outdir` Name of output directory. DEFAULT: replicateQC\n\n- `--running_mode` The mode in which to run the analysis. This allows you to choose whether the analysis will be run as is, or submitted as a job through sge or slurm. Available options are: \"NA\" (default, no jobs are submitted). Coming soon: \"sge\", \"slurm\"\n\n- `--concise_analysis` Set this flag to obtain a concise analysis, which means replicateQC is measured but plots that might be more time/memory consuming are not created. This is useful for quick testing or running large-scale analyses on hundreds of comparisons.\n\n- `--subset_chromosomes` Comma-delimited list of chromosomes for which you want to run the analysis. By default the analysis runs on all chromosomes for which there are data. This is useful for quick testing\n\nAnalyzing multiple dataset pairs\n======\nTo analyze multiple pairs of contact maps, all you need to do is add any additional datasets you want to analyze to the `--metadata_samples` file and any additional pairs of datasets you want to compare to the `--metadata_pairs` files. \n\nParameters file\n======\n\nThe parameters file specifies the parameters to be used with GenomeDISCO (and any of the other methods GenomeDISCO supports). The format of the file is: `method_name parameter_name parameter_value`. The default parameters file used by GenomeDISCO is:\n\n```\nGenomeDISCO|subsampling\tlowest\nGenomeDISCO|tmin\t3\nGenomeDISCO|tmax\t3\nGenomeDISCO|norm\tsqrtvc\nGenomeDISCO|scoresByStep\tno\nGenomeDISCO|removeDiag\tyes\nGenomeDISCO|transition\tyes\nSGE|text\t\"-l h_vmem=3G\"\nslurm|text\t\"--mem 3G\"\n```\nNote: all of the above parameters need to be specified in the parameters file.\n\nHere are details about setting these parameters:\n\n- `GenomeDISCO|subsampling` This allows subsampling the datasets to a specific desired sequencing depth. Possible values are: `lowest` (subsample to the depth of the sample with the lower sequencing depth from the pair being compared), `<samplename>` where <samplename> is the name of the sample that is used to determine the sequencing depth to subsample from. \n\n- `GenomeDISCO|tmin` The minimum number of steps of random walk to perform. Integer, > 0.\n\n- `GenomeDISCO|tmax` The max number of steps of random walk to perform. Integer, > tmin.\n \n- `GenomeDISCO|norm` The normalization to use on the data when running GenomeDISCO. Possible values include: `uniform` (no normalization), `sqrtvc`.\n\n- `GenomeDISCO|scoresByStep` Whether to report the score at each t. By default (GenomeDISCO|scoresByStep no), only the final reproducibility score is returned.\n\n- `GenomeDISCO|removeDiag` Whether to set the diagonal to entries in the contact map to 0. By default (GenomeDISCO|removeDiag yes), the diagonal entries are set to 0.\n\n- `GenomeDISCO|transition` Whether to convert the normalized contact map to an appropriate transition matrix before running the random walks. By default (GenomeDISCO|transition yes) the normalized contact map is converted to a proper transition matrix, such that all rows sum to 1 exactly.\n\n- `SGE|text` Text to append to the job submission for SGE. The default is \"-l h_vmem=3G\".\n\n- `slurm|text` Text to append to the job submission for slurm. The default is \"--mem 3G\". \n\nRunning GenomeDISCO step by step\n============================================\nGenomeDISCO consists of multiple steps, which are run in sequence by default. However, the user may decide to run the steps individually, which can be useful for instance when running GenomeDISCO with job submission engines that runs the comparisons in parallel as separate jobs.\n\n**GenomeDISCO steps**\n\n**preprocess**\n\nPreprocesses all datasets provided in `--metadata_samples`.\n\nExample command: \n```\ngenomedisco preprocess --metadata_samples examples/metadata.samples --bins examples/Bins.w50000.bed.gz --outdir examples/output --parameters_file examples/example_parameters.txt\n```\n\n**concordance**\n\nRuns GenomeDISCO on all samples pairs provided in `--metadata_pairs`. \n\nExample command: \n```\ngenomedisco concordance --metadata_pairs examples/metadata.pairs --outdir examples/output \n```\n\n**summary**\n\nSummarizes scores across all comparisons.\n\nExample command: \n```\ngenomedisco summary --metadata_samples examples/metadata.samples --metadata_pairs examples/metadata.pairs --bins examples/Bins.w50000.bed.gz --outdir examples/output \n```\n\n**cleanup**\n\nClean up superfluous files, leaving only the scores.\n\nExample command: \n```\ngenomedisco cleanup --outdir examples/output\n```\n\nRunning GenomeDISCO with job submission engines\n============\n\nIt is possible to run GenomeDISCO with job submission engines, specifically either SGE or slurm.\nTo do so, modify the parameters `SGE|text` or `slurm|text` respectively, to add any additional parameters to the job run.\n\nThen, run the steps sequentially (that is, wait for all jobs of a given step to complete before launching the next step), while specifying `--running_mode` to either `sge` or `slurm`.\n\nFor instance, an example analysis workflow for SGE would be:\n```\ngenomedisco preprocess --running_mode sge --metadata_samples examples/metadata.samples --bins examples/Bins.w50000.bed.gz --outdir examples/output --parameters_file examples/example_parameters.txt\ngenomedisco concordance --running_mode sge --metadata_pairs examples/metadata.pairs --outdir examples/output \ngenomedisco summary --running_mode sge --metadata_samples examples/metadata.samples --metadata_pairs examples/metadata.pairs --bins examples/Bins.w50000.bed.gz --outdir examples/output \ngenomedisco cleanup --running_mode sge --outdir examples/output\n```\n\nSimilarly, for slurm, change sge to slurm for the `--running_mode`.\n\nMore questions?\n====\nSubmit an issue for this repository.\n\nThis code was put together by Oana Ursu (oursu@stanford.edu).\n\n\n\n\n\n\n",
    "readme_length": 10421
  },
  {
    "name": "dna-sculpture",
    "full_name": "PaulKlinger/dna-sculpture",
    "description": "3D printed sculpture of a DNA molecule, showing my own genome",
    "stars": 24,
    "forks": 0,
    "language": "Python",
    "url": "https://github.com/PaulKlinger/dna-sculpture",
    "topics": [
      "bioinformatics",
      "electronics",
      "raspberry-pi"
    ],
    "created_at": "2019-12-26T00:14:25Z",
    "updated_at": "2025-02-16T17:00:12Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# DNA Sculpture\n\n[![](video_link_image.jpg)](https://youtu.be/C1H_zHTX7Ds \"Project video\")\n\nThere are some in-progress photos of the build [here](https://imgur.com/a/wToFaz0).\n\n## DNA\nI had my DNA sequenced by [dantelabs.com](https://www.dantelabs.com/) (the 30X whole genome sequencing). They provide fastq, aligned bam, and vcf files, but just to understand some more about the process I redid the alignment with [bwa](https://github.com/lh3/bwa) and the variant calling with [bcftools](https://github.com/samtools/bcftools). (There are some issues with the called variants, see comments in dna.py, but that doesn't really matter for this project.) I've released all my genome files as [creative commons](https://creativecommons.org/publicdomain/zero/1.0/): [VCF](https://almoturg.com/paul_klinger_vcf_grch38.zip), [BAM](https://api.sequencing.com/download.ashx?id=27a5d439-af53-4887-940c-dd9895c1915e) (34 GiB!), [BAI](https://api.sequencing.com/download.ashx?id=ad2bf9d3-d068-4d9a-8b01-ec432784baf2).\n\n## 3D Printed Parts\nThe 3D printed parts were printed on my [Prusa MK3S](https://shop.prusa3d.com/en/3d-printers/180-original-prusa-i3-mk3-kit.html) 3d printer.\n\nThe helices need quite a lot of support material, so the surface finish wasn't great. I removed the worst of it with some 300 grit sandpaper and then used a heat gun to smooth the surface.\n\nThe two parts of the case are held together with three m2 screws that screw into heat-set metal inserts in the top plate. The PCBs are retained with m2 screws into metal inserts in the bottom plate. The helices are friction fit into the top part of the case and secured with an m2 screw that screws into the plastic.\n\nThe base pairs between the two helices are printed in transparent PLA and the holes are filled with short pieces of 4mm PMMA side-glow fiberoptic. I glued a bit of aluminium foil to the end of the fiberoptic (where the two pieces meet in the middle) so the colors of the complementary bases don't mix. The base pairs are just friction fit into the helices, and the LEDs are sort-of retained by the little tabs at the end of the bases, although most of them broke off during assembly.\n\n## Electronics & Software\nThe electronics are very simple, just a raspberry pi zero w, 18 WS2812B RGB LEDS in two strings of nine, a momentary button (for turning it on/off) and a diode (necessary for the button due to overlapping pin functions, see below).\nThe LEDS are connected with some 30 AWG wire, soldered directly to them. I assembled the LEDs already inside the two helices, adding one and then pulling the cables through to the next hole. That's absolutely horrible work, I'm pretty amazed it works at all. When putting the connecting transparent parts in they put pressure on the LEDs and it's quite likely that some of the connection fail and you have to fix it in place... \nTheoretically the WS2812B require an external capacitor (except for the new v5 version) but that would make assembly even more horrible. Thankfully it works fine without them.\nThe second PCB just contains a micro usb connector and a big capacitor, I used the [same PCB as for my satellite tracker](https://github.com/PaulKlinger/satellite_tracker/tree/master/PCBs/auxiliary).\n\nThe default I2C baudrate of the raspberry pi zero w is quite low, which limits the display to ~2 fps. Adding\n```\ndtparam=i2c_arm=on\ndtparam=i2c1=on\ndtparam=i2c1_baudrate=800000\n```\nto boot.config fixes this.\n\n### On/off button\nGetting both a single button for startup/shutdown and I2C (for the display) to work is a bit harder than it should be. When the RPi is shut down (\"halt\" state) it can only be woken up by connecting pin 3 (BCM numbering) to ground (or disconnecting and reconnecting power). But pin 3 is also the clock pin for the standard I2C port (i2c-1). Theoretically there's a second I2C, i2c-0 on pins 0/1 but I didn't manage to increase the baudrate for that. (Theoretically this should be possible using \"dtparam=i2c_vc=on\" to enable it and \"dtparam=i2c_vc_baudrate=800000\" to set the baudrate, but for me it stayed at the default (low) speed.)\n\nThe solution I found is by [Christian U. on stackexchange](https://raspberrypi.stackexchange.com/a/85316) (thanks!). He suggests connecting a diode from pin 3 to another GPIO (pin 4 in my case) and the button between pin 4 and ground. That way when the Pi is off it can be turned on by pushing the button (as current can flow from pin 3 to pin 4) but when the I2C peripheral pulls the clock line low this doesn't affect pin 4. When the button is pressed pin 4 is pulled low which triggers the shutdown (I'm using the gpiozero library for button handling). Pushing the button also pulls pin 3 low, so this messes up the I2C interface, but as the device is shutting down anyway this doesn't matter.\n\nTo make sure that the Pi wakes up when the button is pressed the forward voltage of the diode must be small enough. I tried a red LED but that didn't work, so I'm using an SS24 power diode now (the only other kind of diode I had at hand).\n\n\n## Code\nThe code here is not very nice, and it's pretty slow, mostly because of the display library. But it's just fast enough for ~10 updates per second, which is about the limit of what is nice to look at anyway.\n\nThe file dna.service is the systemd service configuration used to start main.py on startup (mostly as a syntax reminder to myself).\n",
    "readme_length": 5375
  },
  {
    "name": "GenomeFlow",
    "full_name": "jianlin-cheng/GenomeFlow",
    "description": "GenomeFlow: A Graphical Tool for Modeling and Analyzing 3D Genome Structure",
    "stars": 22,
    "forks": 5,
    "language": "Java",
    "url": "https://github.com/jianlin-cheng/GenomeFlow",
    "topics": [],
    "created_at": "2017-09-11T21:51:05Z",
    "updated_at": "2024-12-10T15:23:17Z",
    "homepage": "",
    "license": "N/A",
    "readme": "------------------------------------------------------------------------------------------------------------------------------------\n# GenomeFlow : A Graphical Tool for Modeling and Analyzing 3D Genome Structure \n------------------------------------------------------------------------------------------------------------------------------------\nA comprehensive graphical tool to facilitate the entire process of 3D genome organization, modeling and analysis of Hi-C data. \n\nGenomeFlow is capable of creating index for reference genome, generating maps from raw fastq data files, and reconstructing two-dimensional (2D) and \nthree-Dimensional (3D) chromosome and genome models with a graphical user interface.\n\nIf you have further difficulties using GenomeFlow, please do not hesitate to contact us (chengji@missouri.edu) \n\n--------------------------------------------------------------------\t\t\n\n# Documentation\n\nPlease see [the wiki](https://github.com/jianlin-cheng/GenomeFlow/wiki) for an extensive documentation, or download the [user guide](https://github.com/jianlin-cheng/GenomeFlow/blob/master/user_guide/UserGuide_4.3.1.pdf)\n\n--------------------------------------------------------------------\t\n\n# Distribution\n\nIn this repository, we include the folowing:\n* **executable**: contains sample datasets.  [Download latest version of the GenomeFlow executable from here](https://github.com/jianlin-cheng/GenomeFlow/releases)\n* src: contains the GenomeFlow source codes\n* user_guide: contains user guide\n* lib: contains the external libraries used in source code\n\n--------------------------------------------------------------------\t\n# Hardware and Software Requirements\n\nGenomeFlow consists of three parts: the pipeline that takes raw fastq files to create a formatted text files called the [1D-Functions](https://github.com/jianlin-cheng/GenomeFlow/wiki/1D-Functions), the analysis of binned Hi-C file called [2D-Functions](https://github.com/jianlin-cheng/GenomeFlow/wiki/2D-Functions), and three-dimensional (3D) models analysis called [3D-Functions](https://github.com/jianlin-cheng/GenomeFlow/wiki/3D-Functions) tools.  \n\n## Requirements\nGenomeFlow requires the use of a computer, with ideally >= 4 cores (min 1 core) and >= 4 GB RAM (min 2 GB RAM)\n\n\n## GenomeFlow tools requirements\nThe minimum software requirement to run GenomeFlow is a working Java installation (version >= 1.7) on Windows, Linux, and Mac OSX. We recommend using the latest Java version available, but please do not use the Java Beta Version. Minimum system requirements for running Java can be found at https://java.com/en/download/help/sysreq.xml\n\nTo download and install the latest Java Runtime Environment (JRE), please go to https://www.java.com/download\n\n\n## Burrows-Wheeler Aligner (BWA) for 1D-Function\nThe latest version of BWA should be installed from http://bio-bwa.sourceforge.net/. \n\n\n## Bowtie2 for 1D-Function \n\nThe latest version of Bowtie2 should be installed from  http://bowtie-bio.sourceforge.net/index.shtml\n\n## Samtools for 1D-Function\t\n\nThe latest version of Bowtie2 should be installed Samtools (http://samtools.sourceforge.net/)\n\n\nInstallation Instructions for `BWA`, `Bowtie2`, and `Samtools` can be found [here](https://github.com/jianlin-cheng/GenomeFlow/wiki/Installation)\n\n--------------------------------------------------------------------\t\n\n# Building from source code\nThe project uses JUnit for testing and Maven to manage dependencies and builds. It can be imported into Eclipse as a Maven project.\n\n## Requirements\n* Maven: manage builds and dependencies. <br>\nIf using Eclipse, there is no need to install Maven. Otherwise, Maven needs to be installed to compile the code. \n\n<br>\n\n* Example commands <br>\n`mvn compile`: to compile <br>\n`mvn test`: to run test <br>\n`mvn package`: to compile, test and package <br>\n\n\n## Compiling source code\nRequires Java JDK 1.8 (not JRE but JDK, set JAVA_HOME to JDK folder)\n\n\n--------------------------------------------------------------------\t\n",
    "readme_length": 4001
  },
  {
    "name": "rambutan",
    "full_name": "jmschrei/rambutan",
    "description": "Prediction of the 3D structure of the genome through statistically significant Hi-C contacts.",
    "stars": 21,
    "forks": 5,
    "language": "Jupyter Notebook",
    "url": "https://github.com/jmschrei/rambutan",
    "topics": [],
    "created_at": "2015-10-16T17:43:19Z",
    "updated_at": "2020-10-23T16:20:48Z",
    "homepage": "https://rambutan-py.readthedocs.io/en/latest/",
    "license": "MIT License",
    "readme": "# Rambutan\n\n[![Travis-CI](https://travis-ci.org/jmschrei/rambutan.svg?branch=master)](https://travis-ci.org/jmschrei/rambutan) [![Documentation Status](https://readthedocs.org/projects/rambutan-py/badge/?version=latest)](http://rambutan-py.readthedocs.io/en/latest/?badge=latest)\n\nRambutan is a deep convolutional neural network which predicts 3D chromatin architecture using only nucleotide sequence and DNaseI sensitivity. Specifically it predicts whether a pair of 1kb loci engage in a statistically significant contact with respect to the genomic distance effect as defined by Fit-Hi-C. If you've previously used Fit-Hi-C to identify relevant contacts in experimentally acquired Hi-C contact maps, you can now use Rambutan to do that for human cell types which don't have Hi-C data! Rambutan is trained off the deeply sequenced GM12878 experiment from the Rao 2014 paper and so can make predictions at 1kb resolution, far higher than most experimentally acquired contact maps.  \n\nRead the manuscript here! <a href=\"https://www.biorxiv.org/content/early/2018/07/15/103614\">Nucleotide sequence and DNaseI sensitivity are predictive of 3D chromatin architecture</a>\n\n**NOTE: After our original submission we discovered an error in our calling of statistically significant contacts. Briefly, when calculating the prior probability of a contact, we used the number of contacts at a certain genomic distance in a chromosome but divided by the total number of bins in the full genome. When we corrected this mistake we noticed that the Rambutan model, as it curently stands, did not outperform simply using the GM12878 contact map that Rambutan was trained on as the predictor in other cell types. While we investigate these new results, we ask that readers treat this manuscript skeptically.**\n\nThe code used to recreate most figures in the paper can be found in Biological_Validation.ipynb. \n\n## Dependencies\n\nRambutan is written to be used in Python 2.7, but should work for Python 3 as well. Please open an issue on the issue tracker if this is not the case.\n\nRambutan requires sklearn, joblib, numpy, progressbar, mxnet, and cython. Of these dependencies, the first four can easily be installed using pip. The last two may be more tricky to get installed due to their efficiency needs. In particular, mxnet is a deep learning package and so requires cuda and cudnn for installation. Please see the <a href=\"http://mxnet.io/get_started/setup.html\">mxnet installation guide</a> for instructions on how to install mxnet. Cython requires a working C++ compiler, which should not be a problem if you are on Ubuntu or a mac (gcc and clang both work well). If you are on a Windows machine you will have to download one. For Python 2 this minimal version of Visual Studio 2008 works well: https://www.microsoft.com/en-us/download/details.aspx?id=44266. For Python 3 this version of the Visual Studio Build Tools has been reported to work: http://go.microsoft.com/fwlink/?LinkId=691126. \n\nUsing the Anaconda distribution may help in the in installation of these dependencies.\n\n## Installation\n\nRambutan can be installed once all of the dependencies are successfully installed. Currently you can install Rambutan by cloning this repo and installing from source using the following commands:\n\n```\ngit clone https://github/com/jmschrei/rambutan\ncd rambutan\npython setup.py install\n```\n\n## Usage\n\nRambutan comes with parameters from a model which has been pre-trained on 12.8 million samples from GM12878 chromosomes 1 through 20. This is the model which is used for all tasks in the manuscript. Making predictions is as simple as calling the predict function:\n\n```\nfrom rambutan import Rambutan\nmodel = Rambutan(\"path/to/model/file/\", iteration=25)\ny_pred = model.predict(\"chr21.fa\", \"chr21.GM12878.dnase.bedgraph\", ctxs=[0, 1, 2, 3])\n```\n\nThe predict function takes in a filename for a FastA file and a filename for a bedgraph file containing fold change DNaseI values. The context parameter defines which GPUs to use in prediction. The prediction task is parallelized in a manner such that there is a linear speedup with the number of contexts. The resulting matrix of predictions will be sparse, only filling in the upper triangle between the band of 50kb to 1Mb. `min_dist` and `max_dist` can be passed in to the Rambutan object initialization to consider a different band.\n",
    "readme_length": 4368
  },
  {
    "name": "scGHOST",
    "full_name": "ma-compbio/scGHOST",
    "description": "single-cell Hi-C, scHi-C, Hi-C, 3D genome, nuclear organization, genome subcompartment",
    "stars": 21,
    "forks": 2,
    "language": "Jupyter Notebook",
    "url": "https://github.com/ma-compbio/scGHOST",
    "topics": [
      "3d-genome",
      "machine-learning",
      "single-cell"
    ],
    "created_at": "2022-01-14T00:03:03Z",
    "updated_at": "2025-09-22T08:25:28Z",
    "homepage": "",
    "license": "MIT License",
    "readme": "# Overview of scGHOST\r\n\r\n![Overview of scGHOST](scghost_overview.png)\r\n\r\nscGHOST is an unsupervised single-cell subcompartment annotation method based on graph embedding with constrained random walk sampling.\r\nscGHOST is designed to be run on a single-cell Hi-C (scHi-C) dataset which has undergone imputation by [Higashi](https://github.com/ma-compbio/Higashi) ([Zhang et al. 2022](https://www.nature.com/articles/s41587-021-01034-y)).\r\nscGHOST assigns embeddings to genomic loci in the genomes of individual cells by viewing scHi-C as graphs whose vertices are genomic loci and edges are the contact frequencies among loci.\r\nWhile scGHOST is developed for scHi-C data, it can also identify single-cell subcompartments in single-cell genome imaging data.\r\n\r\n# Running scGHOST\r\n\r\n## Input data\r\n\r\nscGHOST uses the outputs from [Higashi](https://github.com/ma-compbio/Higashi) as its inputs.\r\nSpecifically, it requires the scHi-C imputations (hdf5 format), per-cell embeddings (numpy format), sparse raw scHi-C adjacency maps (numpy format), the scA/B scores (hdf5 format), and the label info file (pickle format) describing the cell types corresponding to each cell in the dataset.\r\n\r\n## Installation\r\n\r\nBefore installing any Python packages, we strongly recommend using Anaconda (please refer to the [Anaconda](https://anaconda.org/) webpage for `conda` installation instructions) to create a python 3.10 environment using the following command:\r\n\r\n`conda install --name scghost python=3.10`\r\n\r\nAfter creating the environment, activate it using:\r\n\r\n`conda activate scghost`\r\n\r\n### Dependencies\r\n\r\n#### Conda installations\r\n- PyTorch (2.1.0) with CUDA (11.8)\r\n- Scikit-learn (latest)\r\n- h5py\r\n#### Pip installations\r\n- [cuML for CUDA 11.8](https://docs.rapids.ai/install#selector)\r\n- [Thread-pool Controls](https://pypi.org/project/threadpoolctl/) (> 3)\r\n\r\nUsers can install scGHOST dependencies using the `conda` or `pip` commands following the specifications above.\r\n\r\nSystems without a CUDA-capable GPU can also install scGHOST using the same dependencies and installing PyTorch for CPU only, but will have to modify the source code in `modules/clustering.py` to use `SKMeans` instead of `KMeans` under the `scghost_clustering_reworked` function. We may add a flag in the config file to run CPU only instead, but from our experience running scGHOST on the CPU only takes far longer than on a GPU and is not recommended.\r\n\r\n## Hardware Requirements\r\n\r\nscGHOST can use up to 40 GB of memory for a single-cell dataset of 4,238 cells.\r\nConsidering operating system overhead, we recommend running scGHOST on a machine with at least 64 GB of memory to avoid poor performance or out-of-memory errors at runtime.\r\n\r\nscGHOST was developed on a system with a 12-core 12th generation Intel CPU, an Nvidia RTX 3090 GPU with 24GB of VRAM, and 64GB of system memory. With GPU caching enabled, scGHOST uses a maximum of 15 GB of VRAM on the PFC dataset. With GPU caching disabled, VRAM becomes less of a limiting factor and scGHOST should run on any CUDA-capable GPU with at least 4 GB of VRAM.\r\n\r\n## Usage\r\n\r\nUsers can run scGHOST using the following command:\r\n\r\n`python scghost.py --config <configuration.json>`\r\n\r\nSample JSON config files for scGHOST have been provided.\r\n\r\n`configuration` is the filepath to a custom configuration file adhering to the JSON format for scGHOST. By default, scGHOST uses the included config.json file, which can be modified to the user's specifications.\r\n\r\n**Note**: users may run into a `RuntimeWarning` after the clustering step. This is normal behavior and should not affect the accuracy of results.\r\n\r\n## Runtime\r\nscGHOST was run on a machine with a 12-core 12th generation Intel CPU and Nvidia RTX 3090 24GB GPU.\r\nFrom scratch, scGHOST takes about 2 hours to run on the sciHi-C GM12878 dataset and about 4 hours to run on the human prefrontal cortex dataset.\r\n\r\n## Configuration file\r\n\r\n- `schic_directory` : the directory containing Higashi-imputed single-cell Hi-C maps.\r\n- `label_info` : `label_info.pickle` file following the [format in Higashi](https://github.com/ma-compbio/Higashi/wiki/Input-Files).\r\n  - `path` : the file path of the `label_info.pickle` file\r\n  - `cell_type_key` : the key in `label_info.pickle` with a list of the cell types in the dataset\r\n- `data_directory` : the output directory of scGHOST\r\n- `chromosomes` : the list of chromosomes to apply scGHOST to. default: autosomes\r\n- `chrom_sizes` : file path to the chromosome sizes file. default: `data/hg38.chrom.sizes`\r\n- `chrom_indices` : file path to chrom indices if previously computed. Development flag to save time over multiple runs on the same dataset. Default: `null`\r\n- `embeddings_path` : file path to the Higashi embeddings `.npy` file for each cell in the dataset\r\n- `higashi_scab_path` : file path to Higashi scA/B scores `.h5` file\r\n- `cell_type` : the cell type in the dataset to apply scGHOST on; use `null` to apply scGHOST to all cell types in the dataset. default: `null`\r\n- `random_walk` : random walk parameters\r\n  - `num_walks` : number of random walks per iteration. default: 50\r\n  - `ignore_top` : the top and bottom percentile to be ignored, to remove extreme values in the input matrix. default: 0.02\r\n  - `top_percentile` : the top percentiles within which random walks are performed. default: 0.25\r\n- `eps` : small float value to prevent dividing by zero in some functions. default: 1e-8\r\n- `num_clusters` : number of clusters to partition chromosomes into\r\n- `neighbor_contacts` : determine whether to use the average of nearest neighbor contacts as the target label during node embedding.\r\n- `nearest_neighbor_override` : use a custom numpy array to define nearest neighbors. The format should be an `N x (k+1)` array with `N` denoting the number of cells in the dataset and `k` denoting the number of nearest neighbors. Row `i` in the array should contain entries denoting which cells are the nearest neighbors of cell `i`.\r\n- `cluster_gpu_caching` : toggle caching chromosome embeddings on the GPU prior to clustering to reduce CPU overhead converting embedding vectors to cuda variables. We recomend disabling this if your GPU memory is less than 16 GB.\r\n- `gpu_uniques` : determine whether to use the GPU to compute unique random walk samples. On machines with higher CPU core counts, CPU processing may be faster than GPU processing.\r\n- `kmeans_init` : the `n_init` parameter in scikit-learn/cuML's `KMeans`. We set this value at a default of 1 to reduce clustering runtime.\r\n\r\n## Tutorials\r\n\r\nPlease follow our tutorial notebooks in the root directory for examples on how to run scGHOST with and without first running Higashi. For a sample run of scGHOST, users can download the smaller WTC-11 dataset [here](http://genome.compbio.cs.cmu.edu:8008/~kxiong/data/scghost/wtc11/). After downloading the sample data, please change the `sample_configs/config_wtc.json` configuration file accordingly to point to the correct paths and run the following command:\r\n\r\n`python scghost.py --config sample_configs/config_wtc.json`\r\n\r\n## Contact\r\nPlease email jianma@cs.cmu.edu or raise an issue in the github repository with any questions about installation or usage or any encountered bugs.\r\n",
    "readme_length": 7221
  },
  {
    "name": "TADbit",
    "full_name": "3DGenomes/TADbit",
    "description": "TADbit is a complete Python library to deal with all steps to analyze, model and explore 3C-based data. With TADbit the user can map FASTQ files to obtain raw interaction binned matrices (Hi-C like matrices), normalize and correct interaction matrices, identify and compare the so-called Topologically Associating Domains (TADs), build 3D models from the interaction matrices, and finally, extract structural properties from the models. TADbit is complemented by TADkit for visualizing 3D models",
    "stars": 107,
    "forks": 62,
    "language": "Python",
    "url": "https://github.com/3DGenomes/TADbit",
    "topics": [
      "3d-models",
      "chromatin",
      "hi-c",
      "mapping",
      "ngs",
      "python"
    ],
    "created_at": "2013-08-29T12:31:54Z",
    "updated_at": "2025-11-13T14:08:38Z",
    "homepage": "",
    "license": "GNU General Public License v3.0",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "NeoLoopFinder",
    "full_name": "XiaoTaoWang/NeoLoopFinder",
    "description": "A computation framework for genome-wide detection of enhancer-hijacking events from chromatin interaction data in re-arranged genomes",
    "stars": 75,
    "forks": 17,
    "language": "Python",
    "url": "https://github.com/XiaoTaoWang/NeoLoopFinder",
    "topics": [
      "enhancer-hijacking",
      "hi-c",
      "structural-variation"
    ],
    "created_at": "2021-02-09T21:43:44Z",
    "updated_at": "2025-11-14T02:34:34Z",
    "homepage": "",
    "license": "Other",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "HiCLift",
    "full_name": "XiaoTaoWang/HiCLift",
    "description": "A fast and efficient tool for converting chromatin interaction data between genome assemblies",
    "stars": 75,
    "forks": 5,
    "language": "Python",
    "url": "https://github.com/XiaoTaoWang/HiCLift",
    "topics": [
      "3d-genome",
      "hi-c",
      "liftover"
    ],
    "created_at": "2021-01-23T04:43:26Z",
    "updated_at": "2025-09-26T03:33:13Z",
    "homepage": "",
    "license": "Other",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "TADLib",
    "full_name": "XiaoTaoWang/TADLib",
    "description": " A Library to Explore Chromatin Interaction Patterns for Topologically Associating Domains",
    "stars": 44,
    "forks": 11,
    "language": "Python",
    "url": "https://github.com/XiaoTaoWang/TADLib",
    "topics": [
      "bioinformatics",
      "chromatin",
      "contact-matrix",
      "cooler",
      "genomics",
      "hi-c",
      "python",
      "tad",
      "tads"
    ],
    "created_at": "2014-11-25T12:24:49Z",
    "updated_at": "2025-07-02T04:43:27Z",
    "homepage": "",
    "license": "GNU General Public License v3.0",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "deepC",
    "full_name": "Hughes-Genome-Group/deepC",
    "description": "A neural network framework for predicting the Hi-C chromatin interactions from megabase scale DNA sequence",
    "stars": 33,
    "forks": 14,
    "language": "HTML",
    "url": "https://github.com/Hughes-Genome-Group/deepC",
    "topics": [],
    "created_at": "2019-08-02T08:30:33Z",
    "updated_at": "2024-06-12T05:19:13Z",
    "homepage": "",
    "license": "GNU General Public License v3.0",
    "readme": "<img src=\"docs/logo_1_transparent.png\" width=\"150\">\n\n# deepC\nA Tensorflow DL framework for predicting Hi-C chromatin interactions using megabase scale DNA sequence.\n\n-------------------------------------------------------------------------------\n\n### Description\n\nThis repository contains the core deepC python code, R scripts and functions for downstream analysis as well as tutorials and links to example data.\n\nThe core code is implemented in python (v3.5+) and tensorflow (v1). For downstream analysis and visualizations we use R and custom functions for handling HiC data\nand deepC predictions.\n\n### Requirements\n\n  * python 3.5 +\n  * tensorflow (tensorflow-gpu)\n    * GPU support is preferable for predictions and essential for training\n  * additional python modules:\n    * numpy (v1.16.4 or above)\n    * pysam (tested with v0.15.2)\n    * pybedtools and a compatible version of bedtools installed\n\n  * R version 3.4.4 +\n    * packages:\n      * tidyverse (v1.2.1 or above)\n      * RColorBrewer (v1.1-2 or above)\n      * cowplot (v0.9.2 or above)\n      * for plotting 1D tracks (e.g. DNase, ChIP-seq) rtracklayer rtracklayer (v1.38.3 or above) and dependencies are required\n    * Rstudio (not required but recommended)\n\n  * some processing helper scripts require perl (v5.26.0 or above)\n\n### Installation\n\n* Make sure python 3.5-3.7 as supported by tensorflow is installed.\n\n* Install [tensorflow](https://www.tensorflow.org/install) preferably with [GPU support](https://www.tensorflow.org/install/gpu).\n  * We recommend tensorflow 2.1 but deepC was developed under v1.8 and supports (v1.8, 1.14 and 2.1 other versions have not been tested).\n  * The tensorflow docker containers are the easiest way to set up tensorflow with GPU and come with the correct CUDA and cuDNN versions packaged.\n  * If installing CUDA, cuDNN and tensorflow separately make sure to follow the [compatibility advice](https://www.tensorflow.org/install/source#linux)\n  * To install an older version e.g. tensorflow 1 follow [this route](https://www.tensorflow.org/install/pip)\n\n* Install additional python library (pysam and pybedtools) using e.g. pip or bioconda\n  * `pip install pybedtools`\n  * `pip install pysam`\n\n* Clone the **deepC** github repository\n* Check which version of tensorflow you have installed and choose the appropriate compatibility version of deepC\n\n| tensorflow version |  CUDA version | deepC version  |\n| ------------------ |:-------------:| --------------:|\n| 2.1+               | 10.1          | [tensorflow2.1plus_compatibility_version](./tensorflow2.1plus_compatibility_version) |\n| 2.0               | 10          | [tensorflow2.0_compatibility_version](./tensorflow2.0_compatibility_version)* |\n| 1.14               | 10          | [tensorflow1_version](./tensorflow1_version) |\n| 1.8               | 9          | [legacy_version_tf1.8](./legacy_version_tf1.8) |\n\n*Compatibility with v2.0 not yet tested.\n\n### Required Resources\n\n  * Training deepC models requires running with GPU support for several hours (up to days depending on the dataset and epochs aimed for)\n  * Running predictions is feasible without but runs significantly faster with a GPU\n  * For example predicting the impact of a variant as in the tutorial provided requires ~ 5 mins with GPU support and ~ 2h on CPU.\n\n### Installation\n\nClone the repository. Make sure all dependencies are available.\nTo use from within a python script import as `import deepCregr`.\n\n### Tutorials\n\nFind tutorials [here](./tutorials).\n\n### Trained Models\n\nDownload links to trained models are provided under `./models`. See the README\nfile there for details.\n\n### Publication\n\nPlease refer to the Nature Methods article [here](https://www.nature.com/articles/s41592-020-0960-3)\n\n### Acknowledgements\n\nImplementation of dilated convolutions was adapted from [wavenet](https://github.com/ibab/tensorflow-wavenet).\n",
    "readme_length": 3870
  },
  {
    "name": "GraphReg",
    "full_name": "karbalayghareh/GraphReg",
    "description": "Chromatin interaction aware gene regulatory modeling with graph attention networks",
    "stars": 27,
    "forks": 5,
    "language": "Python",
    "url": "https://github.com/karbalayghareh/GraphReg",
    "topics": [],
    "created_at": "2021-08-10T21:25:58Z",
    "updated_at": "2025-07-31T15:29:50Z",
    "homepage": "https://genome.cshlp.org/content/32/5/930.short",
    "license": "N/A",
    "readme": "# GraphReg\n\n<img\n  src=\"assets/distal_enhancers.png\"\n  alt=\"Alt text\"\n  title=\"\"\n  style=\"display: inline-block; margin: 0 auto; max-width: 300px\">\n\n**GraphReg** ([Chromatin interaction aware gene regulatory modeling with graph attention networks](https://genome.cshlp.org/content/32/5/930.short)) is a graph neural network based gene regulation model which integrates DNA sequence, 1D epigenomic data (such as chromatin accessibility and histone modifications), and 3D chromatin conformation data (such as Hi-C, HiChIP, Micro-C, HiCAR) to predict gene expression in an informative way. **GraphReg** is a versatile model which can be used to answer interesting questions in regulatory genomics such as:\n\n- How well we can predict expression of a gene by using the epigenomic features of its promoter and candidate enhancers and enhancer-promoter interactions? Can this model be used in unseen cell types to predict gene expression?\n\n- What are the cis regulatory elements of the genes in each cell type? Which candidate enhancers are functional and play a role in gene regulation?\n\n- Which transcription factor (TF) motifs are important for gene regulation? How do distal TF motifs regulate their target genes?\n\nThis repository contains all the codes for training **GraphReg** models and all the downstream analyses for gene expression prediction, enhancer validation, and discovering regulating TF motifs.\n\n## Data preparation\n\n### 1D data (epigenomic and CAGE)\nWe need a coverage file `bigwig` for each epigenomic track. We have used some useful functions from [Basenji](https://github.com/calico/basenji) for reading and writing the `bigwig` files, which can be found in [utils](https://github.com/karbalayghareh/GraphReg/tree/master/utils). \n\n We can use two different approaches to generate `bigwig` files from alignment `BAM` files:\n\n- [`bam_cov.py`](https://github.com/karbalayghareh/GraphReg/blob/master/utils/bam_cov.py) from Basenji. This works best when we want to work with each cell type individually. The coverage tracks from different cell types are not normalized by this method. In **Epi-GraphReg** if we are interested in cross-cell-type generalization, the coverage tracks should be normalized by other techniques such as DESeq, otherwise there would be batch effect between cell types due to sequencing depths, which would hurt the generalization performance. \n\n- [`bamCoverage`](https://deeptools.readthedocs.io/en/develop/content/tools/bamCoverage.html) from [deepTools](https://deeptools.readthedocs.io/en/develop/index.html). This is more suitable for cross-cell-type analyses, as they offer some normalization methods for `bigwig` files. In particular, we use 1x normalization or reads per genome coverage (RPGC), which normalizes the coverage in each bin by sequencing depth. We run `bamCoverage` with bin size 100 for epigenomic tracks and 5000 for CAGE-seq.\n\nAfter generating the `bigwig` files, we use [data_read.py](https://github.com/karbalayghareh/GraphReg/blob/master/utils/data_read.py) to read the `bigwig` files and save the coverage signals in `hdf5` format. We use `pool_width = 100` (to get the coverage in 100bp bins) for epigenomic tracks and `pool_width = 5000` (to get the coverage in 5Kb bins) for CAGE. The reason of using 5Kb bins for CAGE is that we use 5Kb resolution of 3D assays and want to have corresponding bins. If we use `bam_cov.py` to generate `bigwig` files, we set `sum_stat = 'sum'` to sum all the base-pair coverage in each bin; otherwise, if we use `bamCoverage` to generate `bigwig` files, we set `sum_stat = 'max'` as the coverage per bin has already been computed per bin. \n\n### 3D data (chromatin conformation: Hi-C/HiChIP/Micro-C/HiCAR)\nThe chromatin conformation `fastq` data from various 3D assays such as Hi-C, HiChIP, Micro-C, HiCAR could be aligned to any genome (using packages like [Juicer](https://github.com/aidenlab/juicer) or [HiC-Pro](https://github.com/nservant/HiC-Pro)) to get `.hic` files. **GraphReg** needs connecivity graphs for each chromosome. As these 3D data are very noisy, we need some statistical tools to get the significant interactions for the graphs, otherwise it would be very noisy. To this end, we use [HiCDCPlus](https://github.com/mervesa/HiCDCPlus) which gets the `.hic` files and returns the significance level (FDR) for each genomic interaction (of resolution 5Kb) based on a Negative Binomial model. We filter the interactions and keep the ones with `FDR <= alpha` to form the graphs and adjacency matrices. We have worked with three different values of `alpha = 0.1, 0.01, 0.001` and noticed that its ideal value depends on the 3D data. But, we recommend `alpha = 0.1` as a default and less stringent cutoff. \n\nThe outputs of HiCDCPlus is given to [hic_to_graph.py](https://github.com/karbalayghareh/GraphReg/blob/master/utils/hic_to_graph.py) to generate the adjacency matrices for each chromosome, which are saved as sparce matrices. \n\n### TSS bins and positions\nWe need to have a `BED` file for TSS annotations. This file could be extracted from any gene annotation `GTF` files for any genome build. We have used GENCODE annotations which can be found [here](https://www.gencodegenes.org/). The TSS annotation `BED` file is given to [find_tss.py](https://github.com/karbalayghareh/GraphReg/blob/master/utils/find_tss.py) to compute the number of TSS's in each 5Kb bin. `find_tss.py` saves four outputs as numpy files: start position of each bin, number of TSS's in each bin, and the gene names (if existent) and their TSS positions in each bin. With 5Kb bins, the majority of them would have one TSS. However, there is a chance that a bin has 2 or 3 TSS's, in which case we save the first TSS position and all the genes (in the format `gene_name_1+gene_name_2`), because we want to keep track of all the genes appearing in each bin. \n\n### Writing all data (1D, 3D, TSS) to TFRecords \n**GraphReg** has been implemented in TensorFlow. [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) is an efficient format to store and read data in TensorFlow. We use [data_write.py](https://github.com/karbalayghareh/GraphReg/blob/master/utils/data_write.py) to read (1) epigenomic coverage files (saved in `h5` format), (2) sparse adjacency matrices (saved in `npz` format), and (3) TSS files (saved in `np` format) and save them sequentially in TFRecords in each chromosome. We start from beginning of each chromosome and write the epigenomic and CAGE coverages, adjacency matrices, and TSS annotations for the regions of 6Mb. Then we sweep the entire chromosome by steps of 2Mb. This way, there is no overlap for the middle 2Mb regions where we predict gene expression values. For each batch of 6Mb, the dimensions of data would be: `60,000` for each epigenomic track, `1200` for CAGE, and `1200 x 1200` for adjacency matrices. The predicted CAGE values in the middle `400` bins would appear in the loss function so that all the genes could see their distal enhancers up to 2Mb up- and downstream of their TSS. \n\nThe TFRecord files are slightly different for **Epi-GraphReg** and **Seq-GraphReg** models: (1) TFRecords for **Seq-GraphReg** also contain one-hot-coded DNA sequences of the size `6,000,000 x 4`, as the DNA sequence is an input for these models, (2) The epigenomic signals for **Epi-GraphReg** undergo an extra log-normalization, via function `log2(x+1)`, to reduce their dynamic ranges, as they are inputs in  **Epi-GraphReg** models.\n\nNow that we have generated TFRecord files, we are ready to train the models.\n\n## Training GraphReg models\n\n### Epi-GraphReg\n\nUse [`Epi-GraphReg.py`](https://github.com/karbalayghareh/GraphReg/blob/master/train/Epi-GraphReg.py) to train the **Epi-GraphReg** models. You should specify the validation and test chromosomes. The remaining autosomal chromosomes are used for training. For example:\n```\npython Epi-GraphReg.py -c K562 -p $data_path -a HiChIP -g 1 -q 0.1 -v 3,13 -t 4,14\n```\ntrains **Epi-GraphReg** on cell line K562, using graphs extracted from HiChIP with FDR (q-value) cutoff 0.1, in generalizable mode `-g 1`, with Chrs 3 and 13 as the  validation and Chrs 4 and 14 as the test chromosomes. `$data_path` is the directory where TFRecords have been stored. Training on generalizable mode means that the model uses the normalized epigenomic coverage tracks (for example the ones obtained from RPGC normalization) so that the trained model can be used in other cell types as well.\n\n### Seq-GraphReg\n\nThe **Seq-GraphReg** models can be trained in two ways: (1) end-to-end, (2) separate. End-to-end training means that both epigenomic and CAGE data are predicted in a multi-task learning fashion. However, separate training means that we train two tasks (epigenomic and GACE) separately: we first use CNN layers to predict the epigenomic tracks from DNA sequence (similar to Basenji) and then feed the bottleneck representations to the graph attention layers to predict the CAGE values. So, which one should you use? It depends on the amount of GPU memory the users have access to. End-to-end training requires high GPU memory as it needs to load the entire 6Mb DNA sequence to the GPU memory. One advantage of an end-to-end model is the ability to do gradient-based feature attribution from output of any gene back to the base pair level. However, if a high GPU memory is not available, we can employ separate training, where we can use smaller genomic regions of length 100Kb (instead of 6Mb) to predict the epigenomic data from DNA sequences as these are local features and no graph is used for this task. Then after predicting the entire 6Mb (60 mini batches of 100Kb), we concatenate their corresponding bottleneck representations (with the size `60,000 x 64`, where 64 is the dimension of bottleneck representations) and feed that to the graph attention layers along with the corresponding graph of 6Mb region. \n\n#### End-to-end training\n\nUse [Seq-GraphReg_e2e.py](https://github.com/karbalayghareh/GraphReg/blob/master/train/Seq-GraphReg_e2e.py) to train end-to-end **Seq-GraphReg** models. You should specify the validation and test chromosomes. The remaining autosomal chromosomes are used for training. For example:\n```\npython Seq-GraphReg_e2e.py -c K562 -p $data_path -a HiChIP -q 0.1 -v 3,13 -t 4,14\n```\ntrains end-to-end **Seq-GraphReg** on cell line K562, using graphs extracted from HiChIP with FDR (q-value) cutoff 0.1 with Chrs 3 and 13 as the validation and Chrs 4 and 14 as the test chromosomes.\n\n#### Separate training\n\n1- Use [Seq-CNN_base.py](https://github.com/karbalayghareh/GraphReg/blob/master/train/Seq-CNN_base.py) to first train a CNN model to predict the epigenomic data from DNA sequence. For example:\n```\npython Seq-CNN_base.py -c K562 -p $data_path -v 3,13 -t 4,14\n```\ntrains a CNN model on cell line K562 with Chrs 3 and 13 as the validation and Chrs 4 and 14 as the test chromosomes. We have implemented four flavors of such epigenomic CNN models: with/without dilation layers, and with/without FFT (Fast Fourier Transform) loss (so overall four models), which can be found [here](https://github.com/karbalayghareh/GraphReg/tree/master/train). The idea of adding FFT attribution prior is borrowed from [here](https://proceedings.neurips.cc/paper/2020/file/1487987e862c44b91a0296cf3866387e-Paper.pdf) to improve the interpretability of the deep learning models using DNA sequences.\n\n2- Use [Seq-GraphReg.py](https://github.com/karbalayghareh/GraphReg/blob/master/train/Seq-GraphReg.py) to train the graph attention networks of **Seq-GraphReg** to predict the CAGE values. For example:\n```\npython Seq-GraphReg.py -c K562 -p $data_path -a HiChIP -q 0.1 -f 1 -d 0 -v 3,13 -t 4,14\n```\nfirst loads the epigenomic CNNs trained in step (1) on cell line K562 without dilation layers `-d 0` and with FFT loss `-f 1`. Then it uses the bottleneck representation as the input for the second task: predicting CAGE using the graphs extracted from HiChIP with FDR cutoff 0.1. Note that the validation (Chrs 3 and 13) and test (Chr 4 and 14) chromosomes are the same as step (1).\n\n## Testing GraphReg models\n\nAfter training the **GraphReg** models, it is time to use them to predict gene expression (CAGE) in held-out test chromosomes or cell types. All the scripts for saving the predictions and plotting the results are in [test](https://github.com/karbalayghareh/GraphReg/tree/master/test). We first run the prediction scripts (explained below) to save the CAGE predictions and all meta data (such as number of enhancer-promoter interactions, name of the genes, TSS positions, etc.) for the TSS bins in the test chromosomes in a `csv` file. Then we run the plotting scripts (which call the saved `csv` files) to plot the results. \n\n- For **Epi-GraphReg**, run [Epi-GraphReg_test_multiple_runs.py](https://github.com/karbalayghareh/GraphReg/blob/master/test/Epi-GraphReg_test_multiple_runs.py) to save predictions in the test chromosomes, and run [plot_results_epi_models.py](https://github.com/karbalayghareh/GraphReg/blob/master/test/plot_results_epi_models.py) to plot the results. \n\n- For generalizable (cross cell type) **Epi-GraphReg**, run [Epi-GraphReg_generalizable_test_multiple_runs.py](https://github.com/karbalayghareh/GraphReg/blob/master/test/Epi-GraphReg_generalizable_test_multiple_runs.py) to save predictions in the test chromosomes, and run [plot_results_epi_models_generalizable.py](https://github.com/karbalayghareh/GraphReg/blob/master/test/plot_results_epi_models_generalizable.py) to plot the results. Note that we recommend cross cell type and cross chromosome predictions, meaning that the chromosomes that we use in the test cell type for predictions are the same chromosomes that are held out as the test chromosomes in the training cell type. This makes sure that the model does not just use the memorized values of the training cell types in the test cell type for the genes that do not vary a lot between the cell types. \n\n- For end-to-end **Seq-GraphReg**, run [Seq-GraphReg_e2e_test_multiple_runs.py](https://github.com/karbalayghareh/GraphReg/blob/master/test/Seq-GraphReg_e2e_test_multiple_runs.py) to save predictions in the test chromosomes, and run [plot_results_seq_models.py](https://github.com/karbalayghareh/GraphReg/blob/master/test/plot_results_seq_models.py) to plot the results.\n\n- For separate **Seq-GraphReg**, run [Seq-GraphReg_test_multiple_runs.py](https://github.com/karbalayghareh/GraphReg/blob/master/test/Seq-GraphReg_test_multiple_runs.py) to save predictions in the test chromosomes, and run [plot_results_seq_models.py](https://github.com/karbalayghareh/GraphReg/blob/master/test/plot_results_seq_models.py) to plot the results.\n\n## Feature attributions of GraphReg models\n\n### Enhancer validation\n\nWe can use feature attributions of **GraphReg** models to find out which regions are important for gene expression, meaning that they could be considered as the enhancers of any target gene. To validate enhancer predictions, we need experimental enhancer perturbation data. We have used two such datasets in the K562 cell line: (1) CRISPRi FlowFISH from [this](https://www.nature.com/articles/s41588-019-0538-0) paper and (2) Targeted Perturb-seq (TAP-seq) from [this](https://www.nature.com/articles/s41592-020-0837-5) paper. We can use the feature attributions of both **Epi-GraphReg** and **Seq-GraphReg** models. We have tried [DeepSHAP](https://github.com/slundberg/shap) and gradient-by-input for feature attribution. Note that in **Epi-GraphReg** we do the gradients of output with respect to the input epigenomic bins (of size 100bp), while in **Seq-GraphReg** we do the gradients of outputs with respect to the bottleneck representation bins (of size 100bp). This approach has been suggested by Basenji. All the scripts for these kind of analyses are in [here](https://github.com/karbalayghareh/GraphReg/tree/master/feature_attribution).\n\n### In-silico TF motif knockout and ISM\n\nWe can use **Seq-GraphRe** models to get insights about how TF motifs regulate their target genes. To have experimental validation data, we downloaded CRISPRi TF knockout experiments in K562 cells from [ENCODE](https://www.encodeproject.org/search/?searchTerm=ENCSR016WFQ&limit=all). These data show which genes are down- (up) regulated after knockout of a TF. To see if we can predict such effects, we delete the motifs of that TF in the genome and then observe the difference in the predictions of each gene. If a TF motif is important for the expression of a gene, the predicted value of gene expression would go down. By doing so, we can find the direct effects of each TF on their target genes. However, this prediction is not perfect as there is no way for the model to consider the indirect effects of the TFs on their target genes. [Seq-GraphReg_TF_KO.py](https://github.com/karbalayghareh/GraphReg/blob/master/feature_attribution/Seq-GraphReg_TF_KO.py) does these kinds of analyses. \n\nWe can also perform an in-silico saturation mutagenesis (ISM) in some distal enhancers of the genes to get an idea about how single mutations get affect the expression predictions of the genes. [Seq-GraphReg_ISM.py](https://github.com/karbalayghareh/GraphReg/blob/master/feature_attribution/Seq-GraphReg_ISM.py) can do ISM using **Seq-GraphReg** models. \n\n\n",
    "readme_length": 17291
  },
  {
    "name": "HiCcompare",
    "full_name": "dozmorovlab/HiCcompare",
    "description": "Joint normalization of two Hi-C matrices, visualization and detection of differential chromatin interactions. See multiHiCcompare for the analysis of multiple Hi-C matrices",
    "stars": 23,
    "forks": 4,
    "language": "R",
    "url": "https://github.com/dozmorovlab/HiCcompare",
    "topics": [
      "difference-detection",
      "hi-c",
      "hic",
      "normalization",
      "visualization"
    ],
    "created_at": "2017-06-30T19:44:45Z",
    "updated_at": "2025-09-28T06:02:23Z",
    "homepage": "https://dozmorovlab.github.io/HiCcompare/",
    "license": "Other",
    "readme": "# HiCcompare\n\nStansfield, John C., Kellen G. Cresswell, Vladimir I. Vladimirov, and Mikhail G. Dozmorov. [HiCcompare: An R-Package for Joint Normalization and Comparison of HI-C Datasets](https://doi.org/10.1186/s12859-018-2288-x).â€ BMC Bioinformatics 19, no. 1 (December 2018).\n\n## Overview \n\n`HiCcompare` provides functions for joint normalization and difference detection in multiple Hi-C datasets. `HiCcompare` operates on processed Hi-C data in the form of chromosome-specific chromatin interaction matrices. `HiCcompare` is available as an R package, the major releases can be found on Bioconductor [here](https://bioconductor.org/packages/HiCcompare/). \n\nIf you have more than two Hi-C datasets which you need to normalize or compare please see our other package, `multiHiCcompare`, which is available on Bioconductor [here](https://bioconductor.org/packages/multiHiCcompare/).\n\n`HiCcompare` accepts three-column tab-separated text files storing chromatin interaction matrices in a sparse matrix format which are available from several sources such as the [http://aidenlab.org/data.html](http://aidenlab.org/data.html) and [http://cooler.readthedocs.io/en/latest/index.html](http://cooler.readthedocs.io/en/latest/index.html). HiCcompare is designed to give the user the ability to perform a comparative analysis on the 3-Dimensional structure of the genomes of cells in different biological states. `HiCcompare` first can jointly normalize two Hi-C datasets to remove biases between them. Then it can detect signficant differences between the datsets using a genomic distance based permutation test. The novel concept of the MD plot, based on the commonly used MA plot or Bland-Altman plot is the basis for these methods. The log **M**inus is plotted on the y axis while the genomic **D**istance is plotted on the x axis. The MD plot allows for visualization of the differences between the Hi-C datasets. \n\nThe main functions are:\n+ `hic_loess()` which performs joint `loess` normalization on the Hi-C datasets\n+ `hic_compare()` which performs the difference detection process to detect significant changes between Hi-C datasets and assist in comparative analysis\n\nSeveral Hi-C datasets are also included in the package.\n\nRead the full paper describing the methods behind `HiCcompare` [here](https://doi.org/10.1186/s12859-018-2288-x)\n\n\n## Installation\n\nFirst make sure you have all dependencies installed in R.\n\n```\ninstall.packages(c('dplyr', 'data.table', 'ggplot2', 'gridExtra', \n\t\t\t\t   'mgcv', 'parallel', 'devtools'))\n\nif (!requireNamespace(\"BiocManager\", quietly=TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(c(\"InteractionSet\", \"GenomicRanges\", \"IRanges\", \n\t\t   \"BiocParallel\", \"QDNAseq\", \"GenomeInfoDbData\"))\t\t\t   \n```\n\nTo install `HiCcompare` from bioconductor open R and enter the following commands. Currently it is recommended to use the GitHub release or the development version of the bioconductor release.\n\n```\n# Bioconductor development version and Github Release contain major changes for difference detection\n# it is recommended to use the github release until the next Bioconductor update\n## try http:// if https:// URLs are not supported\nif (!requireNamespace(\"BiocManager\", quietly=TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"HiCcompare\")\nlibrary(HiCcompare)\n```\n\n\nOr to install the latest version of `HiCcompare` directly from the github release open R and enter the following commands.\n\n```\nlibrary(devtools)\ninstall_github('dozmorovlab/HiCcompare', build_vignettes = TRUE)\nlibrary(HiCcompare)\n```\n\n\n## Usage\n\nFirst you will need to obtain some Hi-C data. Data is available from the sources listed in the overview along with many others. You will need to extract the data and read it into R as either a 3 column sparse upper triangular matrix or a 7 column BEDPE file. For more details on data extraction see the vignette included with `HiCcompare`.\n\nBelow is an example analysis using `HiCcompare`. The data in 3 column sparse upper triangular matrix format is loaded and the first step is to create a `hic.table` object using the `create.hic.table()` function. Next, the two Hi-C matrices are jointly normalized using the `hic_loess()` function. Finally, difference detection can be performed using the `hic_compare()` function. The `hic_loess()` and `hic_compare()` functions will also produce an MD plot for visualizing the differences between the datasets. \n\n```\n# load data\nlibrary(HiCcompare)\ndata(\"HMEC.chr22\")\ndata(\"NHEK.chr22\")\n\n# create the `hic.table` object\nchr22.table = create.hic.table(HMEC.chr22, NHEK.chr22, chr = 'chr22')\nhead(chr22.table)\n\n# Jointly normalize data for a single chromosome\nhic.table = hic_loess(chr22.table, Plot = TRUE)\nhead(hic.table)\n\n# input hic.table object into hic_compare\nhic.table = hic_compare(hic.table, Plot = TRUE)\nhead(hic.table)\n```\n\nRefer to the `HiCcompare` vignette for full usage instructions. For a full explanation of the methods used in `HiCcompare` see the manuscript [here](https://doi.org/10.1101/147850).\n\nTo view the usage vignette:\n\n`browseVignettes(\"HiCcompare\")`\n\n\n## Tutorial for Differential Analysis of Hi-C Data\n\nFor more detailed instructions and examples on how to perform differential analyses on Hi-C data please see our tutorial paper \"R Tutorial: Detection of Differentially Interacting Chromatin Regions From Multiple Hiâ€C Dataset\" published in Current Protocols in Bioinformatics. https://doi.org/10.1002/cpbi.76\n\n## Branches\n\n- Master: contains the current stable release of `HiCcompare`\n- supplemental: contains supplementary files and data, see Additional Vignettes section below\n- manuscript_bioinformatics: contains write up for submission to Bioinformatics\n- test_version: contains versions of `HiCcommpare` currently in development. This version of the software may be unstable and is not reccomended for users.\n\n## Additional Vignettes\n\nThe `HiCcompare` paper included several supplemental files that showcase some of the usage and reasoning behind the methods. Below are the titles and brief descriptions of each of these vignettes along with links to the compiled `.pdf` and the source `.Rmd` files. \n\n**Normalization method comparison.** \n\nComparison of several Hi-C normalization techniques to display the persistence of bias in individually normalized chromatin interaction matrices, and its effect on the detection of differential chromatin interactions.\n\n[Compiled](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S1_File.pdf)\n\n\n[Source](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S1_File.Rmd)\n\n**S2 File. Estimation of the IF power-law depencence.** \n\nEstimation of the power-law depencence between the $log_{10}-log_{10}$ interaction frequencies and distance between interacting regions. This vignette displays the reasoning behind using a power-law function for the simulation of the signal portion of Hi-C matrices.\n\n[Compiled](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S2_File.pdf)\n\n[Source](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S2_File.Rmd)\n\n**S3 File. Estimation of the SD power-law dependence.** \n\nEstimation of the power-law depencence between the $log_{10}-log_{10}$ SD of interaction frequencies and distance between interacting regions. This vignette displays the reasoning behind using a power-law function for the simulation of the noise component of Hi-C matrices.\n\n[Compiled](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S3_File.pdf)\n\n[Source](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S3_File.Rmd)\n\n**S4 File. Estimation of proportion of zeros.** \n\nEstimation of the depencence between the proportion of zeros and distance between interacting regions. This vignette shows distribution of zeros in real Hi-C data. The results were used for modeling the proportion of zeros in simulated Hi-C matrices with a linear function.\n\n[Compiled](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S4_File.pdf)\n\n[Source](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S4_File.Rmd)\n\n\n**S5 File. Evaluation of difference detection in simulated data.** \n\nExtended evaluation of differential chromatin interaction detection analysis using simulated Hi-C data. Many different classifier performance measures are presented. Note: if trying to compile the source `.Rmd` this will take a long time to knit. \n\n[Compiled](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S5_File.pdf)\n\n[Source](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S5_File.Rmd)\n\n**S6 File. Evaluation of difference detection in real data.** \n\nExtended evaluation of differential chromatin interaction detection analysis using real Hi-C data. Many different classifier performance measures are presented. Note: if trying to compile the source `.Rmd` this will take a long time to knit. \n\n[Compiled](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S6_File.pdf)\n\n[Source](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S6_File.Rmd)\n\n**S7 File. `loess` at varying resolution.** \n\nVisualization of the `loess` loint normalization over varying resolutions. This vignette shows that increasing sparsity of Hi-C matrices with increasing resolution causes loess to become less useful for normalization at high resolutions. \n\n[Compiled](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S7_File.pdf)\n\n[Source](https://github.com/dozmorovlab/HiCcompare/raw/supplemental/supplemental_files/S7_File.Rmd)\n\n\n\n## Citation\n\nPlease cite `HiCcompare` if you use it in your analysis.\n\nJohn C. Stansfield, Kellen G. Cresswell, Vladimir I. Vladimirov, Mikhail G. Dozmorov, HiCcompare: an R-package for joint normalization and comparison of HI-C datasets. BMC Bioinformatics. 2018 Jul 31;19(1):279. doi: 10.1186/s12859-018-2288-x.\n\n## Contributions & Support\n\nSuggestions for new features and bug reports are welcome. Please create a new [issue](https://github.com/dozmorovlab/HiCcompare/issues) for any of these or contact the author directly: [@jstansfield0](https://github.com/jstansfield0) (stansfieldjc@vcu.edu)\n\n## Contributors\n\nAuthors: [@jstansfield0](https://github.com/jstansfield0) (stansfieldjc@vcu.edu) & [@mdozmorov](https://github.com/mdozmorov) (mikhail.dozmorov@vcuhealth.org)\n",
    "readme_length": 10524
  },
  {
    "name": "HiPore-C",
    "full_name": "zhengdafangyuan/HiPore-C",
    "description": "We developed a protocol of in situ high throughput multi-way contact long read Pore-C sequencing (in situ HiPore-C), a strategy that integrated multi-fragment ligates preparation with third-generation sequencing technology. With HiPore-C approach, we could explore higher-order chromatin interaction genome-widely.",
    "stars": 20,
    "forks": 5,
    "language": "Jupyter Notebook",
    "url": "https://github.com/zhengdafangyuan/HiPore-C",
    "topics": [],
    "created_at": "2022-04-24T05:01:52Z",
    "updated_at": "2025-07-19T10:17:50Z",
    "homepage": null,
    "license": "MIT License",
    "readme": "# HiPore-C\nWe developed a protocol of in situ high throughput multi-way contact long read Pore-C sequencing (in situ HiPore-C), a strategy that integrated multi-fragment ligates preparation with third-generation sequencing technology. Compared to the reported Pore-C method, HiPore-C can yield more chromatin interactions than traditional Hi-C and Pore-C at the same cost using simple procedures.And base on the high-order and allele-specific feature of HiPore-C, we have globally characterized single-allele topologies with unprecedented depth to reveal elusive genome folding principles.\n\nThis is the code used to make a pipeline for analysing HiPore-C data. In this work, We use the human hg38 genome (GRCh38_no_alt_analysis_set_GCA_000001405.15) as reference, and we obtained the public Hi-C, Chip-seq, DNase-seq and RNA-seq datasets of GM12878 and K562 cell lines from 4DN porter or ENDCODE database. \n\n\nDue to a cyber-attack last year, our management decided to shut down www.tgsbioinformatics.com as a precaution, with an uncertain timeline for its return. However, we've ensured the availability of our processed data elsewhere: alignment data is accessible on GEO (accession number GSE202539), and we've uploaded contact matrix and methylation data to Zenodo (https://zenodo.org/records/10822184).\n\n# Software\nThis pipeline has a number of dependencies including the following:\n\n- python (3.7);\n- Guppy (4.5.3);  \n- Megalodon (2.3.4);  \n- Nanoplot (1.30.1);  \n- seqkit (2.2.0);\n- seqtk (1.3-r106);\n- ngmlr (0.2.7);  \n- minimap2 (2.17-r941);  \n- samtools (1.10);    \n- Pore-C-Snakemake pipeline (0.3.0);  \n- FIMO (5.3.3);   \n- cooltools (0.5.1);  \n- cooler (0.8.6);  \n- HiCrep (1.2.0);  \n- Juicer(1.22.01);  \n- Juicebox(2.10.01);  \n- HiCExplorer (3.6);  \n- HiGlass (1.11.7);  \n- FAN-C (0.9.23);\n\n\n# Basecalling and Methylation calling\n\nIn this step, Guppy and Megalodon software were used. And due to the large amount of Nanopore data, it is recommended to generate a corresponding fastq or modification callling bam result for each multfast5 file (typically containing 4000 reads) to facilitate the subsequent analysis. The input is fast5 file and the output is fastq and mod bam file.\n\n``` \nConf_file=\"~/ont-guppy/data/dna_r9.4.1_450bps_hac_prom.cfg\"\n\nguppy_basecaller \\\n-i ${Fast5_file_dir} \\\n-s ${Exportdir} \\\n-c ${Conf_file}  \\\n-x cuda:0 \\\n-r -q 0 \\\n--min_qscore 7 \\\n--compress_fastq\n``` \n\n``` \nmegalodon \\\n    ${Fast5_file_dir} \\\n    --guppy-server-path \"~/ont-guppy/bin/guppy_basecall_server\" \\\n    --guppy-params \"-d ~/rerio/basecall_models/\" \\\n    --guppy-config res_dna_r941_prom_modbases_5mC_v001.cfg \\\n    --outputs mod_basecalls \\\n    --output-directory \"${Exportdir}\" \\\n    --mod-motif m CG 0 \\\n    --devices all --processes 48 --overwrite\n``` \n\n# Mapping and Fragment Annotation\nIn this step, to obtain more accurate alignment results for high-order reads, and to improve the effective usage of HiPore-C data, we introduced the NGMLR and Minimap2 to construct the HiPore-C alignment pipeline. NGMLR algorithm has advantages for sequence breakpoint identification and Minimap2 algorithm has advantages for long noisy sequence reads alignment. Similar to the above, we propose to perform alignment and annotation for each single fastq file (generated from a 4000 reads multifast5 file).\n\nUse the ./Scripts/Alignment.sh  and ./Scripts/Fragment_Annotation.sh script to perform this analysis. The fastq data needs to be placed in the ```${Sampledir}/Rawdata``` directory, the generated alignment result files will be placed to ```${Sampledir}/Mapping``` , and the annotation files will be placed to ```${Sampledir}/vdFAnnotation``` directory.  A python script for annotating fragments ```annopy=\"./Scripts/Read_Fragment_Annotation.py\"```, a table of genomic restriction enzyme  in-silicon digested fragments ```genome_digest_frag=\"./Scripts/DpnII_GRCh38.vd.fragments.csv\"``` , and a shell script for re-alignment ```ReAlignBash=\"./Scripts/minimap2_subreads_remapping.sh\"``` and alignemtn validation ```ChromCheckBash=\"./Scripts/multichrom_check.sh\"``` are also used in this step of the analysis.\n\nTo generate the DpnII_GRCh38.vd.fragments.csv type:\n```\ncooler digest -o hg38_DpnII_digetstion_fragment.bed hg38.chromosomes.size GCA_000001405.15_GRCh38_major_chr.fa DpnII\necho \"chrom,start,end,fragment_length,fragment_id\" > DpnII_GRCh38.vd.fragments.csv  #DpnII_GRCh38.vd.fragments.csv with columns [chrom, start, end, fragment_length, fragment_id]\nawk -v OFS=\",\" '{pirnt $1,$2,$3,$3-$2,NR}' hg38_DpnII_digetstion_fragment.bed >> DpnII_GRCh38.vd.fragments.csv\n``` \n\nTo run the pipeline type:\n\n``` \nbase ./Scripts/Alignment.sh $Sampledir $refgenome $fastq $threads\nbase ./Scripts/Fragment_Annotation.sh $annopy $Sampledir $refgenome $fastq $genome_digest_frag  $ReAlignBash $ChromCheckBash\n\n``` \n\n\n\n# Generate pairwise contact matrix\nTo compare with the Hi-C data and the subsequent visualization, the multiway contacts of HiPore-C reads were decomposed into pairwise contacts using ```Generate_Contact_juiceMatrix.py```ï¼Œ and then  mcool and hic format file were generated by cooler and Juicer tools, repectively.\n\nTo generate pairwise contact matrix, merge all ```Align_Fragment_RvdF.csv``` files from above steps, and type:\n``` \ninputfile=\"Merge_Align_Fragment_RvdF.csv\"\njuice_matrix=\"juice_matrix.txt\"\npython ./Scripts/Generate_Contact_juiceMatrix.py -p ${inputpaf} -o ${output_dir} -s 0 -t 20 -c 1000000 &\n``` \n\n\nTo generate .cool, .mcool and .hic files type:\n``` \nDataDir=\"${SampleDir}/juiceMatrix\"\nPAIRS_FILE=\"${DataDir}/juice_matrix.txt\"\nPAIRS_FILE_Sort=\"${DataDir}/juice_matrix_sort.txt\"\n\nCHROMSIZES_FILE=\"~/hg38.chromosomes.size\"\nBINSIZE=1000\nCOOL_FILE=\"${DataDir}/output.cool\"\nHIC_FILE=\"${DataDir}/output.hic\"\n\n# sorted\nawk '{if ($3 > $7) { print $1, $6, $7, $8, $9, $2, $3, $4, $5, $11, $10} else {print $0} }' ${PAIRS_FILE} | sort -k3,3d -k7,7d | tr \" \" \"\\t\"  >  ${PAIRS_FILE_Sort}\n\n# contact matrix to cool and mcool\ncooler cload pairs -c1 3 -p1 4 -c2 7 -p2 8 $CHROMSIZES_FILE:$BINSIZE $PAIRS_FILE_Sort $COOL_FILE && \\\ncooler zoomify -p 20 -r 1000,2000,5000,10000,25000,50000,100000,250000,500000,1000000 --balance ${COOL_FILE} &\n\n# juice_matrix to hic\njava -Xmx20g -jar ~/juicer_tools_1.22.01.jar pre --threads 12 ${PAIRS_FILE_Sort} ${HIC_FILE} hg38 &\n``` \n\n\n# High order chromatin structure analysis\n\nFor the validation of chromation structure, we analyszed the compartment eigenvector scores, insulation scores and performed APA analysis.\n``` \nDataDir=\"${SampleDir}/juiceMatrix\"\nmCOOL_FILE=\"${DataDir}/output.mcool\"\nmCOOL_FILE_10kb=\"${mCOOL_FILE}::resolutions/10000\"\nmCOOL_FILE_100kb=\"${mCOOL_FILE}::resolutions/100000\"\n\n# Eigenvector and Insulation scores\ncooltools eigs-cis --bigwig -o Cis_Compartment.100k $mCOOL_FILE_100kb &\ncooltools insulation -o Insulation.tsv --window-pixels --append-raw-scores --bigwig $mCOOL_FILE_10kb 1 2 5 10 &\n\n# APA\nloopfile=\"~/Rao2014_Loops/GSE63525_GM12878_hg38_looplist.txt\"\nhicfile=\"${SampleDir}/juiceMatrix/out.hic\"\nExportdir=\"${SampleDir}/juiceMatrix/APA\"\nmkdir -p ${Exportdir}\njava -Xmx20g -jar ~/juicer_tools_1.22.01.jar apa --threads 20 -r 10000 -w 10 -u ${hicfile} ${loopfile} ${Exportdir}\n``` \n",
    "readme_length": 7169
  },
  {
    "name": "pySCENIC",
    "full_name": "aertslab/pySCENIC",
    "description": "pySCENIC is a lightning-fast python implementation of the SCENIC pipeline (Single-Cell rEgulatory Network Inference and Clustering) which enables biologists to infer transcription factors, gene regulatory networks and cell types from single-cell RNA-seq data.",
    "stars": 562,
    "forks": 199,
    "language": "Python",
    "url": "https://github.com/aertslab/pySCENIC",
    "topics": [
      "gene-regulatory-network",
      "single-cell",
      "transcription-factors",
      "transcriptomics"
    ],
    "created_at": "2018-03-07T13:57:49Z",
    "updated_at": "2025-11-27T05:34:07Z",
    "homepage": "http://scenic.aertslab.org",
    "license": "GNU General Public License v3.0",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "Beeline",
    "full_name": "Murali-group/Beeline",
    "description": "BEELINE: evaluation of algorithms for gene regulatory network inference",
    "stars": 197,
    "forks": 58,
    "language": "Python",
    "url": "https://github.com/Murali-group/Beeline",
    "topics": [],
    "created_at": "2019-03-14T01:14:49Z",
    "updated_at": "2025-10-30T16:35:56Z",
    "homepage": "",
    "license": "GNU General Public License v3.0",
    "readme": "# :honeybee: BEELINE: Benchmarking gEnE reguLatory network Inference from siNgle-cEll transcriptomic data :honeybee:\n![Overview of BEELINE](docs/figs/overview-graphic.png )\n\nThis is the main repository for BEELINE. The documentation is available at: [https://murali-group.github.io/Beeline/](https://murali-group.github.io/Beeline/).\n\nQuick setup:\n- BEELINE can be run on Windows or Linux distributions. To install Docker Desktop, you can find it here (https://www.docker.com/). If running BEELINE on a server such that you require a CLI, see Docker Engine (https://docs.docker.com/engine/install/).\n- Setup docker to run docker without sudo using ` sudo usermod -aG docker $USER`, if you haven't already. See more details [here](https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo)\n- The Docker images of 12 algorithms tested in BEELINE are available at [https://hub.docker.com/u/grnbeeline](https://hub.docker.com/u/grnbeeline). Alternatively, to build the docker containers from scratch (instead of using pre-built versions) for each of the algorithms run `. initialize.sh` (this step will take a while)\n- We recommend using [Anaconda](https://www.anaconda.com/) for Python. Run the `. setupAnacondaVENV.sh` command to automatically create an Anaconda virtual environment named BEELINE from requirements.txt and install necessary libraries required to run BEELINE. Alternatively, you can create virtual environment for python using vnev from requirements.txt as detailed [here](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)\n\n\nWe provided an example dataset under inputs/example/GSD/ and a corresponding configuration file necessary for running GRN inference using 12 methods described in BEELINE. \n- To compute proposed reconstructions on the example dataset, run `python BLRunner.py --config config-files/config.yaml`. Running this script for the first time can be slow as it involves downloading the contianers from Docker hub.\n- To compute areas under the ROC and PR curves for the proposed reconstructions, run `python BLEvaluator.py --config config-files/config.yaml --auc`. To display the complete list of evalutation options, run `python BLEvaluator.py --help`.\n\nIf you use BEELINE in your research, please cite:\n\nPratapa, A., Jalihal, A.P., Law, J.N., Bharadwaj, A., Murali, T. M. (2020) \"Benchmarking algorithms for gene regulatory network inference from single-cell transcriptomic data.\" _Nature Methods_, 17, 147â€“154.\n\nLink to the pubication: [https://www.nature.com/articles/s41592-019-0690-6](https://www.nature.com/articles/s41592-019-0690-6)\n\nThe preprint version of this article is available at: [https://doi.org/10.1101/642926](https://doi.org/10.1101/642926)\n\nThe repository for BoolODE is located at: [https://github.com/Murali-group/BoolODE](https://github.com/Murali-group/BoolODE)\n\nThe input datasets used in BEELINE are available at: [https://doi.org/10.5281/zenodo.3378975](https://doi.org/10.5281/zenodo.3378975)\n\nTwitter thread link: https://twitter.com/t_m_murali/status/1215302095601119234?s=20\n",
    "readme_length": 3085
  },
  {
    "name": "SCENICprotocol",
    "full_name": "aertslab/SCENICprotocol",
    "description": "A scalable SCENIC workflow for single-cell gene regulatory network analysis",
    "stars": 169,
    "forks": 66,
    "language": "Python",
    "url": "https://github.com/aertslab/SCENICprotocol",
    "topics": [],
    "created_at": "2019-03-05T13:51:41Z",
    "updated_at": "2025-12-01T06:13:51Z",
    "homepage": "",
    "license": "GNU General Public License v3.0",
    "readme": "# A scalable SCENIC workflow for single-cell gene regulatory network analysis\n\nThis repository describes how to run a pySCENIC gene regulatory network inference analysis alongside a basic \"best practices\" expression analysis for single-cell data.\nThis includes:\n* Standalone Jupyter notebooks for an interactive analysis\n* A Nextflow DSL1 workflow, which provides a semi-automated and streamlined method for running these steps\n* Details on pySCENIC installation, usage, and downstream analysis\n\nSee also the associated publication in **Nature Protocols**: https://doi.org/10.1038/s41596-020-0336-2.\n\nFor an advanced implementation of the steps in this protocol, see **[VSN Pipelines](https://github.com/vib-singlecell-nf/vsn-pipelines)**, a Nextflow DSL2 implementation of pySCENIC with comprehensive and customizable pipelines for expression analysis.\nThis includes additional pySCENIC features (multi-runs, integrated motif- and track-based regulon pruning, loom file generation).\n\n## Overview\n\n* [Quick start](#quick-start)\n* [Requirements](#general-requirements-for-this-workflow)\n* [Installation](docs/installation.md)\n* Case studies\n  * PBMC 10k dataset (10x Genomics)\n    * Full SCENIC analysis, plus filtering, clustering, visualization, and SCope-ready loom file creation:\n      * [Jupyter notebook](notebooks/PBMC10k_SCENIC-protocol-CLI.ipynb) \n        | \n        [HTML render](http://htmlpreview.github.io/?https://github.com/aertslab/SCENICprotocol/blob/master/notebooks/PBMC10k_SCENIC-protocol-CLI.html)\n    * Extended analysis post-SCENIC:\n      * [Jupyter notebook](notebooks/PBMC10k_downstream-analysis.ipynb)\n        | \n        [HTML render](http://htmlpreview.github.io/?https://github.com/aertslab/SCENICprotocol/blob/master/notebooks/PBMC10k_downstream-analysis.html)\n    * To run the same dataset through the VSN Pipelines DSL2 workflow, see [this tutorial](https://vsn-pipelines-examples.readthedocs.io/en/latest/PBMC10k.html).\n  * Cancer data sets\n    * [Jupyter notebook](notebooks/SCENIC%20Protocol%20-%20Case%20study%20-%20Cancer%20data%20sets.ipynb)\n        | \n        [HTML render](http://htmlpreview.github.io/?https://github.com/aertslab/SCENICprotocol/blob/master/notebooks/SCENIC%20Protocol%20-%20Case%20study%20-%20Cancer%20data%20sets.html)\n  * Mouse brain data set\n    * [Jupyter notebook](notebooks/SCENIC%20Protocol%20-%20Case%20study%20-%20Mouse%20brain%20data%20set.ipynb)\n        | \n        [HTML render](http://htmlpreview.github.io/?https://github.com/aertslab/SCENICprotocol/blob/master/notebooks/SCENIC%20Protocol%20-%20Case%20study%20-%20Mouse%20brain%20data%20set.html)\n* [References and more information](#references-and-more-information)\n\n\n<p align=\"center\">\n<img src=\"docs/figs/Figure01.png\" width=\"600\" alt=\"SCENIC workflow diagram\">\n</p>\n\n\n---\n## Quick start\n\n### Running the pySCENIC pipeline in a Jupyter notebook\nWe recommend using \n    [this notebook](notebooks/PBMC10k_SCENIC-protocol-CLI.ipynb) \n    as a template for running an interactive analysis in Jupyter.\nSee the \n    [installation instructions](docs/installation.md)\n    for information on setting up a kernel with pySCENIC and other required packages.\n\n### Running the Nextflow pipeline on the example dataset\n\n#### Requirements (Nextflow/containers)\n\nThe following tools are required to run the steps in this Nextflow pipeline:\n* [Nextflow](https://www.nextflow.io/)\n* A container system, either of:\n    * [Docker](https://docs.docker.com/)\n    * [Singularity](https://www.sylabs.io/singularity/)\n\nThe following container images will be pulled by nextflow as needed:\n* Docker: [aertslab/pyscenic:latest](https://hub.docker.com/r/aertslab/pyscenic).\n* Singularity: [aertslab/pySCENIC:latest](https://www.singularity-hub.org/collections/2033).\n* [See also here.](https://github.com/aertslab/pySCENIC#docker-and-singularity-images)\n\n#### Using the test profile\n\nA quick test can be accomplished using the `test` profile, which automatically pulls the testing dataset (described in full below):\n\n    nextflow run aertslab/SCENICprotocol \\\n        -profile docker,test\n\nThis small test dataset takes approximately 70s to run using 6 threads on a standard desktop computer.\n\n#### Download testing dataset\n\nAlternately, the same data can be run with a more verbose approach (this is more illustrative for how to substitute other data into the pipeline).\nDownload a minimum set of SCENIC database files for a human dataset (approximately 78 MB).\n\n    mkdir example && cd example/\n    # Transcription factors:\n    wget https://raw.githubusercontent.com/aertslab/SCENICprotocol/master/example/test_TFs_tiny.txt\n    # Motif to TF annotation database:\n    wget https://raw.githubusercontent.com/aertslab/SCENICprotocol/master/example/motifs.tbl\n    # Ranking databases:\n    wget https://raw.githubusercontent.com/aertslab/SCENICprotocol/master/example/genome-ranking.feather\n    # Finally, get a tiny sample expression matrix (loom format):\n    wget https://raw.githubusercontent.com/aertslab/SCENICprotocol/master/example/expr_mat_tiny.loom\n\n\n#### Running the example pipeline\n\nEither Docker or Singularity images can be used by specifying the appropriate profile (`-profile docker` or `-profile singularity`).\nPlease note that for the tiny test dataset to run successfully, the default thresholds need to be lowered.\n\n##### Using loom input\n\n    nextflow run aertslab/SCENICprotocol \\\n        -profile docker \\\n        --loom_input expr_mat_tiny.loom \\\n        --loom_output pyscenic_integrated-output.loom \\\n        --TFs test_TFs_tiny.txt \\\n        --motifs motifs.tbl \\\n        --db *feather \\\n        --thr_min_genes 1\n\nBy default, this pipeline uses the container specified by the `--pyscenic_container` parameter.\nThis is currently set to `aertslab/pyscenic:0.9.19`, which uses a container with both pySCENIC and Scanpy `1.4.4.post1` installed.\nA custom container can be used (e.g. one built on a local machine) by passing the name of this container to the `--pyscenic_container` parameter.\n\n##### Expected output\n\nThe output of this pipeline is a loom-formatted file (by default: `output/pyscenic_integrated-output.loom`) containing:\n* The original expression matrix\n* The pySCENIC-specific results:\n    * Regulons (TFs and their target genes)\n    * AUCell matrix (cell enrichment scores for each regulon)\n    * Dimensionality reduction embeddings based on the AUCell matrix (t-SNE, UMAP)\n*  Results from the parallel best-practices analysis using highly variable genes:\n    * Dimensionality reduction embeddings (t-SNE, UMAP)\n    * Louvain clustering annotations\n\n## General requirements for this workflow\n* Python version 3.6 or greater\n* Tested on various Unix/Linux distributions (Ubuntu 18.04, CentOS 7.6.1810, MacOS 10.14.5)\n\n---\n\n## References and more information\n\n### SCENIC\n* [SCENIC (R) on GitHub](https://github.com/aertslab/SCENIC)\n* [SCENIC website](http://scenic.aertslab.org/)\n* [SCENIC publication](https://doi.org/10.1016/j.cell.2018.05.057)\n* [pySCENIC on GitHub](https://github.com/aertslab/pySCENIC)\n* [pySCENIC documentation](https://pyscenic.readthedocs.io/en/latest/)\n* [VSN Pipelines](https://github.com/vib-singlecell-nf/vsn-pipelines), a repository of pipelines for single-cell data in Nextflow DSL2, including an implementation of pySCENIC.\n\n### SCope\n* [SCope webserver](http://scope.aertslab.org/)\n* [SCope on GitHub](https://github.com/aertslab/SCope)\n* [SCopeLoomR](https://github.com/aertslab/SCopeLoomR)\n* [SCopeLoomPy](https://github.com/aertslab/SCopeLoomPy)\n\n### Scanpy\n* [Scanpy on GitHub](https://github.com/theislab/scanpy)\n* [Scanpy documentation](https://scanpy.readthedocs.io/)\n* [Scanpy publication](https://doi.org/10.1186/s13059-017-1382-0)\n\n\n\n\n",
    "readme_length": 7706
  },
  {
    "name": "dictys",
    "full_name": "pinellolab/dictys",
    "description": "Context specific and dynamic gene regulatory network reconstruction and analysis",
    "stars": 123,
    "forks": 15,
    "language": "Python",
    "url": "https://github.com/pinellolab/dictys",
    "topics": [
      "dynamic-network",
      "gene-regulatory-network",
      "network-analysis",
      "network-inference",
      "network-visualization",
      "single-cell-analysis",
      "single-cell-multiomics",
      "single-cell-network"
    ],
    "created_at": "2022-07-01T13:54:32Z",
    "updated_at": "2025-11-30T08:03:37Z",
    "homepage": "",
    "license": "GNU Affero General Public License v3.0",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "SERGIO",
    "full_name": "PayamDiba/SERGIO",
    "description": "A simulator for single-cell expression data  guided by gene regulatory networks",
    "stars": 69,
    "forks": 31,
    "language": "Python",
    "url": "https://github.com/PayamDiba/SERGIO",
    "topics": [],
    "created_at": "2019-07-17T20:10:02Z",
    "updated_at": "2025-10-29T13:04:48Z",
    "homepage": null,
    "license": "GNU General Public License v3.0",
    "readme": "# SERGIO (Single-cell ExpRession of Genes In silicO)\n[![DOI](https://zenodo.org/badge/197455649.svg)](https://zenodo.org/badge/latestdoi/197455649)\n\nSERGIO v1.0.0\n\n\nSaurabh Sinhaâ€™s Lab, University of Illinois at Urbana-Champaign [Sinha Lab](https://www.sinhalab.net/sinha-s-home)\n\nDeveloped by Payam Dibaeinia\n\n## Description\nSERGIO is a simulator for single-cell expression data guided by gene regulatory networks. A command-line, easy-to-use version of SERGIO will be soon uploaded to PyPI. Here is the documentation for using SERGIO v1.0.0 as a module in python.\n\n## Dependencies\nPython >= 2.7.14\n\nnumpy >= 1.13.3\n\nscipy >= 1.1.0\n\nnetworkx >= 2.0\n\nThe tool has been succefully tested on MacOS Sierra (v10.12.6) and ScientificLinux 6.9.\n\n## Getting Started\nTo download SERGIO, clone the repository via the following command (should take < 1 minute):\n\n```git clone https://github.com/PayamDiba/SERGIO```\n\n\n## Usage\n\nrun_sergio.ipynb is a jupyter notebook that runs SERGIO for steady-state and differentiation simulations as well as adding technical noise. SERGIO with an easier interface for simulations and adding technical noise will be soon uploaded to PyPI. \n### Simulating Clean Data\nA synthetic data set can be simulated in four lines of python code:\n\n1. An instance of SERGIO simulator is constructed as below:\n\n```python\nimport numpy as np\nfrom sergio import sergio\nsim = sergio(number_genes, number_bins, number_sc, noise_params,\n    noise_type, decays, dynamics, sampling_state, dt, bifurcation_matrix, \n    noise_params_splice, noise_type_splice, splice_ratio, dt_splice)\n```\n\n* number_genes: total number of genes present in GRN\n* number_bins: total number of distinct cell types to be simulated\n* number_sc: total number of cells per cell type to be simulated\n* noise_params: a single scalar or a list of size number_genes containing the genesâ€™ noise amplitude parameter q in steady-state simulations or unspliced transcriptsâ€™ noise parameter in differentiation simulations. For differentiation simulations, small values (<0.5) are recommended.\n* noise_type: The type of genes' stochastic noise in steady-state or unspliced transcripts' noise type in differentiation simulations. Options: â€œdpdâ€, â€œspâ€, â€œsdâ€ (For more details, see the paper)\n* decays: a single scaler or a list of size number_genes containing the genesâ€™ decay parameter for steady-state simulations or unspliced transcriptsâ€™ decay in differentiation simulations.\n* sampling_state: an integer determining the length of simulations in stationary region. In steady-state simulations, for each cell type, simulations are continued for sampling_state times number_sc time steps after reaching to steady-state region. In differentiation simulations, for each cell type, if takes n steps till reaching to steady-state, simulations are continued for sampling_state times n more time steps in steady-state region. \n* dt: integration time step in steady-state simulations (default: 0.01).\n* dynamics: a Boolean showing whether to simulate steady-state (False) or differentiation (True).\n* bifurcation_matrix: only needed for dynamics simulations (default: None). A 2d (number_bins times number_bins) python list containing >=0 floats showing the differentiation graph. The element in row i and column j shows the migration rate (r) from cell type i to cell type j. Therefore, r times number_sc paths between cell type i and j is simulated. Increasing r slows down simulations but increases the density of simulated cells differentiating from cell type i to j, also r=0 denotes no differentiation from cell type i to j. Typically values of r around 1 result in desirable differentiation trajectories.\n\t- Example: system of three cell types with a linear differentiation graph:\n\t    bifurcation_matrix = [[0, 0.8, 0 ],[0, 0, 1.1], [0,0,0]]\n\n* noise_params_splice: only needed for dynamics simulations (default: None). A single scalar or a list of size number_genes containing the spliced transcriptsâ€™ noise parameter. Small values (<0.5) are recommended.\n* noise_type_splice: only needed for dynamics simulations (default: None). The type of stochastic noise for simulations of spliced transcripts. Options: â€œdpdâ€, â€œspâ€, â€œsdâ€ (For more details, see the paper)\n* splice_ratio: only needed for dynamics simulations (default: 4). A single scalar or a list of size number_genes containing the ratio of the expected expression of spliced to unspliced transcripts of genes in differentiation simulations. This tunes the degradation rate of spliced RNA.\n* dt_splice: only needed for dynamics simulations (default: 0.01). Integration time step in differentiation simulations.\n\n2. GRN structure and master regulatorsâ€™ profile is fed into the simulator by invoking `build_graph` method:\n\n```python\nsim.build_graph(input_file_taregts, input_file_regs, shared_coop_state)\n```\n\tNote: Before preparing the input files, use zero-based numerical indexing for naming all gene IDs (both master regulators and non-master regulators) in the GRN. For example if there are 10 genes in the GRN, naming them starting 0 to 9.\n\n* input_file_taregts: path to a comma separated file containing GRN structure and its parameters. Each row in this file corresponds to a target gene in the GRN. Every row contains the parameters of the hill functions of all the regulators of that rowâ€™s target gene. \nColumn order is: target gene id, number of targetâ€™s regulators, regulator ID_1,â€¦, regulator ID_n, K_1,â€¦,K_n, hill_coeff_1, â€¦, hill_coeff_n\n\n\twhere â€œKâ€ denotes the maximum interaction strength (see equation 6 in the manuscript). For activating interactions use positive â€œKâ€ and for repressive ones use negative values. Since master regulators do not have any regulator they should not be included in this file as a target gene. \n\t- Example: input_file_taregets for GRN of three genes  g0 --> g1 --| g2   \n\t1, 1, 0, 2.5, 2   \n\t2, 1, 1, -1.3, 2\n\n\n* input_file_regs: path to a comma separated file containing master regulatorsâ€™ basal production rate in all cell types. So, if there are three cell types to be simulated, each row in this file has four entries: master regulator id, production rate cell type_1,â€¦,  production rate cell type_3.\n\t- Example: input_file_regs, for GRN g0 --> g1 --| g2,  in three cell types:   \n\t   0, 0.5, 1.5, 3\n\n* shared_coop_state: in case of using >0 values, the same value is used for all hill coefficients in simulations and therefore there is no need to specify these values (hill_coeff) in the input_file_taregets (they are ignored otherwise). In case of using any <=0 value, hill coefficients will be read from input_file_taregets. Recommended values of hill coefficient is between 1 and 3 (default: 0).\n\n3. For running steady-state simulations invoke `simulate` method:\n```python\nsim.simulate()\n```\n\nFor running differentiation simulations invole `simulate_dynamics` method:\n```python\nsim.simulate_dynamics()\n```\n4. To get the clean simulated expression matrix after steady_state simulations invoke `getExpressions` method:\n```python\nexpr = sim.getExpressions()\n```\n\nThis returns a 3d numpy array (#cell_types * #genes * #cells_per_type). To convert into a 2d matrix of size (#genes * #cells) do:\n```python\nexpr = np.concatenate(expr, axis = 1)\n```\n\nNow each row represents a gene and each column represents a simulated single-cell. Gene IDs match their row in this expression matrix, also cell types are groupd by columns such that the first #cells_per_type columns correspond to the first simulated cell type, the next #cells_per_type columns correpond to the second cell type and ... .\n\nTo get the clean simulated expression matrix after differentiation simulations invoke `getExpressions_dynamics` method:\n```python\nexprU, exprS = sim.getExpressions_dynamics()\n```\n\nThis returns two 3d numpy array (#cell_types * #genes * #cells_per_type) for unspliced (exprU) and spliced (exprS) transcripts. To convert them into a 2d matrix of size (#genes * #cells) do:\n```python\nexprU = np.concatenate(exprU, axis = 1)\nexprS = np.concatenate(exprS, axis = 1)\n```\n\nNow each row represents a gene and each column represents a simulated single-cell. Gene IDs match their row in this expression matrix, also cell types are groupd by columns such that the first #cells_per_type columns correspond to the first simulated cell type, the next #cells_per_type columns correpond to the second cell type and ... .\n\n\n### Adding Technical Noise\nSERGIO can add three type of technical noise (outlier genes, library size, and dropouts) to the clean simulated data. These noise modules can be invoked in any combination and order. Also, there is a fourth module that converts an expression matrix to an mRNA count matrix. All of these modules work on the 3d expression matrix (not the 2d concatenated version).\n\nFirst use SERGIO to simulate a clean data set and obtain the 3d expression matrix:  \nIn steady-state simulations:  \n```python\nexpr = sim.getExpressions()\n```\n\nIn differentiation simulations:\n```python\nexprU, exprS = sim.getExpressions_dynamics()\n```\n\n\nHere we show how to add outlier genes followed by library size and then dropouts. Please refer to the manuscript for the definitions of the input parameters to the each of the noise modules:\n1. **Outlier Genes**: \n\nIn steady-state simulations invoke the `outlier_effect` method:\n```python\nexpr_O = sim.outlier_effect(expr, outlier_prob, mean, scale)\n```\n\nIn differentiation simulations invoke the `outlier_effect_dynamics` method:\n```python\nexprU_O, exprS_O = sim.outlier_effect_dynamics(exprU, exprS, outlier_prob, mean, scale)\n```\n\n2. **Library Size**: \n\nIn steady-state simulations invoke the `lib_size_effect` method:\n```python\nexpr_O_L = sim.lib_size_effect(expr_O, mean, scale)\n```\n\nIn differentiation simulations invoke the `lib_size_effect_dynamics` method:\n```python\nexprU_O_L, exprS_O_L = sim.outlier_effect_dynamics(exprU_O, exprS_O, mean, scale)\n```\n\n3. **Dropouts**: \n\nIn steady-state simulations invoke the `dropout_indicator` method:\n```python\nbinary_ind = sim.dropout_indicator(expr_O_L, shape, percentile)\nexpr_O_L_D = np.multiply(binary_ind, expr_O_L)\n```\n\nIn differentiation simulations invoke the `dropout_indicator_dynamics` method:\n```python\nbinary_indU, binary_indS = sim.dropout_indicator_dynamics(exprU_O_L, exprS_O_L, shape, percentile)\nexprU_O_L_D = np.multiply(binary_indU, exprU_O_L)\nexprS_O_L_D = np.multiply(binary_indS, exprS_O_L)\n```\n\n4. **mRNA Count Matrix**: \n\nIn steady-state simulations invoke the `convert_to_UMIcounts` method:\n```python\ncount_matrix = sim.convert_to_UMIcounts(expr_O_L_D)\n```\n\nIn differentiation simulations invoke the `convert_to_UMIcounts_dynamics` method:\n```python\ncount_matrix_U = sim.convert_to_UMIcounts_dynamics(exprU_O_L_D)\ncount_matrix_S = sim.convert_to_UMIcounts_dynamics(exprS_O_L_D)\n```\n\nThe output of each of these modules including the \"count matrix conversion\" module are 3d numpy arrays of size (#cell_types * #gene * #cells_per_type). To convert them into a 2d expression matrix invoke numpy.concatenate as shown before. \n\n## Repository Contents\n* SERGIO/ contains the python codes required for simulations.\n\n* data_sets/ cotains 11 data sets including 6 steady-state and 5 differentiation simulated data. Each data set's folder contains the input files used in simulations as well the ground truth (gt) GRN. Differentiation data sets' folders also contain the differentiation graph (bMat) used in simulations.\n\n* GNW_sampled_GRNs/ contains four networks sampled from the known regulatory network in Ecoli and Yeast using GeneNetWeaver (doi: 10.1093/bioinformatics/btr373). These networks might contain auto-regulatory edges and cycles.\n\n* Demo/ contains demo input files for both steady-state and differentiation simulations. It also contains a jupyter notebook that runs demo simulations. Expected run time on a normal desktop computer for demo steady-state simulation is about 150 seconds and for demo differentiation simulations is about 120 seconds. \n",
    "readme_length": 11942
  },
  {
    "name": "inferelator",
    "full_name": "flatironinstitute/inferelator",
    "description": "Task-based gene regulatory network inference using single-cell or bulk gene expression data conditioned on a prior network.",
    "stars": 54,
    "forks": 12,
    "language": "Python",
    "url": "https://github.com/flatironinstitute/inferelator",
    "topics": [
      "inference",
      "network"
    ],
    "created_at": "2019-02-11T14:35:42Z",
    "updated_at": "2025-11-11T02:18:10Z",
    "homepage": "",
    "license": "BSD 2-Clause \"Simplified\" License",
    "readme": "# Inferelator 3.0\n\n[![PyPI version](https://badge.fury.io/py/inferelator.svg)](https://badge.fury.io/py/inferelator)\n[![CI](https://github.com/flatironinstitute/inferelator/actions/workflows/python-package.yml/badge.svg)](https://github.com/flatironinstitute/inferelator/actions/workflows/python-package.yml/)\n[![codecov](https://codecov.io/gh/flatironinstitute/inferelator/branch/release/graph/badge.svg)](https://codecov.io/gh/flatironinstitute/inferelator)\n[![Documentation Status](https://readthedocs.org/projects/inferelator/badge/?version=latest)](https://inferelator.readthedocs.io/en/latest/?badge=latest)\n\nThe [Inferelator 3.0](https://doi.org/10.1093/bioinformatics/btac117) is a package for gene regulatory network inference that is based on regularized regression. \nIt is an update of the [Inferelator 2.0](https://ieeexplore.ieee.org/document/5334018), which is an update of the original [Inferelator](https://doi.org/10.1186/gb-2006-7-5-r36)\nIt is maintained by the Bonneau lab in the [Systems Biology group of the Flatiron Institute](https://www.simonsfoundation.org/flatiron/center-for-computational-biology/systems-biology/).\n\nThis repository is the actively developed inferelator package for python. It works for both single-cell and bulk transcriptome experiments.\nIncludes [AMuSR](https://github.com/simonsfoundation/multitask_inferelator/tree/AMuSR/inferelator_ng) \n[(Castro et al 2019)](https://doi.org/10.1371/journal.pcbi.1006591), \nelements of [InfereCLaDR](https://github.com/simonsfoundation/inferelator_ng/tree/InfereCLaDR) \n[(Tchourine et al 2018)](https://doi.org/10.1016/j.celrep.2018.03.048), \nand single-cell workflows [(Jackson et al 2020)](https://elifesciences.org/articles/51254).\n\nWe recommend installing this package from PyPi using `python -m pip install inferelator`. \nIf running locally, also install `joblib` by `python -m pip install joblib` for parallelization.\nIf running on a cluster, also install `dask` by `python -m pip install dask[complete] dask_jobqueue` for dask-based parallelization.\n\nThis package can also be installed from the github repository. \nClone the [inferelator GitHub](https://github.com/flatironinstitute/inferelator) repository and run `python setup.py install`.\n\nDocumentation is available at [https://inferelator.readthedocs.io](https://inferelator.readthedocs.io/en/latest/), and\nbasic workflows for ***Bacillus subtilis*** and ***Saccharomyces cerevisiae*** are included with a tutorial. \n\nAll current example data and scripts are available from Zenodo \n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3355524.svg)](https://doi.org/10.5281/zenodo.3355524).",
    "readme_length": 2633
  },
  {
    "name": "TENET",
    "full_name": "neocaleb/TENET",
    "description": "TENET is a tool for reconstructing gene regulatory networks from pseudo-time ordered single-cell transcriptomic data.",
    "stars": 41,
    "forks": 8,
    "language": "Python",
    "url": "https://github.com/neocaleb/TENET",
    "topics": [],
    "created_at": "2019-01-24T19:18:26Z",
    "updated_at": "2025-11-17T11:42:24Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# TENET\nA tool for reconstructing Transfer Entropy-based causal gene NETwork from pseudo-time ordered single cell transcriptomic data \n\n<div>\n<img src=\"https://user-images.githubusercontent.com/33104430/83049138-fe23bb00-a04a-11ea-9d8a-59ca7582e759.png\" width=\"90%\"></img>\n</div>\n\n## Citation\n\nNucleic Acids Research, gkaa1014, https://doi.org/10.1093/nar/gkaa1014\n\n## Dependency\n\n\tpython3\n\topenmpi (>4.0)\n\tJPype\n\n## 1. Run TENET using expression data in a csv file and pseudotime result in a text file\n#### Usage\n\n\t./TENET [expression_file_name] [number_of_threads] [trajectory_file_name] [cell_select_file_name] [history_length]\n\n#### example\n\n\t./TENET expression_data.csv 10 trajectory.txt cell_select.txt 1\n\n\n#### Input \n\n###### (1) expression_file (raw count is recommended) - a csv file with N cells in the rows and M genes in the columns (same format with wishbone pseudotime package).\n\n\t\tGENE_1\tGENE_2\tGENE_3\t...\tGENE_M\n\n\tCELL_1\t\n\n\tCELL_2\n\n\tCELL_3\n\n\t.\n\t.\n\t.\n\n\tCELL_N\n\n###### (2) number_of_threads - You can use this multi-threads option. This will take lots of memory depending on the squared number of genes * the number of cells. If the program fail, you need to reduce this.\n\n###### (3) trajectory_file - a text file of pseudotime data with N time points in the same order as the N cells of the expression file.\n\n\t0.098\n\t0.040\n\t0.023\n\t.\n\t.\n\t.\n\t0.565\n\n###### (4) cell_select_file - a text file of cell selection data with N Boolean (1 for select and 0 for non-select) data in the same order as the N cells of the expression file.\n\n\t1\n\t1\n\t0\n\t.\n\t.\n\t.\n\t1\n\n###### (5) history_length - the length of history. In the benchmark data TENET provides best result when the length of history set to 1.\n\n#### Output\n\n\tTE_result_matrix.txt - TEij, M genes x M genes matrix representing the causal relationship from GENEi to GENEj.\n\n\tTE\tGENE_1\tGENE_2\tGENE_3\t...\tGENE_M\n\tGENE_1\t0\t0.05\t0.02\t...\t0.004\n\tGENE_2\t0.01\t0\t0.04\t...\t0.12\n\tGENE_3\t0.003\t0.003\t0\t...\t0.001\n\t.\n\t.\n\t.\n\tGENE_M\t0.34\t0.012\t0.032\t...\t0\n\n## 2. Run TENET with hdf5 file including PAGA pseudotime result\n#### Usage\n\n\t./TENET4PAGAhdf5 [hdf5_file_name] [number_of_threads] [history_length] [variable_in_adata]\n\n#### example\n\n\t./TENET4PAGAhdf5 Data.Tuck/Tuck_PAGA510genes.h5ad 10 1 X\n\t./TENET4PAGAhdf5 Data.Tuck/Tuck_PAGA510genes.h5ad 10 1 raw\n\n#### Input\n\n###### (1) hdf5 file stored after running PAGA.\n\n###### (2) [variable_in_adata]\n\tIf the expression matrix stored in adata.X, then choose X. If it is adata.raw.X, then choose raw.\n\n## 3. Run TENET from TF to target using expression data in a csv file and pseudotime result in a text file\n#### Usage\n\n        ./TENET_TF [expression_file_name] [number_of_threads] [trajectory_file_name] [cell_select_file_name] [history_length] [species]\n\n#### example\n\n        ./TENET_TF expression_data.csv 10 trajectory.txt cell_select.txt 1 mouse\n\n#### Input\n\n###### (6) species - [human/mouse/rat]\n\n#### Output\n\n        TE_result_matrix.txt\n\n## 4. Run TENET single core version\n#### Usage\n\n\tpython TENETsinglecore [expression_file_name] [trajectory_file_name] [cell_select_file_name] [history_length]\n\n#### example\n\n\tpython TENETsinglecore expression_data.csv trajectory.txt cell_select.txt 1\n\n#### Output\n\n\tTE_result_matrix.txt\n\n## 5. Downstream analysis\n\n#### (1) Reconstructing GRN\n###### Usage\n\tpython makeGRN.py [cutoff for FDR]\n\tpython makeGRNsameNumberOfLinks.py [number of links]\n\tpython makeGRNbyTF.py [species] [cutoff for FDR]\n\tpython makeGRNbyTFsameNumberOfLinks.py [species] [number of links]\n\t** Note that \"TE_result_matrix.txt\" should be in the same folder.\n\n###### Example\n\tpython makeGRN.py 0.01\n\tpython makeGRNsameNumberOfLinks.py 1000\n\tpython makeGRNbyTF.py human 0.01\n\tpython makeGRNbyTFsameNumberOfLinks.py human 1000\n\n###### Output file\n\tTE_result_matrix.fdr0.01.sif\n\tTE_result_matrix.NumberOfLinks1000.sif\n\tTE_result_matrix.byGRN.fdr0.01.sif\n\tTE_result_matrix.byGRN.NumberOflinks1000.sif\n\n###### Parameter\n\t[cutoff for fdr] - A cutoff value for FDR by z-test\n\t[number of links] - The number of links of the GRN\n\t[species] - User can choose [human/mouse/rat]\n\n#### (2) Trimming indirect edges\n###### Usage\n\tpython trim_indirect.py [name of GRN] [cutoff]\n###### Example\n\tpython trim_indirect.py TE_result_matrix.fdr0.01.sif 0\n###### Output file\n\tTE_result_matrix.fdr0.01.trimIndirect0.0.sif\n###### Parameter\n\t[cutoff] - A cutoff value for trimming indirect edges. Recommended range is -0.1 to 0.1\n\n#### (3) Counting out-degree of a given GRN\n###### Usage\n\tpython countOutdegree.py [name of GRN]\n###### Example\n\tpython countOutdegree.py TE_result_matrix.fdr0.01.sif\n###### Output file\n\tTE_result_matrix.fdr0.01.sif.outdegree.txt\n",
    "readme_length": 4644
  },
  {
    "name": "GRGNN",
    "full_name": "juexinwang/GRGNN",
    "description": "Gene Regulatory Graph Neural Networks",
    "stars": 36,
    "forks": 8,
    "language": "Python",
    "url": "https://github.com/juexinwang/GRGNN",
    "topics": [],
    "created_at": "2019-09-22T14:34:50Z",
    "updated_at": "2025-09-12T12:25:24Z",
    "homepage": "",
    "license": "N/A",
    "readme": "GRGNN -- Gene Regulatory Graph Neural Network\n===============================================================================\n\nAbout\n-----\n\nGene regulatory graph neural network (GRGNN): an end-to-end approach to reconstruct GRNs from scratch utilizing the gene expression data, in both a supervised and a semi-supervised framework. \n\nPreprocessing script is provided, readers can use the data directly or generate the data by downloading the DREAM5 challenge data from https://www.synapse.org/#!Synapse:syn3130840\n\nRequirements\n------------\n\nTested with Python 3.7.3, Pytorch 1.12.0 on Ubuntu 16.04\n\nRequired python libraries: gensim and scipy; all python libraries required by pytorch_DGCNN are networkx, tqdm, sklearn etc.\n\nIf you want to enable embeddings for link prediction, please install the network embedding software 'node2vec' in \"software\" (if the included one does not work).\n\nInstallation\n------------\nType\n\n    bash install.sh\n\nto install the required software and libraries. [Node2vec](https://github.com/aditya-grover/node2vec) and [DGCNN](https://github.com/muhanzhang/pytorch_DGCNN) are included in software folder. \n\n\nUsages\n------\n1. Unzip DREAM5 data\n\n    cd data/dream\n\n    unzip dreamdata.zip\n\n    cd ../../\n\n2. (Optional): Preprocessing DREAM5 data\n\n    cd preprocessing\n\n    python Preprocessing_DREAM5.py 3\n\n    python Preprocessing_DREAM5.py 4\n\n3. In this program, for simple, data3 means E.coli dataset, data4 means S. cerevisae dataset\nTrain S. cerevisae and test on E. coli with default parameters, Type:\n\n    python Main_inductive_ensemble.py  --traindata-name data4 --testdata-name data3\n\nTrain S. cerevisae and test on E. coli with hop 1 and embedding, Type:\n\n    python Main_inductive_ensemble.py  --traindata-name data4 --testdata-name data3 --hop 1 --use-embedding\n\nTrain E. coli and test on S. cerevisae with hop 1 and embedding, Type:\n\n    python Main_inductive_ensemble.py  --traindata-name data3 --testdata-name data4 --hop 1 --use-embedding\n\n\nReferences:\n------------\n1. SEAL code: https://github.com/muhanzhang/SEAL\n2. Dream data: http://dreamchallenges.org/project/dream-5-network-inference-challenge/ \n\n",
    "readme_length": 2147
  },
  {
    "name": "GRouNdGAN",
    "full_name": "Emad-COMBINE-lab/GRouNdGAN",
    "description": "A causal implicit generative model for simulating single-cell RNA-seq data guided by a gene regulatory network ðŸ§¬",
    "stars": 35,
    "forks": 5,
    "language": "Python",
    "url": "https://github.com/Emad-COMBINE-lab/GRouNdGAN",
    "topics": [],
    "created_at": "2023-07-25T11:04:58Z",
    "updated_at": "2025-11-27T21:28:01Z",
    "homepage": "https://emad-combine-lab.github.io/GRouNdGAN/",
    "license": "GNU Affero General Public License v3.0",
    "readme": "# <img src=\"https://github.com/Emad-COMBINE-lab/GRouNdGAN/blob/master/docs/_static/logo.svg\" width=\"250\"> \r\n \r\n\r\n_**GR**N-guided in silic**o** sim**u**lation of single-cell R**N**A-seq **d**ata using Causal **G**enerative **A**dversarial **N**etworks_\r\n\r\n[![Website](https://img.shields.io/website?url=https%3A%2F%2Femad-combine-lab.github.io%2FGRouNdGAN%2F)](https://emad-combine-lab.github.io/GRouNdGAN/)\r\n[![CI](https://github.com/Emad-COMBINE-lab/GRouNdGAN/actions/workflows/documentation.yaml/badge.svg?branch=master)](https://github.com/Emad-COMBINE-lab/GRouNdGAN/actions)\r\n[![Docker build status](https://img.shields.io/github/actions/workflow/status/Emad-COMBINE-lab/GRouNdGAN/docker-build.yml?logo=docker&label=docker%20build)](https://github.com/Emad-COMBINE-lab/GRouNdGAN/actions/workflows/docker-build.yml)\r\n[![Docker Image Size with architecture (latest by date/latest semver)](https://img.shields.io/docker/image-size/yazdanz/groundgan?logo=docker)](https://hub.docker.com/r/yazdanz/groundgan)\r\n<a href=\"https://github.com/psf/black\"><img alt=\"Code style: black\" src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"></a>\r\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.11068246.svg)](https://doi.org/10.5281/zenodo.11068246)\r\n\r\n---\r\nImplementation of GRouNdGAN as described in:\r\n> Zinati, Y., Takiddeen, A. & Emad, A. GRouNdGAN: GRN-guided simulation of single-cell RNA-seq data using causal generative adversarial networks. Nat Commun 15, 4055 (2024). https://doi.org/10.1038/s41467-024-48516-6\r\n\r\n\r\n## Simulated Datasets and Ground Truth GRNs\r\nSimulated dataset and their underlying ground truth GRNs are available for download on [our website](https://emad-combine-lab.github.io/GRouNdGAN/benchmarking.html#bonemarrow-paul-et-al-2015).\r\n\r\n\r\n## Tutorials and Documentation\r\nFor a detailed tutorial and comprehensive API references, please visit our project's documentation [here](https://Emad-COMBINE-lab.github.io/GRouNdGAN/).\r\n\r\n## BibTex Citation\r\n```\r\n@article{zinati2024groundgan,\r\n  title={GRouNdGAN: GRN-guided simulation of single-cell RNA-seq data using causal generative adversarial networks},\r\n  author={Zinati, Yazdan and Takiddeen, Abdulrahman and Emad, Amin},\r\n  journal={Nature Communications},\r\n  volume={15},\r\n  number={1},\r\n  pages={1--18},\r\n  year={2024},\r\n  publisher={Nature Publishing Group}\r\n}\r\n```\r\n\r\n## License \r\nCopyright (C) 2023 Emad's COMBINE Lab: Yazdan Zinati, Abdulrahman Takiddeen, and Amin Emad. \r\n\r\nGRouNdGAN is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\r\n\r\nGRouNdGAN is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.\r\n\r\nYou should have received a copy of the GNU Affero General Public License along with GRouNdGAN. If not, see <https://www.gnu.org/licenses/>.\r\n",
    "readme_length": 3095
  },
  {
    "name": "velorama",
    "full_name": "rs239/velorama",
    "description": "Gene regulatory network inference for RNA velocity and pseudotime data",
    "stars": 27,
    "forks": 4,
    "language": "Python",
    "url": "https://github.com/rs239/velorama",
    "topics": [],
    "created_at": "2022-10-18T16:10:52Z",
    "updated_at": "2025-11-06T03:08:10Z",
    "homepage": null,
    "license": "MIT License",
    "readme": null,
    "readme_length": 0
  },
  {
    "name": "scDoRI",
    "full_name": "bioFAM/scDoRI",
    "description": "scDoRI (single-cell Deep multi-Omic Regulatory Inference) is a deep learning model to infer enhancer-mediated gene regulatory networks (eGRNs) from single-cell multiome data (RNA + ATAC).",
    "stars": 26,
    "forks": 7,
    "language": "Python",
    "url": "https://github.com/bioFAM/scDoRI",
    "topics": [],
    "created_at": "2025-04-09T16:27:36Z",
    "updated_at": "2025-11-18T01:20:36Z",
    "homepage": "https://scdori.readthedocs.io/en/latest/",
    "license": "MIT License",
    "readme": "# scDoRI: Single-cell Deep Multi-Omic Regulatory Inference\n\n[![Documentation Status](https://readthedocs.org/projects/scdori/badge/?version=latest)](https://scdori.readthedocs.io/en/latest/)\n[![codecov](https://codecov.io/gh/bioFAM/scDoRI/graph/badge.svg?token=RK6G4LBUHL)](https://codecov.io/gh/bioFAM/scDoRI)\n\n![graphical abstract](https://raw.githubusercontent.com/bioFAM/scDoRI/refs/heads/main/docs/_static/scdori_schematic_main.png)\n\n**scDoRI** is a deep learning model for single-cell **multiome** data (RNA + ATAC in same cell) that infers **enhancer-mediated gene regulatory networks (eGRNs)**. By combining an **encoderâ€“decoder** approach with mechanistic constraints (enhancerâ€“gene links, TF binding logic), scDoRI learns **topics** that group co-accessible peaks, their cis-linked genes and upstream activator and repressor TFs â€“ all while scaling to large datasets via mini-batches.\n\n## Highlights\n- ðŸ”„ **Unified** approach: a single model for dimensionality reduction + eGRN inference\n- ðŸ§  Learns **topics** that represent cell-state-specific regulatory programs\n- ðŸ§¬**Continuous eGRN modelling** : each cell is a mixture of topics, allowing the study of changes in GRNs. No need for predefined clusters\n- ðŸ§° **Scalable** to large datasets via **mini-batch training**\n\n## Input Requirements\n\nscDoRI expects **single-cell multiome data** with the following inputs:\n\n- `RNA`: an AnnData `.h5ad` object with **cells Ã— genes** expression matrix\n- `ATAC`: an AnnData `.h5ad` object with **cells Ã— peaks** accessibility matrix\n  - Peaks must include genomic coordinates in `.var` (columns: `chr`, `start`, `end`)\n\nThese datasets must be paired â€” i.e., RNA and ATAC should come from the **same cells**. Make sure that **.X** of anndata objects contain raw counts.\n\n\n## Installation\n\nTo install all dependencies for scDoRI, we recommend using [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) or [Micromamba](https://mamba.readthedocs.io/en/latest/installation/micromamba-installation.html). The installation should take a couple of minutes.\n\n\n```bash\ngit clone https://github.com/bioFAM/scDoRI.git\ncd scDoRI\n\n# create a new environment\nconda create -n scdori python=3.12 -y\nconda activate scdori\n\n# Install BEDTools dependencies, e.g., via conda (if not installed on your system)\nconda install --channel conda-forge --channel bioconda bedtools htslib -y\n\n# Install the scDoRI package\npip install -e .\n```\n\nMake sure the `-e` flag is used to install the package in editable mode.\n\nThe training process is GPU-accelerated and **highly recommended** to be run on a GPU-enabled machine. While preprocessing can run on CPU, training large datasets on CPU is not advised due to slow performance. If a GPU is available, the GPU-accelerated version will be installed automatically.\n\n\n## Usage\nYouâ€™ll work through three notebooks - Preprocessing, Training and Downstream analysis. This will require two separate config files to set parameters for your dataset preprocessing and training.\n### Step 1: Preprocessing\n#### Edit paths and parameters in:\n```bash\nsrc/scdori/pp/config.py\n```\nto specify the location of RNA and ATAC anndata .h5ad files, motif file, and set number of peaks/genes/TFs to train on.\n#### Run preprocessing notebook\n```bash\ndocs/notebooks/preprocessing.ipynb\n```\nThis step can take between a few hours to a day depending on the dataset size and number of features selected.\n### Step 2: Training\n\n#### Edit paths and parameters in:\n```bash\nsrc/scdori/_core/config.py\n```\nfor scDoRI hyperparameters (number of topics, learning rate, epochs etc.) and specify path for preprocessed anndata objects and insilico-chipseq files\n#### Run training notebook\n```bash\ndocs/notebooks/training.ipynb\n```\nThis step again can take between a few hours to a day depending on the dataset size and number of features selected.\n### Step 3: Downstream analysis\n\n#### Run downstream analysis notebook\n```bash\ndocs/notebooks/downstream.ipynb\n```\n\n## Dataset Demonstration\n\nThe provided notebooks use the **mouse gastrulation dataset** from:\n\nðŸ“„ Paper: [Argelaguet et al., Bioarxiv 2022](https://www.biorxiv.org/content/10.1101/2022.06.15.496239v1)\nðŸ“¦ Download: [Dropbox link](https://www.dropbox.com/scl/fo/9inmw43pz2bygtqepxl82/ALeeNjuEqw4qp0L9Z9t71xo/data/processed?rlkey=5ihgkvafegkke9jnldlnhw1x6&subfolder_nav_tracking=1&st=cixvwynt&dl=0)\n\n## Configuration Notes\n\n`preprocessing_pipeline/config.py` provides flexible options:\n\n- You can **set the number of peaks, genes, and TFs** to use for model training\n  - ðŸ’¡ Tip: Adjust based on your available **GPU memory**\n- You can also **force inclusion of specific genes or TFs**, even if they arenâ€™t highly variable\n  - Useful for focusing on known regulators/ genes of interest\n\n## Documentation\nðŸ“– Full documentation and API reference is hosted at: https://scdori.readthedocs.io/en/latest/\n\nIncludes:\n- API reference (docstrings)\n- In-depth method overview\n- Preprocessing + training guides\n- (upcoming) Customization tips\n\n## Citation\nIf you use scDoRI in your work, please cite our [preprint](https://www.biorxiv.org/content/10.1101/2025.05.13.653733v1).\nFeel free to open an issue or get in touch at manu.saraswat@dkfz.de\n",
    "readme_length": 5199
  },
  {
    "name": "RegDiffusion",
    "full_name": "TuftsBCB/RegDiffusion",
    "description": "Diffusion model for gene regulatory network inference. ",
    "stars": 23,
    "forks": 5,
    "language": "Python",
    "url": "https://github.com/TuftsBCB/RegDiffusion",
    "topics": [
      "diffusion-models",
      "gene-regulatory-network",
      "grn",
      "scenic",
      "single-cell"
    ],
    "created_at": "2023-11-02T19:36:47Z",
    "updated_at": "2025-11-02T04:06:53Z",
    "homepage": "https://tuftsbcb.github.io/RegDiffusion/",
    "license": "Apache License 2.0",
    "readme": "# RegDiffusion <a href=\"https://tuftsbcb.github.io/RegDiffusion/\"><img src=\"https://raw.githubusercontent.com/TuftsBCB/RegDiffusion/master/docs/_static/rd_logo_horizontal.png\" align=\"right\" alt=\"logo\" width=\"200\" height = \"56\" style = \"border: none; float: right;\"></a>\n\n[![Downloads](https://static.pepy.tech/badge/regdiffusion)](https://pepy.tech/project/regdiffusion)\n[![Downloads](https://static.pepy.tech/badge/regdiffusion/month)](https://pepy.tech/project/regdiffusion)\n![PyPI - Version](https://img.shields.io/pypi/v/regdiffusion)\n\nRegDiffusion is a very fast unsupervised regulatory network inference algorithm (just like GENIE3 and GRNBoost2), based on probabilistic diffusion model. It works well on genes and is capable to rapidly (<5min) predict biologically verifiable links from large single cell RNA-seq data with 14,000+ genes.\n\n```\nZhu H, Slonim D. From Noise to Knowledge: Diffusion Probabilistic Model-Based Neural Inference of Gene Regulatory Networks. J Comput Biol. 2024 Nov;31(11):1087-1103. doi: 10.1089/cmb.2024.0607. Epub 2024 Oct 10. PMID: 39387266; PMCID: PMC11698671.\n```\n\n![](https://raw.githubusercontent.com/TuftsBCB/RegDiffusion/master/resources/regdiffusion_structure.png)\n\n## Installation\n\nRegDiffusion is on pypi.\n\n```\npip install regdiffusion\n```\n\nCheck out the [this tutorial](https://tuftsbcb.github.io/RegDiffusion/quick_tour.html) for a quick tour of how to use RegDiffusion! If you would like to integrate results from RegDiffusion into the SCENIC pipeline, checkout [this tutorial](https://tuftsbcb.github.io/RegDiffusion/downstream_with_pyscenic.html). \n\n## Inferred Networks from RegDiffusion\nHere are two examples of inferred networks from regdiffusion. The networks are coherent with existing literature and across datasets. \n\n![](https://raw.githubusercontent.com/TuftsBCB/RegDiffusion/master/resources/apoe_net.png)\n\n## Inference Speed\nInference on networks with 15,000 genes takes under 5 minutes on an A100 GPU. \nIn contrast, previous VAE based models would take more than 4 hours on the same \ndevice. Even if you don't have access to those fancy GPU cards, RegDiffusion \nstill works. Inference on the same large network takes roughly 3 hours on a \nmid-range 12-core CPU. \n\n## CLI tool\nregdiffusion has a CLI tool now! It takes a count matrix as the input (different from the main API, which needs the data to be log transformed) and returns a table of inferred edges. \n\n```\nusage: regdiffusion [-h] [--output OUTPUT] [--top_gene_percentile TOP_GENE_PERCENTILE] [--k K] [--workers WORKERS] input\n\nInfer a gene regulatory network (GRN) from a single-cell count dataset.\n\npositional arguments:\n  input                 Input single-cell count dataset file (CSV or H5AD format).\n\noptions:\n  -h, --help            show this help message and exit\n  --output OUTPUT       Output file path for the edgelist (CSV). Default: rd_grn.csv\n  --top_gene_percentile TOP_GENE_PERCENTILE\n                        Percentile cutoff to filter weak edges (e.g., 50 for the top 50%). Default: 50\n  --k K                 Number of edges per gene to extract (-1 for all edges). Default: -1\n  --workers WORKERS     Number of workers to use for edgelist extraction. Default: 4\n```\n\n## Citation \n\nIf you find our package useful, consider cite our paper! =)\n\n```\n@article{zhu2024noise,\n  title={From Noise to Knowledge: Diffusion Probabilistic Model-Based Neural Inference of Gene Regulatory Networks},\n  author={Zhu, Hao and Slonim, Donna},\n  journal={Journal of Computational Biology},\n  volume={31},\n  number={11},\n  pages={1087--1103},\n  year={2024},\n  publisher={Mary Ann Liebert, Inc., publishers 140 Huguenot Street, 3rd Floor New~â€¦}\n}\n```",
    "readme_length": 3669
  },
  {
    "name": "DeepGRN",
    "full_name": "jianlin-cheng/DeepGRN",
    "description": "Deep learning for modeling gene regulatory network",
    "stars": 22,
    "forks": 6,
    "language": "Python",
    "url": "https://github.com/jianlin-cheng/DeepGRN",
    "topics": [],
    "created_at": "2017-09-27T19:19:44Z",
    "updated_at": "2025-10-10T16:24:43Z",
    "homepage": null,
    "license": "N/A",
    "readme": "# DeepGRN\r\nDeep learning for modeling gene regulatory network\r\n\r\nLicense\r\n-------\r\nÂ© Contributors, 2019. Licensed under an [Apache-2] license.\r\n\r\nContribute\r\n---------------------\r\nDeepGRN has been developed and used by the Bioinformatics, Data Mining and Machine Learning Laboratory (BDM)\r\n. Help from every community member is very valuable to make the tool better for everyone.\r\nCheckout the [Lab Page](http://calla.rnet.missouri.edu/cheng/).\r\n\r\n## Required software:\r\n\r\n```\r\nPython 3.6.6\r\nbedtools 2.26.0\r\n```\r\n\r\n## Required Python modules:\r\n\r\n```\r\npandas 0.24.2\r\nnumpy 1.16.2\r\ntensorflow 1.11.0\r\npyfaidx 0.5.5.2\r\npyfasta 0.5.2\r\npybedtools 0.8.0\r\npyBigWig 0.3.14\r\nKeras 2.2.4\r\nh5py 2.9.0\r\ndeepTools 3.2.1\r\n```\r\n\r\nOptional(for training with GPUs):\r\n\r\n```\r\ncuda 10\r\ntensorflow-gpu 1.11.0\r\n```\r\n\r\n## Training (train.py)\r\nTrain models for TF binding site prediction:\r\n\r\n### Arguments\r\n\r\n  * `-h, --help`            show this help message and exit\r\n  * `--data_dir DATA_DIR, -i DATA_DIR`\r\n                        path to the input data (required)\r\n  * `--tf_name TF_NAME, -t TF_NAME`\r\n                        name of the transcription factor (required)\r\n  * `--output_dir OUTPUT_DIR, -o OUTPUT_DIR`\r\n                        output path (required)\r\n  * `--genome_fasta_file GENOME_FASTA_FILE, -gf GENOME_FASTA_FILE`\r\n                        genome fasta file (required)\r\n  * `--val_chr VAL_CHR, -v VAL_CHR`\r\n                        name for validation chromosome (default: chr11)\r\n  * `--bigwig_file_unique35 BIGWIG_FILE_UNIQUE35, -u BIGWIG_FILE_UNIQUE35`\r\n                        35bp uniqueness file, will not use this feature if left empty (default: '')\r\n  * `--rnaseq_data_file RNASEQ_DATA_FILE, -r RNASEQ_DATA_FILE`\r\n                        RNA-Seq PCA data file, will not use this feature if left empty (default: '')\r\n  * `--gencode_file GENCODE_FILE, -g GENCODE_FILE`\r\n                        Genomic annotation file, will not use this feature if left empty (default: '')\r\n  * `--attention_position ATTENTION_POSITION, -ap ATTENTION_POSITION`\r\n                        Position of attention layers, can be attention_after_lstm, attention_before_lstm,attention_after_lstm,attention1d_after_lstm (default: 'attention_after_lstm')\r\n  * `--flanking FLANKING, -f FLANKING`\r\n                        flanking length (default: 401)\r\n  * `--epochs EPOCHS, -e EPOCHS`\r\n                        epochs (default: 60)\r\n  * `--patience PATIENCE, -p PATIENCE`\r\n                        training will stop early if no improvements after n epochs (default: 5)\r\n  * `--batch_size BATCH_SIZE, -s BATCH_SIZE`\r\n                        batch size (default: 64)\r\n  * `--learningrate LEARNINGRATE, -l LEARNINGRATE`\r\n                        learningrate (default: 0.001)\r\n  * `--kernel_size KERNEL_SIZE, -k KERNEL_SIZE`\r\n                        kernel size for Conv1D (default: 34)\r\n  * `--num_filters NUM_FILTERS, -nf NUM_FILTERS`\r\n                        number of filters for Conv1D (default: 64)\r\n  * `--num_recurrent NUM_RECURRENT, -nr NUM_RECURRENT`\r\n                        Output dim for LSTM (default: 32)\r\n  * `--num_dense NUM_DENSE, -nd NUM_DENSE`\r\n                        Output dim for dense layers (default: 64)\r\n  * `--dropout_rate DROPOUT_RATE, -d DROPOUT_RATE`\r\n                        dropout rate for all layers except LSTM (default: 0.1)\r\n  * `--rnn_dropout1 RNN_DROPOUT1, -rd1 RNN_DROPOUT1`\r\n                        dropout rate for LSTM (default: 0.1)\r\n  * `--rnn_dropout2 RNN_DROPOUT2, -rd2 RNN_DROPOUT2`\r\n                        RNN dropout rate for LSTM (default: 0.1)\r\n  * `--merge MERGE, -me MERGE`\r\n                        merge method, max or ave (default: ave)\r\n  * `--num_conv NUM_CONV, -nc NUM_CONV`\r\n                        Number of Conv1D layers (default: 1)\r\n  * `--num_lstm NUM_LSTM, -nl NUM_LSTM`\r\n                        Number of LSTM layers (default: 1)\r\n  * `--num_denselayer NUM_DENSELAYER, -dl NUM_DENSELAYER`\r\n                        Number of additional dense layers (default: 1)\r\n  * `--ratio_negative RATIO_NEGATIVE, -rn RATIO_NEGATIVE`\r\n                        Ratio of negative samples to positive samples in each epoch (default: 1)\r\n  * `--use_peak, -a`        should the positive bins sampled from peak regions? (default: OFF)\r\n  * `--use_cudnn, -c`       use cudnnLSTM instead of LSTM, faster but will disable LSTM dropouts (default: OFF)\r\n  * `--single_attention_vector, -sa`\r\n                        merge attention weights in each position by averaging (default: OFF)\r\n  * `--positive_weight POSITIVE_WEIGHT, -pw POSITIVE_WEIGHT`\r\n                        weight for positive samples (default: 1)\r\n  * `--plot_model, -pl`     if the model architecture should be plotted (default: OFF)\r\n  * `--random_seed RANDOM_SEED, -rs RANDOM_SEED`\r\n                        random seed (default: 0)\r\n  * `--val_negative_ratio VAL_NEGATIVE_RATIO, -vn VAL_NEGATIVE_RATIO`\r\n                        ratio for negative samples in validation (default: 19)\r\n\r\n## Prediction (predict.py)\r\n\r\nUse models trained from train.py for new data prediction:\r\n\r\n### Arguments\r\n\r\n  * `-h, --help`            show this help message and exit\r\n  * `--data_dir DATA_DIR, -i DATA_DIR`\r\n                        path to the input data (required)\r\n  * `--model_file MODEL_FILE, -m MODEL_FILE`\r\n                        path to model file (required)\r\n  * `--cell_name CELL_NAME, -c CELL_NAME`\r\n                        cell name (required)\r\n  * `--predict_region_file PREDICT_REGION_FILE, -p PREDICT_REGION_FILE`\r\n                        predict region file (required)\r\n  * `--output_predict_path OUTPUT_PREDICT_PATH, -o OUTPUT_PREDICT_PATH`\r\n                        output path of prediction (required)\r\n  * `--bigwig_file_unique35 BIGWIG_FILE_UNIQUE35, -bf BIGWIG_FILE_UNIQUE35`\r\n                        35bp uniqueness file  (default: '')\r\n  * `--rnaseq_data_file RNASEQ_DATA_FILE, -rf RNASEQ_DATA_FILE`\r\n                        RNA-Seq PCA data file  (default: '')\r\n  * `--gencode_file GENCODE_FILE, -gc GENCODE_FILE`\r\n                        Genomic annotation file (default: '')\r\n  * `--batch_size BATCH_SIZE, -b BATCH_SIZE`\r\n                        batch size  (default: 512)\r\n  * `--blacklist_file BLACKLIST_FILE, -l BLACKLIST_FILE`\r\n                        blacklist_file to use, no fitering if not provided  (default: '')\r\n\r\nTo generate the figures that we use in our experiment, please refer to [these instructions](analysis/README.md) to extract data from trained models and create the plot you are interested in.\r\n\r\n## Prepare input features for training and prediction\r\n\r\nThe following sections are for users who wish to generate their own training/prediction dataset. If you are interested in the DREAM-ENCODE Challenge 2016 data that we use in our experiment, we have prepared the [step by step guideline](example/README.md) to generate the input for training and prediction.\r\n\r\n### Genomic sequence\r\n\r\nGenomic sequence is provided as fasta format. You can download these files from [UCSC Genome Browser](https://hgdownload.soe.ucsc.edu/downloads.html)\r\nPlease notice that if your validation chromosome name should be one of the chromosome names in this file.\r\n\r\n### Chromatin accessibility data\r\n\r\nChromatin accessibility data should be prepared as BigWig format from DNase-Seq experiment. You should name your files as {cell_type}.1x.bw and put them into your/data/folder/DNase/  That is to say, one file for each cell type. These files should be generated from the read alignment files in the standard BAM file format. If you have replicates for the same cell type, you should first merge them with samtools:\r\n\r\n```\r\nsamtools merge {cell_type}.bam {cell_type}.rep1.bam {cell_type}.rep2.bam\r\nsamtools index {cell_type}.bam\r\n```\r\n\r\nThen you can run the bamCoverage in [deeptools](https://deeptools.readthedocs.io/en/develop/content/tools/bamCoverage.html) to generate the required .bw file for each cell type. Here we use human genome as example:\r\n\r\n`bamCoverage --bam ${i}.bam -o ${i}.1x.bw --outFileFormat bigwig --normalizeTo1x 2478297382 --ignoreForNormalization chrX chrM --Offset 1 --binSize 1  --blackListFileName blacklist.bed.gz --skipNonCoveredRegions`\r\n\r\nYou can provide a BED or GTF file containing regions that should be excluded from bamCoverage analyses with the `--blackListFileName` option. For human genome hg19, we use the [low mapability region provided by UCSC Genome Browser](http://hgdownload.cse.ucsc.edu/goldenpath/hg19/encodeDCC/wgEncodeMapability/wgEncodeDacMapabilityConsensusExcludable.bed.gz) \r\n\r\n### Sequence uniqueness data [optional]\r\nAccording to [UCSC Genome Browser](http://genome.ucsc.edu/cgi-bin/hgTrackUi?db=hg18&g=wgEncodeMapability). We use the Duke uniqueness score as an additional input. The Duke excluded regions track displays genomic regions for which mapped sequence tags were filtered out before signal generation and peak calling for Duke/UNC/UTA's Open Chromatin tracks. This track contains problematic regions for short sequence tag signal detection (such as satellites and rRNA genes). The Duke excluded regions track was generated for the ENCODE project.\r\n\r\nThe Duke uniqueness score tracks display how unique is each sequence on the positive strand starting at a particular base and of a particular length. Thus, the 35 bp track reflects the uniqueness of all 35 base sequences with the score being assigned to the first base of the sequence. Scores are normalized to between 0 and 1 with 1 representing a completely unique sequence and 0 representing the sequence occurs >4 times in the genome (excluding chrN_random and alternative haplotypes). A score of 0.5 indicates the sequence occurs exactly twice, likewise 0.33 for three times and 0.25 for four times. For Human genome, you can use the [Duke uniqueness tracks](http://hgdownload.soe.ucsc.edu/goldenPath/hg18/encodeDCC/wgEncodeMapability/wgEncodeDukeUniqueness35bp.wig.gz)  generated for the ENCODE project as tools in the development of the Open Chromatin tracks. For other genomes, you can generate your own sequence uniqueness data following this rule and use [wigToBigWig](https://www.encodeproject.org/software/wigtobigwig/) to convert it to the BigWig format.\r\n\r\n### Gene expression profile [optional]\r\nFollwing the [FactorNet](https://www.sciencedirect.com/science/article/pii/S1046202318303293) framework, we use the first eight principal components from the RNA-Seq experiments for each cell type. These principal components should be generated using the TPM(Transcripts Per Kilobase Million) with replicates merged by averging. You should save this input in your/data/folder/rnaseq_data.csv as a Comma Separated Values file with the first line indicate the type of the cells.\r\n\r\nWe provided a simple R script data/generate_pca.R to generate this data from TPM data. The input of this script should be a csv file containing a gene by cell type matrix with first row indicate the types of cell and the first column indicate the gene names. Usage:\r\n\r\n`Rscript generate_pca.R [path/to/input.csv] [path/to/rnaseq_data.csv]`\r\n\r\n### Genomic annotations [optional]\r\nThe annotation feature for each bin is encoded as a binary vector of length 6, with each value representing if there is an overlap between the input bin and each of the six genomic features (coding regions, intron, promoter, 5'/3'-UTR, and CpG island). For Human genome Hg19, we use annotations from the [FactorNet GitHub repo](https://github.com/uci-cbcl/FactorNet/tree/master/resources) to generate the required input and save them in example/hg19. For preparing this input for your own genome, you can use our python script: data/generate_annotation.py Usage:\r\n\r\n`Python generate_annotation.py [gencode_path] [genome_sizes_file] [bed_file] [outfile]`\r\n\r\nYou should prepare your own (gzipped) bed files indicting the genomic features (coding regions, intron, promoter, 5'/3'-UTR, and CpG island) and save them as [cpgisland|cds|intron|promoter|utr5|utr3].bed.gz under the `gencode_path`. \r\n\r\n`genome_sizes_file` is the file containing size of each chromosome. For Hg19 the content is:\r\n\r\n```\r\nchr1\t249250621\r\nchr10\t135534747\r\nchr11\t135006516\r\nchr12\t133851895\r\nchr13\t115169878\r\nchr14\t107349540\r\nchr15\t102531392\r\nchr16\t90354753\r\nchr17\t81195210\r\nchr18\t78077248\r\nchr19\t59128983\r\nchr2\t243199373\r\nchr20\t63025520\r\nchr21\t48129895\r\nchr22\t51304566\r\nchr3\t198022430\r\nchr4\t191154276\r\nchr5\t180915260\r\nchr6\t171115067\r\nchr7\t159138663\r\nchr8\t146364022\r\nchr9\t141213431\r\nchrX\t155270560\r\n```\r\n\r\n`bed_file` is the file indicating the target region and should be the same as the one you use for the prediction script.\r\n\r\n## Prepare label data for custom training\r\n\r\nIn additional to the input feature information mentioned in [Prepare input features for training and prediction](#Prepare-input-features-for-training-and-prediction), you need to prepare the label information for training.\r\n\r\n### label data\r\n\r\nIf you need to train your own models with custom data, instead of preparing a file indicating the target region, you should prepare a label file indicating the true labels of the binding status along the chromosome. In the DREAM-ENCODE challange, each bin falls into one of the three types: bound(B), unbound(U), or ambiguous(A), which is determined from the ChIP-Seq results. Bins overlapping with peaks and passing the Irreproducible Discovery Rate (IDR) check with a threshold of 5%  are labeled as bound. Bins that overlap with peaks but fail to pass the reproducibility threshold are labeled as ambiguous. All other bins are labeled as unbound. We do not use any ambiguous bins during the training or validation process according to the common practice. Therefore, each bin in the genomic sequence will either be a positive site (bounded) or a negative site (unbounded).\r\nExample:\r\n\r\n```\r\nchr     start   stop    A549    H1-hESC HeLa-S3 HepG2   IMR-90  K562    MCF-7\r\nchr10   600     800     U       U       U       U       U       U       U\r\nchr10   650     850     U       U       U       U       U       U       U\r\nchr10   700     900     U       U       U       U       U       U       U\r\nchr10   750     950     U       U       U       U       U       U       U\r\nchr10   800     1000    U       U       U       U       U       U       U\r\n```\r\n\r\nFor each TF, you will need one such label file. You should save these (gzipped) label files under data_dir/label/train/ and name them as [TF_name].train.labels.tsv.gz\r\n\r\n### Narrowpeak files for data augmentation during training [optional]\r\n\r\nFor TFs that have abundant binding sites, training performance and speed could benefit from sampling positive sample regions from the ChIP-Seq peak regions during each epoch. To do so, you would need to prepare a peak annotation file. We have provided the peak annotations as example/hg19/peak_annotation.gz for the TF and cells we used. For your own customized input, you can use data/generate_peaks.py to generate those peak annotations. Usage:\r\n\r\n`Python generate_peaks.py [gencode_path] [genome_sizes_file] [bed_file] [outfile] [narrowPeak_path] [blacklist_file] [label_path] [label_peak_path] [genome_window_size] [flanking]`\r\n\r\n[gencode_path] [genome_sizes_file] [bed_file] [outfile] are the same as described in the \"Genomic annotations\" section. \r\n\r\nnarrowPeak_path is the path you store the ChIP-Seq narrowPeak files. In our experiment, these peaks are the reproducible peaks across pseudo-replicates that pass the 10% IDR threshold. The peaks are provided in the [narrowPeak format](https://genome.ucsc.edu/FAQ/FAQformat.html#format12). For each cell type and TF pair, the file  should be named as  ChIPseq.[train_cell].[tf_name].conservative.train.narrowPeak.gz'\r\n\r\n`blacklist_file` is the file you want to exclude during analysis and can be empty if you do not wish to miss any regions\r\n\r\n`label_path` is where the label files are stored. They should be located in data_dir/label/train/\r\n\r\n`label_peak_path` is where the output annotation files are stored. They should be located in data_dir/label/train_positive/\r\n\r\n`genome_window_size` is the size of each bin in your label files. In our experiment we set it to 200.\r\n\r\n`flanking` is the length of upstream and downstream region that around your training sample. In our experiment we set it to 401.\r\n\r\n## Prepare target region for prediction\r\n\r\nIn additional to the input feature information mentioned in [Prepare input features for training and prediction](#Prepare-input-features-for-training-and-prediction), you need to specify your target region for prediction. The region to predict should be a bed format file indicate the region you wish to predict along the chromosome. If you use our trained model, we recommend you consider use 200bp for each prediction since the models are trained using this format. Example:\r\n\r\n```\r\nchr1\t600\t800\r\nchr1\t650\t850\r\nchr1\t700\t900\r\nchr1\t750\t950\r\nchr1\t800\t1000\r\n```\r\n",
    "readme_length": 16844
  },
  {
    "name": "MINI-EX",
    "full_name": "VIB-PSB/MINI-EX",
    "description": "Motif-Informed Network Inference of cell type-specific gene regulatory networks in plants",
    "stars": 22,
    "forks": 10,
    "language": "Python",
    "url": "https://github.com/VIB-PSB/MINI-EX",
    "topics": [],
    "created_at": "2021-10-19T12:24:18Z",
    "updated_at": "2025-10-06T09:12:10Z",
    "homepage": "",
    "license": "Other",
    "readme": "# MINI-EX\n\nMotif-Informed Network Inference based on single-cell EXpression data  \n\nThe pipeline is built using Nextflow DSL2 and has the purpose of infer cell-type specific gene regulatory network using scRNA-Seq data in plants.\n\nMINI-EX uses a [dual license](https://github.com/VIB-PSB/MINI-EX/blob/main/LICENSE) to offer the distribution of the software under a proprietary model as well as an open source model.\n\n**MINI-EX v3.\\* is released!** Main features:\n* Added new metrics:\n  * transcription factor cluster specificity\n  * median coexpression score of transcription factors and their target genes\n* Updated centrality metrics:\n  * for regulons where closeness and betweenness are not defined at the cluster level (e.g., when the TF doesnâ€™t have incoming edges), the closeness and betweenness values are now computed based on the original network after motif mapping filtering\n  * per-cluster closeness and betweenness are always assigned better ranks compared to the original closeness and betweenness values\n* Improved TF prioritization in the Reference procedure (used when a list of (GO) terms of interest is specified by the user)\n* New parameter: users can now specify a custom background for the enrichment analysis (when not specified, the background defaults to the list of genes that are present in the expression matrix)\n* Expanded input options: terms of interest can now also be specified as ontology terms (e.g., GO:0009819), in addition to (or instead of) plain English words (e.g., drought)\n* The complete list of new features can be found in the release notes for [v3.0](https://github.com/VIB-PSB/MINI-EX/releases/tag/v3.0) and [v3.1](https://github.com/VIB-PSB/MINI-EX/releases/tag/v3.1)\n\n## Pipeline summary\n**1\\.** Run expression-based gene regulatory network (GRN) inference ([GRNBoost2](https://arboreto.readthedocs.io/en/latest/algorithms.html#grnboost2)) given a list of transcription factors (TFs) and a gene-to-cell count matrix<br/>\n**2\\.** Run TF binding site (TFBS) enrichment on the expression-based regulons and filter for TF or TF-Family motifs (default TF-Family)<br/>\n**3a.** Filter the previously identified regulons by target genes' (TGs) expression among the defined cell clusters (cluster specificity) using the provided markers<br/>\n**3b.** Filter the cell cluster specific regulons by TF expression<br/>\n**4a.** Calculate network statistics (out-degree, betweenness, closeness), median coexpression score, cluster specificity of TF, cluster specificity of TGs, and functional (GO) enrichment of the target genes of each regulon (if a list of GO terms is provided)<br/>\n**4b.** Generate a list of ranked regulons based on Borda ranking and generate an edge table containing edge scores\n\nFor the last step, if a list of GO terms of interest is provided:\n- First all the combinations of weighted metrics (network statistics, median coexpression score, cluster specificity of the TF, cluster specificity of TGs, and functional enrichment) are evaluated\n- The combination which returns half of the expected regulons earlier in the ranks (R50) is chosen for the weighted Borda ranking\n\nelse:\n- The network statistics, median coexpression score, cluster specificity of the TF, and cluster specificity of TGs are used to calculate the Borda ranking (calculated on the geometric mean of the single metrics)\n\n**Note**: step 2 can be omitted when no motif mapping data is available (motif mapping data is provided for Arabidopsis, rice and maize). However, use with caution as without motif data the networks will be less precise.\n\n## Detailed pipeline overview\n\n![MINI-EX_scheme](docs/MINI-EX_schema.png)\n\n## Inputs\n* Gene-to-cell count matrix (genes as rows and cells as columns)\n* List of TFs\n* [Seurat](https://satijalab.org/seurat/) output from [FindAllMarkers](https://www.rdocumentation.org/packages/Seurat/versions/3.1.2/topics/FindAllMarkers)\n* Tab-separated file containing the cluster identity of each cell (cell_barcode \\t cluster_id)\n* Tab-separated file containing the cluster annotation (cluster_id \\t cluster_annotation)\n* (Optional) List of GO terms of interest\n\nAs the pipeline can be run in parallel for multiple datasets all the inputs can be provided as a path to the dedicated directories.  \nAll input files should have specific extensions and names as shown in our [guide on data preparation](docs/data_preparation.md).  \n\n## Outputs\n* **regulons folder** containing a tab-separated files with the inferred regulons, an edge table, info per TF, and an excel file with the ranked regulons and relative metadata\n* **figures folder** containing a clustermap reporting the distribution of the different regulons across the cell clusters, and two heatmaps showing the cell cluster specificity and DE calls of the top 150 regulons, respectively. \n* **go_enrichment folder** containing a tab-separated file with GO enrichment for the different regulons with relative statistics\n* **grnboost2 folder** containing a TF-TG tab-separated file resulted from the GRNBoost2 run\n* **Log file** containing statistics on the provided dataset(s) and information on intermediary results in the workflow\n\n##   \nA detailed overview on necessary input files and expected output files can be found [here](example/).\n\n\n## Requirements\n\n* [Nextflow](https://www.nextflow.io/)\n* [Singularity](https://sylabs.io/guides/3.0/user-guide/index.html)\n\n\n## Usage\n\nDefine paths in the config file to all the required inputs (check our guide on preparing the config file [here](docs/configuration.md)).\n\n```\nnextflow -C miniex.config run miniex.nf\n```\n \nHaving problems running MINI-EX? Check the [FAQ](docs/FAQ.md).\n\n\n## Contact and support\n\nShould you have any questions or suggestions, please send an e-mail to klaas.vandepoele@psb.vib-ugent.be.\n\nShould you encounter a bug, please [open an issue](https://github.com/VIB-PSB/MINI-EX/issues).\n\n## Citation\n\nWhen publishing results generated using MINI-EX, please cite:\n\nFerrari C., Manosalva PÃ©rez N., Vandepoele K. MINI-EX: Integrative inference of single-cell gene regulatory networks in plants. Mol Plant. 2022 Nov 7;15(11):1807-1824. doi: [10.1016/j.molp.2022.10.016](https://doi.org/10.1016/j.molp.2022.10.016). Epub 2022 Oct 27. PMID: 36307979.\n\nStaut, J., Manosalva PÃ©rez, N., Depuydt, T., Vandepoele, K., Lukicheva, S. MINI-EX version 2: cell-type-specific gene regulatory network inference using an integrative single-cell transcriptomics approach. bioRxiv. 24 Dec 2023;573246. doi: [10.1101/2023.12.24.573246](https://doi.org:10.1101/2023.12.24.573246).\n",
    "readme_length": 6543
  }
]