[
  {
    "doc_id": "b3e497ce804aaf41",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_ATACtrans_20260106143058_2008426043602112512",
    "title": "A Bayesian approach for correcting Tn5 transposition bias in ATAC-seq footprinting",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# Alignment and peak calling\n\nATAC-seq data (SRR891268) and WGS data (SRR1554094) used in this study were downloaded from NCBI. The ATAC-Seq library was generated using a human B lymphoblastoid cell line (GM12878) and  $\\mathrm{CD4^{+}}$  T cells<sup>1</sup>. The WGS library was prepared using human genomic DNA <sup>12</sup>. Reads were aligned using Bowtie2 (v2.2.4) to the hg38 reference genome using default parameters, and a MAPQ cutoff of 1 was used. Accessible regions in the ATAC-seq data were determined using MACS2 (v2.1.1 in the \"narrow\" mode using default parameters). Examination of motif instances and transposition patterns in the ATAC-seq and WGS data was restricted to these accessible genomic intervals.\n\n# Identification of motif instances\n\nNonredundant vertebrate TF motifs were downloaded from the JASPAR CORE database as position frequency matrices (PFMs), converted to the MEME format, and FIMO was used to determine instances of the motifs in the accessible genomic intervals (motif matches with p-value  $< 1$ e-04 were considered)  $^{9,13}$ . 100 bp on either side of motif matches were considered for footprinting analysis. Motifs with inadequate coverage were excluded from further analysis.\n\n# Estimating Tn5 positional bias\n\n20-base sequences flanking all read start sites mapping within the accessible peak intervals were obtained. Duplicate reads were ignored and reads mapping to the positive and negative strands were analyzed separately. Read starts mapping to the positive and negative strand were shifted by  $+/-$  4 bases respectively to account for 9-base staggered cut introduced by Tn5. WebLogo was used to visualize the consensus sequence surrounding the read start sites $^{14}$ .\n\n# Testing CENTIPDE on simulated reads\n\nBriefly, CENTIPEDE assesses the likelihood of a putative motif-containing genomic interval exhibiting a particular transposition pattern, given a prior probability of it being bound or not, using a hierarchical mixture model that considers the bound and unbound states of the motif<sup>3</sup>. The read coverage for motif-containing intervals is modeled as a negative binomial distribution, while the positional distribution of reads within individual motif-containing intervals is modeled as\n\na multinomial distribution. The product of this likelihood function and the prior probability of a motif being bound yields a posterior probability, which is used to predict whether a particular instance of a motif is bound or not.\n\nWe generated a matrix of insertion site counts for Smad3 and Tbx5, two transcription factor motifs that do not exhibit footprints (even after Seqbias correction) in the GM12878 ATAC-seq dataset. We ran CENTIPEDE using default runtime parameters without specifying PWM match scores or motif conservation scores as we were interested in direct evidence of TF occupancy based upon steric exclusion of transposition events (Figure 1). To test CENTIPEDE on cases that may yield spurious predictions of footprints, we simulated cut site counts across a set of intervals bearing non-footprinted motifs using R. The total number of cut sites per interval (coverage) was sampled from a uniform distribution ranging from zero to a maximum of 400. The intervals were divided into two sets, one with a uniform distribution of cut-sites and a second with a non-uniform (normal) distribution of cut sites centered on the motif, and the cut-sites were distributed across each interval accordingly (Figure 2). Two sets of data were simulated: (i) with no difference and (ii) with a 2-fold difference in the mean coverage between the intervals bearing a uniform and a non-uniform positional distribution, and CENTIPEDE was run as described above.\n\n# Calculation of footprint scores\n\nFor footprint visualization and downstream processing, position-adjusted positive- and negative-stranded transposition tracks were added together. To create a symmetric footprint trace, the transposition frequency track across the motif containing intervals was added to the reverse of itself and divided by 2. Volatility at each motif position was defined as the square of the differences in transposition frequencies of successive nucleotide positions (a position offset of 1). All motifs were centered and transposition volatility was calculated across each motif. As motifs vary in length, the missing values for shorter motifs were replaced by the median volatility for the particular motif. The median volatility across all motifs is depicted in Figure 4C. We found that a spline model fit using the midpoint of the motif, the ends of the motif match, and positions 19 bases away from the motif as knots was robust to extremes in transposition frequencies within the motif interval (Figure 3A). The choice of 19 bases reflects the span of the estimated Tn5 sequence bias surrounding a read start site. Further, the volatility declined to background levels over this distance (Figure 4C). The footprint score was defined as the difference between the modeled transposition frequencies at the knots corresponding to the 19-base flank and the midpoint of the motif. Footprint scores were not correlated with transposition frequencies at the spline knot positions placed at the 19-base flanks, implying that the footprint scores were not associated with coverage (Figure 3B). In this study, motifs with a footprint Z-score  $>1$  were considered to be footprinted.\n\n# Seqbias\n\nFor each motif, a BED file containing motif matches expanded by 100 bp on either side was imported into Seqbias, along with the BAM alignment files from the ATAC-seq and WGS datasets. Transposition events in these intervals, were estimated by counting the read start sites using the 'binary=TRUE' mode, where a position has a count of 0 if no read maps to it and 1 if at least one does; this counting scheme produces a more robust footprint estimate, devoid of amplification bias. The Seqbias model used for correcting ATAC-seq read counts was trained on the ATAC-seq data, with sequences 20 bases on either site of each read start site considered for the model. For adjusting the WGS read counts, a separate Seqbias model was trained on reads mapping to chromosome 22, comprising  $\\sim 3.6$  million mapped reads. Since the performance of Seqbias appears to level off at  $\\sim 10^{5}$  reads, the model trained on chromosome 22 can be expected to perform comparably to a model built using all reads in the dataset  $^{11}$ . Given that the reads mapping to the positive and negative strands exhibit an identical bias, all reads can be corrected with the same model. We also compared the use of a common Seqbias model trained on read start sites exclusive to chromosome 22 with motif-specific models trained on WGS reads limited to the intervals containing matches for each motif across the entire genome, but did not obtain in any further improvement in performance (data not shown). The Seqbias models used for bias correction are available as YAML files (Supplementary Data). The overall approach for Seqbias-corrected TF footprints is summarized in the flowchart below (Figure S3).\n\n# FIGURES\n\n# FIGURE 1: Transposition patterns surrounding motifs in ATAC-Seq accessible intervals deemed footprinted by CENTIPDE\n\nTransposition pattern around all instances of Smad3 (2383 matches) and Tbx5 (6830 matches) motif matches in ATAC-Seq accessible intervals (+/- 100 bp were considered). Transposition probability is calculated as the proportion of intervals carrying a transposition event at the given position relative to the motif match. The lower panel shows the relationship between CENTIPDE posterior probability (CentPostPr) of footprinting and read coverage (i.e. number of transposition events) per motif-containing interval. Despite no visual indication of diminished transposition at the motif matches, over  $90\\%$  of motif-containing intervals are deemed footprinted (CentPostPr = 1) by the CENTIPDE algorithm.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/570f359dfdec33266a2b359946d35712d957e690fde98bc5984d2ce8347c4af9.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/2c7b549ba951baca11c465b519a8fde34a9a1b506830449d2395d2bf7e91bae1.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/455a7c2b01ecde73cd62ebda2f8a959403f836c9bbb2f6213f77a6b762c7527f.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/a897deeebcf75a9faf26dc039c9a36cd15705b0dd19bb4540898e4eaafb6b841.jpg)\n\n# FIGURE 2: CENTIPDE footprint estimates do not reflect steric occlusion at motif matches.\n\n(a) Schematic depiction of ATAC-seq transposition sites surrounding a motif that would be deemed footprinted (solid line) or not footprinted (dashed line). The positional distribution of cut sites surrounding a non-footprinted motif in ATAC-seq data can be approximated by a normal distribution centered on the motif.\n\n(b) Simulated data of cut site counts across 2000 intervals bearing a non-footprinted motif to test CENTIPDE footprint prediction is shown. The cut sites in each interval were drawn from a normal distribution centered on the motif site (pink) in the top half, and from a uniform distribution (blue) in the lower half. Each row represents cut sites across an interval. The total cut site coverage for each interval was randomly sampled from a uniform distribution ranging from 0 to 400 cuts. Two sets of data were simulated: (i) with no difference and (ii) with a 2-fold difference in the mean coverage between the intervals bearing a uniform and a non-uniform positional distribution. Only the former is shown here.\n\n(c) CENTIPEDE predicts non-uniform intervals (pink) as having a greater posterior probability (centPostPr) of being footprinted than uniform intervals (blue), despite all intervals being explicitly modeled as not exhibiting any footprint. Moreover, higher probabilities are calculated for intervals with greater coverage. This effect is especially pronounced when the non-uniformly distributed intervals have a higher coverage. Two simulated data sets are shown: (i) with no difference (top panel) and (ii) with a 2-fold difference (lower panel) in the mean coverage between the intervals bearing a uniform and a non-uniform cut site distribution.\n\na\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/1723bf5c7ead8ed032291d289a4fc4e78461174da1f58335fba84974949006ff.jpg)\n\nC\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/258582b48c3e78f0db17ed4976f811d52e74dc75d2b0cfa2ca896afbbde71a1e.jpg)\n\nb\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/44f74082356b3030dd7482f93fe44ceff2efec329904642fc5afcee2176eb08c.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/f2bd60a9b0df62b54307c2e5c12eb212b79feeee4786752c43e1207c4e5b1bf8.jpg)\n\n# FIGURE 3: Calculation of footprint scores\n\n(a) A spline function was fit to get a smoothed estimate of position-specific cut-site probabilities with knots placed at the midpoint of the motif, the ends of the motif match, and  $+/-$  19 bases from the motif. The magnitude of the footprint was defined as the difference (green bar) in the cut-site probability at the knots placed in the flanking intervals and the center of the motif. (b) Footprint scores are not correlated with transposition frequencies at the spline knot positions placed at the 19-base flanks (\"non-motif.est\"), implying that the footprint scores are not correlated with read coverage.\n\na\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/8520f0eb5f27226b0673c447234f2ec389be81fdb3a1519dfa54801afa9ef8f3.jpg)\n\nb\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/96259629b64b06889029f6767c25734775bf1b0873f8d5709a5fbc3aec5117be.jpg)\n\n# Figure 4: Sequence bias in Tn5 target sites confounds ATAC-Seq footprint estimates.\n\n(a) Observed 19-base palindromic sequence bias around read start sites of Tn5-tagmented WGS (top) and ATAC-seq (bottom). Only reads that mapped to genomic intervals defined as accessible regions in the ATAC-seq data were considered. Given that Tn5 introduces a 9 bp staggered cut, reads (red arrows) mapping to the positive and negative strand were shifted by +/- 4 bases respectively.\n\n(b) The crystal structure of the Tn5 homodimer complexed with the 19-bp mosaic end (ME) sequences is shown with the proposed path of the target DNA (red lines) on the transposome complex (top panel). The  $3^{\\prime}$  end of the ME DNA strands are transesterified to the target DNA with a 9-bp offset and the resulting sequence reads are shown in the bottom panel. Position-specific bias observed on the reads is depicted (lower panel) and likely arises due to interactions with neighboring residues on the transposome surface.\n\n(c) Transposition volatility is highly elevated within the motifs, and reduces to background levels 19-bases away from the motif. Transposition volatility at a position was defined as the square of the difference in the transposition frequency with the adjacent nucleotide position. All motifs were centered and transposition volatility was calculated for each motif. As motifs vary in length, the missing values were replaced by median volatility for the particular motif. The median volatility across all motifs is depicted here.\n\n(d) Transposition bias was tested in 471 non-redundant vertebrate motifs from the JASPAR CORE database. Footprints in motif instances observed in ATAC-seq exhibit a strong positive correlation  $(R^2 = 0.80)$  with transposition patterns seen in instances found in WGS. Calculation of footprint scores is described in the Online Methods. Positive values indicate under-transposition (interpreted as footprints in ATAC-seq). Dotted lines indicate 1 standard deviation from the mean on each axis. (Plot insets) CTCF motif transposition observed in ATAC-seq (lower right corner) and WGS (upper left corner).\n\na\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/f79a2c06f218e38377e4861a1e6f2a6927cea5abe99796c10d81a29d76c76cf1.jpg)\n\nb\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/5cd82c75b9d0e483972032f1ae03d3f9306159c63f6e22c1244aa905ee32190e.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/85e9bf9d80340a2f5ffbaffe78351a4d2544d9f363bf1854dd6b99f17aebe5e6.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/ac31c77c52d90976946d753f3f01916532f199b0075f94b88380dfea3879a0e7.jpg)\n\nC\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/d967f5f95ff74f88bf00fdc67d51e63d8cd41aea227fc726059fcd880ddb9711.jpg)\n\nd\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/63cb3301eca447e77b7c6014ccefc3af04e592bdb533ababe36cfe5654024b90.jpg)\n\n# Figure 5: The Seqbias Bayesian network model yields corrected footprints\n\n(a) (Top panel) Seqbias corrects nucleotide frequencies around read start sites to near-uniform levels. (Bottom panel) K-L divergence between nucleotide frequencies at positions around read start sites with respect to a uniform background distribution shown before (left) and after (right) Seqbias model application.\n\n(b) The correlation between the footprint scores observed in ATAC-seq and WGS data is reduced after Seqbias model application across the 471 JASPAR CORE motifs  $(R^2 = 0.34)$ . The number of matches for the motifs analyzed ranged from 88 (Rarg) to 20340 (NRF1) matches (median = 1080, IQR = 1498). Footprint scores were calculated as shown in Figure 3A.\n\n(c) 55 motifs (11.67%) are predicted to have genuine footprints in ATAC-seq data (e.g. SP1, shown in the inset). Highlighted points in (c), (d) and (e) indicate TF motif footprint Z-scores in ATAC-seq versus WGS before (blue) and red (after) model application.\n\n(d) 325 motifs (69.00%) are predicted not to be footprinted after Seqbias correction (e.g. POU2F2, shown in the inset).\n\n(e) 22 motifs (4.67%) show incomplete correction of transposition bias after model application (e.g. FOXP1, shown in the inset).\n\nA\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/9125802926afbb35b68d3255a71cf1130f2834ed2e362ea3a0067842aa63ce76.jpg)\n\nB\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/098a8c63e7f8736c36b4ba753bd4a657ad40b88eb04613998896ab9f0ea1a679.jpg)\n\nC\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/b10900d46832e3838ed336580e3000a8bfff3855e6feb3107a664bf2bdd3f4c7.jpg)\n\nD\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/32e31ef0e8d250df970cf239fad737bfd6129c8e1eb9f1451aad34c03703751a.jpg)\n\nE\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d1228c60-656e-425a-9d30-7f243ae55687/88d6f220e16454b779e1b95136cbd69c47b41694fe499ccd879177fd0e2dec3c.jpg)",
    "metadata": {
      "md_filename": "MinerU_markdown_ATACtrans_20260106143058_2008426043602112512.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_ATACtrans_20260106143058_2008426043602112512.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "a943560cb23a5ec5",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_BigTransformer_20260106143139_2008426213202989056",
    "title": "Cross-species regulatory sequence activity prediction",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# Functional genomics data\n\nIn this work, we studied quantitative sequencing assays performed on human and mouse samples. Specifically, we focused on DNase and ATAC-seq profiling DNA accessibility, ChIP-seq profiling TF binding or histone modifications, and CAGE profiling RNA abundance derived from 5' transcription start sites. Preprocessing these data effectively is critical to successful machine learning. Our primary preprocessing objective is to denoise these data to the relevant signal at nucleotide-resolution.\n\nWe largely followed the preprocessing pipeline described in prior research introducing the Basenji framework (5)] The standard pipeline through which experimental data flowed follows:\n\n1. Trim raw sequencing reads using fastp, which can automatically detect and remove unwanted adapter nucleotides (39).\n\n2. Align reads using BWA to hg38 or mm10 and requesting 16 multi-mapping read positions (40).\n\n3. Estimate nucleotide-resolution signal using an open source script (bam_cov.py) from the Basenji software that distributes multi-mapping reads, normalizes for GC bias, and smooths across positions using a Gaussian filter (5).\n\nHowever, we varied from this standard pipeline for all data available from the ENCODE consortium website, which is 4,506 human and 1,019 mouse experiments. These data have been thoughtfully processed using open source pipelines and are available for download at several stages, including log fold change signal tracks in BigWig format (41). Rather than reprocess these data without full knowledge of how replicate and control experiments match, we chose to use these signal tracks directly. The Seattle Organismal Molecular Atlas (SOMA) server provides a single cell mouse ATAC-seq atlas (22). These data are also available in log fold change BigWig format, and we similarly chose to use these rather than reprocess the single cell data. We clipped negative values in all such BigWig tracks to zero.\n\nWe applied several transformations to these tracks to protect the training procedure from large incorrect values. First, we collected blacklist regions from ENCODE and added all RepeatMasker satellite and simple repeats (42), which we found to frequently collect large false positive signal (43). We further defined unapplicable regions of  $>32$  bp where 24-mers align to  $>10$  genomic sites using Umap mappability tracks (44). We set signal values overlapping these regions to the  $25^{th}$  percentile value of that dataset. Finally, we soft clipped high values with the function  $f(x) = \\min(x, t_c + sqrt(max(0, x - t_c)))$ . Above the threshold  $t_c$  (chosen separately for each experiment and source), this function includes only the square root of the residual  $x - t_c$  rather than the full difference.\n\nWhen replicate experiments profiling the same or related samples were available, we averaged the signal tracks. Altogether, the training data includes 638 CAGE, 684 DNase/ATAC, and 3991 ChIP datasets in human and 357 CAGE, 228 DNase/ATAC, and 1058 ChIP datasets in mouse. S1 Table describes all data with preprocessing parameters. Code to preprocess typical functional genomics data formats into TensorFlow input formats is available from https://github.com/calico/basenji.\n\n# Model architecture\n\nWe modeled genomic regulatory sequence activity signal as a function of solely DNA sequence using a convolutional neural network. Such deep learning architectures have excelled for many similar tasks (2, 4-6). We follow our prior work in analyzing large 131 kbp sequences in order to consider long range interactions.\n\nThe first stage of the architecture aims to extract the relevant sequence motifs from the DNA sequence using the following block of operations:\n\n1. Convolution width 5 (or 15 in first layer)\n\n2. Batch normalization\n\n3. Gaussian Error Linear Unit (GELU) activation\n\n4. Max pool width 2\n\nWe applied this block seven times so that each sequence position represents 128 bp, increasing the number of filters from an initial 288 by 1.1776x each block to 768 filters by the end. The GELU activation slightly outperformed the more common ReLU in our benchmarks (45).\n\nThe second stage of the architecture aims to spread information across the sequence to model long range interactions. In prior work, we applied densely connected dilated convolutions for this task (5). Here, we applied a related but more effective variation, which is related to a strategy applied for DNA sequence analysis in the SpliceAI system (46). Recent deep learning research has revealed that skip connections between layers where one layer's representation is directly added to a subsequent layer's representation relieve vanishing gradients and improve gradient descent training (18). Thus, we applied the following series of operations:\n\n1. GELU activation\n\n2. Dilated convolution width 3, dilation rate  $d$ , 384 filters\n\n3. Batch normalization\n\n4. GELU activation\n\n5. Convolution width 1, back to 768 filters\n\n6. Batch normalization\n\n7. Dropout probability 0.3\n\n8. Addition with the block input representation before step 1.\n\nWe applied this block eleven times, increasing the dilation rate  $d$  by 1.5x each time. Relative to the densely connected version, the dilated residual blocks lead to improved generalization accuracy (Figure S1).\n\nIn the final stage, we first transformed this 1024x768 (length x filters) representation of 128 bp windows with an additional width 1 convolution block using 1536 filters and dropout probability 0.05. To make predictions for either 5313 human or 1643 mouse datasets, we applied a final width one convolution followed by a softplus activation  $(f(x) = \\log(1 + e^x))$  to make all predictions positive. We attached a genome indicator bit to each sequence to determine which final layer to apply.\n\nWe trained to minimize a Poisson log likelihood in the center 896 windows, ignoring the far sides where context beyond the sequence is missing. The Poisson model is not technically appropriate for the log fold change tracks. However, by clipping negative values to zero, the distribution of values resembles that from our standard processing. Clipping to zero focuses attention on signal magnitude in regions where relevant signal is present and away from less relevant signal fluctuations in background regions. On a subset of data, we observed that using the log fold change track did not decrease accuracy or the utility of the model for genetic variant analysis.\n\nWe implemented the network in TensorFlow and used automatic differentiation to compute gradients via back propagation (47). We minimized with stochastic gradient descent (SGD) on batches of 4 sequences. We\n\nstopped training when the loss on a validation set had not improved for 30 epochs and returned to the model weights that had achieved the minimum validation loss. We performed several grid searches to choose model and optimization hyper parameters for the following sets: (1) SGD learning rate and momentum; (2) initial convolution filters and convolution filter multiplication rate; (3) dilated convolution filters and dropout rate; (4) final convolution filters and dropout rate.\n\nData augmentation describes a suite of techniques to expand the implicit size of the training dataset from the perspective of model training by applying transformations that preserve annotations to data examples. We tiled the 131,072 bp sequences across the chromosomes by 65,599 bp, representing a  $50\\%$  overlap minus 63 bp in order to also shift the 128 window boundaries and max pooling boundaries. During training, we cycled over combinations of two transformations that maintain the relationship between sequence and regulatory signal while changing the model input: (1) reverse complementing the sequence and reversing the signal; (2) shifting the sequence 1-3 bp left or right. Both transformations improved test accuracy and reduce overfitting in our benchmarks.\n\nModel implementations and instructions for re-training, predicting, and modifying them are available from https://github.com/calico/basenji.\n\n# Multi-genome training\n\nTraining on multiple genomes containing orthologous sequence complicates construction of holdout sets. Independently splitting each genome's sequences would allow training on a human promoter and testing on its mouse orthologue. If the model memorized conserved elements of the sequence, rather than learning a general function, we might overestimate generalization accuracy.\n\nWe used the following procedure to minimize occurrence of this potential issue:\n\n1. Divide each genome into 1 mb regions.\n\n2. Construct a bipartite graph where vertexes represent these regions. Place edges between two regions if they have  $>100$  kbp of aligning sequence in a whole genome alignment.\n\n3. Find connected components in the bipartite graph.\n\n4. Partition the connected components into training, validation, and test sets.\n\nWe used the hg38-mm10 syntenic net format alignment downloaded from the UCSC Genome Browser site (48). Using this procedure, we set aside approximately  $12\\%$  of each genome into validation and test sets respectively. Stricter parameter settings created a single large connected component that did not allow for setting aside enough validation and test sequences.\n\nAnother complication of training on multiple genomes arises from imbalance between each genome's sequences and datasets. We extracted 38.2k human and 33.5k mouse sequences for analysis. We assembled batches of sequences from one genome or the other, chosen randomly proportional to the number of sequences from each genome. The overall loss function comprises a term for every target dataset summed, which leads to larger step magnitudes for batches of human sequences that are annotated with  $>3$  times more datasets. Explicit weighting could be applied to preference training towards a particular species, but we found this to be unnecessary in our experiments for good mouse performance.\n\nJointly training on both human and mouse data constrains the model slightly more than is ideal. We found that training several epochs on only one genome or the other after the full joint procedure improved validation and test set accuracy.\n\n# GTEX SLDP\n\nWe predicted the effect of a genetic variant on various annotations by computing a forward pass through the convolutional network using the reference and alternative alleles, subtracting their difference, and summing across the sequence to obtain a single signed score for each annotation. We averaged scores computed using\n\nthe forward and reverse complement sequence and small sequence shifts to the left and right. We computed scores for all 1000 Genomes SNPs, which we provide for download from [available upon publication].\n\nSigned linkage disequilibrium profile (SLDP) regression is a technique for measuring the statistical concordance between a signed variant annotation  $v$  and a genome-wide association study's marginal correlations between variants and a phenotype  $\\hat{\\alpha}$  (8). The functional correlation between  $v$  and the true variant effects on the phenotype describes how relevant the annotation is for the phenotype's heritability. Our model produces these signed variant annotations, and SLDP offers a validated approach to assessing their relevance to human phenotypes. Briefly, the method estimates this functional correlation using a generalized least-squares regression, accounting for the population LD structure. SLDP performs a statistical test for significance by randomly flipping the signs of entries in  $v$  in large consecutive blocks to obtain a null distribution. We follow previous work in conditioning on background annotations describing minor allele frequency and binary variables for variant overlap with coding sequence (and 500 bp extension), 5' UTR (and 500 bp extension), 3' UTR (and 500 bp extension), and introns.\n\nWe downloaded GTEx v7a summary statistics for 48 tissues (25). We summarized each SNP's effect on all cis-genes using the following transformation suggested for SLDP analysis\n\n$$\n\\hat {\\alpha_ {m}} = \\frac {1}{\\sqrt {| G _ {m} |}} \\sum_ {k \\in G _ {m}} \\hat {\\alpha_ {m} ^ {(k)}}\n$$\n\nwhere  $G_{m}$  is the set of all genes for which a cis-eQTL test was performed for variant  $m$  and  $\\hat{\\alpha}_{m}^{(k)}$  is the marginal correlation of SNP  $m$  and gene  $k$  expression (8). We passed  $\\hat{\\alpha}_{m}$  to SLDP for analysis of variant predictions.\n\nTo assess the orthogonal value of prediction scores derived from mouse datasets relative to those from human, we added the human dataset predictions to the background annotation set. Conditioning on thousands of annotations was computationally intractable. Instead, we included 64 principal components of the human variant scores matrix, which explained  $>99\\%$  of the variance in all cases studied. To assess statistical significance, we performed the Benjamini-Hochberg procedure to correct for multiple hypotheses within each GTEx tissue.\n\n# Simons Simplex Collection\n\nWe downloaded 255,106 de novo variants derived from whole-genome sequencing of 1902 quartet families with an autistic child from the Simons Simplex Collection from the supplement of (29). We filtered these variants for SNPs and computed predictions as described above.",
    "metadata": {
      "md_filename": "MinerU_markdown_BigTransformer_20260106143139_2008426213202989056.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_BigTransformer_20260106143139_2008426213202989056.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "bb8a6121492167c4",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_Borzoi_20260106143133_2008426188456595456",
    "title": "Predicting RNA-seq coverage from DNA sequence as a unifying model of gene regulation",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "The experiments conducted in this study did not require approval from a specific ethics board.\n\n# Training data\n\nThe training data for this analysis consisted of a large set of human and mouse RNA-seq experiments. To help the model use important sequence features for making its RNA coverage predictions, we also included the experimental assays studied by the Enformer and Basenji models in the training data $^{8,9,13}$ . This includes a curated set of human and mouse CAGE assays from the FANTOM5 consortium, which we reasoned would help the model relate TSS usage and strength to RNA-seq coverage between multiple (alternative)  $\\mathrm{TSSs}^{97,98}$ , as well as DNase-seq and ChIP-seq from ENCODE and the Epigenomics Roadmap $^{31,99}$  and pseudo-bulk single-cell ATAC-seq data from CATlas $^{100,101}$ , which focuses the model towards distal regulatory elements. We processed the data slightly differently relative to prior analyses $^{9,13}$ . First, we aggregated the aligned read counts here at 32 bp resolution. Second, we split the CAGE-aligned reads by strand, requiring that the model predict both the forward and anti-sense coverage.\n\nWe collected 867 human and 278 mouse RNA-seq coverage tracks from ENCODE. This set includes samples from a diverse set of tissues and cell types, with measurements spanning the developmental spectrum for both human and mouse. The tracks available for download represent normalized coverage from the STAR alignment program of uniquely mapping reads $^{102}$ . Most experiments used a protocol to enable stranded analysis, creating a forward and anti-sense coverage track. We trained Borzoi to directly predict these continuous coverage values in 32 bp genomic bins. Owing to the relatively large dynamic range of RNA-seq, we normalized each coverage track by exponentiating its bin values by  $3/4$ . If bin values were still larger than 384 after exponentiation, we applied an additional square-root transform to the residual value. These operations effectively limit the contribution that very highly expressed genes can impose on the model training loss. The formula below summarizes the transform applied to the  $j^{\\text{th}}$  bin for tissue  $t$  of target tensor  $y$ :\n\n$$\n\\mathbf {y} _ {j, t} ^ {\\text {(s q u a s h e d)}} = \\left\\{\\mathbf {y} _ {j, t} ^ {(3 / 4)} \\text {i f} \\mathbf {y} _ {j, t} ^ {(3 / 4)} \\leq 3 8 4, \\text {o t h e r w i s e} 3 8 4 + \\sqrt {\\mathbf {y} _ {j , t} ^ {(3 / 4)} - 3 8 4} \\right\\}\n$$\n\nWe refer to this set of transformations as 'squashed scale' in the main text. The parameters were chosen such that most genes had bin values of  $< 1,000$  (a reasonably large maximum value that is handled well by standard tensorflow data types). For most downstream tasks, for example, when calculating log fold changes from predicted values because of a mutation, we first undo the normalization by applying inverse transforms to the predictions (thus operating in 'count' space). One exception is when visualizing reference predictions of test sequences, in which all transforms except the residual exponentiation at 384 are inverted, as small amounts of noise near the threshold would otherwise be amplified.\n\nWe supplemented the training data with 89 tracks from GTEx whole-tissue samples $^{33}$ , uniformly processed by the recount3 project $^{34}$  (GTEX v.8 release). recount3 clustered the 49 GTEx tissues into 30 meta-tissues, combining highly related physiological regions (such as regions of the brain). For each meta-tissue, we chose a subset of samples to include as training data by performing  $k$ -means clustering on the gene expression profiles of all samples with  $k = 3$  (although several meta-tissues collapsed to  $k = 2$ ). For each cluster, we chose to include the sample with the minimum average distance to all cluster members. These data were processed without consideration of strand information in recount3, which means the GTEx training tracks are non-stranded whereas most other RNA-seq tracks are stranded. For these tracks, we scaled the aligned fragment counts by the inverse of their average length to weight\n\neach fragment as a single event, in addition to the exponentiation transform described above.\n\nWe fragmented the human (hg38) and mouse (mm10) chromosomes and randomly divided these fragments into eight roughly evenly sized partitions, pairing orthologous regions into the same partition. One partition was held out for validation and another for testing, and the remainder of the data  $(-75\\%)$  was used for training. Note that all coverage measurements of all experimental assays (RNA, DNase, CAGE, ATAC, ChIP) are held out (and not seen by the model) whenever a particular  $524~\\mathrm{kb}$  sequence window is not in the training set.\n\n# Model\n\nThe model is based on the Enformer network architecture but introduces a number of simplifications and enhancements to optimize for RNA-seq prediction<sup>13</sup>. Supplementary Fig. 14 shows the full architecture. Enformer comprises two main stages. First, repeated application of a convolution block that achieves a twofold reduction of the sequence length extracts local sequence patterns until each position in the sequence represents 128 bp. Second, repeated application of a self-attention (or transformer) block enables long-range interaction and exchange between every pair of sequence positions<sup>27,28</sup>. Enformer accepts a 196 kb input sequence and predicts coverage data aggregated at 128 bp resolution.\n\nRNA-seq is a base-resolution readout of transcribed RNAs. We believed that it was important to both increase the sequence length and decrease the prediction resolution to model RNA-seq well. Mammalian genes regularly exceed a full span of  $>100$  kb, and if the  $5^{\\prime}$  or  $3^{\\prime}$  end of a gene extends outside of the training sequence window (such that its promoter and other regulatory signals are not captured in the receptive field of the network), it will probably obstruct learning. Conversely, mammalian exons regularly cover fewer than 128 bp, and modeling the coverage patterns around these exons at such a coarse resolution can obstruct splice site learning. However, computational limitations make these joint objectives challenging. Therefore, we aimed for a compromise of 524 kb input sequences, predicting at 32 bp resolution.\n\nHalting the convolution and pooling blocks in the vanilla Enformer architecture at 32 bp would mean that the self-attention blocks processed 16,384-length sequences. These blocks require quadratic memory complexity, which exceeds the capability of contemporary GPU/TPU hardware without complicated optimizations. Therefore, we chose to remain at 128 bp resolution for the self-attention blocks. To predict at 32 bp resolution, we instead make use of U-net upsampling techniques from the image segmentation and object detection literature $^{39,30}$ , which solve an analogous problem of determining image-level content and communicating it back down to pixel resolution annotations. In brief, the output embeddings predicted by the self-attention blocks at 128 bp resolution are upsampled two times by duplicating the embedding vector at each position. We then apply point-wise convolutions to match the number of channels to those of the original convolution tower output (preceding the self-attention blocks) at 64 bp resolution. Finally, we add the upsampled feature map from the self-attention blocks and the intermediate feature map from the convolution tower and apply a separable convolution with a width of three. This workflow is repeated once more using the intermediate feature map with 32 bp resolution from the convolution tower.\n\nAs this architecture is still very computationally expensive, we simplified several Enformer components. First, we used max pooling instead of attention pooling, which requires an additional convolution but generally only minimally boosts performance. Second, we apply only a single convolution with a width of five in each block of the initial convolution tower, forgoing the second convolution added in with a residual connection used by Enformer. Third, we reduced the number of self-attention blocks from 11 to 8 to reduce memory usage. Fourth, we used only central mask relative position embeddings given that additional distance functions minimally affected performance.\n\n# Training\n\nWe trained the model in a multi-task setting to predict coverage for all assays from one species, with a species-specific head attached to the shared model trunk. During training, we alternated human and mouse training batches by dynamically swapping in the corresponding species-specific head. To avoid less accurate predictions on the sequence boundaries (owing to asymmetric visibility), we cropped from each side to focus the loss computation on the center 196,608 bp. We used a Poisson loss function but decomposed the loss analogous to BPnet to separate magnitude and shape terms $^{7}$ . Having independent Poisson distributions at each sequence position is mathematically equivalent to a single Poisson distribution representing their sum, followed by allocating the counts to sequence positions using a multinomial distribution. Thus, we apply a Poisson loss on the sum of the observed and predicted coverage and a multinomial loss on the normalized observed and predicted coverage across the sequence length. This decomposition allows us to weight the multinomial shape loss by a greater amount (five times), which we found boosts performance.\n\nUsing TensorFlow (v.2.11), backpropagation of this model on a 524 kb sequence maxes out the 40 GB of RAM of a standard NVIDIA A100 GPU. Each model instance was trained using the Adam optimizer with a batch size of two, split across two GPUs for  $\\sim 25$  days, and training stopped when the validation set accuracy plateaued.\n\nWe trained four replicate models with random weight initialization and sequence training order. We constructed an ensemble predictor from these four replicates that generally performed better than any individual model. Note that for all analyses in Figs. 1 and 2 in which we evaluate model performance, we do so strictly on fragments from the held-out test set. In subsequent analyses (for example, variant effect prediction in Fig. 5), we make no distinction between train or test splits of hg38. This technically means that the ensemble is applied to genomic loci seen during training. We argue that these are still unbiased analyses, as the evaluations are done on out-of-domain measurements not trained on (for example, the alternative alleles of fine-mapped QTLs and their estimated effects were not part of the training data).\n\n# Model ablation experiments\n\nInstances of the Borzoi model were trained on smaller subsets of the original training data to assess the contribution of various data modalities to final performance. We varied whether or not the model was trained on mouse data in addition to human experiments, whether or not the model was trained on additional assays (for example, DNase-seq, ATAC-seq, ChIP-seq and CAGE) in addition to the core RNA-seq modality and whether or not the model used a U-net component to increase the output resolution. Owing to the large number of combinations, it was difficult to acquire a sufficient set of NVIDIA A100 GPUs that would allow training them as full-sized Borzoi models in a reasonable amount of time. Therefore, we reduced their size (393,192 bp input length,  $\\sim 30$  million trainable parameters, four self-attention heads per layer) such that we could fit them with a batch size of two on either NVIDIA RTX 4090 GPUs or NVIDIA TITAN RTX GPUs. We trained two cross-validation folds per ablation condition, choosing a different held-out validation and test set from the eight genomic hg38 or mm10 partitions per fold. We trained four folds for the baseline condition (with all features included). Training lasted 30-90 days, depending on condition, and was stopped when the validation accuracy saturated.\n\nThe following model instances were trained: ['Multispecies'] Training data - CAGE, DNase-, ATAC-, ChIP- and RNA-seq in human (hg38) and mouse (mm10). Architecture changes - N/A (baseline model). ['Multispecies (No U-net)'] Training data - CAGE, DNase-, ATAC-, ChIP- and RNA-seq in human and mouse. Architecture changes - U-net removed. Trained at 128 bp output resolution. ['Multispecies (D/A/RNA)'] Training data - DNase-, ATAC- and RNA-seq in human and mouse. Architecture changes - N/A. ['Multispecies (RNA)'] Training data - RNA-seq in human and mouse. Architecture changes - N/A.\n\n['Human'] Training data - CAGE, DNase-, ATAC-, ChIP- and RNA-seq in human. Architecture changes - N/A. ['Human (D/A/RNA)'] Training data - DNase-, ATAC- and RNA-seq in human. Architecture changes - N/A. ['Human (GTEX RNA)'] Training data - GTEX RNA-seq (human). Architecture changes - N/A. ['K562'] Training data - CAGE, DNase-, ChIP- and RNA-seq in K562 cells. Architecture changes - N/A. ['K562 (D/A/RNA)'] Training data - DNase-, and RNA-seq in K562 cells. Architecture changes - N/A. ['K562 (RNA)'] Training data - RNA-seq in K562 cells. Architecture changes - N/A.\n\n# Enformer comparison\n\nOur research objective was to extend this modeling framework to new data (that is, RNA-seq) and not to exceed Enformer performance on the set of overlapping tracks, which includes CAGE, DNase, ATAC and ChIP assays. Several modeling decisions make comparisons between Borzoi and Enformer imperfect. First, working with larger sequences required reprocessing the genome so that the held-out test set of Borzoi does not exactly match that of Enformer. Second, we aggregated the data at 32 bp resolution, whereas Enformer works with 128 bp, thus altering the distribution of bin values. Third, we split the aligned reads from the CAGE datasets by strand. Nevertheless, we examined test accuracies for Borzoi versus Enformer (v.3.0) on these overlapping datasets and found them to be broadly similar despite these modifications (Extended Data Fig. 1a-d).\n\n# Tissue-specific expression, TSS and APA predictions\n\nWe evaluated three different statistics derived from the predicted GTEx RNA-seq coverage tracks to quantify (tissue-specific) gene expression, alternative TSS usage and APA isoform abundance (Fig. 2). Gene expression is quantified as the sum of predicted coverage overlapping exonic bins. Alternative TSS usage is quantified by taking the maximum coverage among the nine bins immediately downstream of each annotated TSS in GENCODE (v.41) (maximum given that the exon may be shorter than nine bins) and computing the ratio between the  $3^{\\prime}$ -most and  $5^{\\prime}$ -most TSSs of each gene. Only TSSs that were within 50 bp of an annotated TSS in FANTOM5 were included[97]. APA site usage is quantified by calculating the ratio of average coverage between the four bins immediately upstream of the distal-most PAS and the four bins upstream of the proximal-most PAS, based on polyadenylation sites annotated in PolyADB[44].\n\nExamples visualized in Fig. 2 and Extended Data Fig. 3 were chosen as follows: (1) differentially expressed examples were selected from the genes with the largest measured fold change between exon-aggregated coverage in the target tissue and the average coverage in the four other tissues, based on the GTEx RNA-seq data; (2) tissue-specific TSS examples were selected from the set of genes with largest measured differential TSS usage according to tissue-matched FANTOM5 CAGE data; and (3) tissue-specific APA examples were selected from the genes with the largest measured fold change in coverage ratio in the target tissue with respect to the average coverage ratio in the four other tissues. To reduce the risk of picking genes in which the perceived APA is driven by  $3^{\\prime}$  bias in the GTEx RNA-seq data, we required that the genes also exhibited differential distal polyadenylation in cell-type-matched experiments from the PolyASite 2.0 database $^{42}$ . All example genes were picked from the held-out test set, and coverage was predicted using the four-replicate ensemble.\n\n# Input sequence attribution\n\nTo visualize important features in the input sequence (such as TF or RNA-binding protein motifs) and quantify their contribution to the prediction (their saliency score), we apply a number of different attribution methods, each with their own strengths and limitations. In summary, we either use methods based on gradient saliency, which are computationally efficient for single outputs but tend to be noisier owing to moving off the one-hot coding simplex, or in-silico\n\nmutagenesis, which often give better-calibrated attributions for all outputs, but are too computationally expensive to run on long sequences. The shared goal of these methods is to estimate the contribution of each nucleotide in the input with respect to scalar statistics derived from the predicted coverage tracks, resulting in a matrix  $s \\in \\mathbb{R}^{524,288 \\times 4}$  of saliency scores for each coverage track. In this study, we focus solely on interpreting Borzoi's RNA-seq tracks. Furthermore, by computing distinct summary statistics from the predicted RNA coverage tracks, we dynamically isolate distinct regulatory mechanisms in the attribution scores; namely, transcription, polyadenylation and splicing.\n\nAs preliminaries, let  $\\mathcal{M}$  be the Borzoi model,  $x \\in \\{0,1\\}^{524,288 \\times 4}$  be the one-hot coded input sequence,  $\\pmb{y} = \\mathcal{M}(\\pmb{x}) \\in (0, +\\infty]^{16,384 \\times 7,611}$  be the (human) coverage prediction and  $\\mathcal{T} = \\{t_0, \\dots, t_T\\}$  be the set of  $T$  indices of the coverage tracks in  $\\pmb{y}$  that we want to average over (for example, to combine all blood-specific tracks) and compute the attribution scores for. Note that Borzoi's raw prediction  $\\pmb{y}$  is based on training data that had been subjected to various transforms intended to stabilize training (exponentiating by  $3/4$ , additional exponentiation of residuals above a target value and re-scaling). Here, we assume that we have applied the inverse transforms to  $\\pmb{y}$  such that the tensor can be reasonably assumed to reflect counts (also note that these transforms are differentiable, which means gradient saliency can be propagated through the inverse operations).\n\nBelow are the definitions of three distinct summary statistics used for expression attribution, polyadenylation attribution and splicing attribution, respectively:\n\nLog sum of exon coverage (expression attribution). The summary statistic  $u \\in \\mathbb{R}$  is computed by aggregating the set of 32 bp bins  $\\mathcal{B} = \\{b_0, \\dots, b_B\\}$  in  $y$  overlapping the exons of the gene of interest (with optional pseudo count  $C \\in \\mathbb{R}$ ):\n\n$$\nu = \\log \\left(C + (1 / T) \\times \\sum_ {t \\in \\mathcal {T}} \\sum_ {b \\in \\mathcal {B}} \\mathbf {y} _ {b, t}\\right)\n$$\n\nLog ratio of PAS coverage (polyadenylation attribution). The statistic  $u \\in \\mathbb{R}$  is computed by summing coverage in five adjacent bins immediately upstream of bin  $b_{\\mathrm{prox}}$ , which overlaps the PAS of interest, and dividing by the coverage of a matched set of bins upstream of bin  $b_{\\mathrm{dist}}$ , where a competing PAS is located (or immediately downstream of  $b_{\\mathrm{prox}}$  if the gene of interest is not subject to APA):\n\n$$\nu = \\log \\left(\\frac {C + (1 / T) \\times \\sum_ {t \\in \\mathcal {T}} \\sum_ {b = b _ {\\text {p r o x}} - 5} ^ {b _ {\\text {p r o x}}} \\boldsymbol {y} _ {b , t}}{C + (1 / T) \\times \\sum_ {t \\in \\mathcal {T}} \\sum_ {b = b _ {\\text {d i s t}} - 5} ^ {b _ {\\text {d i s t}}} \\boldsymbol {y} _ {b , t}}\\right)\n$$\n\nNote that the formula above assumes that the gene is on the forward (plus) strand. Coverage must be summed from  $b_{\\mathrm{prox}} + 1$  to  $b_{\\mathrm{prox}} + 5 + 1$  (and from  $b_{\\mathrm{dist}} + 1$  to  $b_{\\mathrm{dist}} + 5 + 1$ ) if the gene is on the minus strand.\n\nLog ratio of exon-to-intron coverage (splicing attribution). The statistic  $u \\in \\mathbb{R}$  is computed by summing coverage in bins  $\\mathcal{B}_{\\mathrm{exon}} = \\{b_0, \\dots, b_E\\}$  overlapping the exon and dividing by the sum of coverage in a matched number of bins  $\\mathcal{B}_{\\mathrm{intron}} = \\{b_0, \\dots, b_I\\}$  overlapping the adjacent intron or, alternatively, a neighboring exon (which occasionally resulted in less noisy attributions when intronic polyadenylation sites created non-uniform intronic coverage):\n\n$$\nu = \\log \\left(\\frac {C + (1 / T) \\times \\sum_ {t \\in \\mathcal {T}} \\sum_ {b \\in \\mathcal {B} _ {\\text {e x o n}}} \\boldsymbol {y} _ {b , t}}{C + (1 / T) \\times \\sum_ {t \\in \\mathcal {T}} \\sum_ {b \\in \\mathcal {B} _ {\\text {i n t r o n}}} \\boldsymbol {y} _ {b , t}}\\right)\n$$\n\nThe summary statistics defined above are used in conjunction with the following attribution methods:\n\nGradient  $\\times$  input (gradients). Given summary statistic  $u(x)$ , the attribution scores  $\\pmb{s} \\in \\mathbb{R}^{524,288 \\times 4}$  are computed by taking the gradient with respect to input  $\\pmb{x}$  and subtracting the mean at each position across nucleotides<sup>103</sup>:\n\n$$\n\\boldsymbol {s} _ {i, j} = \\frac {\\partial u (\\boldsymbol {x})}{\\partial \\boldsymbol {x} _ {i , j}} - (1 / 4) \\times \\sum_ {k = 1} ^ {4} \\frac {\\partial u (\\boldsymbol {x})}{\\partial \\boldsymbol {x} _ {i , k}}\n$$\n\nWhen visualizing  $s$ , we extract the score at position  $i$  corresponding to the reference nucleotide  $j$  only (which is easily implemented by multiplying with  $x$  and aggregating across nucleotides):\n\n$$\n\\boldsymbol {s} _ {i} ^ {\\mathrm {(v i s)}} = \\sum_ {j = 1} ^ {4} \\boldsymbol {s} _ {i, j} \\times \\boldsymbol {x} _ {i, j}\n$$\n\nISM. Given a start and end position,  $p_{\\mathrm{start}}$  and  $p_{\\mathrm{end}}$ , in  $x$  to compute ISM over, the attribution scores  $s \\in \\mathbb{R}^{524,288 \\times 4}$  are computed as follows: create a new tensor  $\\tilde{\\pmb{x}} \\in \\{0,1\\}^{(p_{\\mathrm{end}} - p_{\\mathrm{start}}) \\times 4 \\times 524,288 \\times 4}$  and let each matrix  $\\tilde{\\pmb{x}}_{u,v}$  hold a mutated copy of  $x$  where the reference nucleotide at position  $u$  is substituted for nucleotide  $v$ . Then compute the ISM scores  $s$  as:\n\n$$\n\\boldsymbol {s} _ {i, j} = u (\\boldsymbol {x}) - u \\left(\\bar {\\boldsymbol {x}} _ {i - p _ {\\text {s t a r t}}, j}\\right), \\text {i f} p _ {\\text {s t a r t}} \\leq i \\leq p _ {\\text {e n d}}, 0 \\text {o t h e r w i s e}.\n$$\n\nWhen visualizing  $s$ , we average the scores across the four nucleotides:\n\n$$\n\\boldsymbol {s} _ {i} ^ {\\mathrm {(v i s)}} = (1 / 4) \\times \\sum_ {j = 1} ^ {4} \\boldsymbol {s} _ {i, j}\n$$\n\nWindow-shuffled ISM (ISM shuffle). Given a start and end position,  $p_{\\text{start}}$  and  $p_{\\text{end}}$ , a window size  $M$  and a number of re-shuffles  $N$ , the attribution scores  $s \\in \\mathbb{R}^{524,288 \\times 4}$  are computed as follows: create tensor  $\\tilde{\\pmb{x}} \\in \\{0,1\\}^{(p_{\\text{end}} - p_{\\text{start}}) \\times N \\times 524,288 \\times 4}$  containing  $(p_{\\text{end}} - p_{\\text{start}}) \\times N$  copies of input pattern  $x$ . For each matrix  $\\tilde{\\pmb{x}}_{u,v}$  (where  $v$  denotes one of  $N$  independent samples), either dinucleotide-shuffle the local region  $[u - M/2, u + M/2 + 1]$  or replace the reference nucleotides in this region with uniformly random nucleotides. Dinucleotide shuffling (with  $M = 7$  and  $N = 24$ , or  $N = 8$  for large window sizes) is performed when computing enhancer saliency, whereas uniform random substitution ( $M = 5$  and  $N = 24$ , or  $N = 8$  for large window sizes) is used for promoters, splice sites and PASs (where salient features are often stretches of repeating nucleotides). Then compute the attribution scores  $s$  as:\n\n$$\n\\boldsymbol {s} _ {i, n} = u (\\boldsymbol {x}) - u \\left(\\tilde {\\boldsymbol {x}} _ {i - p _ {\\text {s t a r t}}, n}\\right), \\text {i f} p _ {\\text {s t a r t}} \\leq i \\leq p _ {\\text {e n d}}, 0 \\text {o t h e r w i s e}.\n$$\n\nWhen visualizing  $s$ , we average the scores across the  $N$  samples:\n\n$$\n\\pmb {s} _ {i} ^ {\\mathrm {(v i s)}} = (1 / N) \\times \\sum_ {n = 1} ^ {N} \\pmb {s} _ {i, n}\n$$\n\n# Tissue-specific motif discovery\n\nWe visualized learned tissue-specific cis-regulatory motifs driving RNA coverage in GTEx tracks through a combination of (1) picking a large set of (measured) highly tissue-specific genes, (2) computing their gradient saliencies and normalizing out tissue-shared saliency and (3) clustering and annotating the saliency scores using TF-MoDISco  $(v.0.5.14.1)^{51}$  and Tomtom MEME suite  $(v.5.5.2)^{52}$ . We first downloaded measured TPMs for GTEx (v.8) (GTEx_Analysis_2017-06-05_v8_RNASEQCv1.1.9_gene_median_tpm.gct.gz). We heuristically cleaned the data by adding a small pseudo-TPM that was roughly the first percentile of all values (to avoid zeros), followed by clipping at a value slightly larger than the  $99^{\\text{th}}$  percentile per tissue (to avoid extremely large numbers). Then, for each of the five prospective GTEx tissues whole blood, liver, brain - cortex, muscle - skeletal and esophagus - muscularis,\n\nwe computed gene-specific log fold changes of TPM expression for the tissue of interest relative to the average TPM expression of the four other tissues. For each tissue, we sorted the TPM matrix in descending order of this metric and selected the top 1,000 most differentially expressed genes, resulting in a total of 5,000 genes.\n\nWe computed nucleotide-level attribution scores (input gradients) with respect to the log of aggregated exon coverage for each of the 5,000 genes, repeating the gradient computation for each of the five GTEx tissues. Specifically, we matched each GTEx tissue to the corresponding two to three RNA coverage tracks obtained from recount3 that we trained on (for example, for brain - cortex, we computed the input gradient saliency with respect to the three GTEx brain meta-tissue tracks). The gradient computation was repeated for all four model replicates, for both forward-complemented and reverse-complemented input sequences, and averaged.\n\nThe gradient computation outlined above produces five separate sets of saliency scores for all 5,000 genes (one set of scores per tissue). Next, we performed de novo motif discovery for tissue  $x$  by slicing out the 1,000 genes originally selected to be differentially upregulated in tissue  $x$  and running TF-MoDISco on the residual gradient scores for tissue  $x$ . The residual scores were calculated by subtracting the average gradient of the four other tissues from those of tissue  $x$ , thus dampening the saliency of shared regulatory motifs and accentuating motifs specific to tissue  $x$ . Additionally, before running MoDISco, we first re-weighted the gradients by computing the standard deviation at each position across the four nucleotides, applying a Gaussian filter (s.d. = 1,280; truncate = 2) to the resulting vector of standard deviations and dividing the gradient scores by this smoothed vector. This operation results in down-weighting of regulatory regions with long contiguous stretches of large magnitude (often promoter regions) and up-weights sparser regulatory regions (transcriptional enhancers). To increase computational efficiency, we extracted the centered-on 131 kb gradient scores (as opposed to the full 524 kb) before calling MoDISco. TF-MoDISco was executed with the following parameters: 'revcomp = true', 'trim_to_window_size = 24', 'initial flank_to_add = 8', 'sliding_window_size = 18', 'flank_size = 8' and 'max_seqlets_per_metacluster = 40,000'. Other parameters were kept at their default values.\n\nThe five tissue-specific MoDISco result objects were filtered and pooled as follows: Tomtom MEME was used to match the position weight matrices of each MoDISco cluster to HOCOMOCO (v.11) $^{53}$  motifs (each position weight matrix was trimmed by an information content threshold of  $>0.1$ ). Only matches with  $E$  values of  $\\leq 0.1$  were retained. The match with the lowest  $P$  value was chosen as the representative motif for that cluster. The five MoDISco objects were pooled by matching clusters with identical HOCOMOCO motifs and merging the seqlet coordinates, resulting in a single list of seqlet coordinates for each putative motif. A scalar tissue-specific saliency score was then computed for each seqlet by averaging the input-gated gradients overlapping its coordinates. The distributions of these seqlet-level gradient saliencies were used to assess the tissue-specificity of each motif.\n\nReplicating the entire analysis with pseudo counts added to the predicted sum of exon coverage before applying log and computing gradients resulted in nearly identical results. Replicating the analysis without running TF-MoDISco on residual attribution scores but rather using the raw gradients from each tissue-specific coverage track as input to TF-MoDISco similarly produced negligible differences.\n\n# Tissue-pooled splice motif discovery\n\nSplice-regulatory motifs were generated by computing input gradients with respect to the splicing attribution statistic (log ratio of exon-to-intron coverage) for one randomly chosen exon in each of the 4,778 genes from the Gasperini dataset $^{58}$ . The gradients were computed with respect to the average predicted coverage taken across all 89 of Borzoi's GTEx RNA-seq tracks. The gradients were normalized across genes as follows: we first computed the standard deviation across the\n\nfour nucleotides and found the maximum standard deviation across all 524,288 positions per gene. We clipped the lower end of the 4,778 maximum deviations at the  $25^{\\text{th}}$  percentile (to avoid up-weighting gradients with very low magnitudes) and divided each gene's gradient by this number. We tried varying the percentile threshold (from 1 to 100) and the results were robust to this parameter (the same motif clusters were identified with roughly the same number of supporting seqlets). Finally, to obtain  $5^{\\prime}$  splice motifs, we extracted a 192 bp window centered on the splice donor from each of the gradients. To obtain  $3^{\\prime}$  splice motifs, we extracted a 192 bp window around the splice acceptor.\n\nTF-MoDISco was executed on the resulting  $4,778 \\times 192 \\times 4$  hypothetical scores, using custom parameter settings that we empirically found worked better for degenerate RNA-binding protein motifs: 'revcomp = false', 'trim_to_window_size = 8', 'initial_flank_to_add = 2', 'sliding_window_size = 6', 'flank_size = 2', 'max_seqlets_per_metacluster = 40,000', 'kmer_len = 5', 'num_gaps = 2' and 'num_mismatches = 1'.\n\n# Tissue-pooled polyadenylation motif discovery\n\nSalient motifs related to PASs were obtained in a process similar to the procedure for splice-regulatory motif discovery. We computed tissue-pooled gradients with respect to the polyadenylation statistic (log ratio of PAS coverage) for the distal-most PAS of each gene from the Gasperini dataset $^{58}$ . The gradients were normalized by the (clipped) maximum standard deviation per gene. Finally, a 192 bp window centered on the mode of saliency in the  $3^{\\prime}$  UTR of each gene was used to extract short gradient slices. These gradient slices were used as hypothetical scores for TF-MoDISco, which was executed using the same custom parameters as was used for splice motif discovery.\n\n# Attention matrix visualization\n\nWe visualized higher-order structures and long-range interactions learned by Borzoi directly through the attention score matrices of the self-attention layers. Examples of such higher-order structures include intronic and exonic regions, UTRs, promoters and gene spans. Long-range interactions describe relationships or dependencies between these structures learned by Borzoi, which would be observed as off-diagonal intensities in the attention matrix. Such examples include phenomena in which an intron attends to its nearest exon junction, a  $3^{\\prime}$  UTR attends to its PASs or gene spans attend to promoters and transcriptional enhancers. After exploring the predicted attention maps for several different loci, we noticed that higher-order structures matching GENCODE annotations $^{104}$  were generally found in the later self-attention layers. However, to mitigate capturing potential assay-specific or experiment-specific biases and focus on general knowledge, we decided not to use the two final attention layers and instead used the two penultimate self-attention layers for all analyses. We further noted that different attention heads tended to capture mostly the same trends, leading us to analyze the mean attention of all eight heads.\n\nLet  $\\pmb{a}_{i,j}^{l,h} =$  softmax  $\\left(\\pmb{q}_i\\pmb{k}_j^T / \\sqrt{K} + \\pmb{r}_{i,j}\\right) \\in \\mathbb{R}^{N \\times N}$  be the attention matrix for head  $h$  of layer  $l$ , where  $\\pmb{q}_i$  is the  $i^{\\text{th}}$  query vector,  $\\pmb{k}_j$  is the  $j^{\\text{th}}$  key vector,  $\\pmb{r}_{i,j}$  is the positional encoding and  $K$  is the key or query size. We obtain the final attention matrix to be visualized as an unweighted average of all heads of the two penultimate layers:  $(1/16) \\times \\sum_{l=6}^{7} \\sum_{h=1}^{8} \\pmb{a}_{ij}^{lh}$ . When zooming in on smaller sections of the attention matrix, we apply a small Gaussian filter to smooth out high-frequency noise ( $\\sigma = 0.5$ , truncate  $= 2.0$ ). We further average the attention matrix over four independent model replicates and reverse-complemented input sequences. Promoters generally had higher magnitude attention values than exons, leading us to clip individual entries in the average attention matrix at 0.005 (each row of 4,096 entries sums to 1.0).\n\n# Fine-mapped eQTL classification and regression tasks\n\neQTL studies deliver valuable data for evaluating whether Borzoi identifies the correct nucleotides driving expression and their sensitivity\n\nto specific alternative alleles. We studied GTEx (v.8) eQTL results from 49 tissues of varying sample sizes. We made use of summary statistics and fine-mapping results generated with SuSiE in a previous publication<sup>1</sup>. Only fine-mapped causal eQTLs with a posterior causal probability (PIP) of  $\\geq 0.9$  were kept as positives. We focused all analyses on single nucleotide variants only because insertions and deletions (indels) introduce technical variance caused by shifted prediction boundaries, which we aspire to alleviate in future work. To visualize the measured RNA-seq coverage tracks in individuals with or without the minor allele(s) of interest, we also made use of whole genome sequencing genotyping data of GTEx subjects obtained through dbGAP (http://www.ncbi.nlm.nih.gov/gap).\n\nInspired by the expression modifier score construction presented in a previous work, in which the authors demonstrated that functional eQTL classification probabilities enable improved fine-mapping, we evaluated Borzoi and other models at the task of discriminating fine-mapped causal eQTLs from a negative set chosen to control for TSS distance. To compare against models with multiple generic outputs, we constructed a feature vector based on the model predictions for each variant and trained a random forest classifier with the eQTL causal and non-causal labels. We considered a 'SUM' score and an 'L2' score to define these SNP features. For both score types, we start by centering the 524 kb input window on the SNP of interest and predict coverage  $\\mathbf{y}^{(\\mathrm{ref})} = \\mathcal{M}(\\mathbf{x}^{(\\mathrm{ref})}), \\mathbf{y}^{(\\mathrm{alt})} = \\mathcal{M}(\\mathbf{x}^{(\\mathrm{alt})}) \\in \\mathbb{R}^{16,384 \\times 7,611}$  for the reference and variant patterns, respectively. When computing the SUM score vector  $\\mathbf{u}(\\mathbf{x}^{(\\mathrm{ref})}, \\mathbf{x}^{(\\mathrm{alt})}) \\in \\mathbb{R}^{7,611}$  for the 7,611 distinct Borzoi tracks, we aggregate the difference between coverage predictions  $\\mathbf{y}^{(\\mathrm{ref})}$  and  $\\mathbf{y}^{(\\mathrm{alt})}$  across the length axis independently per track:\n\n$$\n\\boldsymbol {u} _ {t} = \\sum_ {j = 1} ^ {1 6, 3 8 4} \\left(\\boldsymbol {y} _ {j, t} ^ {(\\text {a l t})} - \\boldsymbol {y} _ {j, t} ^ {(\\text {r e f})}\\right)\n$$\n\nFor the L2 score vector, we compute the L2 norm of the difference between predictions  $\\mathbf{y}^{(\\mathrm{ref})}$  and  $\\mathbf{y}^{(\\mathrm{alt})}$  across the length axis independently for each track. Before applying the L2 norm, we first log transform the coverage track bins to focus on fold change rather than absolute change. The final metric is calculated as:\n\n$$\n\\boldsymbol {u} _ {t} = \\sqrt {\\sum_ {j = 1} ^ {1 6 , 3 8 4} \\left(\\log_ {2} \\left(1 + \\boldsymbol {y} _ {j , t} ^ {(\\text {a l t})}\\right) - \\log_ {2} \\left(1 + \\boldsymbol {y} _ {j , t} ^ {(\\text {r e f})}\\right)\\right) ^ {2}}\n$$\n\nThe L2 score extracts more information and achieves greater performance on this task for Borzoi. All previous Enformer work uses the SUM score, but we observed here that it also benefits from L2, though less than Borzoi.\n\nFor the second task, we evaluated models on their ability to predict eQTL effect sizes, which is a critical component of a system tasked with predicting gene expression values across a population of individuals. Given that the Borzoi and Enformer models make use of gene annotation differently to map predictions to genes, we chose to perform a gene-agnostic analysis for a less biased comparison. Thus, we filtered the variant set for only those with a consistent sign of the estimated eQTL effect sizes across genes and chose the effect size with maximum absolute value as the representative effect size for that particular fine-mapped SNP. For a subset of GTEx tissues, we were able to select an appropriately matched CAGE experiment from Enformer's outputs and computed the SUM score. For Borzoi, we selected the matching GTEx tissue RNA-seq output and computed a 'logSUM' score, in which we transformed the bin predictions  $\\mathbf{y}$  by  $\\log_2(\\mathbf{y} + 1)$  before taking a sum over the length axis. In supplementary analyses, we performed gene-specific coefficient analyses using a variant statistic termed 'logSED' ('sum of expression differences'), in which we aggregated predicted coverage in the bins  $\\mathcal{B} = \\{b_{1},\\dots,b_{K}\\}$  overlapping the exons of the target gene, and compared the log fold change between alternate and reference alleles:  $\\log_2\\left(\\sum_{k = 1}^{K}\\mathbf{y}_{\\mathcal{B}(k)}^{\\mathrm{(alt)}}\\right) - \\log_2\\left(\\sum_{k = 1}^{K}\\mathbf{y}_{\\mathcal{B}(k)}^{\\mathrm{(ref)}}\\right)$ .\n\nFor the third task, we evaluated Borzoi's ability to identify the gene(s) affected by an eQTL from the set of local genes, which is intended to estimate how accurately the model can prioritize the correct gene at more general GWAS loci. We downloaded fine-mapped eQTL credible sets and their associated eGenes for 49 GTEx tissues from the eQTL catalog (release 5) $^{79,80}$ . The credible set files were downloaded from ftp://ftp.ebi.ac.uk/pub/databases/spot/eQTL/credible_sets/ (e.g. ftp://ftp.ebi.ac.uk/pub/databases/spot/eQTL/credible_sets/GTEX_gen_adipose_subcutaneous.purity-filtered.txt.gz).\n\nNote: These file paths have since changed but historical versions can be found at https://github.com/eQTL-Catalogue/eQTL-Catalogueresources/blob/00ea8a7abca895f26c3aee74ece1307dc5054ace/tabix/tabixftp_paths.tsv. To download credible sets with the latest file path table, use column 'ftp_cs_path' (e.g. for adipose_subcutaneous, download file ftp://ftp.ebi.ac.uk/pub/databases/spot/eQTL/susie/QTS000015/QTD000116/QTD000116.credible_sets.tsv.gz).\n\nFor each variant within a credible set, we predicted a gene-specific L2 score, which considers only sequence positions overlapping the genes' exons, for all genes within a 360,448 bp sequence window centered on the variant. For each credible set, we computed a single score for each surrounding gene by averaging the gene's score across variants weighted by their posterior causal probabilities. For each GTEx tissue, we computed a variant's L2 score using model predictions for the matched GTEx RNA-seq tracks. We analyzed only credible sets associated with protein-coding genes. Owing to the indel challenge described above, we further removed credible sets in which a fine-mapped variant (PIP > 0.1) is an indel. We predicted a credible set's target gene as the gene with the highest aggregate PIP-weighted L2 score for that credible set. As a baseline, we predicted a credible set's target gene as the nearest gene. We define 'nearest gene' as the gene with the maximum PIP-weighted inverse distance from the credible set. Maximizing the PIP-weighted inverse distance outperforms the previously described approach of minimizing the PIP-weighted distance $^{105}$ . Notably, a single distal credible set variant can inflate the minimum average distance statistic, resulting in an incorrect eGene prediction, whereas maximizing the inverse distance does not lead to this problem.\n\n# Fine-mapped paQTL classification task\n\nWe benchmarked Borzoi's ability to predict genetic variants that alter the relative abundance of mRNA  $3^{\\prime}$  isoforms using fine-mapped  $3^{\\prime}$  QTLs (referred to in this paper as polyadenylation QTLs) obtained from the eQTL catalog via txrevise processing $^{79,80}$ . The file paths to the fine-mapping results were obtained from https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/tabix/tabixftp_paths.tsv.\n\nTable rows were filtered by study = 'GTEX' and quant_method = 'txrev'. The resulting sumstat files (for example, 'GTEX_txrev_adipose_subcutaneous.all.tsv.gz') were changed to fine-map files ('GTEX_txrev_adipose_subcutaneous.purity-filtered.txt.gz') and downloaded from ftp://ftp.ebi.ac.uk/pub/databases/spot/eQTL/credible_sets/ (e.g. ftp://ftp.ebi.ac.uk/pub/databases/spot/eQTL/credible_sets/GTEX_txrev_adipose_subcutaneous.purity_filterd.txt.gz).\n\nNote: These file paths have since changed but a historical version of the file path table can be found at https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/00ea8a7abca895f26c3aee74ece1307dc5054ace/tabix/tabixftppaths.tsv. To download credible sets with the latest file path table, use column 'ftp_csc_path' (e.g. for adipose_subcutaneous, download file ftp://ftp.ebi.ac.uk/pub/databases/spot/eQTL/susie/QTS000015/QTD000119/QTD000119.credible_sets.tsv.gz).\n\nTo build negative sets of GTEx SNPs that are not part of any txrevise credible set, we obtained rows from the file path table where quant_method = 'ge' and downloaded the full sumstat files from ftp://ftp.ebi.ac.uk/pub/databases/spot/eQTL/sumstats/GTEX/ge/ (e.g. ftp://ftp.ebi.ac.uk/pub/databases/spot/eQTL/sumstats/GTEX/ge/GTEX_ge_adipose_subcutaneous.all.tsv.gz). These file paths have also changed;\n\nto download sumstat files with the latest file path table, use column 'ftp_path' (e.g. for adipose_subcutaneous, download file ftp://ftp.ebi.ac.uk/pub/databases/spot/eQTL/sumstats/QTS000015/QTD000116/QTD000116.all.tsv.gz).\n\nFine-mapped causal paQTLs for a given tissue were obtained from the corresponding fine-mapping file ('XYZ.purity-filtered.txt.gz') by filtering on rows in which molecular_traid_id contained the substring 'downstream,' the SNP occurred at most 50 bp outside of a gene span (GENCODE v.41), the distance to the nearest annotated  $3^{\\prime}$  UTR PAS in PolyADB  $(\\mathrm{v}.3)^{44}$  was at most 10,000 bp and PIP was  $\\geq 0.9$ . Valid negatives were obtained from the tissue's sumstat file ('XYZ.all.tsv.gz') with identical gene-span and PAS distance filters as the fine-mapped paQTLs. Negative SNPs had to be either absent from all credible sets or have PIP  $< 0.01$  across all GTEx tissues. Finally, we selected one negative SNP for each fine-mapped causal paQTL by requiring that they have identical distances to an annotated PAS and that the negative SNP occurs in a gene with expression levels that are within (and less than) 1.5-fold the expression level of the paQTL gene (in the same tissue). This resulted in 1,058 retained unique fine-mapped causal paQTLs. The following procedure was used to efficiently search for negative SNPs fulfilling these requirements for a given tissue:\n\nStep 1. Discretize and bin the  $\\log_2(TPM)$  values of all genes (GTEX v.8) into buckets of size 0.4 (in  $\\log_2$ -space). Step 2. For a given query gene (and its associated  $\\log_2(TPM)$  value), take all candidate genes that map into the same bucket. Scan this subset of genes for any gene that contains a distance-matched non-causal SNP. Step 3. If none of the genes in the bucket are suitable candidates (none have a non-causal distance-matched SNP), then subtract 0.15 from the query  $\\log_2(TPM)$  value and take all candidate genes that were binned into the new bucket (if subtracting 0.15 does not change the bucket, skip to Step 4). Scan this new bucket for suitable genes. Step 4. If no suitable gene has been found, repeat Step 3 but instead add 0.15 rather than subtract 0.15 to the original  $\\log_2(TPM)$  value. Scan this (potentially) new bucket for suitable genes. Step 5. If no suitable gene has been found, exit with an error (unmatchable).\n\nThe maximum  $\\log_2$  fold change that two genes can be within and still match is  $0.4 + 0.15 = 0.55$  (-1.464-fold). With these parameter settings, each bucket contained at least 100 genes, and we never exited Step 5 with an error.\n\nNote that owing to the relatively small number of fine-mapped paQTLs, we decided to pool all tissues rather than benchmark separately per tissue. Given that many of the positives are shared between tissues (there are a total of 1,058 unique paQTLs, each occurring in at least one tissue), we end up with  $\\sim 2.5\\times$  the amount of unique negative SNPs after merging across tissues. Hence, for the benchmark, we performed 100 permutations of randomly matching one of the multiple valid negative SNPs (from different tissues) to each corresponding positive SNP and evaluated performance on each permutation set of 1,058 positives and 1,058 sampled negatives.\n\nIntronic paQTLs (and matched negatives) were extracted from the same files as above but had to occur in intronic regions and be closer to an annotated intronic polyadenylation site than any  $3^{\\prime}$  UTR polyadenylation site. Negatives were now matched by distance to the nearest intronic PAS. A total of 567 fine-mapped causal intronic paQTLs were retained.\n\n# Polyadenylation variant effect prediction\n\nWe compute polyadenylation-centric variant effect scores from Borzoi's predicted RNA coverage tracks as the maximum ratio of coverage fold change between any annotated  $3^{\\prime}$  cleavage junction within the UTR of the same gene as the SNP. Specifically, we center the 524 kb input window on the SNP, predict coverage tracks  $\\pmb{y}^{(\\mathrm{ref})} = \\mathcal{M}(\\pmb{x}^{(\\mathrm{ref})}),\\pmb{y}^{(\\mathrm{alt})} = \\mathcal{M}(\\pmb{x}^{(\\mathrm{alt})})\\in \\mathbb{R}^{16,384\\times 7,611}$  given the reference and alternate allele sequences  $\\pmb{x}^{(\\mathrm{ref})}$  and  $\\pmb{x}^{(\\mathrm{alt})}$  as input and compute the statistic  $\\pmb{u}(\\pmb{y}^{(\\mathrm{ref})},\\pmb{y}^{(\\mathrm{alt})})_t$  for coverage track  $t$  as follows:\n\n$$\n\\begin{array}{l} \\boldsymbol {u} _ {t} = \\max  _ {k = 1} ^ {K - 1} \\| \\log_ {2} \\\\ \\left. \\frac {(1 / k) \\times \\sum_ {u = 1} ^ {k} \\left(\\left(\\sum_ {j \\in \\mathcal {B} (u) - 4} \\mathbf {y} _ {j , t} ^ {\\text {(a l t)}}\\right) / \\left(\\sum_ {j \\in \\mathcal {B} (u) - 4} \\mathbf {y} _ {j , t} ^ {\\text {(r e f)}}\\right)\\right)}{\\left. \\frac {1}{(1 / (K - k - 1))} \\times \\sum_ {u = k + 1} ^ {K} \\left(\\left(\\sum_ {j \\in \\mathcal {B} (u) - 4} \\mathbf {y} _ {j , t} ^ {\\text {(a l t)}}\\right) / \\left(\\sum_ {j \\in \\mathcal {B} (u) - 4} \\mathbf {y} _ {j , t} ^ {\\text {(r e f)}}\\right)\\right)}\\right) \\Bigg |}\\right) \\\\ \\end{array}\n$$\n\n$K$  in the equation above denotes the total number of PASs within the UTR.  $\\mathcal{B} = \\{b_1,\\dots,b_K\\}$  is the ordered set of bin indices in  $y$  overlapping the KPASs. The final score used in the benchmarks was the average statistic computed from all of Borzoi's 89 GTEx coverage tracks. The score was also averaged over all four model replicates in both forward-complemented and reverse-complemented input formats.\n\n# Comparison to APARENT2 and Saluki\n\nWe compare Borzoi's classification performance to APARENT2 (v.1.0.2) in two ways. First, we score the reference and alternate PAS sequence affected by the variant using APARENT2 and simply use the absolute value of the predicted log odds ratio as the variant effect score. Second, we use the predicted odds ratio to scale the tissue-pooled reference PAS usage (as reported in PolyADB) and use the absolute value of the difference in PAS usage as the final variant effect score. The latter statistic effectively dampens the magnitude of variants, which, based on APARENT2's prediction, has a large predicted fold change but, according to measurements, occur in lowly used PASs (owing to competing PASs).\n\nWhen comparing performance to an ensemble consisting of both APARENT2 and Saluki (v.1.0.0) on the paQTL classification task, we follow the methodology from the APARENT2 paper[22]. In brief, we curate the PAS sequences and corresponding mRNA isoforms of each gene (at most 30) based on annotations from PolyADB and fit a logistic regression model to predict tissue-pooled distal isoform proportions (as reported in PolyADB) given both APARENT2's PAS scores (at most 30 scalars) and Saluki's isoform scores (at most 30 vectors of top four PCA components extracted from the penultimate layer of Saluki) as input. Using this calibrated ensemble model, we predict the reference and alternate distal proportions of a gene when inducing a particular variant (which may affect multiple PAS- and isoform sequences). We estimate a final odds ratio from the predicted distal proportions and use the odds ratio to recalculate the alternate distal proportion based on the measured reference distal proportion. Finally, we subtract the alternate distal proportion from the reference proportion and use the absolute value of this difference as the final variant effect score.\n\n# Fine-mapped sQTL classification task\n\nFine-mapped sQTLs and matched negatives were obtained from the eQTL catalog $^{79,80}$  using the same sumstat and fine-mapping files as were used for the paQTL classification task. The fine-mapped causal sQTLs were extracted by filtering on rows in which molecular_trait_id contained the substring 'contained.' These QTLs were further filtered on  $\\mathrm{PIP} \\geq 0.9$  and on a maximum distance of  $\\leq 10,000$  bp to an annotated splice junction (GENCODE v.41). A set of expression-matched and distance-matched negatives were constructed per tissue in an identical fashion to the paQTL task, with the exception of matching by nearest distance to splice junctions. We retained a total of 4,105 unique fine-mapped causal sQTL SNPs.\n\n# Splicing variant effect prediction\n\nPurely isolating splicing impact from other mechanisms proved challenging. We focus on a simple statistic that worked well in practice; namely, the maximum difference in normalized coverage across the gene span. Specifically, we center the  $524\\mathrm{kb}$  input window on the SNP, predict coverage tracks  $\\mathbf{y}^{(\\mathrm{ref})} = \\mathcal{M}(\\mathbf{x}^{(\\mathrm{ref})}),\\mathbf{y}^{(\\mathrm{alt})} = \\mathcal{M}(\\mathbf{x}^{(\\mathrm{alt})})\\in \\mathbb{R}^{16,384\\times 7,611}$  and compute the statistic  $\\mathbf{u}(\\mathbf{y}^{(\\mathrm{ref})},\\mathbf{y}^{(\\mathrm{alt})})_t$  for coverage track  $t$  as follows:\n\n$$\n\\boldsymbol {u} _ {t} = \\max  _ {j = b _ {\\text {s t a r t}}} ^ {b _ {\\text {e n d}}} \\left| \\frac {\\boldsymbol {y} _ {j , t} ^ {(\\text {a l t})}}{\\sum_ {k = b _ {\\text {s t a r t}}} ^ {b _ {\\text {e n d}}} \\boldsymbol {y} _ {k , t} ^ {(\\text {a l t})}} - \\frac {\\boldsymbol {y} _ {j , t} ^ {(\\text {r e f})}}{\\sum_ {k = b _ {\\text {s t a r t}}} ^ {b _ {\\text {e n d}}} \\boldsymbol {y} _ {k , t} ^ {(\\text {a l t})}} \\right|\n$$\n\nThe indices  $b_{\\text{start}}$  and  $b_{\\text{end}}$  refer to the bins in  $y$  overlapping the start and end positions of the gene span. The relatively large number of fine-mapped causal sQTLs allows for a tissue-specific benchmark comparison. To that end, for a given SNP and GTEx tissue, we average the statistic only over the subset of tracks corresponding to the tissue.\n\n# Comparison to Pangolin\n\nWe used the pre-packaged command-line utility to score sQTL SNPs with Pangolin (v.1.0.1) $^{16}$ . To make comparisons easier, we modified the program to output scores with six rather than two decimals. We used the following command to score the positive and negative vcf files: pangolin -d 2,000 -m False < sqtl file >.vcf hg38.fa gencode41basic_nort_protein.db < out dir >.\n\nAlthough this command allows at most a distance of 2,000 bp from an annotated splice junction, Pangolin will also score potential de novo splice gains at the variant position, meaning that the command will produce variant effect scores for all variants (even those separated by  $>2,000$  bp from a splice site). We parsed the command-line output and matched the gene identifier of the Pangolin output to the gene in which the SNP occurs. The final variant effect score is calculated as the sum of the absolute values of the predicted maximum increase and decrease.\n\n# Splice site identification task\n\nIdentifying splice sites in DNA sequences has formed the basis for a successful approach to interpreting the splicing code and prioritizing pathogenic splicing variants $^{15,16}$ . To evaluate Borzoi's ability to identify splice sites, we constructed an analogous classification task and compared it to Pangolin $^{16}$ . We downloaded the splicing junction counts for all GTEx samples from recount3 and selected positive examples from annotated junctions with coverage above the  $50^{\\text{th}}$  percentile of aligned read counts. We filtered this set for those that fall in the intersection of Pangolin's and Borzoi's test sets. For each positive example, we selected a matching negative site that had the same tri-nucleotide context, was between 100 bp and 2,000 bp away and lacked evidence of being a splice junction itself. For Borzoi, we scored each site as the predicted log ratio of exon-to-intron coverage around the junction, averaged across samples from the corresponding GTEx tissue. For Pangolin, we scored each site with its predicted splice site probability, averaged across all tissues.\n\n# Classifying rare and common variation from gnomAD\n\nWe sampled a set of 14,198 singletons and 14,198 matched common variants (allele frequency  $>5\\%$ ) from the GnomAD (v.3.1) database (https://gnomad.broadinstitute.org), with sampling restricted to regions overlapping ENCODE candidate cis-regulatory elements. To control for sequence mutability, we excluded variants within CpG islands and low-complexity regions. For each singleton sampled, we sampled a negative example as a matched common variant with the same reference and alternate allele as the singleton. We also matched the variants' background DNA contexts, sampling common variants that lie within the same tri-nucleotide as the singleton. Finally, we removed variants overlapping gene exons in coding sequences (GENCODE v.41), focusing only on regulatory variants for our evaluation. For all sampled variants, we used their CADD raw score and CADD phred scores (v.1.6) from the GnomAD (v.3.1) dataset. We trained ridge regression models to discriminate common variants from singletons and used tenfold cross-validation to evaluate the models. The CADD-based model uses the CADD scores as features, whereas the Borzoi-based model uses the L2 scores across all RNA-seq tracks as features, averaged across the four model replicates. We derived a third (combined) model by averaging predicted variant ranks for the Borzoi-based and CADD-based models. For a second genome-wide benchmark, we sampled uniformly from across the genome instead of restricting the variant sampling to ENCODE candidate cis-regulatory elements. This resulted in a variant set containing 17,360 singletons and 17,360 matched common variants.\n\n# Predicting TRIP expression\n\nWe downloaded TRIP insertion coordinates and measured expression levels for seven distinct promoters from the supplementary material of a previous publication[68]. The promoter sequences are listed in Table S1 and the insertion coordinates (and measurements) are listed in Data S2 of that paper. To predict the activity of TRIP reporters, we iterated over each promoter sequence and coordinate, centered the  $524\\mathrm{kb}$  input window on the insertion coordinate and inserted the sequence. When deriving statistics from Borzoi's RNA-seq or CAGE predictions, we inserted the entire TRIP reporter into the genomic location (including the promoter sequence, the GFP CDS, the PAS and the PiggyBac terminal repeat regions). By contrast, when deriving statistics from Borzoi's DNase or histone modification tracks (for example, H3K4me3) we only inserted the promoter, as these predictions became marginally worse when inserting the full reporter. We attribute this phenomenon to the PiggyBac transposable elements flanking the reporter, which Borzoi inherently does not predict well owing to the clipping of unmappable regions during the original training data processing.\n\nGiven the predicted coverage  $\\pmb{y} = \\mathcal{M}(\\pmb{x}) \\in \\mathbb{R}^{16,384 \\times T}$  for the  $T$  coverage tracks considered (for example, K562 DNase tracks), we calculate a scalar prediction  $u(x) \\in \\mathbb{R}$  by averaging the coverage tracks, aggregating the signal in a local window of size  $W$  centered at the insertion site and applying a  $\\log_2$  transform:\n\n$$\nu = \\log_ {2} \\left((1 / T) \\times \\sum_ {t = 1} ^ {T} \\sum_ {j = - W / 2} ^ {W / 2} \\mathbf {y} _ {8, 1 9 2 + j, t}\\right)\n$$\n\nFor CAGE and RNA-seq outputs, we used a 4,096 bp window size that tightly covered the full reporter construct (and tightly covered the average signal profile, as exemplified in Supplementary Fig. 8a for promoter ARHGEF98). Although this was technically a sub-optimal choice (a narrow 128 bp window maximized the average Spearman's  $R$  across promoters for RNA-seq; see Supplementary Fig. 8b), the difference in Spearman's  $R$  was small (for example,  $< 0.02$  for ARHGEF98) and a 4,096 bp window size was a more intuitive choice. Similarly, the average optimal CAGE window size was 8,813 bp, but the 4,096 bp window had near-identical performance ( $< 0.01$  difference in average Spearman's  $R$  across promoter types). For DNase and histone ChIP tracks, we used a slightly wider 8,192 bp window size as we noticed that the correlation to measured expression saturated less quickly than CAGE as a function of window size (for example,  $-0.02$  difference in average Spearman's  $R$  across promoter types when comparing a window size of 4,096 bp to 8,192 bp for H3K4me3).\n\n# Gene-enhancer prioritization task\n\nWe evaluated Borzoi's ability to link distal regulatory elements to genes by analyzing experiments in which CRISPRi was used to block the regulatory element followed by measuring gene expression. These experiments have been performed on a small set of specifically chosen genes in which expression was measured by various techniques $^{60-65}$  and a large set of all expressed genes in which perturbation and expression were measured by single-cell RNA-seq (scRNA-seq) $^{58}$ . These datasets were analyzed to consider whether each tested regulatory element significantly altered gene expression, defining a set of binary labels. The flow/proliferation dataset contains 117 positives out of 2,194 tested within  $262\\mathrm{kb}$  of the gene's TSS. After filtering for genes with  $\\geq 3$  elements tested, the scRNA-seq dataset contains 404 positives of 19,104 tested within  $262\\mathrm{kb}$  of the gene's TSS. These numbers shrunk further on a per-analysis basis after requiring that each enhancer-gene pair is within the input window of the current set of models evaluated in a given benchmark.\n\nFor both Enformer and Borzoi, we scored putative enhancers using input gradient analysis. For Enformer, we computed the gradient of the K562 CAGE prediction in the two 128 bp bins centered at the gene's\n\nTSS. Computing the gradient using three bins (as in the original paper) resulted in marginally worse performance. The gradient score statistic was averaged for genes with multiple TSSs, which performed better than taking either the max or sum. For Borzoi, we computed the gradient of the K562 RNA-seq prediction for all bins overlapping the gene's exons in GENCODE (v.41). For each nucleotide, we took the absolute value of the reference nucleotide gradient. For each regulatory element, we computed a weighted average of the nucleotide scores using Gaussian weights  $(\\mathrm{s.d.} = 300)$ , centered at the element's midpoint. This approach improved performance for both Enformer and Borzoi compared to a simpler strategy of averaging the absolute-valued gradients in a  $2\\mathrm{kb}$  window centered on the enhancer. To calibrate scores across genes with different expression levels, we normalized the scores by the mean nucleotide score across the entire region.\n\nThe analysis was repeated using an in-silico perturbation approach instead of input gradients. The putative enhancers were independently dinucleotide-shuffled with a 2 kb window. Using Borzoi, each shuffle was repeated 16 times for both forward and reverse orientations and for all four model replicates (128 times total). For Enformer, each shuffle was repeated 64 times in forward and reverse orientations. For Borzoi, the absolute-valued percent change in exon-aggregated RNA-seq coverage was used as the final statistic. For Enformer, the absolute-valued percent change in aggregated CAGE signal was used (within two or three output bins). Smaller or larger window sizes only marginally affected the results (as shown in Supplementary Fig. 7a).\n\n# Saturation mutagenesis MPRA benchmark\n\nThe saturation mutagenesis experiment from a previous publication[106] was used to compare Borzoi to Enformer on non-QTL variation data. Each measured variant was induced in the hg38 reference sequence and centered on when making predictions. For DNase, CAGE and histone ChIP tracks, variant effects were estimated as the log fold change in coverage within a 4 kb window, whereas scores for RNA-seq were computed as the log fold change in exon-aggregated coverage. The final predictions were calculated as an unweighted average of (potentially a subset of) the different assays' scores. Using a narrow 512 bp window for aggregation as in the Enformer paper[13] resulted in worse concordance with measured effects for some promoters and better concordance for other promoters. We settled on the wider 4 kb window as it led to better performance on the majority of promoters. Only promoters and enhancers with better performance using cell-type-matched outputs in the Enformer paper were included to simplify the benchmark. The same cell-type mappings were used except for promoters F9 (K562 instead of HepG2), LDLR (adrenal gland instead of HepG2) and HNF4A, MSMB, TERT and MYC (adrenal gland RNA-seq instead of HEK293T RNA-seq). These changes led to better performance for Borzoi and were reasonable choices with respect to the target genes' expression patterns. The same changes were made to Enformer's mappings if they resulted in an improvement.\n\n# Codon stability comparison\n\nPrior work has demonstrated strong relationships between codon usage and mRNA half-life $^{23,78}$ . We constructed a Borzoi codon statistic to compare to those previously measured. For the Gasperini $^{58}$  scRNA-seq enhancer screen, we computed input gradients for a set of 4,778 genes for K562 gene expression. We made use of these gradients here to quantify codon contributions to expression. For each reference codon in these genes, we used the gradients to approximate the predicted effect of changing it to all alternative codons with a single base-pair mutation. We used least squares regression to fit a coefficient for each codon on this set of possible codon mutations and effects. Finally, we compared these coefficients to codon stability coefficients computed in previous work $^{78}$  as the Pearson correlation between codon frequency and mRNA half-life in three mammalian cell lines: HeLA, mouse embryonic stem cells and CHO cells $^{78}$ .",
    "metadata": {
      "md_filename": "MinerU_markdown_Borzoi_20260106143133_2008426188456595456.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_Borzoi_20260106143133_2008426188456595456.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "fcd1d9f090fa362a",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_BPNet_20260106143130_2008426172165922816",
    "title": "Base-resolution models of transcription-factor binding reveal soft motif syntax",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "Cell culture. R1 ESCs were cultured on  $0.1\\%$  gelatin-coated plates without feeder cells in N2B27 medium (DMEM/F12 with 1:1 mix of GlutaMax/N2 and Neurobasal medium/B27, Invitrogen) supplemented with  $2\\mathrm{mM}$  L-glutamine (Stemcell Technologies),  $1\\times 2$ -mercaptoethanol (Millipore),  $1\\times$  NEAA (Stemcell Technologies),  $3\\mu \\mathrm{M}$  CHIR99021 (Stemcell Technologies),  $1\\mu \\mathrm{M}$  PD0325901 (Stemcell Technologies),  $0.033\\%$  BSA solution (Invitrogen) and  $10^{7}\\mathrm{U}\\mathrm{ml}^{-1}$  LIF (Millipore).\n\nChIP-nexus, PATCH-CAP and ChIP-seq experiments. For each ChIP-nexus experiment, 10 million ESCs were used. Cells were washed with PBS and cross-linked with  $1\\%$  formaldehyde (Fisher Scientific) in PBS for  $10\\mathrm{min}$  at room temperature. The reaction was quenched with  $125\\mathrm{mM}$  glycine. Fixed cells were washed twice with cold PBS, resuspended in cold lysis buffer  $(15\\mathrm{mM}$  HEPES pH 7.5,  $140\\mathrm{mMNaCl},$ $1\\mathrm{mM}$  EDTA,  $0.5\\mathrm{mM}$  EGTA,  $1\\%$  Triton X-100,  $0.5\\%$  N-lauroylsarcosine,  $0.1\\%$  sodium deoxycholate and  $0.1\\%$  SDS), incubated for  $10\\mathrm{min}$  on ice and sonicated with a Bioruptor Pico (Diagenode) for five cycles of  $30\\mathrm{s}$  on and  $30\\mathrm{s}$  off. The ChIP-nexus procedure and data processing were performed as previously described[49], except that the ChIP-nexus adapter mix contained four fixed barcodes (ACTG, CTGA, GACT and TGAC) and PCR library amplification was performed directly after circularization of the purified DNA fragments (without addition of the oligo and BamHI digestion). PATCH-CAP was performed as previously described[59] with  $10\\%$  of sheared chromatin from 10 million ESCs. ChIP-seq experiments were performed as previously described[119] with 10 million ESCs per ChIP.\n\nFor each ChIP,  $5\\mu \\mathrm{g}$  of antibody was coupled to  $50\\mu \\mathrm{l}$  of Protein A or Protein G Dynabeads (Invitrogen). The following antibodies were used: anti-Oct3/4 (Santa Cruz, no. sc-8628), anti-Sox2 (Santa Cruz, no. sc-17320), anti-Sox2 (Active Motif, no. 39843), anti-Nanog (Santa Cruz, no. sc-30328), anti-Klf4 (R&D Systems, no. AF3158), anti-Klf4 (Abcam, no. ab106629), anti-Esrrb (Abcam, no. ab19331), anti-Pbx 1/2/3 (Santa Cruz, no. sc-888) and anti-Zic3 (Abcam, no. ab222124). For all experiments, at least two biological replicates were preparedthat is, the experiments were performed on different days starting with cells from a different passage number. Single-end sequencing was performed on either an Illumina HiSeq (50 cycles) or NextSeq 500 instrument (75 cycles).\n\nMutation of binding motifs using CRISPR/Cas9 technology. Using mouse R1 ESCs, the predicted Nanog motif on chr10: 85,539,756-85,539,765 (mm10) was mutated from CTGATGGCT (wild type) to CGGCTGGCT (mutant). The predicted Sox2 motif on chr10: 85,539,634-85,539,643 (mm10) was mutated from CCTTTGTCC (wild type) to CCTAGGTTCC (mutant). Guide RNA target sites were designed using the CCTop target predictor tool[120] by evaluation of the predicted on-target efficiency score and off-target potential[121]. The single-stranded donor oligonucleotides (ssODN) were designed containing  $\\sim 40$  bases of homology from the targeted cut site (gRNA and ssODN sequences are shown in Supplementary Table 3). A ribonucleoprotein (RNP) complex was formed by combining 90 pmol of gRNA (ordered as Alt-R single-guide RNA; IDT) and 10 pmol of Cas9 HiFi protein (IDT) with hybridization for  $10\\mathrm{min}$  at room temperature. The RNP was combined with 100 pmol of ssODN donor and delivered to cells by Neon electroporation (1,500 V, 10 ms, three pulses; Neon Transfection System, Model MPK5000, Life Technologies). Single cells were screened for the expected mutations through paired-end sequencing on an Illumina MiSeq instrument (250 cycles). On-target indel frequency and expected mutations were analyzed using CRIS.py[122]. Only clones with the intentional mutation and sequence alignments  $>90\\%$  were chosen for future experiments.\n\nPer target site, three monoclonal cell lines were selected and used as replicate experiments: clones B07, B09 and F10 for the mutant Nanog motif, and clones B07, B11 and C10 for the mutant Sox2 motif. For the wild-type R1 ESC control samples, at least two biological replicates were prepared as above. ChIP-nexus was performed as described above (ChIP-nexus, PATCH-CAP and ChIP-seq experiments) with 20 million ESCs and  $5\\mu \\mathrm{g}$  of anti-Nanog (Abcam, no. ab214549) or anti-Sox2 (Active Motif, no. 39843) per replicate. The following fixed barcodes were used: AGTC, CAGT, GTCA and TCAG. Single-end sequencing was performed on an Illumina NovaSeq instrument (100 cycles) to obtain a coverage of  $\\sim 400$  million reads per experiment.\n\nChIP-nexus data processing pipeline. Random barcodes and fixed barcodes were trimmed off the reads and reassigned to FASTQ labels using nimnexus (v.0.1.1). The adapters were then trimmed using cutadapt (v.1.8.1) $^{123}$ . Next, the reads were aligned with Bowtie (v.1.1.12) $^{124,125}$  using the command bowtie --chunkmbs 512 -k 1 -m 1 -v 2 --best --strata to the mouse genome assembly mm10. Mutant samples were aligned to a modified mm10 genome that accommodated the CRISPR changes. Mapping statistics were computed using SAMtools flagstat (v.1.2) $^{126}$ . Reads were filtered using SAMtools view to remove unmapped reads and mates, nonprimary alignments, PCR or optical duplicates (-F 1804) and reads that failed platform or vendor quality checks or had poor mapping quality (<30 MAPQ score). Reads aligned to the same position with the same barcode, CIGAR string and the SAM flag were deduplicated using nimnexus dedup (v.0.1.1). The total number of final (filtered) aligned reads was 243 million for Oct4, 140 million for\n\nSox2, 214 million for Nanog and 176 million for Klf4. The final filtered BAM file was converted to tagAlign format (BED  $3 + 3$ ) using bedtools 'bamtobed' (v.2.26) $^{127}$ . Cross-correlation scores were obtained for each file using phantompeakquality (v.1.2) $^{128}$ . BigWig tracks containing the strand-specific number of aligned  $5^{\\prime}$  read ends (pooled across all replicates) were generated using bedtools genomecov -5 -bg -strand  $< + / - >$ , followed by bedGraph to BigWig conversion using UCSC bedGraphToBigWig v.4 (ref. $^{129}$ ).\n\nPeaks were called using MACS2 (v.2.1.1.20160309) by extending the  $5^{\\prime}$  ends of reads on each strand using a 150-bp window  $(\\pm 75\\mathrm{bp})$  and then computing coverage of extended reads across both strands (shift  $= -75$ , extsize  $= 150$ ). For each TF, peak calling was performed on filtered, aligned reads from each replicate using a relaxed  $P$  threshold of 0.1 and retaining the top 300,000 peaks as described[128]. Relaxed peak calls were similarly performed on pseudoreplicates, which were obtained by pooling filtered, aligned reads from all replicates for a TF and randomly splitting the pooled reads into two balanced pseudoreplicates. Peaks overlapping the blacklisted regions listed in https://www.encodeproject.org/files/ENCFF547MET/ were excluded. The irreproducible discovery rate (IDR) framework was used to obtain reproducible peaks across the true replicates and pseudoreplicates[130]. The set with the larger number of peaks was defined as the IDR optimal peaks for each TF: 25,849 for Oct4, 10,999 for Sox2, 56,459 for Nanog and 57,601 for Klf4. Regions of 1 kb centered on the peak summits were used as inputs to BPNet. All samples passed quality control metrics used in the ENCODE TF ChIP-seq pipeline[128] (Supplementary Table 1).\n\nThe nim-nexus code is available at https://github.com/Avsecz/nimnexus/. The ChIP-nexus pipeline performing the described steps (for example, turning raw reads in FASTQ format to BigWig coverage tracks and called peaks) is available at https://github.com/kundajelab/chip-nexus-pipeline. A detailed pipeline specification is available at https://docs.google.com/document/d/1h9IZ0GyVWd02RCmtaFWSaSFzrcNHoH_OgyPHMpU7b04. ChIP-seq datasets were processed using the ENCODE ChIP-seq pipeline: https://github.com/ENCODE-DCC/chip-seq-pipeline2/releases/tag/v1.2.2. This is identical to the ChIP-nexus pipeline except that it uses the SPP peak caller $^{29}$  and does not use barcodes for read dedduplication.\n\nBPNet architecture. BPNet is a sequence-to-profile convolutional neural network that uses one-hot-encoded DNA sequence ( $A = [1,0,0,0]$ ,  $C = [0,1,0,0]$ ,  $G = [0,0,1,0]$ ,  $T = [0,0,0,1]$ ) with adjustable length as input to predict base-resolution read count profiles as output. For flexibility, the architecture of BPNet can be compartmentalized into body- and multiple task-specific output heads. The body of BPNet consists of a sequence of convolutional layers with residual skip connections and rectified linear activations[57]. The first convolutional layer uses 64 filters of width 25 bp, followed by nine dilated convolutional layers (each with 64 filters of width 3) where the dilation rate (number of skipped positions in the convolutional filter) doubles at every layer. This results in a receptive field of  $\\pm 1,034$  bp for any position in the input sequence. The output of the final convolutional layer within the BPNet body (also referred to as the bottleneck activation map) serves as input for two output heads per TF: (1) a deconvolutional layer (filter width 25a typical ChIP-nexus footprint width) predicting the strand-specific probabilities of observing a particular read at a particular position in the input sequence (shape or profile prediction); and (2) a global average pooling layer followed by the fully connected layer predicting the total number of read counts aligned to the input sequence for each strand (total read count prediction). The training occurs for all TF ChIP-nexus experiments together in a multitask fashion. BPNet architecture (without bias correction) implementation in Keras v.2.2.4 is provided in Supplementary Methods.\n\nBPNet loss function. Let  $\\mathbf{k}^{\\mathrm{obs}}$  be the vector of length  $L$  of observed read counts for a particular strand and a particular task (that is, TF) along the sequence of length  $L$ . Let  $\\mathbf{p}^{\\mathrm{pred}}$  be the vector of length  $L$  of predicted probabilities along the sequence, such that  $\\sum p_i = 1$  and let  $n^{\\mathrm{obs}} = \\sum k_i^{\\mathrm{obs}}$  be the total number of observed counts and  $n^{\\mathrm{pred}}$  the total number of predicted counts for the sequence. The following loss function is used for each sequence, strand and task:\n\n$$\n\\operatorname {L o s s} = - \\log \\mathbf {p} _ {\\text {m u l t .}} \\left(\\mathbf {k} ^ {\\text {o b s}} | \\mathbf {p} ^ {\\text {p r e d}}, n ^ {\\text {o b s}}\\right) + \\lambda \\left(\\log \\left(1 + n ^ {\\text {o b s}}\\right) - \\log \\left(1 + n ^ {\\text {p r e d}}\\right)\\right) ^ {2}.\n$$\n\nThe first term evaluates the error in the shape of the predicted profile. It is the multinomial  $\\left(\\mathbf{p}_{\\mathrm{mult.}}(\\mathbf{k}|\\mathbf{p},\\mathbf{n}) = \\frac{n!}{k_1!\\cdots k_d!} p_1^{k_1}\\dots p_d^{k_d}\\right)$  negative log-likelihood of observed base read counts given the predicted probabilities and total number of observed counts. The second term evaluates the squared error of the log total number of reads in the region. During BPNet training, the total loss function is the sum of individual loss functions across both strands, all input sequences and all tasks.\n\nThe key hyperparameter is  $\\lambda$ . In Supplementary Methods (relationship between Poisson log-likelihood, mean-squared error and multinomial log-likelihood), we show that if  $\\lambda = \\bar{n}^{\\mathrm{obs}} / 2$ , where  $\\bar{n}^{\\mathrm{obs}}$  is the average number of total counts across all sequences in our training set, profile loss and total count loss will be given roughly equal weight. To upweight the profile predictions relative to the total count predictions,  $\\lambda = \\frac{\\alpha}{2} n^{\\mathrm{obs}}$  with  $\\alpha < 1$  can be used.\n\nControl for biases by BPNet. Experimental assays often have biases that can be measured by control experiments (input for ChIP-seq and PATCH-CAP for ChIP-nexus $^{59}$ ). To prevent the sequence-to-profile model from learning these noninformative bias signals, the model tries to explain the target experimental track (for example, the Oct4 profile) using both the sequence-based model predictions  $\\mathbf{f}_{\\mathrm{model}}^{h}$  (seq;  $\\mathbf{w}^{h}$ ) for specific head  $h$  and the control experiment track, ctr:\n\n$$\n\\mathbf {y} _ {\\text {p r e d}} ^ {h} = \\mathbf {f} _ {\\text {m o d e l}} ^ {h} (\\operatorname {s e q}; \\mathbf {w} ^ {h}) + \\mathbf {f} _ {\\text {c t l}} ^ {h} (\\operatorname {c t l}; \\mathbf {w} _ {\\text {c t l}} ^ {h}),\n$$\n\nwhere  $\\mathbf{f}_{\\mathrm{ctl}}^h (\\mathrm{ctl};\\mathbf{w}_{\\mathrm{ctl}}^h)$  is a neural network-based transformation of the control track aimed at explaining data for head  $h$ . Integration with the control data therefore occurs after the task-specific model head  $\\mathbf{f}_{\\mathrm{model}}^h$ . We require that  $\\mathbf{f}_{\\mathrm{ctl}}^h (\\mathrm{ctl};\\mathbf{w}_{\\mathrm{ctl}}^h) = 0$  if the control track is 0 (that is, bias not present) so that the model  $\\mathbf{f}_{\\mathrm{model}}^h$  represents the bias-free part of the signal. Each head/track will have a different bias transformation either by having different parameters,  $\\mathbf{w}_{\\mathrm{ctl}}^h$ , or even a different architecture for  $\\mathbf{f}_{\\mathrm{ctl}}^h$ . For the total count prediction head,  $\\mathbf{f}_{\\mathrm{ctl}}^h (\\mathrm{ctl};\\mathbf{w}_{\\mathrm{ctl}}^h)$  is simply  $\\mathbf{w}_{\\mathrm{ctl}}^h\\log (1 + n_{\\mathrm{ctl}})$ , where  $n_{\\mathrm{ctl}}$  is the total number of reads from the control experiment in the modeled local region. For the profile prediction head,  $\\mathbf{f}_{\\mathrm{ctl}}^h (\\mathrm{ctl};\\mathbf{w}_{\\mathrm{ctl}}^h)$  is a weighted sum of (1) the raw counts and (2) a smoothed version of the raw counts using a sliding window sum of  $50\\mathrm{bp}$  (since control data are often sparse). During model training, the parameters of  $\\mathbf{f}_{\\mathrm{ctl}}^h (\\mathrm{ctl};\\mathbf{w}_{\\mathrm{ctl}}^h)$  are also trained to best explain the output using the control track. This framework readily integrates multiple control tracks, or control tracks predicted from sequence, using a bias model learned on other data such as deproteinized genomic DNA for DNase-seq[13].\n\nBPNet training and hyperparameter tuning. ChIP-nexus profiles of Oct4, Sox2, Nanog and Klf4 were used to train and evaluate BPNet. Regions from mouse chromosomes 2, 3 and 4 (20%) were used as the tuning set for hyperparameter tuning. Regions from chromosomes 1, 8 and 9 (20%) were used as the test set for performance evaluation (Supplementary Methods). The remaining regions were used for model training. Hyperparameters were manually adjusted to yield best performance on the tuning set. All neural network models were implemented and trained in Keras (v.2.2.4)[132] (TensorFlow backend v.1.6) using the Adam optimizer[133] (learning rate = 0.004) and early stopping with patience of five epochs.\n\nDeepLIFT contribution scores for sequence-to-profile models. DeepLIFT is a feature attribution method for computing the contribution of each base (feature) in an input sequence to a specific scalar output prediction from a neural network model[65]. DeepLIFT decomposes the difference between the output prediction from an input sequence versus that of a neutral reference input sequence as an additive combination of contribution scores of all bases ( $D$  features) in the input sequence:\n\n$$\n\\mathbf {f} (\\mathbf {x}) - \\mathbf {f} (\\mathbf {r}) = \\sum_ {i} ^ {D} c _ {i}\n$$\n\nwhere  $c_{i}$  is the contribution of feature  $i$  in input  $\\mathbf{x}$  to the model output prediction  $\\mathbf{f}(\\mathbf{x})$  compared to model prediction  $\\mathbf{f}(\\mathbf{r})$  based on the reference input  $\\mathbf{r}$ .\n\nThe output of BPNet for each head is, however, not a scalar, but a  $2D$  tensor of size  $L \\times S$ , where  $L$  is the sequence length and  $S$  is the number of output channels or strands for ChIP-nexus. We therefore needed to adapt DeepLIFT and defined the profile contribution score of a base with respect to the entire output profile as follows:\n\n$$\nc ^ {(\\text {p r o f i l e})} = \\sum_ {i, s} c _ {i s} P _ {i s}\n$$\n\nwhere  $P_{is}$  is the predicted probability values for position  $i$  and strand  $s$ , obtained by normalizing the profile predictions on the logit scale using the softmax function along the sequence axis:  $\\mathbf{p} = \\mathrm{softmax}(\\mathbf{f}(\\mathbf{x}))$ .  $c_{is}$  is the contribution score of the base with respect to the (scalar) profile prediction on the logit scale at position  $i$  and strand  $s$ . A weighted sum is used to ensure that positions with high predicted profile output values are given more weight, but has the disadvantage that it would normally require the contribution scores to be computed  $L \\times S$  (2,000) times for each 1-kb input sequence per TF. To drastically accelerate this computation, we exploit the backpropagation algorithm used in DeepLIFT and the additive decomposition of DeepLIFT scores. We define a new TensorFlow operation as follows:\n\n$$\n\\hat {f} (\\mathbf {x}) = \\sum_ {i} \\operatorname {C o n s t} \\left(p _ {i} (\\mathbf {x})\\right) f _ {i} (\\mathbf {x}),\n$$\n\nwhere Const denotes the tf.stop_gradients operation, which treats the wrapped expression  $p_{i}(\\mathbf{x})$  as a constant. By applying DeepLIFT to  $\\hat{f} (\\mathbf{x})$ , we obtain the desired result in a single DeepLIFT backpropagation step:\n\n$$\nc ^ {(\\text {p r o f i l e})} = \\sum_ {i, s} c _ {i s} p _ {i s}.\n$$\n\nPseudocode of the described operation in TensorFlow code is:\n\nwn = tf.reduce_mean(tf.reduce_sum(tf.stop_gradients(tf. nn softmax(f, dim=-2)) * f, axis=-2), axis=-1).\n\nFor the reference input  $\\mathbf{r}$ , all zeroes were used since it showed the highest correlation with in silico mutagenesis contribution scores, defined as the weighted sum of the profile prediction changes at all profile locations after introduction of a mutation at a particular position. The DeepLIFT contribution scores were computed with TensorFlow v.1.6 using the DeepExplain implementation of DeepLIFT (repository fork available at https://github.com/kundajelab/DeepExplain/, with commit hash: 738c7145e915a7a48f3a4248d088bcc2e1a94614).\n\nMotif discovery using TF-Modisco. TF-Modisco (v.0.5.1.1) was run on DeepLIFT profile contribution scores for each TF separately (using all 1-kb peak regions bound by the TF on autosomes). Significant seqlets were selected by computing contribution scores over a width of 21 bp and using the false-discovery rate threshold of 0.01 (target_seqlet_fdr). The null distribution was estimated from 4,800 randomly selected peaks with contribution scores computed on reshuffled sequences while preserving dinucleotide counts. A total of 145,748 nonoverlapping significant seqlets were identified. Due to memory constraints (250 GB), 50,000 seqlets were used for each TF during the clustering/motif-discovery phase of TF-Modisco. For all discovered motifs, PFM and CWM are computed from the aligned seqlets by averaging the base frequencies and contribution scores, respectively (Supplementary Methods).\n\nClustering of discovered motifs. Motifs were aligned to each other in all possible offsets and strand combinations, and a pairwise distance metric was generated using the smallest continuous Jaccard distance metric on the PFM information content between each motif pair. Hierarchical clustering was performed in scipy (v.1.2.1) using the Ward variance minimization algorithm $^{134}$  (method='ward') and optimal leaf ordering $^{135}$  (Extended Data Fig. 2d). From these clusters, 11 representative TF motifs were manually selected.\n\nIdentification of motif instances by CWM scanning. Once BPNet is trained it is possible, but not necessary, to use the experimentally measured ChIP-nexus profiles during model interpretation. For the mapping of motifs with TF-Modisco and CWM scanning, no information from the experimental profiles was used. CWM scanning was developed because TF-Modisco analyzes only 50,000 seqlets per run. Trimmed CWMs were used to scan the contribution scores of all 147,974 peak regions (as done by TF-Modisco) and by computing the following similarity metric. Let  $\\mathbf{w}^{\\mathrm{CWM}}\\in \\mathbb{R}^{L_W\\times 4}$  denote the CWM of length  $L_{W}$  and  $\\mathbf{C}\\in \\mathbb{R}^{L_S\\times 4}$  denote the contribution scores for one-hot-encoded sequence  $s$  of length  $L_{S}\\geq L_{W}$ . The contribution score  $C_{i,b}$  for base  $b$  at position  $i$  is 0 if base  $b$  was not observed in the actual sequence (that is, if  $s_{i,b} = 0$ ). We decompose the similarity metric between the CWM scanning position  $i$  of the contribution scores into the 'contrib' score, computed as the L1 norm of the contribution scores at positions between  $i$  and  $i + L_{W}$  in the scanned sequence:\n\n$$\n\\operatorname {S c o r e} _ {\\text {c o n t r i b}} \\left(\\mathbf {w} ^ {\\mathrm {C W M}}, \\mathbf {C}, i\\right) = \\sum_ {j = 1} ^ {L _ {W}} \\sum_ {b = 1} ^ {4} \\left| C _ {i + j - 1, b} \\right|,\n$$\n\nand the 'match' score, which represents its similarity to the CWM computed using the continuous Jaccard distance metric $^{41}$  between the CWM and L1-normalized contribution scores:\n\n$$\n\\operatorname {S c o r e} _ {\\text {m a t c h}} \\left(\\mathbf {w} ^ {\\text {C W M}}, \\mathbf {C}, i\\right) = \\operatorname {J a c c a r d} \\left(\\frac {\\mathbf {w} ^ {\\text {C W M}}}{\\left| \\left| \\mathbf {w} ^ {\\text {C W M}} \\right| \\right| _ {1}}, \\frac {\\mathbf {C} _ {i : i + L _ {W} , b}}{\\left| \\left| \\mathbf {C} _ {i : i + L _ {W} , b} \\right| \\right| _ {1}}\\right),\n$$\n\nAt each position  $i$ , the maximum 'match' score ( $\\text{Score}_{\\text{match}}$ ) between  $\\mathbf{w}^{\\text{CWM}}$  and its reverse-complement version is chosen. To call motif instances from the CWM scanning scores, three criteria were defined based on thresholds identified from the TF-Modisco corresponding seqlets: (1) The match score  $>20$ th percentile of those of the seqlets. This stringent threshold more effectively discriminates between similar motifs. (2) The contrib score is higher than the seqlets lowest contrib score. (3) The log odds score with respect to the PWM derived from the PFM is  $>0$ .\n\nIn silico motif interaction analysis. In the synthetic approach, two consensus motifs were inserted into 128 random background sequences of  $1\\mathrm{kb}$ : MotifA at the center and MotifB downstream at distance  $d$  between the motif centers (maximum at 160 bp). The average strand-specific ChIP-nexus profile predictions,  $P_{\\mathrm{AB}}$ , for the TF that binds MotifA were then obtained using the trained BPNet model as oracle. Additional profiles were predicted by (1) inserting only MotifA in the center  $(P_{\\mathrm{A}})$ , (2) inserting only MotifB  $d$ -bases downstream of the center  $(P_{\\mathrm{B}})$  and (3) not inserting any motif  $(P_{\\mathrm{O}})$ . The strand-specific summit (maximum) location of the footprint was then determined for each strand from profile  $P_{\\mathrm{A}}$  within 35 bp of the MotifA center. These summit locations were used to determine the footprint height,  $h$ , within all four profiles to obtain  $h_{\\mathrm{A}}, h_{\\mathrm{B}}, h_{\\mathrm{AB}}$  and  $h_{\\mathrm{O}}$ . The influence of MotifB on MotifA was then defined by the corrected binding fold change  $(h_{\\mathrm{AB}} - (h_{\\mathrm{B}} - h_{\\mathrm{O}})) / h_{\\mathrm{A}}$  as a function of  $d$ . The procedure was repeated to quantify the influence of MotifA on the binding of TFB to MotifB. In the genomic motif interaction approach, motif pair interactions were calculated in the same way using motif instances that were mapped by CWM scanning in genomic sequences underlying ChIP-nexus peaks, excluding motif instances overlapping\n\nretrotransposons. Rather than inserting motifs into the random sequence, motifs were removed from the genomic sequence by replacing them with random sequences (Supplementary Methods and Supplementary Fig. 10).\n\nReproducibility. All ChIP-nexus and ChIP-seq replicate experiments passed quality control metrics used by ENCODE $^{128}$  (Supplementary Table 1). For Sox2 and Nanog, we used two different antibodies for each with reproducible results: the initial wild-type Sox2 ChIP-nexus experiments used two different antibodies (sc-17320 and Active Motif 39843) with IDR rescue ratio  $< 2$ ; the wild-type and CRISPR Nanog ChIP-nexus experiments also used two different antibodies (sc-30328 and ab-214549) with consistent Nanog footprints on Nanog motifs (Extended Data Fig. 3). The entire pipeline, including the training of BPNet, computation of contribution scores, obtaining motif representations and analysis of motif interactions, was performed in fivefold cross-validation, which supports our claims (Supplementary Information and Supplementary Figs. 4, 11 and 12). The CRISPR mutant and wild-type experiments were consistent in both profile and counts at control enhancers (Extended Data Fig. 8), and replicate experiments were highly reproducible (Supplementary Fig. 14).\n\nReporting Summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article.",
    "metadata": {
      "md_filename": "MinerU_markdown_BPNet_20260106143130_2008426172165922816.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_BPNet_20260106143130_2008426172165922816.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "82309fc23cf8915e",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_Caduceus_20260106143124_2008426152565932032",
    "title": "Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# 4. Caduceus\n\nBelow we describe Caduceus, a novel bi-directional DNA LM architecture that enforces RC equivariance. We introduce two versions of this model, each of which maintains equivariance in a different manner: either (1) via parameter sharing (Shrikumar et al., 2017), Caduceus-PS, or (2) via a technique used during downstream task inference, known as post-hoc conjoining (Zhou et al., 2021), Caduceus-Ph.\n\n# 4.1. Caduceus-PS\n\nArchitecture For Caduceus-PS, we leverage both of the architectural innovations introduced in Section 3. Namely,\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/5aa969b5-d87e-42e1-8c0c-202fbe4d4a57/78b1b628a481435e2f2322dc1d2095fe8c493122b026c7028937f85156c5d569.jpg)\n\n\n\nFigure 2. Caduceus Architecture. Bi-directional, RC equivariant Mamba modules are used in conjunction with equivariant word embeddings and language model head to form Caduceus-PS. Using only BiMamba blocks with RC data augmentation during pretraining and post-hoc conjoining for downstream task inference yields Caduceus-Ph. CADUCEUS IMAGE LICENSE: CREATIVE COMMONS CC0 1.0 UNIVERSAL PUBLIC DOMAIN DEDICATION.\n\n\nwe wrap a BiMamba module within a MambaDNA block. Additionally, preceding the Mamba blocks of this architecture is an RC equivariant token embedding module. Denoting by  $\\mathrm{Emb}_{\\theta}$  the linear projection that takes one-hot vectors  $\\mathbf{X}_{1:T}^{1:4}$  and produces embeddings in  $\\mathbb{R}^{D/2}$ , the RC equivariant version of this embedding is defined as:\n\n$$\n\\operatorname {E m b} _ {\\mathrm {R C e}, \\theta} \\left(\\mathbf {X} _ {1: T} ^ {1: 4}\\right) :=\n$$\n\n$$\n\\operatorname {c o n c a t} \\left(\\left[ \\operatorname {E m b} _ {\\theta} \\left(\\mathbf {X} _ {1: T} ^ {1: 4}\\right), \\operatorname {R C} \\circ \\operatorname {E m b} _ {\\theta} \\left(\\operatorname {R C} \\left(\\mathbf {X} _ {1: T} ^ {1: 4}\\right)\\right) \\right]\\right)\n$$\n\nAdditionally, the logits of the Caduceus model are produced by passing the output of its final MambaDNA block through a RC equivariant language model head. To our knowledge, Caduceus-PS is the first model to incorporate RC equivariance into the LM pre-training paradigm. This can be formalized by first defining a channel flip operator flip chan  $\\left(\\mathbf{X}_{1:T}^{1:D}\\right) := \\left(\\mathbf{X}_{1:T}^{D:1}\\right)$ . Then, letting  $\\mathrm{LM}_{\\theta}$  be the linear projection from sequences with  $D/2$  channels to vectors in  $\\mathbb{R}^4$ , we define the equivariant version of the language modeling head as:\n\n$$\n\\operatorname {L M} _ {\\mathrm {R C e}, \\theta} \\left(\\mathbf {X} _ {1: T} ^ {1: D}\\right) :=\n$$\n\n$$\n\\operatorname {L M} _ {\\theta} \\left(\\mathbf {X} _ {1: T} ^ {1: (D / 2)}\\right) + \\text {f l i p \\_ c h a n} \\circ \\operatorname {L M} _ {\\theta} \\left(\\mathbf {X} _ {1: T} ^ {D: (D / 2)}\\right).\n$$\n\nDepicted in Figure 2 with the black path, Caduceus-PS enables RC equivariant pre-training: the predictions it produces for the RC of a given sequence are equivalent to reversing the predictions of the original sequence along the length dimension and complementing outputs: A-T and C-G. We formalize this claim in the following statement:\n\nTheorem 4.1. Composing  $\\mathrm{LM}_{RCe,\\theta} \\circ \\mathrm{M}_{RCe,\\theta}^{(n)} \\circ \\mathrm{Emb}_{RCe,\\theta}$ , where  $\\mathrm{M}_{RCe,\\theta}^{(n)}$  denotes  $n$  compositions of Mamba RC equivariant modules, yields an operator that is RC equivariant.\n\nProof. See Appendix B.\n\nPre-training Given the bi-directionality of this model, we train Caduceus-PS with the masked language modeling (MLM) objective, using the standard masking recipe proposed in BERT (Devlin et al., 2018). The RC equivariant language modeling of Caduceus-PS means that we do not need RC data augmentation at pre-training, since predictions are inherently symmetric with respect to this operation.\n\nDownstream Usage For downstream tasks, since either strand of an assayed sequence will carry the same label, we wish to enforce RC invariance. The token embedding parameter sharing in Caduceus-PS means that its intermediate and final hidden states are twice the (channel) dimensionality of a standard Mamba-based language model with an equivalently sized token embedding matrix. To enforce RC invariance at downstream training and inference, final hidden states are split and the two splits are averaged.\n\n# 4.2. Caduceus-Ph\n\nArchitecture The Caduceus-Ph model is depicted with the blue path in Figure 2. The core of this model is a stack of BiMamba blocks.\n\nPre-training As with Caduceus-PS, this model is pretrained using the same MLM objective. However, as the model is not an RC equivariant LM, we instead rely on data augmentation during pre-training.\n\nDownstream Usage In order to make the downstream task representations RC invariant, we leverage a technique called post-hoc conjoining (Zhou et al., 2021). Namely, for downstream task training the backbone model is unchanged, but we employ RC data augmentation. However, for downstream task inference, we apply the model twice, once on the original sequence and once on a corresponding RC sequence, and average the two, effectively performing a version of 'RC ensembling' (Mallet & Vert, 2021).",
    "metadata": {
      "md_filename": "MinerU_markdown_Caduceus_20260106143124_2008426152565932032.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_Caduceus_20260106143124_2008426152565932032.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "83d294a97fa869c6",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_CREME_20260106143119_2008426130235457536",
    "title": "Interpreting cis-regulatory interactions from large-scale deep neural networks",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# Inclusion and ethics\n\nThis study relied solely on computational models that were trained using public resources. It did not make use of individual-level data, and no specific ethics approval was required. Some of the data sources might present biases toward European ancestries.\n\n# Enformer\n\nEnformer is a previously established DNN that takes genomic sequences of the length 196,608 bp as input and predicts 5,313 epigenetic tracks for human and 1,643 epigenetic tracks for mouse biosamples through two output heads<sup>1</sup>. For each track, Enformer's predictions cover 896 binned positions, with each bin representing 128 bp. This represents the central 114,688 bp of the input sequence. The extended input sequence, provides context for the edge cases, that is the start and end of the predictions. The epigenetic tracks consist of processed coverage values of expression (CAGE), DNA accessibility (DNase-seq), TF binding and histone modification (ChIP-seq). Enformer is composed of convolutional layers that initially summarize the input sequence into representations of 128 bp bins. This is followed by 11 transformer blocks that use multihead self-attention<sup>71</sup>. We acquired code for the Enformer model along with trained weights from https://tfhub.dev/deepmind/enformer/1as per instructions in the 'Methods' section of ref. 1.\n\n# Borzoi\n\nBorzoi is a DNN similar to Enformer in that it predicts a range of epigenetic tracks from input DNA sequences. However, it considers a larger input size of 524,288 bp and makes predictions for strand specific outputs of CAGE (and other tracks) at a 32 bp resolution. Borzoi is an ensemble of four models trained on different data splits. Here, we adopted the same strategy as the authors of Borzoi of running inference using all four models and averaging the results. Each model in the ensemble is composed of convolutional layers that summarize the input into representations of 128 bp bins, eight transformer layers that use multihead self-attention and deconvolution layers that upsample the representations to 32 bp resolution. We acquired code for the Borzoi model along with trained weights from https://github.com/calico/borzoi/tree/main as per instructions in the 'Methods' section of ref. 4.\n\n# Transcription start site selection\n\nWe acquired human annotations from GENCODE $^{26}$  (https://www.gencodegenes.org/human/) and filtered for 'transcript' annotations and 'protein coding' genes. We then extracted sequences of the length 196,608 bp (or 524,288 bp for Borzoi) from the GRCh38 reference genome centered at each filtered TSS. We converted the sequences to a one-hot representation, treating  $N$  characters as a uniform probability (that is 0.25). We calculated Enformer's prediction for these sequences and considered the mean at positions 447 and 448 (of the 896 binned predictions), which corresponds to the central TSS. We used tracks 4,824, 5,110 and 5,111 of the human output head (corresponding to PC-3, GM12878 and K562 CAGE predictions, respectively). We refer to this scalar coverage value per cell line as the TSS activity. To focus our study on genes that yield high TSS activity, we considered the top 10,000 unique genes per cell line with the highest predicted read coverage.\n\n# Creme: cis-regulatory element model explanations\n\nCREME is an in silico perturbation toolkit that can uncover rules of gene regulation learned by a large-scale DNN. The rationale behind CREME stems from the concept that DNNs are function approximators. Thus, by fitting experimental data, the DNN is effectively approximating the underlying 'function of the experimental assay'. By treating the DNN as a surrogate for the experimental assay, CREME can be queried with new sequences and provide in silico 'measurements',\n\nalbeit through the lens of the DNN. Inspired by wet laboratory experiments, such as CRISPR $^{24,25,72}$ , that perturb genomic loci to uncover how CREs influence gene expression, we devised a suite of in silico perturbation experiments that interrogate a DNN's understanding of long-standing questions of gene regulation, including the context dependence of gene expression $^{31,73}$ , identification of enhancing and silencing CREs and their target genes $^{24,30}$ , distance dependence of CREs to target genes on gene expression, the complex higher-order interactions of CREs and the effect of finer-scale elements on gene expression $^{34,35,58-60,64}$ . Since DNN predictions may not fully capture the underlying biology when fitting experimental data, Creme is strictly a model interpretability tool. Below, we detail the different in silico perturbation tests explored in this paper.\n\nRegaring the CREME investigation of Enformer, for the vast majority of the experiments, we only considered TSS activity, which we define as the central 5 kb tile centered on the input sequence. Enformer's receptive field for this tile covers roughly 200 kb sequences, so the 200 kb region centered on the sequence is what is probed in our experiments. We split the central 200 kb sequences into 38 nonoverlapping 5 kb tiles (such that the tiles are fully within the input sequence), with the central tile corresponding to the TSS of an annotated gene. We define the TSS activity as the mean of Enformer's prediction for bins 447 and 448, which are the central bins. Predictions for tracks 4,824, 5,110 and 5,111 of the human output head correspond to PC-3, GM12878 and K562 cell lines.\n\nContext dependence test. The context dependence test aims to measure the effect size of TSS activity in random contexts (derived from dinucleotide-shuffled versions of the WT sequence). This test measures the extent to which a prediction of a given TSS activity is influenced by its context which may contain enhancers and silencers. To do this, we computed the difference between WT TSS activity and that of a dinucleotide-shuffled context case. We normalized by WT TSS activity. Detailed steps can be found in Supplementary Note 1.\n\nIn an example case, assuming WT prediction is 10 and mutant is 5 (that is, shuffling leads to a drop in activity), the normalized effect would equal\n\n$$\n\\frac {\\mathrm {W T} - \\mathrm {M U T A N T}}{\\mathrm {W T}} = \\frac {1 0 - 5}{1 0} = 0. 5.\n$$\n\nFor the interpretation, the effect size of 0 means that the context is neutral and has no effect on the TSS predictions (that is, WT and mutant yield the same prediction). Positive effect size means that the central TSS prediction for the mutated sequence is lower than WT, which indicates that we have perturbed an enhancing context. Negative effect size means that the central TSS prediction for the mutated sequence is higher than WT, which suggests that we have perturbed a silencing context.\n\nFor analysis, we categorized the sequences into silencing, neutral and enhancing contexts based on their context effect on TSS. We identified three regions: (1) enhancing context were chosen based on an effect size of more than 0.95, (2) neutral context was chosen if the absolute effect size was less than 0.05 and (3) silencing context was chosen based on an effect size of less than  $-0.3$ . If the number of data points in a category was above 200, we randomly sampled 200 sequences to cap the number for further experiments. We used these groups per cell line throughout the experiments.\n\nFor each cell line, the breakdown of contexts in each category is given in Supplementary Table 2.\n\nFor Borzoi, we chose to proceed with less strict thresholds:  $-0.9$ ,  $0.05$  and  $-0.2$  for enhancing, neutral and silencing contexts, respectively, to address the weaker context effect sizes given by Borzoi. The number of detected sequences in each category is given in Supplementary Table 3.\n\nContext swap test. The context swap test aims to measure the extent that TSS activity depends on a specific genomic context or measure compatibility with other contexts. For this, we embedded the TSS of one sequence into another, to replace the existing TSS. We then obtained TSS activity prediction and normalized it by the WT prediction of the first sequence. Detailed steps can be found in Supplementary Note 1.\n\nFor an example case, assuming WT prediction for a given sequence from which the TSS is taken is 20 and mutant, that is, the activity of the same TSS in a new background, is 10 (that is, moving the TSS to the new background leads to a lower activity), the normalized effect would equal\n\n$$\n\\frac {\\text {M U T A N T}}{\\text {W T}} = \\frac {1 0}{2 0} = 0. 5.\n$$\n\nAssuming both sequences are from enhancing backgrounds, this would indicate that the new context is less enhancing compared with its native TSS (classified based on context dependence test).\n\nFor the interpretation, if the fold change over the control is 1, the TSS activity in the new context is the same as in its native context. If the value is below 1, the TSS is less active in the new context and vice versa for values above 1.\n\nFor analysis, we performed the context swap test on the sequences filtered by the context dependence test per cell line. Specifically, we placed the TSSs in each context category across all other context categories, separately keeping track of the source TSS and the context category.\n\nNecessity test. The necessity test measures the importance of a putative CRE on the central TSS activity for a given sequence context while the other tiles remain intact. To perform this, we independently shuffled each 5 kb tile and obtained mutant TSS predictions. We then normalized these by subtracting the WT activity and divided the difference by WT. Detailed steps can be found in Supplementary Note 1.\n\nFor example, assuming WT activity is 10 and it drops to 5 after shuffling 1 CRE (indicating that the shuffled CRE is enhancing), the normalized shuffle effect would equal\n\n$$\n\\frac {\\mathrm {W T} - \\text {M U T A N T}}{\\mathrm {W T}} = \\frac {1 0 - 5}{1 0} \\approx 0. 5.\n$$\n\nFor the interpretation, an effect size of 0 means that the WT value is similar to the case when the tile is shuffled, and therefore, the tile shuffle has no effect on the TSS. A large positive value means that the shuffled case yields a lower TSS signal than the WT, that is, the tile shuffling leads to a drop in TSS activity, and a large negative value means the shuffling leads to a higher TSS activity compared with WT.\n\nFor analysis, we performed the necessity test on all tiles within the sequences from the context dependence test that had enhancing, silencing or neutral backgrounds (as classified by selected thresholds).\n\nSufficiency test. The sufficiency test measures the effect of a given CRE on its TSS in otherwise random contexts, that is, in isolation from the rest of the tiles from the original WT sequence. This essentially measures whether the CRE by itself is enough to up- or downregulate the TSS. For this, we individually inserted 5 kb tiles into context-shuffled sequences and obtained mutant predictions. We also computed TSS activity in context-shuffled case as control. We then normalized by subtracting control from mutant. For enhancing context sequences we divided this by the WT activity, in others by control. Detailed steps can be found in Supplementary Note 1.\n\nFor example, assume an enhancing sequence with a WT prediction of 10 and control (only TSS) sequence with predicted activity of 2. Given a strong enhancer CRE that recovers the activity in the mutant (TSS and CRE) to 12, the normalized activity would equal\n\n$$\n\\frac {\\text {M U T A N T - C O N T R O L}}{\\text {W T}} = \\frac {1 2 - 2}{1 0} = 1.\n$$\n\nGiven a silencing context sequence with WT activity of 10, control activity of 20 (note that silencing context sequences by definition have higher activity after context shuffle) and a strong silencer that represses the signal to 10, the normalized effect would be computed as\n\n$$\n\\frac {\\text {M U T A N T - C O N T R O L}}{\\text {C O N T R O L}} = \\frac {1 0 - 2 0}{2 0} = - 0. 5.\n$$\n\nFor the interpretation, in case of enhancing context sequences, the TSS activity drops to a small value after the whole context shuffling (control). Therefore, here we compare the value to the original WT sequence, subtracting the small activity of the TSS on its own (control). This can be interpreted as the extent to which a given tile individually restores TSS activity to the original WT value. A value of 0 means that the tile has no positive effect on the TSS (mutant equals control), positive values indicate an enhancing effect (mutant > control) and vica versa for negative values.\n\nIn case of silencing and neutral context sequences, the TSS activity in the CONTROL condition is not always a small value (by definition the silencing contexts are the ones where shuffling the context leads to a high TSS activity). This leads to ambiguities in cases when the WT and control contribute to the normalized values. Therefore, we simply computed the tile effect as fraction change observed when adding the tile compared with CONTROL. A value of 0 means that tile addition has no effect on TSS, a positive value indicates that tile addition activates the TSS and a negative value means that the tile lowers TSS activity.\n\nFor analysis, we performed the sufficiency test on the same subset of sequences as necessity test that had enhancing, silencing or neutral backgrounds (as classified by selected thresholds). Based on sufficiency test results, we denote CREs from enhancing contexts with effect size larger than 0.3 as enhancers. Similarly, we define silencers as tiles from silencing contexts with effect size smaller than  $-0.3$ . In case of Borzoi, we used  $-0.15$ , a higher threshold, for silencers to include more elements and considered those in both silencing and neutral background.\n\nTSS-CRE distance test. The TSS-CRE distance test is a GIA experiment where we systematically shift the position of a tile in shuffled sequences and measure its effect on TSS activity. For this, we inserted a given CRE in various positions across the context-shuffled sequence. We normalized this by dividing the activity at each position by that of the maximum activity across all positions in the sequence. Detailed steps can be found in Supplementary Note 1.\n\nFor example, assuming we consider only five possible test positions with mutant sequence activity values of 2, 5, 10, 5 and 2, the CONTROL would be set to 10, and the normalized activities would equal 0.2, 0.5, 1, 0.5 and 0.2, respectively.\n\nFor analysis, we used the definition of enhancers and silencers based on sufficiency test results. We performed the TSS-CRE distance test on CREs defined as enhancers within enhancing contexts and silencers in silencing contexts for each cell line.\n\nHigher-order tile interaction test. The aim of the higher-order tile interaction test is to dissect CRE networks. Specifically, we compute the combined effect of multiple tile shuffles that have large effects through a greedy search. For enhancers, the iterative greedy search systematically identifies tiles that lead to a lower TSS activity when shuffled. We followed the same steps for silencer search but instead of choosing the minimum predicted value we chose the maximum predicted value in each iteration. Detailed steps can be found in Supplementary Note 1.\n\nFor example, we assume a sequence with two enhancing tiles E1 and E2 and WT activity of 100. In the first iteration, after shuffling E1,\n\nthe prediction drops to 50, and after shuffling E2, it yields a prediction of 70, while shuffling the other tiles maintains activity at WT levels. The normalized TSS activity in case of shuffling E1 would be computed as\n\n$$\n\\frac {\\text {M U T A N T}}{\\text {W T}} = \\frac {5 0}{1 0 0} = 0. 5.\n$$\n\nGiven that E1 elicits the sharpest drop in TSS activity, if searching for enhancers, the E1 tile would be fixed with shuffled sequences and a subsequent round would search among the remaining tiles.\n\nFor analysis, we performed higher-order tile interaction test for maximally enhancing TSS activity and maximally silencing TSS activity for all sequences from different context categories.\n\nComparison with additive effects. To help understand the trajectories from the higher-order tile interaction test, we calculated the hypothetical effects of an additive model. In brief, the additive effects are calculated on the basis of combining the effects on TSS activity from the individual effects of each CRE (that is, calculated in the first round of the greedy search), following the CRE tile order found by the greedy search. This does not take into account cooperative or redundant relationships within sets of CREs, which would be captured in the greedy search.\n\nFor example, assuming a sequence with two strong enhancers E1 and E2 with fully additive effects, we would expect their effect sizes to add up to the prediction from when both are shuffled simultaneously. For instance, if the WT prediction is 100, and in iteration 1 shuffling just E1 leads to a prediction of 60 and E2 of 70, then the effect sizes are  $-40$  and  $-30$ , respectively. The hypothetical additive model would then predict that the simultaneous shuffling of E1 and E2 would lead to a normalized effect size of\n\n$$\n\\frac {\\mathrm {W T} + \\text {e f f e c t (E 1)} + \\text {e f f e c t (E 2)}}{\\mathrm {W T}} = \\frac {1 0 0 - 4 0 - 3 0}{1 0 0} = 0. 3.\n$$\n\nFor the interpretation, if the hypothesis of additive effect holds, we would expect the greedy search trace to be the same as the additive or hypothetical trace for each sequence. If the results are different we can categorize these as superadditive or subadditive for cases when the additive model overestimates the shuffle effect or underestimates it, respectively. To illustrate an example scenario leading to a superadditive case let us assume two enhancers are cooperating, that is, their combined effect is larger than individual effects (a nonadditive case). We would expect their individual shuffle effects to also be larger than shuffling them simultaneously (because disabling one leads to a large effect size already). In contrast, subadditivity can arise if two enhancers are redundant, that is, their roles are overlapping, and the effect size will be small when only a single tile is shuffled (because the other enhancer tile can compensate). Therefore, the estimated additive effect (based on single tile shuffles of iteration 1) will underestimate the effect of shuffling both of the enhancers. This will thus lead to the additive hypothetical trace being higher than the one based on the greedy search.\n\nFor analysis, to characterize deviations, we computed the mean squared error (m.s.e.) of the greedy and hypothetical additive outputs for each sequence. We classified the cases where the m.s.e. value is above 0.1 (arbitrary threshold) and the greedy search results on average is greater than the average of additive as superadditivity cases. Similarly, we classified the cases where the m.s.e. value is above 0.1 (arbitrary threshold) and the greedy search result on average is lower than the average of additive as subadditivity cases. We used a stricter threshold of 0.05 for the m.s.e. value to classify sequences as additive. We applied these thresholds to both the enhancer and silencer search cases after assessing visually that the traces overlap substantially.\n\nComparison with multiplicative effects. Similarly, we compared greedy search outcomes to a multiplicative model. For this we computed the natural logarithm of all predicted activities and otherwise\n\nproceeded with the same steps as in 'Comparison with additive effects' section. We interpreted the results using the same logic as in the additive effect case, but we used different thresholds for classifying sequences into multiplicative and nonmultiplicative categories. We mostly observed cases of the hypothetical multiplicative model predicting higher values than the greedy search yielded and some where they aligned (hence the binary classification). We used the MSE thresholds 0.04 for enhancers (cases of m.s.e. above 0.04 were classified as nonmultiplicative) and 0.01 for silencers. Both were selected after visual inspection of the traces.\n\nSufficiency of CRE sets from higher-order test. In a complementary set of experiments to higher-order interaction test, we tested whether the identified CREs are sufficient to recover or suppress the TSS activity. For this, we inserted tiles into shuffled context sequences progressively adding more tiles, following the order of shuffling from the higher-order interaction test.\n\nMultiplicity test. The multiplicity test measures how TSS activity scales upon repeated addition of an enhancing or silencing tile. With this GIA experiment, we aim to test the model's extrapolation behavior. Specifically, we probed whether TSS activity reaches saturation upon a high dosage of a CRE; saturation is when the predictions reach a plateau when we enrich for enhancers or silencers. The multiplicity test is similar to the greedy search used in the higher-order tile interaction test, with the exception that we are systematically adding the same CRE of interest into optimal positions in each round of dinucleotide-shuffled sequences. Detailed steps can be found in Supplementary Note 1.\n\nFor example, assuming a control activity of 10, if inserting CRE at a new position yields a prediction of 20, the normalized TSS activity would be computed as\n\n$$\n\\frac {\\text {M U T A N T}}{\\text {C O N T R O L}} = \\frac {2 0}{1 0} = 2.\n$$\n\nFor analysis, using the enhancers defined in 'Sufficiency test' section, we performed 15 iterations of the multiplicity test for each CRE.\n\nFine-tile search. This analysis aims to identify subsets of sequences that recover the majority of a given  $5\\mathrm{kb}$  enhancer CRE effect. To formalize the problem, we aimed to identify the smallest set of 50 bp sequences that, when embedded in otherwise shuffled context, recovers at least  $80\\%$  of the enhancer CRE effect on the TSS. We did this using a two-stage nested greedy search, reducing the search space in each stage: first at a 500 bp resolution followed by a 50 bp resolution among the surviving 500 bp tiles. Detailed steps can be found in Supplementary Note 1.\n\nWe compared this approach with XSTREME $^{51}$  (using both denovo motifs and known motifs from the JASPAR database) and saliency-based motif embedding. For XSTREME analysis, we scanned the WT sequence for motif hits with FIMO $^{74}$  and embedded those subsequences into otherwise shuffled background sequences. We evaluated how much of the CRE effect is recovered by XSTREME motifs using the same normalized value (that is, by dividing with the activity when the whole CRE is embedded).\n\nFor this analysis, we used all the enhancing context sequences and the enhancer CREs identified within those in each cell line (74 in K562, 41 in GM12878 and 35 in PC-3). We used the same set of ten backgrounds per sequences for comparing Creme, XSTREME and saliency-based analyses.\n\n# Biochemical characterization of CREs\n\nTo characterize and contrast the chromatin state of the putative enhancing and silencing CREs (at a 5 kb window size) we downloaded (default) bigwig files from ENCODE and summarized the values at coordinates of the CREs according to the mean read coverage across\n\nthe 5 kb read coverage for epigenetic marks and the max read coverage for TF tracks (Supplementary Data 1).\n\nFor Supplementary Figs. 8-10, we sampled neutral tiles as a background for comparison. We selected tiles from neutral contexts, filtered only those with lower than 0.1 absolute sufficiency values, then randomly selected one tile per gene. We additionally subsampled the set to match the the average number of enhancing and silencing tiles.\n\nWe computed the percentage of enhancer and silencer sequences containing repeating sequences using RepeatMasker using the default parameters. The detailed outputs per repeating sequence category and cell line are given in the Supplementary Table 1.",
    "metadata": {
      "md_filename": "MinerU_markdown_CREME_20260106143119_2008426130235457536.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_CREME_20260106143119_2008426130235457536.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "7a7ea86f27a34011",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_data augmentation_20260106143112_2008426100929863680",
    "title": "Innovative data augmentation strategy for deep learning on biological datasets with limited gene representations focused on chloroplast genomes",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# Genomes and corresponding prepared DNA and protein datasets\n\nTo evaluate the efficiency and reproducibility of the data augmentation approaches for biological datasets with limited representations, two strategies were employed concurrently. The method was applied to two primary datasets: one comprising DNA sequences, characterized by four distinct nucleotide bases, and the other consisting of protein sequences, composed of 20 different amino acids. Additionally, the method was independently evaluated on eight diverse datasets representing DNA and protein sequences sourced from\n\nchloroplast reference genomes from microalgae and higher plants to assess its robustness and generalizability. These genomes included Chlamydomonas reinhardtii (NC_005353.1), Chlorella vulgaris (NC_001865.1), Arabidopsis thaliana (NC_000932.1), Nicotiana tabacum (MZ707522.1), Triticum aestivum (NC_002762.1), Oryza sativa (NC_031333.1), Zea mays (NC_001666.2), and Glycine max (NC_007942.1).\n\n# DNA dataset preparation\n\nThe number of genes included in the chloroplast datasets was as follows: 100 for C. reinhardtii, 210 for C. vulgaris, 113 for A. thaliana, 113 for N. tabacum, 116 for T. aestivum, 122 for O. sativa, 125 for Z. mays, and 110 for G. max. The genomic regions including 300 nucleotides upstream of the transcription start sites of genes, within the selected genomes, were analyzed as distinct DNA datasets (S9).\n\n# Protein dataset preparation\n\nTo establish the protein datasets, the protein-coding sequences (CDS) from the aforementioned genomes were operated (S9). To ensure sufficient sequence length for analysis, a minimum threshold of 100 amino acids (aa) was applied to coding sequences. Consequently, the number of proteins meeting this criterion for each genome was as follows: 46 for C. reinhardtii, 70 for C. vulgaris, 56 for A. thaliana, 58 for N. tabacum, 55 for T. aestivum, 57 for O. sativa, 64 for Z. mays, and 56 for G. max. The lengths of the protein sequences varied across these datasets, ranging from 112 to 1995 aa for C. reinhardtii, 103 to 1720 aa for C. vulgaris, 101 to 2294 aa for A. thaliana, 101 to 2280 aa for N. tabacum, 101 to 1479 aa for T. aestivum, 101 to 1513 aa for O. sativa, 101 to 1527 aa for Z. mays, and 101 to 2287 aa for G. max.\n\n# Applying data augmentation for deep learning model inputs\n\nA method for generating overlapping subsequences was employed to address the limitations posed by the datasets with few and limited gene representations of nucleotide and amino acid sequences, thereby enabling the dataset's utility for deep learning models. The original DNA datasets comprised between 100 and 210 sequences, while the protein datasets included 46 and 70 sequences (S9).\n\nA sliding window approach was employed to ensure that sufficient subsequences were generated, producing subsequences of 40 nucleotides with overlaps ranging from 5 to 20 nucleotides. To elaborate further, at least  $50\\%$  (ranging between  $50\\%$  and  $87.5\\%$ ) of each sequence was classified as invariant. This ensured the presence of conserved regions that maintain remarkable differences between subsequences to facilitate effective model training. Conversely,  $12.5\\%$  to  $50\\%$  of each sequence was treated as a variable, introducing diversity into the subsequences and enriching the input dataset for model training. This method enabled the generation of overlapping subsequences by shifting the window across the sequence, ensuring that neighboring subsequences shared common nucleotides. By varying the degree of overlap, the diversity of the generated subsequences was increased, maximizing the coverage of different regions of the sequence. Additionally, overlap patterns were applied to the left side, right side, and both sides of each sequence to achieve a comprehensive representation. Consistency was maintained in the length of the subsequences, fixed at 40 nucleotides, a key requirement for downstream DL models. All overlapping subsequences derived from a single sequence were labeled with the corresponding sequence name, and the same labeling approach was applied to subsequences generated from other sequences. This process ultimately resulted in the creation of the input dataset for subsequent model training (S10).\n\n# Implementation of CNN-LSTM hybrid model\n\nTo validate the efficiency and robustness of the data augmentation approach for DNA and protein datasets, a hybrid deep learning model combining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) layers was implemented in PyTorch $^{47,48}$ . This hybrid model was designed to process the one-hot encoded nucleotide sequences as well as amino acid sequences, and then classify them according to their respective labels.\n\nThe CNN component of the CNN-LSTM architecture was responsible for extracting local features from the sequences by applying two one-dimensional convolutional layers with 64 and 128 filters, respectively. These filters captured meaningful patterns within the sequences. Each filter is followed by ReLU activation[49-51] and max-pooling[52]. The max-pooling layer was included to reduce dimensionality and focus on the most relevant features, thereby improving computational efficiency. Following feature extraction, the data were passed through a two-layer bidirectional LSTM with 128 hidden units per layer, which captured the sequential relationships between nucleotides. The bidirectional configuration of the LSTM doubled the effective hidden size, allowing the model to capture both forward and reverse patterns in the sequences. The output from the LSTM layer was then processed through two fully connected layers with dropout (0.4) to prevent overfitting, and finally mapped to the output layer for classification. For more details, the model was trained for 50 epochs using cross-entropy loss and the Adam optimizer[53], with early stopping implemented to prevent overfitting by halting training once the validation loss ceased improving. Training, validation, and test sets were created by randomly splitting the data, allocating  $80\\%$  for training and  $20\\%$  for testing, with a subsequent  $10\\%$  validation split from the training set.\n\nFurther modifications were applied to the hybrid model utilized on the protein datasets. The model was trained using cross-entropy loss, and in cases of class imbalance, the weight of the classes was calculated using the inverse frequency method to adjust the loss accordingly.\n\nThe optimization of hyperparameters for the CNN-LSTM hybrid model was performed separately for DNA and protein datasets. For the DNA datasets, the hyperparameters were initially optimized using the C. reinhardtii dataset, which served as the representative dataset. The optimized hyperparameter values obtained from this process were subsequently applied to the remaining DNA datasets to ensure consistency and comparability across analyses. Similarly, for protein datasets, hyperparameters were optimized using the representative dataset, and the optimized values were then applied to the other protein datasets. This approach ensured uniformity in\n\nhyperparameter settings across similar dataset types, facilitating a systematic evaluation of model performance across all datasets.\n\nAnalyses to evaluate the effectiveness of the augmentation approach\n\nEvaluation and performance metrics on DNA datasets The performance of the CNN-LSTM hybrid model was evaluated on eight DNA datasets using multiple metrics to ensure a comprehensive assessment of its effectiveness with and without data augmentation. The key evaluation metric was the average test accuracy  $(\\%)$ , calculated across all datasets over multiple trials (5 trials) and epochs.\n\nIn addition to the accuracy, the average area under the precision-recall curve (AUC-PR) was computed for each dataset, and precision-recall curves were plotted for randomly selected classes (10 classes) to visualize performance trends.\n\nTo further assess the model's predictive quality, the Pearson correlation coefficient was calculated to evaluate the relationship between model predictions and experimental results for both augmented and non-augmented datasets.\n\nFinally, feature importance analysis was performed using SHAP (SHapley Additive exPlanations) to quantify the contributions of individual nucleotide positions to the model's predictions. SHAP values were computed using a subset of the training data as a background set and applied to selected test samples. A summary plot was generated to highlight the most impactful nucleotide positions, providing insights into the model's decision-making process and biological relevance.\n\nEvaluation and performance metrics on protein datasets The CNN-LSTM model's performance was further evaluated on eight protein datasets using a series of analyses designed to ensure a robust assessment. To address the class imbalance, a stratified fivefold cross-validation approach was implemented, ensuring balanced representation across all folds. Evaluation metrics included weighted and macro-averaged precision, recall, and F1 scores, calculated per epoch and averaged across all folds for both augmented and non-augmented datasets.\n\nA confusion matrix was generated to provide a detailed breakdown of the model's predictions, capturing true positives, false positives, true negatives, and false negatives for each class. The confusion matrix values from each fold were aggregated to form an overall matrix, visualized as a heatmap to highlight classification accuracy and error distribution across all classes.\n\nAdditionally, precision-recall curve analysis was conducted by collecting true labels and predicted probabilities for each class across all folds. Precision-recall curves were plotted for each class, and the average AUC-PR score along with its standard error, was reported to summarize the model's overall classification ability. Receiver operating characteristic (ROC) curve analysis was also performed to evaluate the sensitivity and positive predictive value.\n\nFinally, the model's performance was tracked using mean loss, F1 score, and recall as primary metrics. These values were computed for each epoch and aggregated across all folds during both the training and validation phases to provide a comprehensive evaluation of the model's performance.\n\n# Data augmentation strategy for unsupervised analysis\n\nIn addition to the data augmentation method designed for deep learning applications with labeled data where each sequence is associated with a corresponding label (e.g., sequence name, species name, etc.)-an alternative approach was employed to maximize feature generation from small datasets lacking labels, enabling the analysis of unlabeled data.\n\n# K-mer analysis and identification of common patterns\n\nEach 300-nucleotide sequence was transformed into a high-dimensional set of k-mer features to enhance feature generation from the limited dataset of genomic sequences. K-mers of varying lengths (ranging from 5 to 12 nucleotides) were extracted to analyze sequence similarity and identify shared patterns. A custom Python script was utilized to systematically identify common k-mers between each pair of sequences by comparing their respective k-mer dictionaries. For every pair, the total count of shared k-mers was calculated by aggregating the minimum occurrences of each common k-mer. The output, which included the total shared k-mer counts for each sequence pair, provided critical insights into conserved sequence motifs for further analysis.\n\n# Clustering and visualization of sequence similarities\n\nTo evaluate sequence similarities and group-related sequences, a clustering and dimensionality reduction approach was employed. A distance matrix was constructed using the total shared k-mer counts between sequence pairs, with the distance defined as inversely proportional to the number of shared k-mers. Sequence pairs with no common k-mers were assigned a distance value of zero.\n\nK-means clustering was then performed, selecting the number of clusters based on biological relevance and the dataset's characteristics. Principal Component Analysis (PCA) was applied for dimensionality reduction, projecting the high-dimensional distance matrix into a two-dimensional space for visualization. The results of PCA, combined with the cluster assignments, were visualized using the seaborn library, with data points color-coded by cluster. This PCA plot provided an intuitive representation of sequence similarities, with distinct clusters highlighting groups of sequences sharing common k-mer patterns.\n\n# Computational resource for the model training and evaluation\n\nThe CNN-LSTM model and the additional analyses, including the data augmentation methods, confusion matrix evaluation, shape feature analysis, correlation analysis, precision-recall metrics, Receiver Operating Characteristic (ROC) curve analysis, and k-means clustering were conducted. The analyses were performed\n\nusing the free version of Google Colaboratory (Google Colab, https://colab.google.com), running Python version 3.10.12, an online platform that provides cloud-based access to a shared computing environment. The corresponding Python scripts are publicly available at https://github.com/MAAbbasi-Vineh/Data-Augmentation.",
    "metadata": {
      "md_filename": "MinerU_markdown_data augmentation_20260106143112_2008426100929863680.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_data augmentation_20260106143112_2008426100929863680.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "9c883c508c70e636",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_DeepRegFinder_20260106143105_2008426069992677376",
    "title": "Software",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "DeepRegFinder trains deep neural networks in a supervised fashion to identify enhancers and promoters. It comprises\n\nthree modules: Preprocessing, Training, and Prediction. Each module can be run independently, providing flexibility for users to train a model once and use it for prediction multiple times. A brief description for each module is provided below.\n\n# 2.1 Preprocessing module\n\nThe preprocessing module obtains read coverage for histone marks and TFs at promoters, enhancers, and background genomic regions to generate training, validation, and test datasets. Users may download processed alignment files (BAM) for ChIP-seq data from the ENCODE (ENCODE Project Consortium 2012) website for a variety of histone marks such as H3K27ac, H3K4me1, H3K4me3, or use their own alignment files. Promoters are defined based on user provided TSS annotation files (BED), wherein each site is slopped to  $2\\mathrm{kb}$ . TSS annotation files can be easily obtained from websites such as Ensembl https://www.ensembl.org/, UCSC Genome Browser https://genome.ucsc.edu/, or GENCODE https://www.gencodegenes.org/.\n\nTo determine promoters that are accessible for a given cell line, DNase I hypersensitive site (DHS) or Assay for Transposase-Accessible Chromatin with sequencing (ATAC-seq) data obtained from the same cell line are utilized to intersect with the TSS annotations. Enhancers are defined using user-provided peak lists for enhancer-specific TFs such as p300 and CBP. The TF peaks are used to intersect with DHS or ATAC-seq peaks and subtract H3K4me3 sites or TSSs to avoid any overlap with promoters. These regions are slopped to  $2\\mathrm{kb}$  as well. For defining background, a total of 30 000 genomic regions of  $2\\mathrm{kb}$  length are selected randomly, after the exclusion of enhancers, promoters, DHS sites, and TF peaks. A detailed explanation of the definition of promoters, enhancers, and background genomic regions can be found in the Preprocessing Module section of the Supplementary Information File.\n\nTo define the input features, the entire genome is divided into windows of  $2\\mathrm{kb}$  in size. Each window is further divided into 20 bins of  $100\\mathrm{bp}$  each. The window size, number of bins, and bin size are all configurable parameters of the preprocessing module. Read counts of histone marks from user-provided BAM files for enhancers, promoters and background regions are obtained for all  $100\\mathrm{bp}$  bins of the genome using featureCounts (Liao et al. 2014). The read counts are normalized to Reads Per Million mapped reads values, which are further averaged across all replicates for each histone mark (see \"Genomic binning and ChIP-seq processing\" section of Supplementary Information). In the case of five-class classification, enhancers and promoters are further divided into poised and active states using GRO-seq (or similar technique) data. GRO-seq experiments map binding sites of RNA polymerase II that are transcriptionally active. Users may provide GRO-seq data to DeepRegFinder in the form of BAM files. The coverage of GRO-seq is obtained for all enhancer and promoter sites by the preprocessing pipeline, and K-means clustering is applied on the GRO-seq coverage to classify them into active and poised states. The coverage of the final enhancers, promoters and random background regions are combined into a 3D tensor of sites  $\\times$  histone marks  $\\times$  bins consisting of the normalized read counts for each region. In the case of 3 class classification, background, enhancer, and promoter regions are assigned class labels 0, 1, and 2 respectively, whereas in the case of five-class classification, the Bgd, PE, AE, PT, and AT are assigned class labels 0,\n\n1, 2, 3, and 4, respectively. This tensor is ultimately divided into training, validation, and test sets to be used in the training module. The split is based on chromosome names (by default) or randomly. Details on the train-validation-test split can be found in the \"Creation of training, validation and test set\" sub-section of the Preprocessing Module section in the Supplementary Information File. For this study, we obtained data for dozens of histone marks across three different cell lines: K562, GM12878, and HepG2 from the ENCODE website. A complete list of the histone marks used for each cell line is provided in Supplementary Table S4. We also obtained TF peak lists from the ENCODE website and they are listed in Supplementary Table S5.\n\n# 2.2 Training module\n\nThe training module trains either a CNN or RNN model on the training set, uses the validation set to save the best model and evaluates on the testing set. Users can specify training-related parameters such as learning rate, number of epochs and batch size.\n\n# 2.2.1 Neural network architectures\n\nThe structures of the CNN and RNN are presented in Fig. 1. We tested several architectures for the CNN through multiple rounds of trial and error and arrived at a model with five convolutional layers. The CNN model incorporates a  $7 \\times 1$  1D convolutional layer at the bottom, serving as a feature extractor to detect histone modification patterns. This layer functions as a \"motif\" detector that extracts binding patterns from the read coverage (represented in gray in Fig. 1). Each of the first four convolutional layers is followed by a batch normalization (Ioffe and Szegedy 2015) layer as well as a ReLU (Nair and Hinton 2010, Dahl et al. 2012) activation layer. The batch normalization layer speeds up learning and prevents overfitting by reducing internal covariate shift. Padding is used to preserve spatial resolution through successive convolutional layers. Max pooling is applied for Layers 2 and 4 to reduce the spatial resolution of the feature maps. The final pooling layer consists of a global average pooling followed by a 1D convolutional layer and a softmax layer for classification. The RNN model was also determined by trial and error. It includes a  $7 \\times 1$  1D convolutional layer at the bottom, followed by two stacked LSTMs with the size of 32. They are followed by a softmax layer for classification. To address overfitting, a dropout layer is incorporated in the RNN model.\n\n# 2.2.2 Network training\n\nBoth CNN and RNN use Adam (Kingma and Ba 2014) as the optimizer. We chose a learning rate of 0.01 for our models because a higher learning rate would result in faster convergence but increase the risk of divergence, while a lower learning rate would require more training epochs to reach convergence. The weight decay was set to 0.0001 as this value yielded the best performance. We employed the negative log likelihood loss (i.e. multinomial cross-entropy loss) for both models. The ReduceLROnPlateau learning rate scheduler was utilized with mode set to \"max\" for maximizing the mean average precision (mAP) for each epoch and the parameters factor and patience set to 0.1 and 5, respectively. The CNN and RNN in DeepRegFinder are both parameter efficient with only 26K and 12K weights and biases,\n\nrespectively. Both models were implemented using PyTorch (Paszke et al. 2019).\n\nFor different cell lines, the sizes of active/poised enhancer/promoter classes vary a lot (Supplementary Table S3). Additionally, we chose to make the background class larger than the non-background classes to represent the diverse genomic background regions. In order to address the issue of class imbalance, we implemented a weighted sampler to construct balanced training batches. Throughout the training process, the model is evaluated on the validation set every 1000 batches. If the performance of the model is greater than that from a previous evaluation, the current model is saved. We found that training for 10 epochs was sufficient to achieve the best performance. The saved best model will be used for making predictions on the test set. The total training time typically ranged from 5 to  $10\\mathrm{min}$ , depending on the user-specified number of epochs and bin size. After training, the training module generates a report that includes precision-recall values for each class along with ROC curves, PR curves, and confusion matrices based on the test set.\n\n# 2.3 Prediction module\n\nThe prediction module utilizes the best model to classify each of the 2 kb windows that cover the entire genome in a step size of 200 bps. The entire genome's read coverage data are obtained during the preprocessing stage. The predictions that pass a probability cutoff of 0.5 for a non-background class are stored as the filtered predictions. All the filtered predictions are then consolidated by grouping adjacent predictions of the same class into larger blocks. The validation rates for consolidated enhancers and promoters are calculated to obtain an estimate of the model's false positive rate by overlapping them with positive markers (PMs). Information regarding the definition of PMs is provided in the \"Defining Positive Markers\" subsection of the Preprocessing Module section in the Supplementary Information File.\n\n# 2.4 Creation of training, validation, test set and external validation set\n\nAt the end of the preprocessing pipeline, the samples consisting of normalized read coverage of enhancers, promoters and randomly selected background genomic regions are merged into a single set. Each sample in this set is a  $2\\mathrm{kb}$  region and represents the normalized read coverage for all histone marks. A 3D tensor of dimension samples  $\\times$  histone marks  $\\times$  bins consisting of the normalized read counts for each region is constructed. This 3D tensor is divided into training, validation and test sets. The user can choose between two different modes for creating the train-validation-test split. The first mode uses a chromosome-wise split of the training, validation, and test sets, i.e. samples belonging to a chromosome will only be assigned to one of the datasets. The second mode randomly splits the samples into training, validation, and test set in the percentage ratio of 60:20:20. For this study, we use the chromosome-wise split. We include regions from chromosome 1-7 in the training set, chromosome 8-16 in the validation set and chromosome 17-22, X and Y in the test set. The total numbers of samples for each class across the three cell lines are listed in Supplementary Table S3.\n\nAs a baseline for machine learning based methods, we downloaded enhancer and promoter annotations from two public databasesSCREEN https://screen.encodeproject.org/ and EnhancerAtlas 2.0 http://www.enhanceratlas.orgto\n\n(a) Convolutional Neural Network\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/3362e2d1084a8cec9a7c91692c9e1846196c2416c248f508ff4fab9df03b9afd.jpg)\n\n(b) Recurrent Neural Network\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/01364ec820c820b78810f01601d98407761844f36f87708f5bd44fd13f93f712.jpg)\n\nFigure 1. Network structures for (a) CNN. The input for CNN is a tensor consisting of the normalized read counts for each histone mark across bins. The first four convolutional layers are followed by a batch normalization layer as well as a ReLU activation layer. Max pooling is applied for layer 2 and 4. The final pooling layer consists of a global average pooling followed by a 1D convolutional layer and a softmax layer for classification. (b) RNN. Input for the RNN is the same as that for the CNN. It includes a  $7 \\times 1$  1D convolutional layer at the bottom which is followed by two LSTMs with the size of 32. They are followed by a softmax layer for classification.\n\ncompare against the test set. Both databases use knowledge-based, ad hoc rules to identify potential DREs on the genome. SCREEN candidate cis-Regulatory Elements (cCREs) were obtained for K562, GM12878, and HepG2 cell line. Only the sites labeled as distal enhancer-like signatures and promoter-like signatures were retained for the comparison. Obtaining BED file of enhancers from EnhancerAtlas database was a straightforward process. We converted the coordinates of EnhancerAtlas database from hg19 to hg38 using the CrossMap Python package (Zhao et al. 2014). The number\n\nof sites used for both databases are included in Supplementary Table S13. For comparison with the test set, we used the command \"bedtools intersect\" to find an overlap between the test set sites of DeepRegFinder and the SCREEN and EnhancerAtlas sites\n\nFor further validation of DeepRegFinder's predictions, we obtained 664 experimentally validated enhancer-gene pairs as well as the negative control regions reported in (Gasperini et al. 2019). We converted the hg19 coordinates to hg38 using the CrossMap Python package (Zhao et al. 2014).\n\n# 3. Results\n\n# 3.1 Model evaluation using three-class classification\n\nWe conducted a comparative analysis of DeepRegFinder against five established methods, namely Random-Forest Based Algorithm for Enhancer Identification from Chromatin State (RFECS) (Rajagopal et al. 2013), enhancer HMM (eHMM) (Zehnder et al. 2019), Probabilistic Enhancer PredictIoN Tool (PREPRINT) (Osmala and Lhdesmki 2020), Enhancer Prediction using Deep Neural Network (EP-DNN or KimNet) (Kim et al. 2016), and ChromHMM (Ernst and Kellis 2017) (see Table 1 for description of each tool). The data preprocessing for each method is provided in the \"Comparison of DeepRegFinder with existing tools\" section of the Supplementary Information File. Because these methods cannot distinguish active and poised enhancers/promoters, we employed the three-class classification mode of DeepRegFinder. For ChromHMM, we manually assigned the chromatin states learned on the three cell lines (Supplementary Fig. S3) to the three classes since it is a purely unsupervised method.\n\nOverall, both CNN and RNN models of DeepRegFinder compare favorably with the other methods in precision and recall scores on the test set across all cell types (Fig. 2 and Supplementary Table S1). Only RFECS achieves performance close to the two models, followed by EP-DNN (i.e. KimNet). The other three methods are significantly worse, with ChromHMM consistently ranked at the bottom. For enhancer prediction, the precision is in the range of 0.71-0.81, while the recall is in the range of 0.84-0.93 for the two DeepRegFinder models. For promoter prediction, the precision is in the range of 0.91-0.95, while the recall is in the range of 0.86-0.89 for the two DeepRegFinder models. Further analysis through the confusion matrices of the two models (Supplementary Fig. S1) reveals a small amount of misclassifications between the promoter and enhancer classes. Only a tiny fraction of the enhancers and promoters are classified as background and vice versa. The performance of the RNN model is on par with that of the CNN model; both reach mean average precision (mAP) in the range of 0.91-0.93 (Supplementary Table S1).\n\nPerformance of the other methods are highly variable, especially on enhancer prediction. For instance, EP-DNN demonstrates poor performance for enhancer identification on the K562 cell line in comparison to DeepRegFinder. However, its performance is more in line with that of DeepRegFinder on the GM12878 and HepG2 cell lines. On the other hand, the method consistently demonstrates good performance for identifying promoters across all cell lines, with slightly lower precision and recall than that of DeepRegFinder. Both PREPRINT and eHMM's performance in enhancer prediction are significantly lower than that of DeepRegFinder, RFECS, and EP-DNN.\n\nTable 1. Summary of all tools utilized in this study.\n\n<table><tr><td>Name</td><td>Reference</td><td>Methodology</td></tr><tr><td>ChromHMM</td><td>Ernst and Kellis (2017)</td><td>Unsupervised HMM + manual labeling</td></tr><tr><td>RFECS</td><td>Rajagopal et al. (2013)</td><td>Random Forest</td></tr><tr><td>EP-DNN</td><td>Kim et al. (2016)</td><td>Multi-layer perceptron</td></tr><tr><td>PREPRINT</td><td>Osmala and Lhdesmki (2020)</td><td>Bayesian derived probabilistic scores + SVM with Gaussian kernel</td></tr><tr><td>eHMM</td><td>Zehnder et al. (2019)</td><td>Supervised HMM</td></tr></table>\n\nPREPRINT is notably better than eHMM on the K562 and GM12878 cell lines but slightly worse on the HepG2 cell line. ChromHMM is significantly worse than all the other methods in both enhancer and promoter prediction across all cell lines. We hypothesize that ChromHMM as a fully unsupervised method requires much more training data than its supervised counterparts for DRE classification. We did an additional analysis to compare the publicly available ChromHMM annotations based on a model trained with 127 cell lines https://egg2.wustl.edu/roadmap/web\\portal/chr\\state\\learning.html with our test set. We found this version of ChromHMM to perform better than PREPRINT, eHMM and the ChromHMM but still worse than DeepRegFinder, especially for enhancer prediction (Fig. 2 and Supplementary Table S6). All methods achieve better performance in promoter prediction than enhancer prediction, showing that the enhancers are more difficult to identify than promoters in general.\n\nTo provide additional baselines for model performance, we used the enhancers and promoters defined in SCREEN and EnhancerAtlas databases. Both databases represent collections of candidate enhancers and promoters based on ad hoc rules. We found SCREEN's promoter annotation to be in line with that of DeepRegFinder but enhancer annotation to be below the PR curve of DeepRegFinder but still above that of PREPRINT, eHMM and ChromHMM (Fig. 2 and Supplementary Table S10). EnhancerAtlas's enhancer annotation has a recall of around 0.9 but a precision of only 0.20-0.35 across the three cell lines (Fig. 2 and Supplementary Table S11). This analysis provides some baseline performance of an enhancer and promoter database that is based on biological knowledge and simple statistical analysis rather than complicated machine learning models.\n\nWe further compared the predicted enhancers from all tools with the 664 experimentally validated enhancers and negative control regions from (Gasperini et al. 2019) (Supplementary Table S12). DeepRegFinder (CNN), eHMM and RFECS show high precision of  $>0.95$  but both eHMM and RFECS have low recall of around 0.6, while DeepRegFinder (CNN) has a recall of 0.85. EP-DNN, DeepRegFinder (RNN), and PREPRINT show high recall at or above 0.95 but relatively lower precision. Judging by the F1 scores, DeepRegFinder (CNN and RNN), EP-DNN, and PREPRINT achieve the top spots with DeepRegFinder (RNN) ranks at the top, followed by PREPRINT and DeepRegFinder (CNN). ChromHMM has high precision of 0.92 but low recall of 0.40, which make it again rank at the bottom among all tools in terms of F1 score. It shall be noted that the experimentally validated enhancers (Gasperini et al. 2019) are only a small subset of all enhancers on the genome and can be biased in terms of their chromatin binding characteristics due to the small sample size. We also inspected the validated enhancers that were not recalled as enhancers for each tool and found the majority of them to be predicted as promoters (Supplementary Table S12). This is in line with the confusion matrix analysis (Supplementary Fig. S1) that enhancers are more likely to be predicted as promoters than background.\n\nLastly, we computed the validation rates of enhancers and promoters predicted by DeepRegFinder's prediction module by overlapping them with PMs (Supplementary Table S8). The validation rates are all above 0.95 for both promoters and enhancers across all cell lines.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/829ba9cb92a8c5c29b3aa7f1ee34f7954cdca3eb0c2130de4712f63ed7dd241a.jpg)\n\nFigure 2. Precision-recall curves for three-class classification. Precision-recall curves to compare the performance of 7 different methods for enhancers and promoters for K562, GM12878, and HepG2 cell lines.\n\n# 3.2 Model evaluation using five-class classification\n\nIn addition to three-class classification, DeepRegFinder can perform five-class classification. Using this mode, our program can distinguish between poised and active states of enhancers and promoters. Poised enhancers are often identified by the decreased level of H3K27ac and the enrichment of H3K27me3 histone marks (Rada-Iglesias et al. 2011). On the contrary, active enhancers are often characterized by the increased level of H3K27ac and the absence of H3K27me3 (Gray et al. 2015). Both poised and active enhancers contain the presence of H3K4me1. While active enhancers are those regions that are actively involved in the regulation of gene expression under normal conditions, poised enhancers can transition to active states in response to specific pathways and developmental cues (Creyghton et al. 2010). It is therefore important to accurately identify both active and poised states of enhancers. Similarly, poised and active promoters contain the presence of H3K27me3 and H3K9ac, respectively. While both contain the enrichment of H3K4me3. The distinction between the poised and active states of promoters and enhancers in histone modifications are not as strong as the distinction between promoter and enhancer, or between enhancer and background, which makes it a more challenging task. Although no other method is designed to distinguish active and poised states of enhancers/promoters, we implemented our own versions of EP-DNN and RFECS to make them perform five-class classifications.\n\nComparing the precision and recall of the four models on the test set (Fig. 3 and Supplementary Table S2), both the CNN and RNN models of DeepRegFinder consistently outperform EP-DNN and RFECS across all classes and all cell types. The mAP scores of CNN and RNN varies between\n\n0.65 and 0.71 for the three cell types. Again, the performance of RNN is on par with that of CNN. There is no clear winner between EP-DNN and RFECS: while RFECS defeats EP-DNN on the K562 and GM12878 cell lines, it slightly underperforms EP-DNN on the HepG2 cell line. We also performed the analysis of comparing the publicly available ChromHMM annotations with the five-class labels of DeepRegFinder and found its performance to vary wildly across classes and cell lines (Fig. 3 and Supplementary Table S7). Although it is sometimes in line with the other models, especially on the active promoter, its scores can be well below the curves of the other models. The only exception is on the active enhancer in the HepG2 cell line where it outperforms all the other models, including DeepRegFinder. We note that the active enhancer class in the HepG2 cell line is unusually small in comparison with other cell lines (Supplementary Table S3). This has caused performance degradation for all models but does not affect ChromHMM annotations.\n\nComparing the four non-background classes, all models achieve the best performance on active promoter on the K562 and GM12878 cell lines, with precision and recall exceed or close to 0.9 for both CNN and RNN. The performance of active promoter is on par with poised enhancer and promoter on the HepG2 cell line. The ranking of the performance on the other three non-background classes varies depending on the cell types. This shows that the active promoter class has the most distinctive histone modification profiles among the four classes to make them easiest to identify. Confusion matrix analysis (Supplementary Fig. S2) shows that poised enhancer is most likely to be misclassified as active enhancer, followed by poised promoter. Active enhancer is most likely to be misclassified as poised enhancer, followed\n\nPoised Enhancers\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/b0cd08b42b5f51d63d31e65dcc53ada39ded1fa38ce11c9dc0b4b98307e77578.jpg)\n\nActive Enhancers\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/a96be959650125dfa39cf403c1cd97e82e201e361d33d153dc0a0c7d2bf14ca4.jpg)\n\nPoised TSS\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/253ffa011bfb27f30c2f3445c37d07d4dc182623e450a4eb3da7ed4de1ddca88.jpg)\n\nActive TSS\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/5597bfad7489fb293d5acd6d25f1a47a82bda1744521124ea48b21b7de10c0fd.jpg)\n\nPrecision\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/5516a9b68d184ef7176171e2de814c2f5b2186c6c9dc487c5a526435f54700b3.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/e9b975f23877c2549f1cfbb7558fec789f91ee9b7a9b877f347680226084988f.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/07f5f721d4c5f507d1cde794a73f38548c7af03471d7bb254f3f6d5225afa54b.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/1a6cfa41eccf2d97d1021692f6dc63ff8242d59a6a047594b55679d519f037fc.jpg)\n\nLegend\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/1ecab7797d61744025e447ffced533a67a902912a8220a5ea6c94b0ad05be57f.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/7aa1b471b5b7c88c603c420667af7c3c9ae967a6577289ca4da2731a5cc5dfb5.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/5495002a105ff2a3f15f05f35ab80a0cb54264d6a0d76cc867300e57789bfdd2.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/763681499b4c9bcc8fcc6c4c0d8e4fe9dee29113eb5f2ebbc2bfabaf4028e5e5.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/6432797cd29a9f114dd5df54483fc2acd80db7bde6cd33288f8babfe4d5efa9e.jpg)\n\nRecall\n\nFigure 3. Precision-recall curves for five-class classification. Precision-recall curves to compare the performance of DeepRegFinder, RFECS and EP-DNN for Poised Enhancers (PE), Active Enhancers (AE), Poised TSS (PT), and Active TSS (AT) for K562, GM12878, and HepG2 cell lines.\n\nby active promoter. Poised promoter is most likely to be misclassified as active promoter or background. On the HepG2 cell line, there is an elevated likelihood for the active promoter to be misclassified as poised promoter.\n\nFigure 4 represents three genome browser screenshots of DeepRegFinder's predictions in the K562 cell line using the CNN model. It demonstrates that the predicted active promoters contain the characteristic enrichment of H3K27ac, H3K4me3, and H3K9ac; the predicted active enhancers contain the enrichment of H3K4me1, H3K27ac and the depletion of H3K4me3; the predicted poised enhancer located upstream of the AK4 gene contains the enrichment of H3K4me1 but is depleted with the activating histone marks.\n\nLastly, we computed the validation rates of poised/active enhancers and promoters predicted by DeepRegFinder's prediction module (Supplementary Table S9). The validation rates are mostly above 0.95 for all classes across all cell lines. Only poised enhancer and poised promoter sometimes show slightly lower validation rate in the range of 0.93-0.95.\n\n# 3.3 Activation heatmap and weight matrices of the first convolution layer filters\n\nOne of the advantages of using a convolutional layer as the first layer of the CNN and RNN is that the convolutional filters of the first layer (Fig. 1aLayer 1) can be interpreted as feature detectors for \"chromatin motifs\". The combinations of such motifs are then learned by the following layers of the model for classification. To demonstrate the chromatin motifs learned by the first layer, we selected the top 100 regions for each of the five classes in the test set by their\n\npredicted class probabilities, resulting in a total of 500 regions. Each region is represented by an array with dimension of  $20 \\times 12$  that represents the coverage across 20 bins for 12 histone marks. In convolution, this region is known to have a length of 20 and a depth of 12, where each dimension of the depth is also known as a channel. The selected regions were passed through a CNN trained on the K562 cell line to obtain the activations of the first convolutional layer. The first convolutional layer contains 32 filters, each of dimension  $7 \\times 12$  that represents the weights spanning 7 bins and across 12 channels (i.e. histone marks). A filter scans the input to derive activation values by computing the dot product between the filter and a segment of 7 bins of the input region. The convolution is designed in a way so that the output has the same length as the input by padding appropriate zero values on both ends of the input.\n\nThe activation values of the first convolutional layer obtained using a single region as input is represented by a matrix of  $20 \\times 32$ . Altogether, the activation values for all 500 regions were contained in a 3-D tensor of  $20 \\times 32 \\times 500$ . Each filter's activation values are represented by a matrix of  $20 \\times 500$ . To characterize the activation patterns of the 32 filters, we applied singular value decomposition (SVD) on each  $20 \\times 500$  matrix independently to derive an eigenvector that represents the reduced activation across the 20 bins for the 500 regions, resulting in a  $1 \\times 500$  vector for each filter. By stacking the 32 eigenvectors together, this process reduced the 3-D tensor to a 2-D matrix of  $32 \\times 500$ . We then employed hierarchical clustering to cluster the rows (i.e. filters) of the matrix (Fig. 5, left panel). The left panel therefore\n\n(a)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/9fcbc57bf25e01efaa694b80bbcbaf88e5231aaf7fa18e1e47c31096882366ac.jpg)\n\n(b)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/23246e740668b4e7e6b9e5e555c8c5c4d264ca6f3ee46f81a9019a4b5469928d.jpg)\n\n(c)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/bca14b702dc58f5935ae2de09382a2ff3fb0617db13706dfba10068e729e93c5.jpg)\n\nFigure 4. Genome browser screenshots. Example screenshots of genome browser displaying DeepRegFinder CNN's predictions in the K562 cell line.\n\nshows the reduced representation of the activation values upon passing the 500 regions through the first convolutional layer of the trained network.\n\nThe left panel of Fig. 5 shows that some filters are activated across multiple classes, which means they extract common features that are shared by the five classes. On the contrary, other filters exhibit specific activations for particular classes. To understand what has been learned in those class-specific filters, we identified four filters (15, 12, 20, 8) and depicted\n\ntheir weights as line plots for six representative histone marksH3K27ac, H3K27me3, H3K4me1, H3K4me2, H3K4me3, and H3K9ac (Fig. 5, right panel). The histone marks were chosen based on our observation and the existing knowledge of chromatin biology about the association between histone marks and DREs. Filter 15 is associated with the active promoter class and contains peak detectors for H3K4me2/3, H3K27ac, and H3K9ac. Filter 12 is associated with the active enhancer class and contains a peak detector\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/dfc2f60d-67e7-4608-bbc5-b68affd5a7d6/38d37d9842ecf38f3d02dff6fb403eea99d17fbc41f20e23b79c8826ba618bb5.jpg)\n\nFigure 5. Clustering of the 32 filters of the first convolution layer of the CNN trained on K562. The top 100 regions for each of the five classes from the test set are selected by their predicted probabilities. The regions are passed through the CNN to obtain the activations for the first convolutional layer. The activations along the bin dimension is reduced to one through SVD (see main text for details) to obtain a matrix of filter  $\\times$  region. Each row of the matrix represents the activation pattern of a filter across the 500 regions. Hierarchical clustering is employed to cluster the rows of the matrix (left panel). Four filters are chosen which show enrichment for specific classes and the corresponding weights are depicted as line plots for six representative histone marks separately (right panel). Each row of line plots in the right panel represents the weights for the six selected histone marks of a filter, which is connected to a row in the left panel via a line with two dots on both ends.\n\nfor H3K27ac and H3K4me1/2 and is deactivated with H3K4me3. Filter 20 is associated with the background class and deactivated for all histone marks except H3K27me3. Filter 8 is associated with the poised promoter, poised enhancer and background classes and contains peak detectors for H3K4me1 and H3K27me3. The feature detectors learned by the class-specific filters are consistent with the existing knowledge of the histone marks in those DREs (ENCODE Project Consortium 2012). It shall be noted that the class predictions are not driven by a single filter in the first convolution layer but rather all filters and their combinations in the following layers.\n\n# 4. Discussion\n\nIn this study, we introduce DeepRegFinder, a pipeline for enhancer and promoter prediction in the genome using histone mark enrichment profiles. Our pipeline is powered by deep learning models and highly customizable, allowing for automated processing of ChIP-seq data. In addition to identifying promoters and enhancers, DeepRegFinder offers the ability to classify them into active and poised states, a feature not found in most other enhancer prediction tools. We conducted a comparison with existing approaches on three cell lines for enhancer and promoter predictions. DeepRegFinder consistently produced higher precision and recall scores than the other methods for both enhancers and promoters across all cell lines. To our surprise, ChromHMM produced the poorest performance among all methods, despite being one of the most popular tools to automatically annotate the genome into \"chromatin states\" including enhancers and promoters. This might not be an entirely fair comparison since ChromHMM is an unsupervised method that does not rely on training labels. However, the training labels for enhancer definition require only ChIP-seq\n\ndata for TFs such as p300 and DHS (or ATAC-seq), which are readily available for many cell lines from the ENCODE project and other studies. Another limitation of ChromHMM is that the learned chromatin states in the emission heatmap are often ambiguous, and it can be challenging to precisely assign them to promoter and enhancer classes.\n\nOur study also raises the important points of evaluating machine learning models properly and avoiding the common pitfalls in a machine learning study design. Both eHMM and PREPRINT report high performance in their own papers but perform poorly in our study. A closer inspection into eHMM's algorithm revealed that eHMM's enhancer definition used the binding profiles of H3K4me1, H3K4me3, H3K27ac, and ATAC-seq, which were later used as model input. This raises concerns of data leakage since the target is partially used as the input. In PREPRINT, statistical models are fit on the entire dataset to generate probabilistic scores that are used for model training before the dataset is split into train and test sets. This again raises concerns of data leakage. We also notice that methods are evaluated using only AUC scores in the PREPRINT study. We did not utilize AUC scores for model evaluation as they can conceal the problem of high false discovery rate (i.e. low precision) when a dataset is highly imbalanced. Instead, we used precision, recall, and mAP to evaluate models, which can better reflect the performance of a tool on a real-world genomic dataset. Additionally, PREPRINT only considers two-class classification of enhancer versus everything else. Our confusion matrix analysis shows that it's more common for misclassifications to happen between enhancer and promoter than between enhancer and generic background. When promoter and generic background are grouped into a single negative class, the AUC is more likely to reflect the performance of enhancer versus background since it is often the largest class in a dataset. We\n\nchose to use the three-class classification problem to compare different methods because it can better illustrate a model's ability to distinguish enhancers from promoters.\n\nThe only two methods whose performance were somewhat close to DeepRegFinder are EP-DNN and RFECS. EP-DNN uses a more traditional deep learning approach called multilayer perceptron (MLP), which consists of about 500K parameters. However, an MLP is not a good network architecture to learn features from chromatin modification data that obviously contain spatial dependencies among adjacent genomic positions. Our CNN and RNN models contain only 26K and 12K parameters yet achieved noticeably better performance than EP-DNN. RFECS's performance is about in line with DeepRegFinder on the three-class classification problem but falls behind on the more challenging five-class classification problem. This shows that a carefully designed deep neural network can be a very effective tool for high-throughput experimental data in genomics.\n\nIn recent years, another important progress in identifying enhancers is to use DNA sequences (Min et al. 2017, Yang et al. 2017, Kaur et al. 2023). The enhancer sites contain specific DNA sequences (i.e. motifs) that allow TFs to bind. A machine learning model can be trained to recognize such motifs and their combinations to predict enhancers. The DNA sequences can serve as a rich resource of information for enhancers and complement the histone mark ChIP-seq data. How to combine the DNA sequences with the ChIP-seq data requires further research. The Hi-C (High-throughput Chromosome Conformation Capture) sequencing can reveal the three-dimensional architecture of genomes (Lieberman-Aiden et al. 2009), which helps us understand how chromosomes fold and how enhancers physically interact with promoters. Novel machine learning methods need to be developed to model large trunks of genomic regions, or even an entire chromosome as a whole, to more accurately identify enhancers and promoters, potentially incorporating 3D chromatin interactions.\n\n# 5. Hardware requirement and running time\n\nRunning DeepRegFinder typically takes about  $2 - 8\\mathrm{h}$  for preprocessing;  $5\\mathrm{min}$  for training; and  $20\\mathrm{min}$  for prediction on the entire human genome. Both training and prediction require only a modest GPU configuration. For this study, we ran the pipeline on a Linux system with 12 CPU cores and 32 GB memory with an NVIDIA GTX 1080 GPU with 12GB VRAM.\n\n# Acknowledgements\n\nWe would like to thank Dr Weijia Zhang for providing the GPU server to facilitate this study.",
    "metadata": {
      "md_filename": "MinerU_markdown_DeepRegFinder_20260106143105_2008426069992677376.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_DeepRegFinder_20260106143105_2008426069992677376.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "3edcd782f4083af5",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_DeepSTARR_20260106143054_2008426025621135360",
    "title": "DeepSTARR predicts enhancer activity from DNA sequence and enables the de novo design of synthetic enhancers",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "UMI-STARR-seq library cloning. Inserts for Drosophila genome-wide and oligonucleotide libraries were amplified (for primers, see Supplementary Table 1) and cloned into the Drosophila STARR-seq vector $^{63}$  containing either the Drosophila synthetic core promoter (DSCP) or Rps12 core promoters using Gibson cloning (New England BioLabs, catalog no. E2611S). The oligonucleotide library for human STARR-seq screens was amplified (for primers, see Supplementary Table 1) and cloned into the human STARR-seq plasmid with the ORI in place of the core promoter $^{98}$ . Genome-wide and oligonucleotide libraries were grown in 61 and 21 LB-Amp (Luria-Bertani medium plus ampicillin,  $100~\\mu \\mathrm{g / ml}$ ), respectively, and purified with a Qiagen Plasmid Plus Giga Kit (catalog no. 12991).\n\nCell culture, transfection and UMI-STARR-seq. Drosophila S2 and human HCT116 cells were cultured as described previously[63,98]. Cells were electroporated using the MaxCyte-STX system at a density of  $50 \\times 10^{7}$  cells per  $100\\mu l$  and  $5\\mu g$  of DNA using the 'Optimization 1' protocol (S2) and at a density of  $1 \\times 10^{7}$  cells per  $100\\mu l$  and  $20\\mu g$  of DNA using the preset 'HCT116' program (HCT116), respectively. We transfected  $400 \\times 10^{6}$  S2 cells total per replicate with  $20\\mu g$  of the input library for Drosophila and  $80 \\times 10^{6}$  HCT116 cells total per replicate with  $160\\mu g$  of the input library for human cells. UMI-STARR-seq was performed as described previously[63,64,98]. Further experimental details can be found in the Supplementary Methods.\n\nIllumina sequencing. Next-generation sequencing was performed at the VBCF NGS facility on an Illumina HiSeq 2500, NextSeq 550 or NovaSeq SP platform, following the manufacturer's protocol, using standard Illumina i5 indexes as well as UMIs at the i7 index.\n\nGenome-wide UMI-STARR-seq data analysis. RNA and DNA input reads were mapped to the Drosophila genome (dm3), excluding chromosomes U, Uextra, and the mitochondrial genome, using Bowtie v.1.2.2 (ref. [99]). Mapping reads with up to three mismatches and a maximal insert size of  $2\\mathrm{kb}$  were kept. For paired-end RNA reads that mapped to the same positions, we collapsed those that have identical UMIs (10 bp, allowing one mismatch) to ensure the counting of unique reporter transcripts (Supplementary Table 2). After processing the two biological replicates separately, we pooled both replicates for developmental and housekeeping screens for further analyses.\n\nPeak calling was performed as described previously[63]. Peaks that had a hypergeometric  $P$  value  $\\leq 0.001$  and a corrected enrichment over input (corrected to the conservative lower bound of a  $95\\%$  confidence interval) greater than 3 were defined as enhancers and resized to 249 bp (Supplementary Table 3). Noncorrected enrichment over input was used as enhancer activity metric. Enhancers were classified as developmental or housekeeping based on the screen with the highest activity.\n\nOligonucleotide library UMI-STARR-seq data analysis. RNA and DNA input reads were mapped to a reference containing 249-bp long sequences containing both wildtype and mutated fragments from the Drosophila or human libraries using Bowtie v.1.2.2 (ref. ). Mapping reads with the correct length, strand and with no mismatches were kept. Both DNA and RNA reads were collapsed by UMIs (10 bp) as above (Supplementary Table 2).\n\nWe excluded oligonucleotides with fewer than ten reads in any of the input replicates and added one read pseudocount to oligonucleotides with zero RNA counts. The enhancer activity of each oligonucleotide in each screen was calculated as the  $\\log_2\\mathrm{FC}$  over input, using all replicates, with DESeq2 (ref. 100).\n\nDeep-learning data preparation. The genome was binned into 249-bp windows with a stride of  $100\\mathrm{bp}$ , excluding chromosomes U, Uextra, and the mitochondrial genome. We selected all windows at the summit of developmental and housekeeping enhancers, in addition to three windows on either side of the regions and a diversity of inactive sequences (Supplementary Methods). We augmented our dataset by adding the reverse complement of each original sequence, with the same output, ending up with 242,026 examples (484,052 postaugmentation). Sequences from the first (40,570;  $8.4\\%$ ) and second half of chr2R (41,186;  $8.5\\%$ ) were held out for validation and testing, respectively.\n\nDeepSTARR model architecture and training. DeepSTARR was designed as a multitask convolutional neural network (CNN) that uses one-hot-encoded 249-bp long DNA sequence  $(\\mathrm{A} = [1,0,0,0],\\mathrm{C} = [0,1,0,0],\\mathrm{G} = [0,0,1,0],\\mathrm{T} = [0,0,0,1])$  to predict both its developmental and housekeeping enhancer activities (Fig. 1c). We adapted the Basset CNN architecture[46] and built DeepSTARR with four one-dimensional (1D) convolutional layers (filters  $= 246$ , 60, 60, 120; size  $= 7$ , 3, 5, 3), each followed by batch normalization, a ReLU nonlinearity, and max-pooling (size  $= 2$ ). After the convolutional layers, there are two fully connected layers, each with 256 neurons and followed by batch normalization, a ReLU nonlinearity, and dropout where the fraction is 0.4. The final layer mapped to both developmental and housekeeping outputs. Further details on model training, hyperparameter tuning and performance evaluation can be found in the Supplementary Methods. The performance of DeepSTARR in the test set sequences was also compared with\n\ntwo different methods: a gapped k-mer support vector machine (gkm-SVM) $^{35}$  and a lasso regression model based on TF motif counts.\n\nNucleotide contribution scores and motif discovery. We used DeepExplainer (the DeepSHAP implementation of DeepLIFT $^{57,65,66}$ ; update from https://github.com/AvantiShri/shap/blob/master/shap/explainers/deep/deep_tf.py) to compute contribution scores for all nucleotides in all sequences with respect to either developmental or housekeeping enhancer activity. We used 100 dinucleotide-shuffled versions of each input sequence as reference sequences. For each sequence, the obtained hypothetical importance scores were multiplied by the one-hot-encoded matrix of the sequences to derive the final nucleotide contribution scores.\n\nTo consolidate motifs, we ran TF-Modisco (v.0.5.12.0 (ref. 58)) on the nucleotide contribution scores for each enhancer type separately using all developmental or housekeeping enhancers. We specified the following parameters: sliding_window_size=15, flank_size=5, max_seqlets_per_metacluster=50,000 and TfModiscoSeqsToPatternsFactory(trim_to_window_size=15, initial flank_to_add=5). Motifs supported by less than 35 seqlets were discarded.\n\nReference compendium of nonredundant TF motifs. A total of 6,502 TF motif models were obtained from iRegulon (http://iregulon.aertslab.org/collections.html (ref. [101]). We systematically collapsed redundant motifs by similarity by a previously described approach[82]. The code and TF motif compendium are available from https://github.com/bernardo-de-almeida/motif-clustering. Details on TF motif enrichment analyses in developmental and housekeeping enhancers can be found in the Supplementary Methods.\n\nDrosophila TF motif mutagenesis oligonucleotide library synthesis and UMI-STARR-seq. We computationally designed a Drosophila enhancers' motif mutagenesis oligonucleotide library containing 524 negative genomic regions; 5,082 wildtype enhancers; variants of 2,375 enhancers with mutations of all instances simultaneously (per motif type) or each instance individually for eight developmental motifs (GATA, AP-1, twist, Trl, SREBP, CREB, ETS, STAT), four housekeeping motifs (Dref, Ohler1, Ohler6, Ohler7) and three control motifs; scanning mutagenesis of five enhancers; variants with swapped GATA motif flanks for 100 enhancers and 249 synthetic enhancer sequences (Supplementary Table 5). All details can be found in the Supplementary Methods. The resulting 21,758-plex 300-mer oligonucleotide library was synthesized by Twist Bioscience. UMI-STARR-seq using this oligonucleotide library was performed and analyzed as described above. We performed three independent replicates for developmental and housekeeping screens (correlation  $\\mathrm{PCC} = 0.94 - 0.98$ ).\n\nTF motif mutation analysis and equivalency. From the candidate 249 bp enhancers, we identified 855 active developmental and 905 active housekeeping Drosophila enhancers (log $_2$  wildtype activity in oligonucleotide UMI-STARR-seq  $\\geq 3.15$  and 2.51, respectively; the strongest negative region in each screen) that we used in the subsequent TF motif mutation analyses. The impact of mutating all instances of a TF motif type simultaneously or each instance individually was measured as the log $_2$ FC enhancer activity between the respective mutant and wildtype sequences (Supplementary Tables 6 and 8). This was done separately for developmental and housekeeping enhancer activities.\n\nMotif nonequivalency across all enhancers or in the same enhancer was assessed by comparing the impact of mutating individual instances of the same TF motif, that is the  $\\log_2\\mathrm{FCs}$  of each instance (Supplementary Table 8). For the comparison between instances in the same enhancer, only enhancers that require the TF motif (greater than twofold reduction in activity after mutating all instances) and contain two or more instances were used. Motif instances with greater than twofold different contributions in the same enhancer were considered as nonequivalent. The same comparison across enhancers or in the same enhancer was performed for the three control motifs.\n\nDeepSTARR predicted global importance of motif types. To quantify the global importance of all known TF motifs to enhancer activity in silico $^{60}$ , we embedded each motif from the 6,502 TF motif compendium at five different locations and in both orientations in 100 random backbone DNA sequences and predicted their developmental and housekeeping enhancer activity with DeepSTARR. For each motif, we used the sequence corresponding to the highest affinity according to the annotated PWM models. The average activity across the different locations per backbone was divided by the backbone initial activity to get the predicted increase in enhancer activity per TF motif. The resultant  $\\log_2\\mathrm{FC}$  was averaged across all 100 backbones to derive the final global importance of each TF motif.\n\nDeepSTARR predictions for the contribution of motif instances. We used two complementary approaches to measure the predicted contribution of each motif instance by DeepSTARR: (1) we measured the predicted importance of all string-matched instances of each TF motif type as the average developmental or housekeeping DeepSTARR contribution scores over all its nucleotides (used in Fig. 4a-c and Supplementary Figs. 8a, 12a,c, 14a and 15); (2) to directly compare with the experimentally derived motif importance through motif mutagenesis, we\n\nused DeepSTARR to predict the  $\\log_2\\mathrm{FC}$  between wildtype and the motif-mutant enhancer sequences included in the oligonucleotide library for all instances of the different motif types (used in Fig. 3b,d and Supplementary Figs. 13 and 17a).\n\nScoring of TF motif instances with PWM motif scores. To assess how the PWM motif models predict the importance of a motif instance, we scored the wildtype sequence of each mutated motif instance with the PWM models of the selected TF motifs. We used the matchMotifs function from R package motifmatchr (v.1.4.0; genome = 'BSgenome.Dmelanogaster.UCSC.dm3', bg = 'even', ref. 102) with a  $P$  value cutoff of 1 to retrieve the PWM scores of all sequences. We tested different PWM models for each TF motif, if available, and reported always the one with the best correlation (Supplementary Table 10).\n\nPredicted contribution of motif-flanking nucleotides. The top 90th and bottom 10th percentile motif instances of each TF were selected based on their predicted (DeepSTARR scores for core sequence) or experimentally derived (minus signed  $(-)$  mutation  $\\log_2\\mathrm{FC}$ ) importance. The DeepSTARR contribution scores of their  $\\pm 50$  flanking nucleotides were shown using box plots (Fig. 4a and Supplementary Fig. 14). For each position, significant differences between top and bottom instances were assessed through a Wilcoxon rank-sum test ( $P < 0.001$ ). The sum of delta between medians of top and bottom instances for the positions with significant differences was used as measure of importance for the upstream and downstream flanking sequences.\n\n# Correlation between motif importance and motif-flanking sequence.\n\nString-matched motif instances of each TF were sorted by their predicted (DeepSTARR) or experimentally derived (minus signed  $(-)$  mutation  $\\log_2\\mathrm{FC}$ ) importance. Their five flanking nucleotides were shown using heatmaps, and the importance of each nucleotide at each flanking position summarized using box plots (Fig. 4b and Supplementary Fig. 15). Significant differences between the four nucleotides per position were assessed through Welch one-way analysis of variance (ANOVA) test followed by false discovery rate (FDR) multiple testing correction.\n\nThe motifs recovered by DeepSTARR were compared with PWM models discovered de novo by HOMER. HOMER (v.4.10.4 (ref.  ${}^{103}$ ) was run on the 249-bp developmental or housekeeping enhancer regions with the findMotifsGenome.pl command and the command line argument -size 249.\n\nIn silico motif distance preferences. Two consensus TF motifs were embedded in 60 random backbone 249-bp DNA sequences, MotifA in the center and MotifB at a range of distances  $(d)$  from MotifA, both up- and downstream. DeepSTARR was used to predict the developmental or housekeeping activity of the backbone synthetic sequences (1) without any motif (b), (2) only with MotifA in the center (A), (3) only with MotifB d-bases up- or downstream (B) and (4) with both MotifA and MotifB (AB). The DeepSTARR predicted activities in log space were converted to linear space as  $2^{\\text{DeepSTARR prediction}}$ . The cooperativity between MotifA and MotifB at each distance  $d$  was then defined as the fold-change between AB and  $(\\mathrm{b} + (\\mathrm{A} - \\mathrm{b}) + (\\mathrm{B} - \\mathrm{b}) = \\mathrm{A} + \\mathrm{B} - \\mathrm{b})$ , where a value of 1 means an additive effect or no synergy between the motifs, and a value higher than 1 means positive synergy. The median of fold-changes across the 60 backbones was used as the final cooperativity scores.\n\n# Enrichment of motif pairs at different distances in genomic enhancers. To\n\ncompute whether MotifA is located within a certain distance (bins: 0-25, 25-50, 50-75, 75-100, 100-125, 125-150, 150-250 bp) of MotifB more or less frequently in enhancers than in negative sequences, we counted the number of times a MotifA instance is at each distance bin to a MotifB instance in enhancers and in negative sequences. The enrichment or depletion of motif pairs at each bin was tested with two-sided Fisher's exact test and the  $\\log_2$  odds ratio used as metric. Obtained  $P$  values were corrected for multiple testing by Benjamini-Hochberg procedure and considered significant if FDR  $\\leq 0.05$ .\n\nAssociation between motif pair distances and enhancer activity. For each pair of motif instances at each distance bin (0-25, 25-50, 50-75, 75-100, 100-125, 125-150, 150-250 bp), we tested the association between enhancer activity and the presence of the pair at the respective distance bin using a multiple linear regression including as independent variables the number of instances for the different developmental or housekeeping TF motif types. The linear model coefficient was used as metric and considered significant if the FDR-corrected  $P$  values  $\\leq 0.05$ .\n\n# Human TF motif mutagenesis oligonucleotide library synthesis and\n\nUMI-STARR-seq. We selected the nine TF motif types with the strongest enrichment in enhancers in human HCT116 cells $^{98}$ : AP-1, p53, MAF, CREB1, ETS, EGR1, MECP2, E2F1 and Ebox/MYC (Supplementary Table 12 and Supplementary Methods). We selected 3,200 enhancer candidates, defining short 249-bp windows (the limits of oligonucleotide synthesis), and mapped the position of all instances of the nine TF motif types in these candidates using the matchMotifs function from R package motifmatchr (v.1.4.0 (ref.  ${}^{102}$ ) with the following parameters: genome = 'BSgenome.Hsapiens.UCSC.hg19', p.cutoff = 5e $^{-04}$ , bg = 'genome'. Overlapping instances (minimum 70%) for the same TF motif were collapsed. We also mapped all instances of four control motifs using string-matching.\n\nWe computationally designed the human enhancers' motif mutagenesis oligonucleotide library containing: 920 249-bp negative genomic regions as controls; 3,200 wildtype enhancers; and 18,780 enhancer variants with all instances of each motif type mutated simultaneously or individually to a motif shuffled variant (Supplementary Table 13). All details can be found in the Supplementary Methods. Apart from the specific sequences, this human motif mutagenesis library exhibits the same specifications as the Drosophila library and was also synthesized by Twist Bioscience. UMI-STARR-seq using this oligonucleotide library was performed and analyzed as described above. We performed two independent replicates (correlation  $\\mathrm{PCC} = 0.99$ ).\n\nHuman TF motif mutation analysis. From the 3,200 designed candidate 249-bp enhancers, we identified 1,083 active short human enhancers  $(\\log_2$  wildtype activity in oligonucleotide UMI-STARR-seq  $\\geq 2.03$ , the strongest negative region) that we used in the subsequent TF motif analyses. The impact of mutating all instances of a TF motif type simultaneously or each instance individually was calculated as the  $\\log_2\\mathrm{FC}$  enhancer activity between the respective mutant and wildtype sequences (Supplementary Tables 14 and 15). Motif nonequivalency across all enhancers or in the same enhancer was assessed as in the Drosophila enhancers.\n\nValidation of important TF motif instances with genomic DNaseI footprinting data. We compared the importance of individual motif instances with genomic DNase I footprinting data of RKO cells (another human colon cancer cell line; https://www.vierstra.org/resources/dgf(ref. $^{82}$ ) as a surrogate for TF occupancy. For each TF motif type, a Wilcoxon rank-sum test was used to determine whether the mutation  $\\log_2\\mathrm{FC}$  of instances overlapping TF footprints (FPR threshold of 0.001) is significantly greater or less than the one of instances not overlapping footprints. Only instances in HCT116 accessible enhancers were used in the analysis.\n\nAssociation between motif syntax rules and the contribution of TF motif instances. For each TF motif type, we built a multiple linear regression model to predict the contribution of its individual instances  $(\\log_2\\mathrm{FCs})$  using as covariates the number of instances of the respective motif type in the enhancer, the motif core (defined as the nucleotides included in each TF motif PWM model) and flanking nucleotides (5 bp on each side), the motif position relative to the enhancer center[75], and the distance to all other TF motifs. All models were built using the Caret R package (v. 6.0-80 (ref. [104])) and tenfold cross-validation. Predictions for each held-out test set were used to compare with the observed  $\\log_2\\mathrm{FCs}$  and assess model performance. The linear model coefficients and respective  $P$  values were used as metrics of importance for each feature.\n\nLuciferase reporter assays. We constructed luciferase reporters by cloning candidate enhancers in both orientations in the pGL3_DSCP_luc+ plasmid either upstream or downstream of the DSCP promoter. One native enhancer, the three strongest synthetic enhancers and five negative controls were amplified from the Twist oligonucleotide pools and plasmids verified by Sanger sequencing (for primers, see Supplementary Table 1). Luciferase assays were performed in quadruplicates as described previously[105].\n\nLuciferase assay data analysis. We first normalized firefly over Renilla luciferase values for each of the eight biological replicates individually. To normalize to the core promoters' intrinsic activity, we then calculated the fold-change luciferase signal over the average signal of the five negative control sequences. For each enhancer candidate and construct, we used the average of the replicates as the final activity together with the s.d. (Supplementary Table 18).\n\nStatistics and data visualization. All statistical calculations and graphical displays were performed in R statistical computing environment (v3.5.1 (ref. 106)) and using the R package ggplot2 (v3.2.1 (ref. 107)). Coverage data tracks have been visualized in the UCSC Genome Browser108 and used to create displays of representative genomic loci. In all box plots, the central line denotes the median, the box encompasses 25th to 75th percentile (interquartile range) and the whiskers extend to  $1.5\\times$  interquartile range.\n\nReporting Summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article.",
    "metadata": {
      "md_filename": "MinerU_markdown_DeepSTARR_20260106143054_2008426025621135360.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_DeepSTARR_20260106143054_2008426025621135360.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "3ff779c22bb63cb3",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_DNABERT2_20260106143048_2008426001986232320",
    "title": "DNABERT-2: EFFICIENT FOUNDATION MODEL AND BENCHMARK FOR MULTI-SPECIES GENOMES",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "In this section, we provide empirical analysis on the BPE tokenizer for genome language ( 3.1) and describe the model architecture ( 3.2) and implementation details ( A.2) of DNABERT-2.\n\n# 3.1TOKENIZER\n\n<table><tr><td>Iteration</td><td>Corpus</td><td>Vocabulary</td></tr><tr><td>0</td><td>AACGCACTATATA</td><td>{A,T,C,G}</td></tr><tr><td>1</td><td>A A C G C A C T A T A T A</td><td>{A,T,C,G,TA}</td></tr><tr><td>2</td><td>A A C G C A C T A T A T A</td><td>{A,T,C,G,TA,AC}</td></tr><tr><td>3</td><td>A A C G C A C T A T A T A</td><td>......</td></tr></table>\n\nFigure 2: Illustration of the BPE vocabulary constructions.\n\nDNABERT-2 adapts SentencePiece (Kudo & Richardson, 2018) with Byte Pair Encoding (BPE) (Sennrich et al., 2016) to perform tokenization for DNA sequences. SentencePiece is a language-agnostic tokenizer that considers each input as a raw stream without assuming any pre-tokenization, which matches greatly with genome sequences where the definitions of word and sentence do not exist. BPE is a compression algorithm that has been widely used in the area\n\nof natural language processing as a word segmentation strategy. It learns a fixed-sized vocabulary of variable-length tokens based on the co-occurrence frequency of the characters. Figure 2 illustrates the process of constructing a vocabulary from a given corpus with BPE. First, we initialize the vocabulary with all unique characters in the corpus. Then, in each iteration, we view the most frequent character segment (e.g., TA at iteration 1) as a new word, add it to the vocabulary, and update the corpus by replacing all the same segments with this new word. The iteration continues till we achieve the desired number of words in the vocabulary. Thus, the target vocabulary size plays a crucial role.\n\nDue to the significant difference between natural language and DNA sequence, vocabulary sizes that are commonly used in the NLP area (Kenton & Toutanova, 2019; Vaswani et al., 2017; Raffel et al., 2020; OpenAI, 2023) may not be appropriate for genome sequences. To determine the most suitable vocabulary size, we constructed 8 vocabularies with target sizes ranging from  $2^{8}$  to  $2^{15}$  on the multi-species genomes (see Sec. 4.1) to empirically evaluate the impact of varying vocabulary sizes. As indicated in Figure 3a, larger vocabularies tend to encompass more lengthy tokens, which enables the tokenizer to represent the same input sequence with fewer tokens. Shorter tokenized sequences consequently reduce the computational cost (See Figure 3b), as the computational complexity of Transformers is quadratic in relation to the input sequence length. Therefore, from the computation efficiency perspective, a larger vocabulary size is favorable.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/c0146663-5f32-4da2-9cb4-7cae485d55bb/8c574fee92c654b1c8fa8abbb6d794387b60419e8b8f418e2951f46a19504066.jpg)\n\n(a) Average token length and the length ratio of original sequence v.s. tokenized sequence.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/c0146663-5f32-4da2-9cb4-7cae485d55bb/88bbb231cf326d66d55e04326936de538e4c5bce5bd28032b48de769d516c79b.jpg)\n\n(b) Training FLOPs on 500-length sequences compared to model with  $2^{8}$  vocabulary.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/c0146663-5f32-4da2-9cb4-7cae485d55bb/42372277e3497d1d44aa946d97a5403267f318582f6599bf52a3c14da89649dc.jpg)\n\n(c) Model performance averaged over each tasks (macro) and individual dataset (micro).\n\nFigure 3: This figure presents the average token length, average sequence length reduced after tokenization, and model performance on the GUE benchmark with different vocabulary sizes.\n\nHowever, a larger vocabulary leads to more sparse updates to the embedding layer, given that each token would be used less frequently, which might compromise the model's performance. We empirically analyzed this issue by pre-training three different DNABERT-2 variants with vocabulary sizes of  $2^{8}$ ,  $2^{12}$ , and  $2^{15}$  on the multi-species genome dataset with a batch size of 2048 for 150000 steps and evaluating them on the GUE benchmark (see Sec. 4.2). Figure 3c displays the performance of each variant, where the model performance is measured by the dataset- and task-average scores. As depicted in the figure, unlike computational efficiency, the model's performance does not consistently improve as the vocabulary size increases. Therefore, we selected a vocabulary size of  $2^{12} = 4096$  for training the final DNABERT-2 model, as it best balances model performance with computational efficiency among the candidates.\n\n# 3.2 MODEL\n\nDNABERT-2 adapts the Transformer Encoder architecture. To address the limitations of existing models, we incorporate a series of recent advances in deep learning to increase the model's efficiency and capability, including: 1) replacing learned positional embeddings with the Attention with Linear Biases (ALiBi) (Press et al., 2021) to overcome the input length limitation; 2) utilizing FlashAttention (Dao et al., 2022) and Low Precision Layer Normalization to increase computation and memory efficiency; 3) employing the Low-Rank Adaptation (LoRA) (Hu et al., 2021) in the fine-tuning stage (if necessary) for parameter-efficient training.\n\nAttention with Linear Biases. Due to the permutation-invariant nature of the attention mechanism, explicit positional information is required in attention-based models. Existing solutions such as Sinusoidal (Vaswani et al., 2017), learned (Kenton & Toutanova, 2019), and Rotary (Su et al., 2021) positional embedding methods either suffer from input length restriction or poor extrapolation capability when applied to sequences longer than training data. Attention with Linear Biases (ALiBi) provides an efficient yet effective solution. Instead of adding position embeddings to the input, ALiBi adds a fixed set of static, non-learned biases to each attention calculation to incorporate positional information into attention scores. Specifically, let  $\\mathbf{q}_i$  define the  $i$ -th query in the input sequence of length  $L$  and  $\\mathbf{K}$  defines the key matrix, the attention score of query  $i$  is calculated as:  $\\text{softmax}(\\mathbf{q}_i\\mathbf{K} + m*[-(i - 1),\\dots, - 2, - 1,0, - 1, - 2,\\dots, - (L - 1 - i)]}$ , where  $m$  is a fixed head-specific constant. ALiBi used a geometric sequence (i.e.,  $\\frac{1}{2^1},\\frac{1}{2^2},\\dots,\\frac{1}{2^n}$ ) of different  $m$  to each attention head. Intuitively, ALiBi increasingly penalizes attention scores between key-query pairs as their distances increase, and  $m$  determines the penalty rate. Replacing learned position embedding with ALiBi allows DNABERT-2 to effectively handle arbitrarily long sequences during fine-tuning and inference despite being pre-trained on relatively short sequences.\n\nFlash Attention. Flash attention is an IO-aware algorithm that implements the exact standard attention calculation in a more time- and memory-efficient way. It identifies a main bottleneck of standard attention implementation as the lack of taking the number of reads and writes to fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory (HBM) into account. To avoid reading and writing to the slow HBM, it splits Key/Query/Value matrices into blocks and incrementally performs softmax over the entire input. It also proposes to recompute large intermediate results like attention scores in backward pass to trade extra computation for fewer IO with HBM, which empirically leads to less computational time without sacrificing model performance.\n\nLow-Rank Adaptation (LoRA). Fine-tuning all the parameters of a model becomes increasingly expensive as the pre-trained model becomes much larger. Thus, we adopt LoRA, a parameter-efficient fine-tuning method that significantly reduces the computation and memory costs with ignorable performance sacrifice. Let  $W_0, W_1 \\in \\mathbb{R}^{m \\times n}$  define the same weight matrix before and after task-specific fine-tuning, and we have  $W_1 = W_0 + \\Delta W$ , where  $\\Delta W \\in \\mathbb{R}^{m \\times n}$  represents the change of each weight element during the fine-tuning. In ordinary fine-tuning, we independently update each weight based on its corresponding gradient, while in LoRA, we represent  $\\Delta W$  with a low-rank decomposition  $\\Delta W = BA$ , where  $B \\in \\mathbb{R}^{m \\times r}$ ,  $A \\in \\mathbb{R}^{r \\times n}$ , and  $r \\ll m, r \\ll n$ . Modeling  $\\Delta W$  with low-rank decomposition reduces the number of trainable parameters from  $m \\times n$  to  $r \\times (m + n)$ , leading to significant improvement in training time and memory usage.\n\nBesides, we replace the Relu activation function with GEGLU (Shazeer, 2020), a variant of GLU (Dauphin et al., 2017) that has been shown to improve the performance of Transformer models. The GEGLU function is calculated as  $\\mathrm{GEGLU}(x,W,V,b,c) = \\mathrm{GELU}(xW + b)\\otimes (xV + c)$ , where  $x$  is the function input,  $W$  and  $V$  are learnable weights, and  $b$  and  $c$  are learnable biases. The GELU function is defined as  $\\mathrm{GELU}(x) = x\\Phi (x)$ , where  $\\Phi (x)$  is the cumulative distribution function (CDF) of the standard normal distribution.\n\n# 4 DATA\n\nIn order to facilitate further research on large-scale genome foundational models, we have collated and made available multi-species genome datasets for both pre-training of models (Sec. 4.1) and benchmarking (Sec. 4.2).\n\n# 4.1 PRE-TRAIN: HUMAN AND MULTI-SPECIES GENOME\n\nTo investigate the impact of species diversity on genome foundation models, we compile and made publicly available two datasets for foundation model pre-training: the human genome and the multi-species genome. The human genome dataset is the one used in DNABERT (Ji et al., 2021), which comprises 2.75B nucleotide bases. The multi-species genome dataset encompasses genomes from 135 species, spread across 6 categories. In total, this dataset includes 32.49B nucleotide bases, nearly 12 times the volume of the human genome dataset. We exclude all sequences with N and retain only sequences that consist of A, T, C, and G. Detailed statistics are presented in Table 11.\n\n# 4.2 BENCKMARK: GENOME UNDERSTANDING EVALUATION (GUE & GUE $^{+}$ )\n\nWe introduce a large benchmark for genome foundation models. Due to the input length limits of existing genome foundation models, we split the benchmark into two parts: GUE and  $\\mathrm{GUE}^{+}$ . GUE (See Table 1) includes 7 genome sequence classification problems with 28 datasets with input lengths ranging from 70 to 1000.  $\\mathrm{GUE}^{+}$  (See Table 2), on the other hand, focuses on relatively longer input sequences, rating from 5000 to 10000. To evaluate the multi-species transferability in genome understanding of each model, this benchmarks contains tasks for various species, including human, fungi, virus, and yeast. We explicitly define evaluation metrics for each task and split each dataset into training, validation, and test data for a fair comparison across different models.\n\nTo calibrate the benchmark's difficulty level and better illuminate each model's capabilities, we carefully selected datasets that are neither too simple nor overly challenging for current models. For example, when the Nucleotide Transformer variants (Dalla-Torre et al., 2023) were tested on the Splice Site Prediction dataset, all variants achieved an accuracy between  $97\\%$  and  $98\\%$ . Similar\n\n<table><tr><td>Species</td><td>Task</td><td>Num. Datasets</td><td>Num. Classes</td><td>Sequence Length</td></tr><tr><td rowspan=\"4\">Human</td><td>Core Promoter Detection</td><td>3</td><td>2</td><td>70</td></tr><tr><td>Transcription Factor Prediction</td><td>5</td><td>2</td><td>100</td></tr><tr><td>Promoter Detection</td><td>3</td><td>2</td><td>300</td></tr><tr><td>Splice Site Detection</td><td>1</td><td>3</td><td>400</td></tr><tr><td>Mouse</td><td>Transcription Factor Prediction</td><td>5</td><td>2</td><td>100</td></tr><tr><td>Yeast</td><td>Epigenetic Marks Prediction</td><td>10</td><td>2</td><td>500</td></tr><tr><td>Virus</td><td>Covid Variant Classification</td><td>1</td><td>9</td><td>1000</td></tr></table>\n\nTable 1: Summarization of the Genome Understanding Evaluation (GUE) benchmark.\n\n<table><tr><td>Species</td><td>Task</td><td>Num. Datasets</td><td>Num. Classes</td><td>Sequence Length</td></tr><tr><td>Human</td><td>Enhancer Promoter Interaction</td><td>6</td><td>2</td><td>5000</td></tr><tr><td>Fungi</td><td>Species Classification</td><td>1</td><td>25</td><td>5000</td></tr><tr><td>Virus</td><td>Species Classification</td><td>1</td><td>20</td><td>10000</td></tr></table>\n\nTable 2: Summarization of the Genome Understanding Evaluation Plus (GUE  ${}^{ + }$  ) benchmark.\n\noutcomes were observed in tasks like Promoter Prediction and Enhancer Prediction. These high scores might suggest these variants perform similarly, but as our experiments in Section 5 show, they vary significantly on more discerning datasets.\n\nThe construction of the benchmark starts with the aggregation of various biologically important genome analysis datasets, followed by the assessment of existing models such as DNABERT (Ji et al., 2021) and Nucleotide Transformer (Dalla-Torre et al., 2023) on these datasets. Datasets where the majority of models yielded moderate (e.g., F1-scores between 0.3 and 0.8) and distinguishable performance scores were retained. On the other hand, datasets that did not meet these criteria underwent a restructuring process involving various strategies such as class balancing, adversarial sample inclusion, and reduction of training sample volume, among others. After several iterations of this process, we ultimately arrived at 36 representative datasets of moderate difficulty. Due to space limits, we present the detailed data processing and statistics of each dataset in Sec. B.2.\n\n# 5 EXPERIMENTS\n\nWe evaluate DNABERT-2 using the proposed benchmark to thoroughly investigate its versatility and robustness across a variety of tasks involving multi-species genomes.\n\n# 5.1 BASELINE\n\nWe compare DNABERT-2 with two state-of-the-art genome foundation models: DNABERT (Ji et al., 2021) and Nucleotide Transformer (Dalla-Torre et al., 2023).\n\nDNABERT was the first pre-trained foundational model for genome sequences, trained on human genome sequences. It has four variants, namely DNABERT (3-mer), DNABERT (4-mer), DNABERT (5-mer), and DNABERT (6-mer), which utilize overlapping  $3/4/5/6$ -kmer tokenization respectively. While DNABERT employs the same architecture as BERT-base, it has a different vocabulary size, which is dependent on the chosen  $k$ -mer.\n\nNucleotide Transformer (NT) scales up the data and model size to achieve state-of-the-art performance in 27 DNA analysis tasks. It also has 4 variants: NT-500M-human, NT-500M-1000g, NT-2500M-1000g, and NT-2500M-multi, where human, 1000g, and multi respectively refers to the GRCh38/hg38 human reference genome, 3202 high-coverage human genomes from the 1000 Genome project (Byrska-Bishop et al., 2021), and genome from 850 different species.\n\nIt is important to note that NT models are 6 to 29 times larger than DNABERT, which precludes standard model fine-tuning on consumer GPUs. Therefore, we perform standard fine-tuning for\n\n<table><tr><td>Model</td><td>Params. </td><td>FLOPs </td><td>Trn. Tokens</td><td>Num. Top-2 </td><td>Ave. Scores </td></tr><tr><td>DNABERT (3-mer)</td><td>86M</td><td>3.27</td><td>122B</td><td>2 || 0</td><td>61.62</td></tr><tr><td>DNABERT (4-mer)</td><td>86M</td><td>3.26</td><td>122B</td><td>0 || 1</td><td>61.14</td></tr><tr><td>DNABERT (5-mer)</td><td>87M</td><td>3.26</td><td>122B</td><td>0 || 1</td><td>60.05</td></tr><tr><td>DNABERT (6-mer)</td><td>89M</td><td>3.25</td><td>122B</td><td>0 || 1</td><td>60.51</td></tr><tr><td>NT-500M-human</td><td>480M</td><td>3.19</td><td>50B</td><td>0 || 0</td><td>55.43</td></tr><tr><td>NT-500M-1000g</td><td>480M</td><td>3.19</td><td>50B</td><td>0 || 1</td><td>58.23</td></tr><tr><td>NT-2500M-1000g</td><td>2537M</td><td>19.44</td><td>300B</td><td>0 || 1</td><td>61.41</td></tr><tr><td>NT-2500M-multi</td><td>2537M</td><td>19.44</td><td>300B</td><td>7 || 9</td><td>66.93</td></tr><tr><td>DNABERT-2</td><td>117M</td><td>1.00</td><td>262B</td><td>8 || 4</td><td>66.80</td></tr><tr><td>DNABERT-2</td><td>117M</td><td>1.00</td><td>263B</td><td>11 || 10</td><td>67.77</td></tr></table>\n\nTable 3: The statistics and performance of each model. The five columns represent the number of model parameters, relative FLOPs compared to DNABERT-2, the number of tokens used in pre-training, and the number of being top-2 among all the models (1st || 2nd) and the average evaluation scores on the 28 datasets of the GUE benchmark.  $\\spadesuit$ : perform further masked language modeling pre-training on the training sets of the GUE benchmark.\n\nDNABERT and DNABERT-2, while adapting the Low-Rank Adaptation (LoRA) technique for fine-tuning the Nucleotide Transformer to enhance efficiency. For a fair comparison, we conducted preliminary experiments to confirm that our implementation of NT achieves comparable results to those reported in their original paper (Dalla-Torre et al., 2023) (see Appendix A.4 for more details).\n\n# 5.2 SETUP AND METRIC\n\nWe evaluate the models from two perspectives: computational efficiency and performance on downstream tasks. To measure each model's computational cost, we consider the number of model parameters and the relative Floating Point Operations (FLOPs)which is the total number of multiplication and addition operations during a forward passcompared to DNABERT-2. We evaluate FLOPs on genome sequences with a length of 500, a commonly used setup in genome analysis. To measure model performance, we utilize F1-Score and Matthews Correlation Coefficient (MCC). We use different metrics for different tasks, following conventional practices (refer to Table 12 for details). Table 3 presents the overall performance of each model on the GUE benchmark. It provides the average score of each model and the number of times it ranks in the top two among all models. The average results across all tasks are reported in Table 4, while task-specific results are presented in Table 6 due to space constraints. We also include statistics on the number of tokens each model processed during its pre-training phase, providing insight into the effects of training steps on model performance. For each model, we keep most of the hyperparameters (e.g., learning rate, batch size, weight decay, etc.) constant across all datasets, adjusting only the maximum sequence length and the number of training steps according to the specific dataset. Hyperparameter tuning tailored to each dataset is left for future work. Throughout the training process, we validate the model every 200 steps, save the model that yields the smallest loss on the validation set, and report its evaluation results on the test set. We train each model using three different random seeds and report the average results.\n\nFurther Pre-Training. We also investigate the impact of additional in-domain pre-training on DNA language models. We combine the training sets of the 28 GUE datasets and further pre-train DNABERT-2 on this combined set. Following Sun et al. (2020), we train the model with a batch size of 32, a maximum sequence length of 128, and a learning rate of  $5e - 5$  for 100,000 steps. This results in 0.41B training tokens, which only constitute  $0.08\\%$  of the tokens processed during the entire training process of DNABERT-2.\n\n# 5.3 RESULTS ON GUE\n\nTable 3 outlines the statistics and aggregate performance of the models. As indicated in the table, despite being  $21 \\times$  smaller and requiring  $19 \\times$  fewer FLOPs, DNABERT-2 delivers a performance comparable to the state-of-the-art model while significantly surpassing other baselines. When DNABERT-2 undergoes additional pre-training on the GUE benchmark, which requires negligible computational overhead, it delivers the highest average performance and consistently ranks in the\n\n<table><tr><td>Species</td><td>Yeast</td><td>Mouse</td><td>Virus</td><td colspan=\"4\">Human</td></tr><tr><td>Task</td><td>EMP</td><td>TF-M</td><td>CVC</td><td>TF-H</td><td>PD</td><td>CPD</td><td>SSP</td></tr><tr><td>DNABERT (3-mer)</td><td>49.54</td><td>57.73</td><td>62.23</td><td>64.43</td><td>84.63</td><td>72.96</td><td>84.14</td></tr><tr><td>DNABERT (4-mer)</td><td>48.59</td><td>59.58</td><td>59.87</td><td>64.41</td><td>82.99</td><td>71.10</td><td>84.05</td></tr><tr><td>DNABERT (5-mer)</td><td>48.62</td><td>54.85</td><td>63.64</td><td>50.46</td><td>84.04</td><td>72.03</td><td>84.02</td></tr><tr><td>DNABERT (6-mer)</td><td>49.10</td><td>56.43</td><td>55.50</td><td>64.17</td><td>81.70</td><td>71.81</td><td>84.07</td></tr><tr><td>NT-500M-human</td><td>45.35</td><td>45.24</td><td>57.13</td><td>50.82</td><td>85.51</td><td>66.54</td><td>79.71</td></tr><tr><td>NT-500M-1000g</td><td>47.68</td><td>49.31</td><td>52.06</td><td>58.92</td><td>86.58</td><td>69.13</td><td>80.97</td></tr><tr><td>NT-2500M-1000g</td><td>50.86</td><td>56.82</td><td>66.73</td><td>61.99</td><td>86.61</td><td>68.17</td><td>85.78</td></tr><tr><td>NT-2500M-multi</td><td>58.06</td><td>67.01</td><td>73.04</td><td>63.32</td><td>88.14</td><td>71.62</td><td>89.36</td></tr><tr><td>DNABERT-2</td><td>55.98</td><td>67.99</td><td>71.02</td><td>70.10</td><td>84.21</td><td>70.52</td><td>84.99</td></tr><tr><td>DNABERT-2</td><td>58.83</td><td>71.21</td><td>68.49</td><td>66.84</td><td>83.81</td><td>71.07</td><td>85.93</td></tr></table>\n\nTable 4: The models' averaged performance on the 7 tasks in the GUE benchmark, including Epigenetic Marks Prediction (EMP), Transcription Factor Prediction on the Human genome and the Mouse genome (TF-H and TF-M), Covid Variants Classification (CVC), Promoter Detection (PD), Core Promoter Detection (CPD), and Splice Site Prediction (SSP).\n\ntop two across the 28 tasks of the GUE benchmark. These results showcase the model's remarkable efficiency and effectiveness.\n\nDespite having  $30\\%$  more parameters than DNABERT, DNABERT-2 requires only one-third the number of FLOPs. This indicates the superiority of the Byte Pair Encoding (BPE)-based tokenization method over overlapping k-mer tokenization in terms of modeling efficiency. Armed with the new tokenization method and the Attention with Linear Biases (ALiBi) module, DNABERT-2 can effectively process long genome sequences arbitrarily, demonstrating enhanced efficiency. This improvement becomes even more significant as the length of the input sequence increases. Moreover, DNABERT-2 consistently outperforms DNABERT by a large margin, indicating the effectiveness of multi-species pre-training and new model architecture.\n\nAlthough DNABERT-2 is 5 times smaller, it surpasses NT-500M while using fewer FLOPs. This underscores the importance of providing the model with adequate data, particularly when the model size is scaled up, and further highlights the inefficiency of overlapping k-mer tokenization. The comparison between DNABERT and NT-2500M-1000g exposes the sample inefficiency of non-overlapping k-mer tokenization. Despite being trained on 2.5 times more tokens, NT-2500M-1000g achieves a performance similar to that of DNABERT.\n\nThe averaged results for each task are displayed in Table 4. DNABERT-2 and NT-2500M-multi consistently achieve top-tier performance across most tasks. Their dominance over other baselines is particularly notable in non-human genome analysis tasks, demonstrating the effectiveness of multi-species genomes pre-training. Furthermore, models trained on multi-species genomes also show strong performance on human genome analysis tasks, proving their ability to develop a comprehensive understanding of multi-species genomes without compromising their grasp of the human genome. We also observe that additional pre-training does not uniformly enhance all tasks, indicating that task-specific further pre-training might be necessary when addressing a certain downstream task.\n\nAdditionally, DNABERT variants achieve optimal performance in the Core Promoter Detection task, where inputs are sequences of length 70. However, their performance diminishes in the similar task of Promoter Detection, where the input length increases to 300. These results highlight a common challenge associated with non-overlapping k-mer tokenization and BPE-based tokenization: the capacity to identify subtle signals from limited input. Although inefficient, the overlapping k-mer tokenization adopted by DNABERT retains most of the information in the original sequences. In contrast, the sequence length is significantly reduced (i.e., from 70 to 15) with non-overlapping k-mer and BPE tokenization, which might limit the retained information and hinder informed decision-making. This identifies a critical area for future exploration in DNA language models.\n\n<table><tr><td>Task</td><td>SC (Fungi)</td><td>SC (Virus)</td><td colspan=\"6\">EPI (Human)</td></tr><tr><td>Dataset</td><td></td><td></td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td>DNABERT (6-mer)</td><td>89.29</td><td>44.51</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>NT-2500M-multi</td><td>92.85</td><td>45.00</td><td>61.91</td><td>72.15</td><td>73.13</td><td>79.49</td><td>86.48</td><td>68.64</td></tr><tr><td>DNABERT-2</td><td>93.04</td><td>48.50</td><td>76.21</td><td>79.19</td><td>83.50</td><td>86.71</td><td>92.90</td><td>73.70</td></tr></table>\n\nTable 5: The models' averaged performance on the 3 tasks in the GUE  $^+$  benchmark, including 6 Enhance Promoter Interaction (EPI) datasets and two Species Classification (SC) datasets on virus and fungi. The - means we are not able to fit the model on the dataset after trying multiple sets of hyperparameters.\n\n# 5.4 RESULTS ON GUE\n\nIn this section, we further compare DNABERT-2 with DNABERT and Nucleotide Transformers on the  $\\mathrm{GUE}^{+}$  benchmark. The input size of datasets in  $\\mathrm{GUE}^{+}$  ranges from 5000 to 10000. For DNABERT, due to its 512 input limitation, we follow its origin setting by splitting the input into 512bp-len pieces, generating embedding independently, and feeding the averaged embedding to the output layer. Table 5 shows DNABERT, Nucleotide Transformer, and DNABERT-2's performance on the  $\\mathrm{GUE}^{+}$  benchmark. As shown in the table, DNABERT-2 consistently outperforms the baselines on all the datasets, showing its good capability in handling long DNA sequences. Notably, despite DNABERT-2 is pre-trained purely on 700bp-len sequences, it performs well even on 10000-bp sequences with a few epochs of fine-tuning, indicating DNABERT-2's extrapolation capability offered by ALiBi.\n\n# 6 CONCLUSION\n\nIn this paper, we introduce DNABERT-2, an efficient foundational model pre-trained on multi-species genomes. We identify the computational and sample inefficiencies of the existing k-mer tokenization method and propose the adaptation of Byte Pair Encoding (BPE) for DNA language modeling. We provide insightful and comprehensive empirical analyses, building DNABERT-2 based on these findings. Moreover, we integrate several techniques such as Attention with Linear Biases (ALiBi) and Low-Rank Adaptation (LoRA) to address the limitations of current DNA language models. We also compile and introduce the Genome Understanding Evaluation, a benchmark for multi-species genome analysis comprising 9 tasks and 36 datasets with well-defined data split, evaluation metrics, and elaborately calibrated difficulty. Empirical analysis on the GUE benchmark shows that DNABERT-2 achieves comparable performance as the SOTA models while being much more efficient. For future work, we are interested in effective modeling strategies for short and extra-long genome sequences and the introduction of training targets and data processing/augmentation methods that leverage the unique double-strand structure of DNA.\n\n# ACKNOWLEDGE\n\nThis work is supported by NIH R01LM01372201.",
    "metadata": {
      "md_filename": "MinerU_markdown_DNABERT2_20260106143048_2008426001986232320.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_DNABERT2_20260106143048_2008426001986232320.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "82aa77eeb9837b05",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_Enformer_20260106143044_2008425981216038912",
    "title": "OPEN",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "Model architecture. The Enformer architecture consists of three parts: (1) 7 convolutional blocks with pooling, (2) 11 transformer blocks, and (3) a cropping layer followed by final pointwise convolutions branching into 2 organism-specific network heads (Extended Data Fig. 1). Enformer takes as input one-hot-encoded DNA sequence  $(\\mathrm{A} = [1,0,0,0],\\mathrm{C} = [0,1,0,0],\\mathrm{G} = [0,0,1,0],\\mathrm{T} = [0,0,0,1],\\mathrm{N} = [0,0,0,0])$  of length 196,608 bp and predicts 5,313 genomic tracks for the human genome and 1,643 tracks for the mouse genome, each of length 896 corresponding to 114,688 bp aggregated into 128-bp bins. The convolutional blocks with pooling first reduce the spatial dimension from 196,608 bp to 1,536 so that each sequence position vector represents 128 bp (although the convolutions do observe nucleotides in the adjacent pooled regions). The transformer blocks then capture long-range interactions across the sequence. The cropping layer trims 320 positions on each side to avoid computing the loss on the far ends because these regions are disadvantaged because they can observe regulatory elements only on one side (toward the sequence center) and not the other (the region beyond the sequence boundaries). Finally, the two output heads predict organism-specific tracks. The Enformer architecture is similar to the state-of-the-art model Basenji2 (ref. 2). However, the following changes helped us improve and exceed its performance: Enformer uses transformer blocks instead of dilated convolutions, attention pooling instead of max pooling, twice as many channels, and 1.5 times longer input sequence (197 kb instead of 131 kb). The detailed model architecture, including the selected hyperparameters, is shown in Extended Data Fig. 1.\n\nAttention pooling summarizes a contiguous chunk of the input sequence  $\\mathbf{x}_{k:k + L_p}^{full} = \\mathbf{x}\\in R^{L_p\\times C}$  across  $L_{\\mathrm{p}}$  positions for each of the  $C$  channels and returns the output value  $\\mathbf{h}\\in \\mathbb{R}^C$  as follows:\n\n$$\nh _ {j} = \\frac {\\sum_ {i} e x p (\\boldsymbol {x} _ {i} \\cdot \\boldsymbol {w} _ {j}) x _ {i j}}{\\sum_ {i} e x p (\\boldsymbol {x} _ {i} \\cdot \\boldsymbol {w} _ {j})},\n$$\n\nwhere  $i$  indexes sequence position in the pooling window, which is weighted by the exponentiated dot product  $\\mathbf{x}_i\\cdot \\mathbf{w}_j$  and  $\\pmb {w}\\in R^{C\\times K}$  is a matrix of learned weights. We apply attention pooling to contiguous chunks of the original input sequence using window size  $L_{\\mathrm{p}} = 2$  and stride of 2. We initialize  $\\mathbf{w}$  to  $2\\times 1$ , where  $\\mathbf{1}$  is the identity matrix to prioritize the larger value, making the operation similar to max pooling. This initialization gave slightly better performance than did random initialization or initialization with zeros, representing average pooling.\n\nWe use multi-head attention (MHA) layers to share information across the sequence and model long-range interactions, such as those between promoters and enhancers. Each head has a separate set of weights  $\\mathbf{w}^{\\mathrm{q}}\\in R^{C\\times K}$ ,  $\\mathbf{w}^{\\mathrm{k}}\\in R^{C\\times K}$ , and  $\\mathbf{w}^{\\mathrm{v}}\\in R^{C\\times 1}$  which transform the input sequence  $\\mathbf{x}\\in R^{L\\times C}$  into queries  $\\mathbf{q}_{\\mathrm{i}} = \\mathbf{x}_{\\mathrm{i}}\\mathbf{w}^{\\mathrm{q}}$ , keys  $\\mathbf{k}_{j} = \\mathbf{x}_{j}\\mathbf{w}^{\\mathrm{k}}$  and values  $\\mathbf{v}_{j} = \\mathbf{x}_{j}\\mathbf{w}^{\\mathrm{v}}$ . Queries represent the current information at each position and keys represent the information each position will be looking for to attend to. Their dot product plus the relative positional encodings  $\\mathbf{R}_{ij}$  forms the attention matrix, which is computed as  $\\mathbf{a}_{ij} = \\text{softmax}(\\mathbf{q}_i\\mathbf{k}_i^{\\mathrm{T}} / \\sqrt{\\mathbf{K} + \\mathbf{R}_{ij}})$ , where the entry  $a_{ij}$  represents the amount of weight query at position  $i$  puts on the key at position  $j$ . Values represent the information that each position will propagate forward to positions that attend to it. Each single attention head computes its output as a weighted sum across all input positions:  $\\mathbf{av}$ . This allows each query position to use information across the whole sequence. The multiple heads compute with independent parameters, and we concatenate the outputs from each head to form the final layer output followed by a linear layer to combine them. Our layers used 8 heads, value size of 192, and key/query size of 64.\n\nMHA applications in NLP typically operate directly on the input sequence, tokenized into words and embedded in a richer embedding space. The convolution tower preceding MHA in the Enformer model serves to perform an analogous operation of embedding nucleotide segments and contributes a compelling inductive bias for adjacent nucleotides to function together in motifs. We chose to compute at 128-bp resolution because it roughly represents a well-studied length of regulatory elements that contain several motifs and is an appropriate bin size at which to aggregate the experimental data to be predicted. Finer resolution has potential benefits when the data support it $^{15}$ , but would extend the sequence length entering the quadratic complexity MHA and make the model engineering intractable on currently available hardware.\n\nTo inject positional information, we add relative positional encodings $^{40}$ $\\mathbf{R}_{ij}$  to the  $\\mathbf{q}_i$ ,  $\\mathbf{k}_j^{\\mathrm{T}}$  attention term as formulated in the Transformer-XL paper $^{41}$ . Relative positional encodings provide a parameterized baseline for how actively two positions in the sequence should influence each other during the layer's transformation as a function of their pairwise distance. Specifically, we use  $\\mathbf{R}_{ij} = \\mathbf{q}_i$ ,  $\\mathbf{r}_{i-j}^{\\mathrm{T}} + \\mathbf{u}$ ,  $\\mathbf{k}_j^{\\mathrm{T}} + \\mathbf{v}$ ,  $\\mathbf{r}_{i-j}^{\\mathrm{T}}$ , where  $\\mathbf{r}_{i-j} = \\mathbf{w}^{\\mathrm{R}}$ ,  $\\mathbf{f}(i-j)$  is a linear function of different relative basis functions  $\\mathbf{f}(i-j)$ , and  $\\mathbf{u}$  and  $\\mathbf{v}$  are the position-agnostic embeddings used to evaluate the preference for specific keys ( $\\mathbf{u}$ ) or relative distances ( $\\mathbf{v}$ ). We use three different basis function classes for  $\\mathbf{f}(i-j)$ , as visualized in Extended Data Fig. 5b:\n\n1.  $f_{i}^{exponential}(r) = e^{-\\log (2)\\frac{r}{r_{1 / 2,i}}},$  where  $r_{1 / 2,i}$  is placed linearly in the log-space between 3 and sequence length.\n\n2.  $f_{i}^{centralmask}(r) = \\left\\{ \\begin{array}{ll}1, & \\text{if} r\\leq 2^{i}\\\\ 0, & \\text{otherwise} \\end{array} \\right.$\n\n3.  $f_{i}^{\\text{gamma}}(r) = \\text{Gamma}\\left(r|\\alpha = \\frac{\\mu_{i}}{\\sigma^{2}},\\beta = \\frac{\\mu_{i}^{2}}{\\sigma^{2}}\\right)$ , where  $\\text{Gamma}(\\mathsf{r}|\\mathsf{a},\\beta)$  is the gamma probability distribution function.  $\\mu_{i}$  is placed linearly from (sequence length / number of features) to sequence length and  $\\sigma =$  sequence length /  $(2\\times$  number of features).\n\nFor each basis function, we use a symmetric  $\\mathrm{f}(|x|)$  and asymmetric sign(x)  $\\times \\mathrm{f}(|x|)$  version to introduce directionality. We use the same number of relative positional basis functions as the value size of MHA (192). The 192 basis functions are equally divided among the basis function classes and the symmetric versus asymmetric versions thereof. With 3 basis function classes, each basis function class provides 64 positional features (32 symmetric and 32 asymmetric).\n\nDropout rates of 0.01 and 0.05 were used for positional encoding features and the final attention matrix respectively in MHA. All other dropout rates are annotated in Extended Data Fig. 1a.\n\nModel training and evaluation. The model was trained, evaluated, and tested on the same targets, genomic intervals, and using the same Poisson negative log-likelihood loss function as Basenji2 (ref. 2). Briefly, the cross-species training/ validation/test sets were constructed using the following procedure to partition homologous sequences into the same set. First, we divided both the human and mouse genomes into  $1\\mathrm{Mb}$  regions. We constructed a bipartite graph, in which the vertices represent these regions. Next, we placed edges between 2 regions if they have  $>100\\mathrm{kb}$  of aligning sequence in the hg38-mm10 syntenic net format alignment downloaded from the UCSC Genome Browser. Finally, we partitioned connected components in the bipartite graph randomly into training, validation, and test sets.\n\nThe dataset contains 34,021 training, 2,213 validation, and 1,937 test sequences for the human genome, and 29,295 training, 2,209 validation, and 2,017 test sequences for the mouse genome. For the human genome, each example contains 2,131 transcription factor (TF) chromatin immunoprecipitation and sequencing (ChIP-seq), 1,860 histone modification ChIP-seq, 684 DNase-seq or ATAC-seq, and 638 CAGE tracks (total 5,313, Supplementary Table 2). For the mouse genome, each example contains 308 TF ChIP-seq, 750 histone modification ChIP-seq, 228 DNase-seq or ATAC-seq, and 357 CAGE tracks (total 1,643, Supplementary Table 3). We modified the Basenji2 dataset by extending the input sequence to 196,608 bp from the original 131,072 bp using the hg38 reference genome.\n\nTo train a model simultaneously on human and mouse genomes, we alternated between a batch containing data from the human genome and the mouse genome. The main Enformer model with 1,536 channels was implemented in Sonnet v2, TensorFlow (v2.4.0), and was trained on 64 TPU v3 cores with batch size of 64 (1 per core) for 150,000 steps (approximately 3 days) using all-reduce gradient aggregation across the cores at every step. Batch normalization statistics were also aggregated across multiple replicas using 0.9 momentum. We used the Adam optimizer from Sonnet v2 (ref.  $^{43}$ ) with a learning rate of 0.0005 and default settings for other hyperparameters:  $\\beta_{1} = 0.9$ ,  $\\beta_{2} = 0.999$ ,  $\\varepsilon = 1 \\times 10^{-8}$ . The optimal learning rate was discovered by grid search yielding the highest performance on the validation set. We linearly increased the learning rate from 0 to target value in the first 5,000 steps of training. We clipped gradients to a maximum global norm of 0.2. We used the same data augmentation as Basenji2 (ref.  $^{2}$ ) during training by randomly shifting the input sequence by up to 3 bp and reverse-complementing the input sequence while reversing the targets. Finally, we fine-tuned the Enformer model on human data for 30,000 steps using a lower learning rate of 0.0001.\n\nWe used the pretrained Basenji2 model for all main model comparisons and retrained an equivalent model for ablation and hyperparameter sweeps shown in Extended Data Fig. 5. In these comparative analyses, we used 768 channels (1/2 of the original Enformer model obtained by using a value size of 96 in MHA),  $131\\mathrm{kb}$  input sequence, and batch size 32 trained on 32 TPU v3 cores. We did not fine-tune these models on the human data. For models using dilated convolutions instead of transformer blocks, we used a higher learning rate of 0.02 without ramp up of the learning rate. As for Enformer, the optimal learning rate was discovered by grid search yielding the highest performance on the validation set. All models were trained for 500,000 steps while only storing the model with the highest Spearman correlation of CAGE TSS gene expression across genes averaged across experiments computed on the validation set every 1,000 steps.\n\nWe used the validation set for hyperparameter selection and the test set for Basenji2 comparison. We considered two evaluation metrics: (1) Pearson correlation computed across all 128-bp binned genomic positions in the validation/test set for each output track; and (2) Pearson correlation of CAGE gene expression values  $(\\log(1 + x)$ -transformed and standardized across genes for each experiment) of all protein-coding genes in the validation/test set computed either for each CAGE experiment across genes (main metric) or across CAGE experiments for each gene (shown in Fig. 1b). Observed and predicted gene expression values were obtained by summing up the observed/predicted CAGE read counts at all unique TSS locations of the gene. For each TSS location, we used the 128-bp bin overlapping the TSS as well as the two neighboring bins (3 bins in total). We used testtime augmentation during model evaluation: we averaged the predictions from 8 sequences randomly augmented the same way as during training ( $\\leq 3$  bp shifts and reverse-complementation). We only evaluated the performance of our\n\nmodel on the test set once to generate Fig. 1 and did not use the test set during model development.\n\nTo select a representative example, we visualized the top 10 transcripts with highest discrepancy between Enformer and Basenji2 performance on the 'Across CAGE experiments' metric measuring tissue specificity for  $33\\%$  of the most tissue-specific genes. We picked the sixth transcript in the list (ENST00000524922) because it cleanly showed differences across all three categories of genomic tracks (DNA accessibility, histone modifications, and gene expression).\n\nEnhancer prioritization. We obtained a set of enhancer-gene pairs tested using a CRISPRi assay perturbing the enhancer of interest while measuring the expression change of the gene in K562 cells from two studies: Gasperini et al. using scRNA-seq to measure expression changes, and Fulco et al. using Flow-FISH. We transformed the enhancer and gene coordinates from hg19 to hg38 using the UCSC liftOver web tool. Each enhancer-gene pair contains a label denoting whether a significant expression change was induced after CRISPRi treatment. We denoted the set of all enhancers as 'candidate' enhancers and those that showed a change in expression as 'validated' enhancers. We evaluated different methods on their ability to classify or prioritize enhancer-gene pairs that exhibited a significant expression change using area under precision-recall curve (auPRC).\n\nTo prioritize enhancer-gene pairs with sequence-based models, we computed three different scores: gradient  $\\times$  input, attention, and in silico mutagenesis (ISM). For each enhancer-gene pair, we determined the major TSS of the gene by taking the highest predicted CAGE value in K562 using Enformer. We extracted the DNA sequence centered at the main TSS and computed the following different enhancer-gene scores:\n\n1. Gradient  $\\times$  input: We computed the absolute value of the gradient of the CAGE targets (either using the K562-specific CAGE targets or all CAGE targets, Extended Data Fig. 7c) at the TSS with regard to the input reference sequence nucleotide. Note that since our input sequence is one-hot encoded, taking the input gradient of the nonzero channel (the reference nucleotide), is equivalent to computing gradient  $\\times$  input attributions[12]. We note that 'CAGE at TSS' always means summing the absolute gradient values from three adjacent bins, as is also done in gene-focused model evaluation. The three bins include the bin overlapping the TSS and one flanking bin on each side. The enhancer-gene score was obtained by summing the absolute gradient  $\\times$  input scores in the 2-kb window centered at the enhancer.\n\n2. Attention: We first averaged transformer attention matrices across all heads and layers. We extracted the row corresponding to the query index positioned at the TSS, so that keys correspond to different spatial positions and the attention values specify how much the model attended to these positions when making predictions for the TSS. We only computed this contribution score for Enformer. The enhancer-gene score was obtained by summing the attention scores in the 2-kb window centered at the enhancer.\n\n3. ISM: The in silico mutagenesis enhancer-gene score was computed by comparing K562 CAGE predictions at the TSS from the reference sequence with predictions from modified sequence where the 2-kb enhancer sequence was replaced by a random sequence:  $|f(\\text{modified}) - f(\\text{reference})|$ .\n\nTo reproduce the ABC score introduced in Fulco et al.13, we obtained the BigWig of H3K27ac ChIP-seq data in K562 from ENCODE with file accession ENCCFF779QTH and DNase with file accessions ENCCF413AHU and ENCCF936BDN. We summed the normalized reads from replicates. For each track and enhancer, we summed up the signal at the enhancer in a fixed window of  $2\\mathrm{kb}$  centered at the enhancer. This fixed and broader window yielded better performance compared to the variable window size of  $\\sim 500$  bp as used in the original ABC score (Extended Data Fig. 4a).\n\nGTEX SLDP. We predicted the effect of a genetic variant on various annotations by computing a forward pass through the model using the reference and alternative alleles, subtracting their difference, and summing outputs across the sequence to obtain a signed score for each training dataset. We averaged scores computed using the forward and reverse complement sequence and small sequence shifts to the left and right. We computed scores for all 1000 Genomes SNPs.\n\nWe used  $\\mathrm{SLDP}^{20}$  to estimate the functional correlation between these scores and GTEx v7a summary statistics for 48 tissues while accounting for population linkage disequilibrium structure (Supplementary Information).\n\nFine-mapped GTEx classification. To study specific eQTLs without needing to consider LD, we studied statistical fine-mapping of GTEx v8 using the SuSiE method $^{20,23}$ . We focused on variants with posterior inclusion probability (PIP) in a credible causal set  $>0.9$ , which ranged from a minimum of 166 variants for substantia nigra to 2,740 for tibial nerve. We arranged a classification task to discriminate between these positive causal variants and a matched set of negative variants. When available, we chose a negative variant matched to each causal variant from the set with  $\\mathrm{PIP} < 0.01$  but  $|Z$ -score|  $>4$  tested for the same gene. When unavailable for the same gene, we chose from the set with  $\\mathrm{PIP} < 0.01$  and  $|Z$ -score|  $>6$  genome-wide.\n\nTo determine how informative different variant annotations are, we trained separate random forest classifiers for each tissue to distinguish causal from noncausal variants using eight-fold crossvalidation. We selected the default hyperparameters of the scikit-learn 0.22 implementation after finding negligible accuracy gains from modifying them<sup>44</sup>. However, owing to the large number of features derived from the training datasets, setting the maximum features considered per decision tree split to  $\\log_2$  of the total number of features greatly improved the computational efficiency. We fit 100 iterations of stochastic crossvalidation shuffling and random forest fitting to delineate a low-variance estimate of model accuracy. We performed statistical tests comparing two different model feature sets by comparing the  $8\\times 100$  distinct test set auROCs.\n\nFor signed GTEx analysis, we benchmarked model predictions on the basis of their ability to discriminate causal variants that increase versus decrease gene expression. In this analysis, we removed variants that affect gene expression in opposite directions for different cis-genes. We manually matched FANTOM5 CAGE sample descriptions to the GTEx tissues. We skipped cases with more than three possible matches. In cases with two or three possible matches, we chose the CAGE sample with the best average concordance between the Basenji2 and Enformer predictions. We computed auROC statistics by ranking causal variants by their signed prediction for that sample.\n\nBenchmarking variant effect predictions on saturation mutagenesis data. We acquired training and test sets as well as the predictive accuracies of individual competition participants from the CAGI5 competition $^{26}$  (M. Kircher, personal communication, https://genomeinterpretation.org/content/expression-variants). For each variant and locus, we evaluated its effect as the predicted difference between the reference and alternative allele summed in four flanking bins representing 512 bp, producing 5,313 features based on the human datasets. All CAGE features were log-transformed after adding a pseudocount of 1 prior to computing this difference. For each allele, we averaged predictions for the forward and reverse-complemented sequence. We scaled the features from the test set with scaling factors computed on the features from the training set, such that the training features had a mean of 0 and s.d. of 1. Following our previous work $^{45}$ , we then trained a lasso regression model for each locus using these features and the corresponding training set. The strength of the regularization was controlled by a single  $\\lambda$  parameter, which was optimized using tenfold crossvalidation for each training set using the cvglmnet function of the glmnet library in R.\n\nFor our training-free comparisons, we selected the subset of features corresponding to cell-type-matched and cell-type-agnostic predictions of changes in CAGE and DNase. For the cell-type-agnostic models, we used the subset of all 638 CAGE or 674 DNase features (Supplementary Table 2). For the cell-type-matched models, we additionally required the CAGE/DNase features to contain the following substrings: (1) 'HepG2' for F9, LDLR, and SORT1, (2) 'K562' for GP1BB, HBB, HBG1, and PKLR, and (3) 'HEK293' for HNF4A, MSMB, TERT (performed in HEK293T cells), and MYCrs6983267. For several loci, a perfectly matched DNase or CAGE sample did not exist. We therefore selected the most closely matched feature based on the following substrings: (1) 'pancreas' for ZFAND3, (2) 'glioblastoma' for TERT (performed in GBM cells), (3) 'keratinocyte' for IRF6, and (4) 'SK-MEL' for IRF4. For each locus, we extracted the features matching the aforementioned substrings, and used the first principal component (PC) of the indicated features as our summary statistic, inverting the sign of the PC if it was negatively correlated to the mean of the features.\n\nReporting Summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article.",
    "metadata": {
      "md_filename": "MinerU_markdown_Enformer_20260106143044_2008425981216038912.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_Enformer_20260106143044_2008425981216038912.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "8b10bd1c45159785",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_EnformerMPRA_20260106143038_2008425959623761920",
    "title": "Massively parallel characterization of transcriptional regulatory elements",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# Design of Agilent oligonucleotide library\n\nHepG2 pilot library. For the HepG2 pilot library, we collected two replicates of DNase I hypersensitivity data derived from HepG2 cells (ENCODE narrowPeak BED files: ENCFF505SRS and ENCFF268DTI, hg19 human genome build) $^{55}$ . For each replicate, we collapsed overlapping peaks using bedtools merge (parameters \"-o collapse -c 2,3,7\") . Then, we identified peaks that intersected between the two replicates, merged these peaks, and removed the subset of merged peaks that overlap promoters (defined as regions  $\\pm 2,500$  nt around any annotated TSS). The resulting distribution of peak sizes was such that  $97\\%$  of peaks were  $\\leq 200$  bp in length. We therefore centred the designed oligonucleotides at each merged DNase peak, consistent with the known region of maximal regulatory activity $^{18}$ , and added  $\\pm 100$  bp to either side. This procedure resulted in a set of 66,017 cCREs. For this pilot library, we sought to evaluate cCREs which overlapped a wide range of putative transcription factor binding sites. We therefore intersected these potential enhancers with wgEncodeRegTfbsClusteredWithCellsV3. bed.gz $^{56}$  in order to count the number of putative HepG2 transcription factor binding sites intersecting these cCREs. We uniformly and randomly sampled  $-1,834$  cCREs with 0-1, 1-5, 5-10, 10-20, and  $>20$  TFBSs for a total of 9,172 elements. Including 50 positive and 50 negative controls from each of two previous studies $^{10,19}$  resulted in a total of 9,372 elements. These 171-bp controls from previous work were linked downstream of a 29-nt random sequence GGTGCTCGATTTAATTTCG CCGACGTGAT to match the 200-bp sequence length of cCREs. For the final oligonucleotide library, each element was linked to the  $5^{\\prime}$  adaptor AGGACCGGATCAACT and  $3^{\\prime}$  adaptor CATTGCGTGAACCGA, designing two 230-bp oligonucleotides per element to minimize the impact of oligonucleotide synthesis errors.\n\nK562 pilot library. An analogous procedure was followed for the K562 pilot library as in 'HepG2 pilot library', with the following modifications: (1) ENCODE narrowPeak BED files ENCFF027HKR and ENCFF154JCN (hg38 human genome build) $^{55}$  were used; (2) merging these peaks resulted in 34,367 potential enhancers; (3) after intersecting the peaks with K562 transcription factor binding sites, we sampled  $\\sim 1,278$  enhancers from each transcription factor binding site bin to test a total of 6,394 cCREs; (4) 250 additional negative controls were chosen by dinucleotide shuffling 250 random potential enhancers possessing 1-5 TFBSs; (5) positive and negative controls were chosen from CRISPRiscreens $^{20,21}$ , a previous MPRA $^{18}$ , and select loci of interest such as  $\\alpha$ -globin and  $\\beta$ -globin; (6) a total of 7,500 elements were tested; and (7) controls were already 200 bp in length, requiring no addition of a random sequence.\n\nHepG2 large-scale library. Following the procedures outlined in 'HepG2 pilot library', we tested all 66,017 previously identified cCREs in both orientations. For human protein-coding gene promoters, we extracted the average signal across cell types in TPM for each CAGE peak listed in hg19.cage_peak_phase1and2combined_tpm_ann. osc.txt.gz from the FANTOM5 consortium $^{57,58}$ . The first exons of all protein-coding gene transcripts were collected from Ensembl v.83 (hg38 genome build) $^{59}$ , transformed into hg19 coordinates using liftOver $^{60}$ , and then intersected with the CAGE peaks to identify a single promoter per gene corresponding to the promoter with the maximal average TPM. To select the final 200-bp oligonucleotide for testing, we identified the centre of the promoter DNase peak on the basis of the HepG2 DNase peaks merged across replicates (described in 'HepG2 pilot library'). In the scenario in which no DNase peak overlapped the promoter, we centred on the midpoint of the CAGE peak. In the scenario in which neither a DNase nor CAGE peak existed, we centred on the TSS defined by the Ensembl annotation. This resulted in a total of 19,104 protein-coding gene promoters, of\n\nwhich 6,181 were centred on a DNase peak, 9,735 were centred on a CAGE peak and 3,188 were centred on a Ensembl TSS definition. The oligonucleotide tested included the  $\\pm 100$ -bp window around this central position in the sense orientation with respect to the gene. A random subset of 12,411 promoters were also tested in the antisense orientation. We tested 102 positive and 102 negative controls from a previous study[19] as well as 175 dinucleotide shuffled negative controls in both orientations. These shuffled controls were derived from shuffling a random subset of 175 DNase peaks. This resulted in a library consisting of 164,307 elements, for which we ordered one 230-bp oligonucleotide per element.\n\nK562 large-scale library. To acquire a set of DNase peaks for testing, we used the 'optimal peak' calls derived from processing ENCODE experiment ID: ENCSR000EOY through the ENCODE DCC Irreproducible Discovery Rate (IDR) pipeline, available at https://github.com/ENCODE-DCC/atac-seq-pipeline (generously provided by A. Kundaje). Removing DNase peaks overlapping human promoters resulted in 87,618 potential enhancers tested in both orientations. The promoters tested were identical to those described in 'HepG2 large-scale library', except that it included all 19,104 promoters tested in both orientations. We tested 50 positive and 200 negative controls from a previous MPRA study $^{18}$  as well as the same 250 dinucleotide shuffled negative controls as tested in 'K562 pilot library'. Finally, 14,918 tiles not overlapping DNase peaks, and subsampled from the  $\\pm 1$  Mb region around the following 7 genetic loci: GATA1, MYC, HBE1, LMO2, RBM38, HBA2, and BCL11A, were chosen using our representative subset selection approach (described in the 'Representative subset selection' section below) and tested in both orientations. This resulted in a library consisting of 243,780 elements, for which we ordered one 230-bp oligonucleotide per element.\n\nWTC11 large-scale library. To acquire a set of DNase peaks for testing, we used the peak calls derived from applying the hotspot2 pipeline (https://github.com/Altius/hotspot2) at FDR = 0.05 to ENCODE experiment ID: ENCSR785ZUI (generously provided by R.Sandstrom) $^{61}$ . This resulted in two independent replicates, which were merged into a unified set using the procedures described in 'HepG2 pilot library'. Removing DNase peaks overlapping human promoters resulted in 83,201 potential enhancers. Together with the 19,104 promoters described in 'HepG2 large-scale library', these elements were subsampled to select 30,121 potential enhancers and 7,500 promoters using our representative subset selection approach described below, and tested in both orientations. We also tested 100 positive and 100 negative controls from a previous study $^{24}$  as well as 100 dinucleotide shuffled negative controls, which were derived from shuffling 100 random sequences from our set of 30,121 potential enhancers. This resulted in a library consisting of 75,542 cCREs, for which we ordered one 230-bp oligonucleotide per element.\n\nJoint library tested in HepG2, K562, and WTC11 cells. Given the measured potential enhancers from the forward orientations in each of the HepG2, K562 and WTC11 large-scale libraries, we binned each set of cCREs into ten equally sized bins spanning the range of measured  $\\log_2(\\mathrm{RNA} / \\mathrm{DNA})$  values in the selected cell type. We randomly sampled an approximately equal number from each bin, resulting in 19,000 HepG2, 18,958 K562 and 18,946 WTC11 potential enhancers. A similar procedure was followed with sense-oriented promoters, except that the ten bins were established on the basis of the mean  $\\log_2(\\mathrm{RNA} / \\mathrm{DNA})$  across all three cell types (that is, instead of performing the procedure independently in each cell type as before), and the top 1,000 promoters exhibiting the greatest variance across three cell lines were also selected for testing. This resulted in the selection of 2,396 out of 19,104 promoters. We also tested 181 positive and 169 negative HepG2 controls from a previous study $^{19}$ , 50 positive K562 controls from a previous study $^{18}$ ,\n\n# Article\n\nand 300 dinucleotide shuffled negative controls. The shuffled controls originated from selecting 100 shuffled controls from each of the three cell types. This resulted in a library consisting of 60,000 cCREs, for which we ordered one 230-bp oligonucleotide per element.\n\nRepresentative subset selection. Given the limited number of testable elements in the large-scale K562 and WTC11 libraries, we designed a subset selection procedure to more optimally sample a non-redundant subset of elements associated with diverse biochemical features. For K562 cells, we used ground sets of non-overlapping 200-bp windows uniformly covering each of 7 genetic loci; for WTC11 cells, we used ground sets of 83,201 potential enhancers and 19,104 promoters. To perform representative subset selection with these ground sets, we utilized an objective function called facility location. This submodular set function can be optimized using a greedy algorithm, and yields a subset of elements that covers the epigenetic diversity of the ground set<sup>62</sup>. The facility location function is given as:\n\n$$\nf (A) = \\sum_ {v \\in V} \\max  _ {a \\in A} \\varphi (v, a)\n$$\n\nwhere  $V$  is the ground set,  $A$  is a subset of  $V$  with  $k$  elements and  $\\varphi$  is a nonnegative similarity function. Optimization of the facility function was performed using the Python package apricot (https://github.com/jmschrei/apricot/)63. For this study, we set  $k = 2,231$  for each of the 7 loci in K562 cells,  $k = 30,121$  for WTC11 potential enhancers, and  $k = 7,500$  for promoters in WTC11. From the 7 loci, we then filtered out the tiles that overlapped DNase peaks as they had already been tested, and then subsampled to ~2,131 tiles per locus to retrieve 14,918 tiles among all loci.\n\nTo assess the pairwise similarity of each element, we utilized hundreds of ENCODE histone and transcription factor ChIP-seq experiments derived from K562 and WTC11-H1 embryonic stem cells (Supplementary Table 6). For each 200-bp tile in the ground set, we computed the mean signal for each ChIP-seq dataset, resulting in a vector of biochemical measurements for each 200-bp tile. We used the Pearson correlation coefficient as a similarity function given these ChIP-seq features.\n\n# Generation of MPRA libraries\n\nThe MPRA libraries were generated as previously described[16]. In brief, the Agilent oligonucleotide pool was amplified by 5-cycle PCR using forward primer (pLSmP-enh-f, Supplementary Table 13) and reverse primer (minP-enh-r, Supplementary Table 13) that adds a minimal promoter and spacer sequences downstream of the cCRE. The amplified fragments were purified with  $0.8\\mathrm{x}$  AMPure XP (Beckman coulter), and amplified for 15 additional cycles using the forward primer (pLSmP-enh-f) and reverse primer (pLSmP-bc-primer-r, Supplementary Table 13) to add 15 bp of random sequence that serves as a barcode. The amplified fragments were then inserted into Sbfl/Agel site of the pLS-Scel vector (Addgene, 137725) using NEBuilder HiFi DNA Assembly mix (NEB), followed by transformation into 10-beta competent cells (NEB, C3020) using the Gemini X2 machine (BTX). Colonies were allowed to grow up overnight on carbenicillin plates and midiprepped (Qiagen, 12945). For HepG2 and K562 pilot libraries, we collected approximately 1 million and 1.3 million colonies, so that on average 50 and 100 barcodes were associated with each cCRE, respectively. For HepG2, K562 and WTC11 large-scale libraries, we collected approximately 8 million, 12 million and 3 million colonies aiming to associate approximately 50, 50 and 40 barcodes per cCRE, respectively. For the joint library, we collected approximately 3.3 million colonies, aiming to associate approximately 55 barcodes per cCRE. To determine the sequences of the random barcodes and their association to each cCRE, the cCRE-mP-barcodes fragment was amplified from each plasmid library using primers that contain flowcell adapters (P7-pLSmP-ass-gfp and P5-pLSmP-ass-i17, Supplementary Table 13). The fragment was\n\nthen sequenced with a NextSeq mid-output 300 cycle kit using custom primers (Read 1, pLSmP-ass-seq-R1; Index read, pLSmP-ass-seq-ind1; Read 2, pLSmP-ass-seq-R2, Supplementary Table 13).\n\n# Cell culture, lentivirus packaging and titration\n\nHepG2 (ATCC, HB-8065) and K562 (ATCC, CCL-243) cell culture were performed as previously described[10]. WTC11 human iPS cells (RRID:CVCL_Y803) were cultured in mTeSR plus medium (Stemcell technologies, 100-0276) and passaged using ReLeSR (Stemcell technologies, 100-0484), according to the manufacturer's instructions. WTC11 cells were used for the MPRA experiments at passage 43-49. Cells were not authenticated or checked for mycoplasma contamination. Lentivirus packaging was performed using HEK293T (ATCC, CRL3216), as previously described with modifications[16]. In brief, 50,000 cells per  $\\mathrm{cm}^2$  HEK293T cells were seeded in T175 flasks and cultured for  $48\\mathrm{h}$ . The cells were co-transfected with  $7.5\\mu \\mathrm{g}$  per flask of plasmid libraries,  $2.5\\mu \\mathrm{g}$  per flask of pMD2.G (Addgene 12259), and  $5\\mu \\mathrm{g}$  per flask of psPAX2 (Addgene 12260) using EndoFectin Lenti transfection reagent (GeneCopoeia) according to the manufacturer's instructions. After  $8\\mathrm{h}$ , cell culture media was refreshed and ViralBoost reagent (Alstem) was added. The transfected cells were cultured for 2 days to complete lentivirus packaging. The lentivirus libraries in the culture media were separated from the HEK293T cells and concentrated using the Lenti-X concentrator (Takara) according to the manufacturer's protocol. To measure DNA titre for the lentiviral libraries in HepG2, K562, or WTC11, cells were seeded at  $1\\times 10^{5}$  cells per well in 24-well plates and incubated for  $24\\mathrm{h}$ . Serial volume (0, 2, 4, 8, 16 and  $32\\mu \\mathrm{l}$ ) of the lentivirus was added along with Polybrene at a final concentration of  $8\\mu \\mathrm{g}\\mathrm{ml}^{-1}$ . The infected cells were cultured for three days and then washed with PBS three times. Genomic DNA was extracted using the Wizard SV genomic DNA purification kit (Promega). Multiplicity of infection (MOI) was measured as relative amount of viral DNA (WPRE region, forward;  $5^{\\prime}$ -TACGCTGCTTTAATGCCTTTG- $3^{\\prime}$ , reverse;  $5^{\\prime}$ -GGGCCACAACTCCTCATAAAG- $3^{\\prime}$ ) over that of genomic DNA (intronic region of LIPC gene, forward;  $5^{\\prime}$ -TCCTCCGGAGTTATTCTTGGCA- $3^{\\prime}$ , reverse;  $5^{\\prime}$ -CCCCCATCTGATCTGTTCAC- $3^{\\prime}$ ) by quantitative PCR using SsoFast EvaGreen Supermix (Bio-Rad), according to the manufacturer's protocol.\n\n# Lentiviral infections and DNA and RNA barcode sequencing\n\nFor the HepG2 and K562 pilot libraries, 2.4 M HepG2 or 10 MK562 cells per replicate were seeded in 10 cm dishes or T75 flasks, respectively, and incubated for 24 h. The HepG2 and K562 cells were infected with the lentiviral libraries along with  $8\\mu \\mathrm{g}\\mathrm{ml}^{-1}$  Polybrene, with an estimated MOI of 50 or 10, respectively. The higher MOI in HepG2 is due to these cells being adherent compared to K562 that grow in suspension. For the large-scale HepG2 library, 15 M HepG2 cells per replicate were seeded in  $3\\times 15$  cm dishes (5 million per dish), incubated for 24 h, and infected with the library along with  $8\\mu \\mathrm{g}\\mathrm{ml}^{-1}$  Polybrene, with an estimated MOI of 50. For the large-scale K562 library, 85 million K562 cells per replicate were seeded in 3 T225 flasks (28.3 M per flask), incubated for 24 h, and infected with the library along with  $8\\mu \\mathrm{g}\\mathrm{ml}^{-1}$  Polybrene, with an estimated MOI of 10. For the large-scale WTC11 library, 38.4 million WTC11 cells per replicate were seeded in four 10 cm dishes (9.6 M per dish), incubated for 24 h, and infected with the library along with  $8\\mu \\mathrm{g}\\mathrm{ml}^{-1}$  Polybrene, with an estimated MOI of 10, due to higher MOIs being lethal for these cells. For the joint library, 5 million HepG2, 28 million K562 and 38.4 million WTC11 cells were infected with the estimated MOI of 50, 10 and 10, respectively. For each experiment, three independent infections were performed to obtain three biological replicates. After three days of culture, genomic DNA and total RNA were extracted from the infected cells using AllPrep DNA/RNA mini kit (Qiagen), and sequencing library preparations were performed as previously described[16]. The libraries were then sequenced with a NextSeq high-output 75 cycle kit using custom primers (Read 1, pLSmP-5bc-seq-R1; Index1 (unique molecular\n\nidentifier (UMI) read), pLSmP-UMI-seq; Index2, pLSmP-5bc-seq-R2; Read 2, pLSmP-bc-seq; Supplementary Table 13) $^{16}$ .\n\n# MPRA processing pipeline\n\nAssociating barcodes to designed elements. For each of the barcode association libraries, we generated FASTQ files with bcl2fastq v.2.20 (parameters \"--no-lane-splitting--create-fastq-for-index-reads --use-bases-mask Y*,I*,I*,Y*\"), splitting the sequencing data into paired-end index files delineating the barcodes (I1 and I2) and paired-end read files delineating the corresponding element linked to the barcode (R1 and R2). These files were used to associate barcodes to elements using the association utility of MPRAflow 1.0 $^{16}$  (run as: nextflow run association.nf --fastq-insert \"R1.fastq.gz\" --fastq-insertPE \"R2.fastq.gz\" --fastq-bc \"I1.fastq.gz\" --fastq-bcPE \"I2.fastq.gz\" --aligner \"bt2_strand\" --design \"designed_sequences.fa\"). Here, designedquences.fa was a FASTA file incorporating all of the element sequences that had been ordered from the corresponding Agilent library, and bt2_strand was used to map elements in a strand-aware fashion to accommodate the existence of elements tested in both orientations. The final output of this utility was a filtered_coordinates_to_barcodes.pickle file mapping barcodes to elements.\n\nReplicates, normalization and RNA/DNA activity scores. For each of the indexed DNA and RNA libraries, we demultiplexed the sequencing run and generated Fastq files with bcl2fastq v.2.20 (parameters \"--barcode-mismatches 2 --sample-sheet SampleSheet.csv --use-bases-mask Y*,Y*,I*,Y* --no-lane-splitting --minimum-trimmed-read-length 0 --mask-short-adapter-reads 0\"), where SampleSheet.csv catalogued the correspondence between the index sequence and DNA or RNA replicate sample of origin. In several instances, the \"--barcode-mismatches 2\" resulted in an index assignment clash, requiring us to instead use \"--barcode-mismatches 1\". These commands split the sequencing data into paired-end read files delineating the barcodes (R1 and R3) and a file indicating the UMI (R2) for each DNA or RNA replicate sample. We compiled a table of these files to indicate the 3 RNA and 3 DNA files for each of the three replicates in the file experiment.csv. Finally, we used the count utility of MPRAflow  $1.0^{16}$  (run as: nextflow run count.nf --e \"experiment.csv\" --design \"designed_sequences.fa\" --association \"filtered_coordinates_to_barcodes. pickle\") to compute activity scores for each element and replicate as  $\\log_2(\\text{RNA/DNA})$ . Elements with which were measured with fewer than 10 independent barcodes were removed to reduce the impact of measurement noise in downstream analysis. This filter led to the following number of retained elements: (1) HepG2 pilot library, 9,153/9,372 (97.7%); (2) K562 pilot library, 7,323/7,500 (97.6%); (3) HepG2 large-scale library, 139,886/164,307 (85.1%); (4) K562 large-scale library, 226,255/243,780 (92.8%); (5) WTC11 large-scale library, 56,093/75,542 (74.2%); (6) HepG2 joint library, 56,018/60,000 (93.4%); (7) K562 joint library, 56,008/60,000 (93.3%); and (8) WTC11 joint library, 55,983/60,000 (93.3%). To combine the data from all three replicates, the distribution of activity values was normalized to the median activity value within each replicate, and then the activity values were averaged across the three replicates.\n\n# Regression modelling\n\nBiochemical model features. We extracted all transcription factor ChIP-seq, histone ChIP-seq, DNase-seq, and ATAC-seq bigWig files available for HepG2, K562, and WTC11 cells for the hg38 human genome assembly under 'released' ENCODE status $^{56}$ . To account for the lack of WTC11 data available, we also collected all such datasets for H1-ESCs for inclusion in the predictive model. This resulted in 1,506 bigWig files for HepG2 cells; 1,206 files for K562 cells; and 277 files for WTC11/H1-ESCs (Supplementary Table 6). For each candidate element aside from controls, we computed the mean bigWig signal extracted from the corresponding region of the human genome using bigWigAverageOverBed $^{60}$ .\n\nAll data was right-skewed, and was therefore log-transformed (that is, after adding a pseudocount of 0.1) to approximate a normal distribution. Finally, for each cell type, multiple replicates corresponding to the same 'experiment target' (Supplementary Table 6) were averaged to compute the consensus signal for each target in each cell type. This led to a total of 655 HepG2 features, 447 KS62 features and 122 WTC11/H1-ESC features considered by the models.\n\nSei and EnformerMPRA model features. For the large-scale libraries, Sei $^{37}$  and Enformer $^{36}$  were used to predict element activity in both orientations (that is, including adaptors in a fixed orientation to simulate the MPRA experiment). The resulting 21,907 Sei and 5,313 Enformer predictions for each of the two orientations were averaged. For the joint library, Sei and Enformer were used to predict element activity in only the forward/sense orientation, and the resulting human predictions were carried forward as features. As Sei requires an input sequence length of 4,000 bp and Enformer requires one of 196,608 bp, all elements were extended with \"N\" padding in both directions while centring on the element sequence.\n\nData pre-processing and model training. For each of the three large-scale libraries, the  $\\log_2(\\mathrm{RNA} / \\mathrm{DNA})$  scores for each element were averaged among both orientations in which the element was tested, and then randomly assigned to one of ten cross-validation folds (Supplementary Table 8). All predictive features (that is, biochemical features from the matched cell type, or all Enformer features) were  $z$ -score-normalized to scale the features similarly. This enabled a direct comparison of coefficients among features derived from the resulting linear models. As described before[10,14,32], for each regression task we optimized the  $\\lambda$  regularization hyperparameter using tenfold cross-validation, and then used the optimal value for  $\\lambda$  to train 10 lasso regression models, each on 9 of the 10 folds of data, to evaluate the performance of each model on the held-out fold. To evaluate the most relevant features selected, we trained a lasso regression model on the full dataset and visualized the 30 coefficients with the greatest magnitude. A similar strategy was used for data from the joint library tested in all three cell types, ensuring that the same element measured in different cell types was always assigned to the same fold (Supplementary Table 12).\n\nTraining MPRALegNet. The LegNet architecture $^{35}$  was adapted to the training data in the following ways: (1) to account for longer sequences but smaller training set size compared to the original LegNet, we added max pooling layers after each local block; (2) the kernel size and number of blocks were selected to match the model's receptive field to the sequence length; (3) weight decay during training was increased to prevent model overfitting; (4) gradient clipping was used to avoid gradient explosion during the one-cycle learning rate policy (see Supplementary Methods for more details). For the training-time augmentation, each sequence was provided twice, both in the forward and reverse complement orientations, along with their corresponding measured element activity scores. At test time, the model predicted four scores: (1) the scores for the forward and reverse element orientations relative to the fixed flanking (adaptor) regions; and (2) as a further augmentation, the scores for the full reverse complement sequences (that is, obtaining the reverse complementary sequence of the element and adaptor regions together) from step (1). The final prediction represented an average of these four values. The reproducible code, including implementation and complete parameter settings, is available on GitHub (https://github.com/autosome-ru/human_legnet) and Zenodo (https://zenodo.org/records/10558183 and https://zenodo.org/records/13908857).\n\nInterpreting motifs identified by MPRAlegNet. As a step towards motif interpretation, ISM was performed for all possible single\n\n# Article\n\nnucleotide variants on each 200-bp sequence. Owing to the nature of our cross-validation strategy, for each sequence there were nine models for which the sequence was held out during training. ISM scores were generated for every sequence by averaging the predictions from these nine models. The average reference sequence prediction was then compared with that of the alternative sequence $^{32}$ . We then interrogated our ISM scores to identify the most pertinent motifs associated with changes in variant activity using TF-MoDISco-lite v.2.0.4 (https://github.com/jmschrei/tfmodisco-lite), a more efficient version of TF-MoDISco $^{38}$ . The TF-MoDISco-lite algorithm was used with default settings and similar seqlet patterns were matched against JASPAR 2022 CORE vertebrate non-redundant database $^{64}$  using Tomtom $^{65}$ .\n\nModelling dose-dependent and combinatorial motif effects learned by MPRALegNet. A non-redundant set of positional weight matrices (PwMs) from each cell type, as ranked by TF-MoDISco-lite (Extended Data Fig. 6), were extracted and scanned (using FIMO 5.5.4 $^{66}$ , parameters \"--text --thresh 0.001\") along each promoter and potential enhancer that was tested bidirectionally. The motif scans were summarized into a matrix of counts for each transcription factor and element tested, as well as log-likelihood (sum of the log(probabilities)), reflecting the likelihood of a given transcription factor binding the element while considering all TFBS instances in both orientations and their respective binding affinities. We then performed an analysis of homotypic (that is, dose-dependent effects for a single transcription factor) as well as heterotypic (that is, combinatorial effects among pairs of transcription factors) for the top 10 activating transcription factors of each cell type.\n\nFor homotypic analysis, we plotted the median element activity (that is, both predicted and observed) for elements possessing 0, 1, 2, 3, 4, or 5 motifs, filtering away elements with  $\\geq 1$  site to any of the other top 10 transcription factors to reduce the chances of a confounding effect. Groups with a sample size of  $< 10$  were also filtered out to minimize the impact of noise. The expected dose-dependent responses (for example, dashed lines in Fig. 3f) were computed using linear regression models examining the relationship between either the observed or MPRALegNet-predicted MPRA activity and the number of TFBSs, given log-transformed and untransformed space to model either multiplicative or additive effects, respectively. The expected trend for multiple sites was extrapolated on the basis of the slope and intercept terms of these linear models.\n\nFor heterotypic analysis, we evaluated every pair of the 10 activating motifs, isolating cases in which the element possessed 0 counts of both transcription factors, 1 count of one transcription factor or the other, or 1 count each of the first and second transcription factor. Again, all elements were filtered to those with  $\\geq 1$  site to any of the other top 10 transcription factors other than the transcription factor pair considered. To further account for confounding effects that could be attributable to all other transcription factors (that is, including those beyond the top 10), we computed the residuals from a linear model which considered the log-likelihood values for all other transcription factors besides the pair of transcription factors under consideration. We call these 'adjusted  $\\log_2(\\mathrm{RNA} / \\mathrm{DNA})'$  (for example,  $y$  axis in Fig. 3h) because they removed variability explained by the binding affinities and occurrences of other transcription factors. Finally, a regression model was fit independently to the predicted and observed activity scores. This model sought to predict activity as a function of the presence of TF1, TF2 or an interaction term (TF1  $\\times$  TF2). The coefficient for the interaction term represented the strength of the super-multiplicative effect (that is, if the coefficient was positive) or the sub-multiplicative effect (that is, if the coefficient was negative) $^{41,42}$ .\n\nPrediction using MPRAlegNet. To generate predictions on an arbitrary sequence, we recommend generating predictions using all 90 pretrained models (considering test-time sequence augmentations such as orientation and shifting for extra precision), and then\n\naveraging the predictions to achieve the final prediction. We recommend replacing the fixed 15-bp adaptors with the surrounding natural genomic sequence context whenever available, to reduce the chances that artefactual motifs occurring at the adaptor-sequence boundaries could bias the results.\n\nCalculation of element specificity scores. To compute element specificity scores (ESSs) using activity scores from the joint library,  $\\log_2(\\mathrm{RNA}/$  DNA) values from each cell line were first  $z$ -score-transformed. Then an ESS for each element was computed by subtracting the element's score in each cell type by the mean element score across cell types. A full table of ESSs is provided (Supplementary Table 10).",
    "metadata": {
      "md_filename": "MinerU_markdown_EnformerMPRA_20260106143038_2008425959623761920.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_EnformerMPRA_20260106143038_2008425959623761920.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "7e0e93e16b811ad2",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_esMPRA_20260106143031_2008425930402041856",
    "title": "Systems biology",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# 3.1 Quantitative quality control\n\nBased on our previous MPRA experiments and data processing experience, we have summarized key quantitative quality control metrics for each step of the workflow. The most important of these metrics fall into the following categories: the proportion of qualified oligos (used to determine whether a sufficient number of oligos are available at each step based on specific qualification criteria), the number of barcodes associated with each oligo (used to assess the complexity of library construction and sequencing quality), the proportion of barcodes detected a specific number of times in sequencing and the mean detection count of barcodes (used to evaluate whether the number of different barcodes is evenly distributed within the sample and whether the sequencing depth is sufficient.), and the correlation between plasmid abundance and RNA abundance (used to assess whether plasmid DNA has been adequately removed). Additional detailed parameters are also included and are described in the QC reports. We calculated these metrics across a large collection of MPRA datasets (Gosai et al. 2024) and used the resulting\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/d0724f64-d741-4d26-bdd2-fbd0ab1c000e/b8f3909fbb0f12a1c4479b01a9a9587268eadd448f9761d587e9e3162b7fe1d4.jpg)\n\nFigure 1. Workflow of esMPRA. esMPRA takes the raw fastq files generated in each experimental step as input and produces corresponding analytical results. While conducting the analysis, esMPRA performs quality control on key parameters in each step, providing reference ranges based on a large amount of existing MPRA data analysis and offering specific experimental suggestions for high-risk items. After completing the quality control and analysis for each step, esMPRA provides the final analytical results, assesses the reproducibility of the experiment, and outputs interface files in a standardized format for further in-depth analysis.\n\nvalues to define reference ranges. When a single step contains risk warning, users are advised to inspect the experimental procedures to address potential issues.\n\nIn addition, we simulated the effect of different sequencing depths on sequence coverage through downsampling and visualized the results to help determine whether the current sequencing depth is sufficient. And we plotted the distribution of barcode counts to assess library complexity and sequencing quality. We also established reference plots based on the collection of MPRA datasets. Furthermore, each report file includes additional analysis parameters and figures at the end to provide readers with a more comprehensive view of the experimental quality.\n\n# 3.2 Mapping oligos with barcodes\n\nMPRA quantifies regulatory activity by appending random barcodes to designed oligo sequences, thereby labeling a vast array of oligos for activity measurement. In addition, it aggregates the activity of the same oligo across multiple barcodes to mitigate experimental variability. In esMPRA, the \"step1_oligo_barcode_map\" module determines the correspondence between oligos and their random barcodes by processing the raw fastq files generated in this step. Two operational modes are available for this process; the recommended approach is to employ theuseflash option after installing the FLASH2 tool (Magoc and Salzberg 2011), which enhances both processing speed and accuracy.\n\nIn this step, a critical metric is the number of unique random barcode types associated with each oligo sequence. It is essential to control the library complexity within an optimal range to ensure the experiment's success. An excessive number of barcodes indicates an overly complex library, which may result in many barcodes going undetected in subsequent experiments. Conversely, a barcode count that is too low can introduce significant noise during the quantification of CRE activity. For additional key metrics and further details, please refer to the README documentation.\n\n# 3.3 Determine plasmid abundance\n\nAfter mapping the correspondence between obligos and random barcodes, typically a reporter gene is inserted between them to enable the assessment of transcriptional regulatory element activity. For accurate quantification of these elements, the relative abundance of plasmids serves as a reference. Consequently, following the insertion of the reporter gene, sequencing is performed to quantify the relative abundance of each random barcode across all plasmids. The \"step2_get_plasmid_counts\" module accomplishes this by processing the raw fastq files generated at this stage.\n\nA key metric in this step is the sequencing coverage of both barcodes and oligos. It is crucial that as many barcodes and oligos as possible are adequately covered by sequencing. Insufficient coverage may suggest low efficiency in plasmid construction or an inadequate sequencing depth.\n\n# 3.4 Getting RNA (cDNA) abundance\n\nAfter the insertion of the reporter gene and the quantification of barcode abundance in the plasmids, the constructed plasmids are transfected into target cells. After a defined period of culture, the RNA transcripts are measured. Typically, RNA quantification is performed by reverse transcription followed by cDNA sequencing, which indirectly determines RNA abundance. The \"step3_get_RNA_counts\" module\n\nprocesses the raw fastq files generated during this step to quantify the relative abundance of random barcodes within the cDNA, thereby enabling the accurate quantification of transcriptional regulatory element activity.\n\nIn this step, the sequencing coverage of barcodes and oligos remains a critical parameter. However, assuming that the previous quality control standards have been met, inadequate coverage in this phase may be due to various factors. The most likely cause of risk values here is an insufficient RNA input for reverse transcription, which should be checked with the provided reports.\n\n# 3.5 Quantifying the activity\n\nMapping oligos with barcodes establishes the correspondence between each oligo and its random barcode. By integrating the relative abundance of barcodes measured in plasmids with that in RNA (cDNA), the regulatory activity associated with each barcode can be quantified. With the established oligo-barcode correspondence, the transcriptional regulatory activity of each oligo sequence can be evaluated, as defined by Equation (1).\n\n$$\n\\begin{array}{l} \\text {C R E a c t i v i t y} = \\\\ \\frac {\\sum (\\text {N o r m a l i z e d R N A c o u n t s f o r a l l b a r c o d e s})}{\\sum (\\text {N o r m a l i z e d p l a s m i d c o u n t s f o r a l l b a r c o d e s})} \\end{array} \\tag {1}\n$$\n\nThe \"step4_get_result\" module quantifies the final CRE activity based on the results of these steps.\n\nThis step focuses on a single key parameter: the correlation coefficient between plasmid and cDNA abundances, which can provide significant quality control insights. An excessively high correlation suggests that DNA was not effectively removed prior to reverse transcription, potentially influencing the final quantification results. Special monitoring of this parameter is recommended.\n\n# 3.6 Extended functionality\n\nAfter completing the standard data processing and quality control steps described above, the CRE activity of the oligos obtained in a single experiment can be determined. In typical applications, several parallel replicates are conducted to ensure experimental reproducibility. To support this common requirement, esMPRA provides the \"step5Compare_diff_rep\" function, which evaluates the pairwise correlations among multiple replicate experiments.\n\nIn addition to the aforementioned data analysis and quality control, users may sometimes require further data processing and analysis using other tools. To facilitate such workflows, esMPRA generates standardized output files for downstream analysis. Specifically, the \"generate_data_for_MPRAnalyse\" function produces interface files formatted according to the requirements of MPRAnalyze (Ashuach et al. 2019).\n\n# 4 Conclusion\n\nWe propose esMPRA with the aim of simplifying MPRA experiment quality control and data analysis for a broad range of users, thereby facilitating the wider adoption of this important technology. esMPRA is designed for one-line installation and operation across diverse computing platforms, drastically reducing the barrier to entryallowing even users with minimal command line experience to get started quickly. The tool\n\ngenerates comprehensive quality control reports and provides experimental recommendations that help researchers rapidly identify potential risks, significantly enhancing experimental efficiency. Furthermore, esMPRA can directly produce analytical results and offers a standardized interface. The flexible and versatile framework of esMPRA also makes it applicable to a wide range of studies involving the quantification realized by random barcodes.\n\n# Acknowledgements\n\nWe would like to thank Xiaocheng Zeng, Qixiu Du, Han Yu, May Nee Poon, and Liyang Liu for their diligent testing and invaluable feedback throughout the development of esMPRA.\n\nConflict of interest: None declared.\n\n# Funding\n\nThis work was supported by the National Key R&D Program of China [2023YFF1204500], the Beijing Municipal Natural Science Foundation [Z230015], and the National Natural Science Foundation of China [62225307].",
    "metadata": {
      "md_filename": "MinerU_markdown_esMPRA_20260106143031_2008425930402041856.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_esMPRA_20260106143031_2008425930402041856.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "fe8e035c5ceed573",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_Evo_20260106143021_2008425888576442368",
    "title": "Sequence Modeling with Unconstrained Generation Order",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "We consider the task of generating a sequence  $Y$  consisting of tokens  $y_{t}$  given some input  $X$ . In order to remove the predefined generation order constraint, we need to reformulate the probability of target sequence in terms of token insertions. Unlike traditional models, there are multiple valid insertions at each step. This formulation is closely related to the existing framework of generating unordered sets, which we briefly describe in Section 2.1. In Section 2.2, we introduce our approach.\n\n# 2.1 Generating unordered sets\n\nIn the context of unordered set generation, Vinyals et al. [9] proposed a method to learn sequence order from data jointly with the model. The resulting model samples a permutation  $\\pi(t)$  of the target sequence and then scores the permuted sequence with a neural probabilistic model:\n\n$$\nP \\left(Y _ {\\pi} \\mid x, \\theta\\right) = \\prod_ {t} p \\left(y _ {\\pi (t)} \\mid X, y _ {\\pi (0)},.., y _ {\\pi (t - 1)}, \\theta\\right). \\tag {1}\n$$\n\nThe training is performed by maximizing the data log-likelihood over both model parameters  $\\theta$  and target permutation  $\\pi(t)$ :\n\n$$\n\\theta^ {*} = \\arg \\max  _ {\\theta} \\sum_ {X, Y} \\max  _ {\\pi} \\log P (Y _ {\\pi} | x, \\theta). \\tag {2}\n$$\n\nExact maximization over  $\\pi(t)$  requires  $O(|Y|!)$  operations, therefore it is infeasible in practice. Instead, the authors propose using greedy or beam search. The resulting procedure resembles the Expectation Maximization algorithm:\n\n1. E step: find optimal  $\\pi (t)$  for  $Y$  under current  $\\theta$  with inexact search,\n\n2. M step: update parameters  $\\theta$  with gradient descent under  $\\pi(t)$  found on the E step.\n\nEM algorithms are known to easily get stuck in local optima. To mitigate this issue, the authors sample permutations proportionally to  $p(y_{\\pi(t)} | x, y_{\\pi(0)}, \\ldots, y_{\\pi(t-1)}, \\theta)$  instead of maximizing over  $\\pi$ .\n\n# 2.2 Our approach\n\nThe task now is to build a probabilistic model over sequences  $\\tau = (\\tau_0, \\tau_1, \\dots, \\tau_T)$  of insertion operations. This can be viewed as an extension of the approach described in the previous section, which operates on ordered sequences instead of unordered sets. At step  $t$ , the model generates either a pair  $\\tau_t = (pos_t, token_t)$  consisting of a position  $pos_t$  in the produced so far sub-sequence  $(pos_t \\in [0, t])$  and a token  $token_t$  to be inserted at this position, or a special EOS element indicating that the generation process is terminated. It estimates the conditional probability of a new insertion  $\\tau_t$  given  $X$  and a partial output  $\\tilde{Y}(\\tau_{0:t-1})$  constructed from the previous inserts:\n\n$$\np (\\tau | X, \\theta) = \\prod_ {t} p \\left(\\tau_ {t} \\mid X, \\tilde {Y} \\left(\\tau_ {0: t - 1}\\right), \\theta\\right). \\tag {3}\n$$\n\nTraining objective We train the model by maximizing the log-likelihood of the reference sequence  $Y$  given the source  $X$ , summed over the data set  $D$ :\n\n$$\n\\begin{array}{l} L = \\sum_ {\\{X, Y \\} \\in D} \\log p (Y | X, \\theta) = \\sum_ {\\{X, Y \\} \\in D} \\log \\sum_ {\\tau \\in T ^ {*} (Y)} p (\\tau | X, \\theta) = \\\\ = \\sum_ {\\{X, Y \\} \\in D} \\log \\sum_ {\\tau \\in T ^ {*} (Y)} \\prod_ {t} p (\\tau_ {t} | X, \\tilde {Y} (\\tau_ {0: t - 1}), \\theta), \\\\ \\end{array}\n$$\n\nwhere  $T^{*}(Y)$  denotes the set of all trajectories leading to  $Y$  (see Figure 2).\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/b60b4881-13d0-477d-8812-64f76af95474/3098785518c14d1708df8566fcc5b5dc5cf0709ed38af9b911621e8d6fdd1fe7.jpg)\n\nFigure 2: Graph of trajectories for  $T^{*}(Y = \\text{\"cat sat\")}$\n\nIntuitively, we maximize the total probability \"flowing\" through the acyclic graph defined by  $T^{*}(Y)$ . This graph has approximately  $O(|Y|!)$  paths from an empty sequence to the target sequence  $Y$ . Therefore, directly maximizing (4) is impractical. Our solution, inspired by [9], is to assume that for any input  $X$  there is a trajectory  $\\tau^{*}$  that is the most convenient for the model. We want the model to concentrate the probability mass on this single trajectory. This can be formulated as a lower bound of the objective (4):\n\n$$\n\\begin{array}{l} L = \\sum_ {\\{X, Y \\} \\in D} \\log p (Y | X, \\theta) = \\sum_ {\\{X, Y \\} \\in D} \\log \\sum_ {\\tau \\in T ^ {*} (Y)} \\prod_ {t} p (\\tau_ {t} | X, \\tilde {Y} (\\tau_ {0: t - 1}), \\theta) \\geq \\\\ \\geq \\sum_ {\\{X, Y \\} \\in D} \\log \\max  _ {\\tau} \\prod_ {t} p \\left(\\tau_ {t} | X, \\tilde {Y} \\left(\\tau_ {0: t - 1}\\right), \\theta\\right) = \\sum_ {\\{X, Y \\} \\in D} \\max  _ {\\tau} \\sum_ {t} \\log p \\left(\\tau_ {t} | X, \\tilde {Y} \\left(\\tau_ {0: t - 1}\\right), \\theta\\right). \\tag {5} \\\\ \\end{array}\n$$\n\nThe lower bound is tight iff the entire probability mass in  $T^{*}$  is concentrated along a single trajectory. This leads to a convenient property: maximizing (5) forces the model to choose a certain \"optimal\" sequence of insertions  $\\tau^{*} = \\arg \\max_{\\tau}\\prod_{t}p(\\tau_{t}|X,\\tilde{Y} (\\tau_{0:t - 1}),\\theta)$  and concentrate most of the probability mass there.\n\nThe bound (5) depends only on the most probable trajectory  $\\tau^{*}$ , thus is difficult to optimize directly. This may result in convergence to a local maximum. Similar to [9], we replace max with an expectation w.r.t. trajectories sampled from  $T^{*}$ . We sample from the probability distribution over the trajectories obtained from the model. The new lower bound is:\n\n$$\n\\sum_ {\\{X, Y \\} \\in D} E _ {\\tau \\sim p (\\tau | X, \\tau \\in T ^ {*} (Y), \\theta)} \\sum_ {t} \\log p \\left(\\tau_ {t} | X, \\tilde {Y} \\left(\\tau_ {0: t - 1}\\right), \\theta\\right). \\tag {6}\n$$\n\nThe sampled lower bound in (6) is less or equal to (5). However, if the entire probability mass is concentrated on a single trajectory, both lower bounds are tight. Thus, when maximizing (6), we also expect most of the probability mass to be concentrated on one or a few \"best\" trajectories.\n\nTraining procedure We train our model using stochastic gradient ascent of (6). For each pair  $\\{X,Y\\}$  from the current mini-batch, we sample the trajectory  $\\tau$  from the model:  $\\tau \\sim p(\\tau |X,\\tau \\in T^{*}(Y),\\theta)$ . We constrain sampling only to correct trajectories by allowing only the correct insertion operations (i.e. the ones that lead to producing  $Y$ ). At each step along the sampled trajectory  $\\tau$ , we maximize  $\\log p(ref(Y,\\tau_{0:t - 1})|X,\\tilde{Y} (\\tau_{0:t - 1}),\\theta)$ , where  $ref(Y,\\tau_{0:t - 1})$  defines a set of all insertions  $\\tau_{t}$  immediately after  $\\tau_{0:t - 1}$ , such that the trajectory  $\\tau_{0:t - 1}$  extended with  $\\tau_{t}$  is correct:  $\\tau_{0:t - 1}\\oplus \\tau_t\\in T^* (Y)$ . From a formal standpoint, this is a probability of picking any insertion that is on the path to  $Y$ . The simplified training procedure is given in Algorithm 1.\n\nAlgorithm 1: Training procedure (simplified)\n\nInputs: batch  $\\{X,Y\\}$  , parameters  $\\theta$  , learning rate  $\\alpha$ $\\vec{g} := \\vec{0}$  //  $\\vec{g}$  is the gradient accumulator for  $X_{i},Y_{i}\\in \\{X,Y\\}$  do  $\\tau \\sim p(\\tau |X_i,\\tau \\in T^* (Y_i),\\theta)$  for  $t\\in 0,1,\\ldots ,|\\tau | - 1$  do ref  $\\coloneqq$  ref(Y,  $\\tau_{0:t - 1}$  // correct inserts  $L_{i,t} = \\log p(ref|X_i,\\tilde{Y}_i(\\tau_{0:t - 1}^*),\\theta)$ $\\vec{g} := \\vec{g} +\\frac{\\partial L_{i,t}}{\\partial\\theta}$  end   \nend   \nreturn  $\\theta +\\alpha \\cdot \\vec{g}$\n\nThe training procedure is split into two steps: (i) pretraining with uniform samples from the set of feasible trajectories  $T^{*}(Y)$ , and (ii) training on samples from our model's probability distribution over  $T^{*}(Y)$  till convergence. We discuss the importance of the pretraining step in Section 5.2.\n\nInference To find the most likely output sequence according to our model, we have to compute the probability distribution over target sequences as follows:\n\n$$\np (Y | X, \\theta) = \\sum_ {\\tau \\in T ^ {*} (Y)} p (\\tau | x, \\theta). \\tag {7}\n$$\n\nComputing such probability exactly requires summation over up to  $O(|Y|!)$  trajectories, which is infeasible in practice. However, due to the nature of our optimization algorithm (explicitly maximizing the lower bound  $E_{\\tau \\sim p(\\tau |X,\\tau \\in T^{*}(Y),\\theta)}p(\\tau |x,\\theta)\\leq \\max_{\\tau \\in T^{*}(Y)}p(\\tau |x,\\theta)\\leq P(Y|X))$ , we expect most of the probability mass to be concentrated on one or a few \"best\" trajectories:\n\n$$\nP (Y | X) \\approx \\max  _ {\\tau \\in T ^ {*} (Y)} p (\\tau | x, \\theta). \\tag {8}\n$$\n\nHence, we perform approximate inference by finding the most likely trajectory of insertions, disregarding the fact that several trajectories may lead to the same  $Y$ . The resulting inference problem is defined as:\n\n$$\nY ^ {*} = \\underset {Y (\\tau)} {\\arg \\max } \\log p (\\tau | X, \\theta). \\tag {9}\n$$\n\nThis problem is combinatoric in nature, but it can be solved approximately using beam search. In the case of our model, beam search compares partial output sequences and extends them by selecting the  $k$  best token insertions. Our model also inherits a common problem of the left-to-right machine translation: it tends to stop too early and produce output sequences that are shorter than the reference. To alleviate this effect, we divide hypotheses' log-probabilities by their length. This has already been used in previous works [10, 11, 12].\n\n# 3 Model architecture\n\nINTRUS follows the encoder-decoder framework. Specifically, the model is based on the Transformer [10] architecture (Figure 3) due to its state-of-the-art performance on a wide range of tasks [13, 14, 15]. There are two key differences of INTRUS from the left-to-right sequence models. Firstly, our model's decoder does not require the attention mask preventing attention to subsequent positions. Decoder self-attention is re-applied at each decoding step because the positional encodings of most tokens change when inserting a new one into an incomplete subsequence of  $Y$ .<sup>3</sup> Secondly, the decoder predicts the joint probability of a token and a position corresponding to a single insertion (rather than the probability of a token, as usually done in the standard setting). Consequently, the\n\npredicted probabilities should add up to 1 over all positions and tokens at each step. We achieve this by decomposing the insertion probability into the probabilities of a token and a position:\n\n$$\np (\\tau_ {t}) = p (t o k e n | p o s) \\cdot p (p o s),\n$$\n\n$$\np (p o s) = \\operatorname {s o f t m a x} \\left(H \\times w _ {l o c}\\right), \\tag {10}\n$$\n\n$$\np (t o k e n | p o s) = \\operatorname {s o f t m a x} \\left(h _ {p o s} \\times W _ {t o k}\\right).\n$$\n\nHere  $h_{pos} \\in \\mathbb{R}^d$  denotes a single decoder hidden state (of size  $d$ ) corresponding to an insertion at position  $pos$ ;  $H \\in \\mathbb{R}^{t \\times d}$  represents a matrix of all such states.  $W_{tok} \\in \\mathbb{R}^{d \\times v}$  is a learned weight matrix that predicts token probabilities and  $w_{loc} \\in \\mathbb{R}^d$  is a learned vector of weights used to predict positions. In other words, the hidden state of each token in the current sub-sequence defines (i) the probability that the next token will be generated at the position immediately preceding current and (ii) the probability for each particular token to be generated next.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/b60b4881-13d0-477d-8812-64f76af95474/7ac7abcea5190550b05b542dbf999cdc1b390452185f01306b1f877149eb4bec.jpg)\n\nFigure 3: Model architecture:  $p(\\tau_t|X, \\tilde{Y}(\\tau_{0:t-1}), \\theta)$  for a single token insertion. Output of the column  $i$  for the token  $j$  defines the probability of inserting the token  $j$  into  $\\tilde{Y}(\\tau_{0:t-1})$  before the token at the  $i$ -th position.\n\nThe encoder component of the model can have any task-specific network architecture. For Machine Translation task, it can be an arbitrary sequence encoder: any combination of RNN [3, 2], CNN [12, 16] or self-attention [10, 17]. For image-to-sequence problems (e.g. Image-To-LaTeX [18]) any 2d convolutional encoder architecture from the domain of computer vision can be used [19, 20, 21].\n\n# 3.1 Relation to prior work\n\nThe closest to our is the work by Gu et al. [22] $^{4}$ , who propose a decoding algorithm which supports flexible sequence generation in arbitrary orders through insertion operations.\n\nIn terms of modeling, they describe a similar transformer-based model but use a relative-position-based representation to capture generation orders. This effectively addresses the problem that absolute positional encodings are unknown before generating the whole sequence. While in our model positional encodings of most of the tokens change after each insertion operation and, therefore, decoder self-attention is re-applied at each generation step, the model by Gu et al. [22] does not need this and has better theoretical time complexity of  $O(len(Y)^2)$  in contrast to our  $O(len(Y)^3)$ . However, in practice our decoding is on average only 50% times slower than the baseline; for the details, see Section 5.2.\n\nIn terms of training objective, they use lower bound (5) with beam search over  $T^{*}(Y)$ , which is different from our lower bound (6). However, we found our lower bound to be beneficial in terms of quality and less prone to getting stuck in local optima. We will discuss this in detail in Section 5.1.\n\n# 4 Experimental setup\n\nWe consider three sequence generation tasks: Machine Translation, Image-To-Latex and Image Captioning. For each, we now define input  $X$  and output  $Y$ , the datasets and the task-specific encoder\n\nwe use. Decoders for all tasks are Transformers in base configuration [10] (either original or INTRUS) with identical hyperparameters.\n\nMachine Translation For MT, input and output are sentences in different languages. The encoder is the Transformer-base encoder [10].\n\nWu et al. [7] suggest that left-to-right NMT models fit better for right-branching languages (e.g., English) and right-to-left NMT models fit better for left-branching languages (e.g., Japanese). This defines the choice of language pairs for our experiments. Our experiments include: En-Ru and Ru-En WMT14; En-Ja ASPEC [23]; En-Ar, En-De and De-En IWSLT14 Machine Translation data sets. We evaluate our models on WMT2017 test set for En-Ru and Ru-En, ASPEC test set for En-Ja, concatenated IWSLT tst2010, tst2011 and tst2012 for En-De and De-En, and concatenated IWSLT tst2014 and tst2013 for En-Ar.\n\nSentences of all translation directions except the Japanese part of En-Ja data set are preprocessed with the Moses tokenizer [24] and segmented into subword units using BPE [25] with 32,000 merge operations. Before BPE segmentation Japanese sentences were firstly segmented into words<sup>5</sup>.\n\nImage-To-Latex In this task,  $X$  is a rendered image of LaTeX markup,  $Y$  is the markup itself. We use the ImageToLatex-140K [18, 26] data set. We used the encoder CNN architecture, preprocessing pipeline and evaluation scripts by Singh [26].\n\nImage captioning Here  $X$  is an image,  $Y$  is its description in natural language. We use MSCOCO [27], the standard Image Captioning dataset. Encoder is VGG16 [19] pretrained<sup>7</sup> on the ImageNet task without the last layer.\n\nEvaluation We use  $\\mathrm{BLEU}^8$  [29] for evaluation of Machine Translation and Image-to-Latex models. For En-Ja, we measure character-level BLEU to avoid influence on word segmentation software. The scores on MSCOCO dataset are obtained via the official evaluation script<sup>9</sup>.\n\nTraining details The models are trained until convergence with base learning rate 1.4e-3, 16,000 warm-up steps and batch size of 4,000 tokens. We vary the learning rate over the course of training according to [10] and follow their optimization technique. We use beam search with the beam between 4 and 64 selected using the validation data for both baseline and INTRUS, although our model benefits more when using even bigger beam sizes. The pretraining phase of INTRUS is  $10^{5}$  batches.\n\n# 5 Results\n\nTable 1: The results of our experiments. En-Ru, Ru-En, En-Ja, En-Ar, En-De and De-En are machine translation experiments. * indicates statistical significance with  $p$ -value of 0.05, computed via bootstrapping [30].\n\n<table><tr><td rowspan=\"2\">Model</td><td>En-Ru</td><td>Ru-En</td><td>En-Ja</td><td>En-Ar</td><td>En-De</td><td>De-En</td><td>Im2Latex</td><td colspan=\"2\">MSCOCO</td></tr><tr><td></td><td></td><td></td><td>BLEU</td><td></td><td></td><td></td><td>BLEU</td><td>CIDEr</td></tr><tr><td>Left-to-right</td><td>31.6</td><td>35.3</td><td>47.9</td><td>12.0</td><td>28.04</td><td>33.17</td><td>89.5</td><td>18.0</td><td>56.1</td></tr><tr><td>Right-to-left</td><td>-</td><td>-</td><td>48.6</td><td>11.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>INTRUS</td><td>33.2*</td><td>36.4*</td><td>50.3*</td><td>12.2</td><td>28.36*</td><td>33.08</td><td>90.3*</td><td>25.6*</td><td>81.0*</td></tr></table>\n\nAmong all tasks, the largest improvements are for Image Captioning: 7.6 BLEU and 25.1 CIDER.\n\nFor Machine Translation, INTRUS substantially outperforms the baselines for most considered language pairs and matches the baseline for the rest. As expected, the right-to-left generation order is better than the left-to-right for translation into Japanese. However, our model significantly outperforms both baselines. For the tasks where left-to-right decoding order provides a strong inductive bias (e.g. in De-En translation task, where source and target sentences can usually be aligned without any permutations), generation in arbitrary order does not give significant improvements.\n\nImage-To-Latex improves by 0.8 BLEU, which is reasonable difference considering the high performance of the baseline.\n\n# 5.1 Ablation analysis\n\nIn this section, we show the superior performance of the proposed lower bound of the data log-likelihood (6) over the natural choice of (5). We also emphasize the importance of the pretraining phase for INTRUS. Specifically, we compare performance of the following models:\n\n- Default  using the training procedure described in Section 2.2;\n\n- Argmax  trained with the lower bound (5) (maximum is approximated with using beam search with the beam of 4; this technique matches the one used in Gu et al. [22]);\n\n- Left-to-right pretraining  pretrained with the fixed left-to-right decoding order (in contrast to the uniform samples in the default setting);\n\n- No pretraining  with no pretraining phase;\n\n- Only pretraining  training is performed with a model-independent order, either uniform or left-to-right.\n\nTable 2: Training strategies of INTRUS. MT task, scores on the WMT En-Ru 2012-2013 test sets.\n\n<table><tr><td rowspan=\"2\">Training strategy</td><td rowspan=\"2\">INTRUS</td><td rowspan=\"2\">Argmax</td><td rowspan=\"2\">Pretraining left-to-right</td><td rowspan=\"2\">No pre-training</td><td colspan=\"2\">Only pretraining</td><td rowspan=\"2\">Baseline left-to-right</td></tr><tr><td>uniform</td><td>left-to-right</td></tr><tr><td>BLEU</td><td>27.5</td><td>26.6</td><td>26.3</td><td>27.1</td><td>24.6</td><td>25.5</td><td>25.8</td></tr></table>\n\nTable 2 confirms the importance of the chosen pretraining strategy for the performance of the model. In preliminary experiments, we also observed that introducing any of the two pretraining strategies increases the overall robustness of our training procedure and helps to avoid convergence to poor local optima. We attribute this to the fact that a pretrained model provides the main algorithm with a good initial exploration of the trajectory space  $T^{*}$ , while the Argmax training strategy tends to quickly converge to the current best trajectory which may not be globally optimal. This leads to poor performance and unstable results. This is the only strategy that required several consecutive runs to obtain reasonable quality, despite the fact that it starts from a good pretrained model.\n\n# 5.2 Computational complexity\n\nDespite its superior performance, INTRUS is more computationally expensive compared to the baseline. The main computational bottleneck in the model training is the generation of insertions required to evaluate the training objective (6). This generation procedure is inherently sequential. Thus, it is challenging to effectively parallelize it on GPU accelerators. In our experiments, training time of INTRUS is 3-4 times longer than that of the baseline. The theoretical computational complexity of the model's inference is  $O(|Y|^3k)$  compared to  $O(|Y|^2k)$  of conventional left-to-right models. However, in practice this is likely not to cause drastic decrease of the decoding speed. Figure 4 shows the decoding speed of both INTRUS and the baseline measured for machine translation task. On average, INTRUS is only  $50\\%$  slower because for sentences of a reasonable length it performs comparably to the baseline.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/b60b4881-13d0-477d-8812-64f76af95474/550cb76c1b74144447c7e9985f7260a72e053fed148d35e5365eb254959625c0.jpg)\n\nFigure 4: Inference time of INTRUS and the baseline models vs sentence length.\n\n# 6 Analyzing learned generation orders\n\nIn this section, we analyze generation orders learned by INTRUS on the Ru-En translation task.\n\nVisual inspection We noticed that the model often follows a general decoding direction that varies from sentence to sentence: left-to-right, right-to-left, middle-out, etc. (Figure 5 shows several examples<sup>10</sup>). When following the chosen direction, the model deviates from it for translation of certain phrases. For instance, the model tends to decode pairs of quotes and brackets together. Also we noticed that tokens which are generated first are often uninformative (e.g., punctuation, determiners, etc.). This suggests that the model has preference towards generating \"easy\" words first.\n\n<table><tr><td>but the researchers gave them pieces of wire.</td><td>actually, it was not so funny.</td><td>the study was conducted among 900 children.</td></tr><tr><td>but the researchers gave them pieces of wire.</td><td>actually, it was not so funny.</td><td>the study was conducted among 900 children.</td></tr><tr><td>but the researchers gave them pieces of wire.</td><td>actually, it was not so funny.</td><td>the study was conducted among 900 children.</td></tr><tr><td>but the researchers gave them pieces of wire.</td><td>actually, it was not so funny.</td><td>the study was conducted among 900 children.</td></tr></table>\n\nFigure 5: Decoding examples: left-to-right (left), right-to-left (center) and middle-out (right). Each line represents one decoding step.\n\nPart of speech generation order We want to find out if the model has any preference towards generating different parts of speech in the beginning or at the end of the decoding process. For each part of speech, $^{11}$  we compute the relative index on the generation trajectory (for the baseline, it corresponds to its relative position in a sentence). Figure 6 shows that INTRUS tends to generate punctuation tokens and conjunctions early in decoding. Other parts of speech like nouns, adjectives, prepositions and adverbs are the next easiest to predict. Most often they are produced in the middle of the generation process, when some context is already established. Finally, the most difficult for the model is to insert verbs and particles.\n\nThese observations are consistent with the easy-first generation hypothesis: the early decoding steps mostly produce words which are the easiest to predict based on the input data. This is especially interesting in the context of previous work. Ford et al. [8] study the influence of token generation order on a language model quality. They developed a family of two-pass language models that depend on a partitioning of the vocabulary into a set of first-pass and second-pass tokens to generate sentences. The authors find that the most effective strategy is to generate function words in the first pass and content words in the second. While Ford et al. [8] consider three manually defined strategies, our model learned to give preference to such behavior despite not having any inductive bias to do so.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/b60b4881-13d0-477d-8812-64f76af95474/a01e65dcd0a8c76160e2d51ac7742024731c185ee458e23e1c528eb6d506401f.jpg)\n\nFigure 6: The distributions of the relative generation order of different parts of speech.\n\n# 7 Related work\n\nIn Machine Translation, decoding in the right-to-left order improves performance for English-to-Japanese [32, 7]. The difference in translation quality is attributed to two main factors: Error Propagation [33] and the concept of language branching [7, 34]. In some languages (e.g. English), sentences normally start with subject/verb on the left and add more information in the rightward direction. Other languages (e.g. Japanese) have the opposite pattern.\n\nSeveral works suggest to first generate the most \"important\" token, and then the rest of the sequence using forward and backward decoders. The two decoders start generation process from this first \"important\" token, which is predicted using classifiers. This approach was shown beneficial for video captioning [6] and conversational systems [35]. Other approaches to non-standard decoding include multi-pass generation models [36, 8, 37, 38] and non-autoregressive decoding [39, 38].\n\nSeveral recent works proposed sequence models with arbitrary generation order. Gu et al. [22] propose a similar approach using another lower bound of the log-likelihood which, as we showed in Section 5.1, underperforms ours. They, however, achieve  $O(|Y|^2)$  time complexity by utilizing a different probability parameterization along with relative position encoding. Welleck et al. [40] investigates the possibility of decoding output sequences by descending a binary insertion tree. Stern et al. [41] focuses on parallel decoding using one of several pre-specified generation orders.\n\n# 8 Conclusion\n\nIn this work, we introduce INTRUS, a model which is able to generate sequences in any arbitrary order via iterative insertion operations. We demonstrate that our model learns convenient generation order as a by-product of its training procedure. The model outperforms left-to-right and right-to-left baselines on several tasks. We analyze learned generation orders and show that the model has a preference towards producing \"easy\" words at the beginning and leaving more complicated choices for later.\n\n# Acknowledgements\n\nThe authors thank David Talbot and Yandex Machine Translation team for helpful discussions and inspiration.",
    "metadata": {
      "md_filename": "MinerU_markdown_Evo_20260106143021_2008425888576442368.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_Evo_20260106143021_2008425888576442368.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "4bc0a81ee554dea2",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_Gating_20260106142839_2008425458597363712",
    "title": "Abstract",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "In this paper we introduce a new neural language model that replaces the recurrent connections typically used in recurrent networks with gated temporal convolutions. Neural language models (Bengio et al., 2003) produce a representation  $\\mathbf{H} = [\\mathbf{h}_0, \\dots, \\mathbf{h}_N]$  of the context for each word  $w_0, \\dots, w_N$  to predict the next word  $P(w_i | \\mathbf{h}_i)$ . Recurrent neural networks  $f$  compute  $\\mathbf{H}$  through a recurrent function  $\\mathbf{h}_i = f(\\mathbf{h}_{i-1}, w_{i-1})$  which is an inherently sequential process that cannot be parallelized over  $i$ .<sup>1</sup>\n\nOur proposed approach convolves the inputs with a function  $f$  to obtain  $\\mathbf{H} = f * w$  and therefore has no temporal dependencies, so it is easier to parallelize over the individual words of a sentence. This process will compute each context as a function of a number of preceding words. Compared to recurrent networks, the context size is finite but we will demonstrate both that infinite contexts are not necessary and our models can represent large enough contexts to perform well in practice (5).\n\nFigure 1 illustrates the model architecture. Words are represented by a vector embedding stored in a lookup table  $\\mathbf{D}^{|\\mathcal{V}| \\times e}$  where  $|\\mathcal{V}|$  is the number of words in the vocabulary and  $e$  is the embedding size. The input to our model is a sequence of words  $w_0, \\ldots, w_N$  which are represented by word embeddings  $\\mathbf{E} = [\\mathbf{D}_{w_0}, \\ldots, \\mathbf{D}_{w_N}]$ . We compute the hidden layers  $h_0, \\ldots, h_L$  as\n\n$$\nh _ {l} (\\mathbf {X}) = \\left(\\mathbf {X} * \\mathbf {W} + \\mathbf {b}\\right) \\otimes \\sigma \\left(\\mathbf {X} * \\mathbf {V} + \\mathbf {c}\\right) \\tag {1}\n$$\n\nwhere  $m, n$  are respectively the number of input and output feature maps and  $k$  is the patch size,  $\\mathbf{X} \\in \\mathbb{R}^{N \\times m}$  is the input of layer  $h_l$  (either word embeddings or the outputs of previous layers),  $\\mathbf{W} \\in \\mathbb{R}^{k \\times m \\times n}$ ,  $\\mathbf{b} \\in \\mathbb{R}^n$ ,  $\\mathbf{V} \\in \\mathbb{R}^{k \\times m \\times n}$ ,  $\\mathbf{c} \\in \\mathbb{R}^n$  are learned parameters,  $\\sigma$  is the sigmoid function and  $\\otimes$  is the element-wise product between matrices.\n\nWhen convolving inputs, we take care that  $\\mathbf{h}_i$  does not contain information from future words. We address this by shifting the convolutional inputs to prevent the kernels\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/c82b8587-5472-4e0c-b79c-d8ec5689e29e/a4872ffc594011651574e02091a53b748c779961752b8cf9a1ed00d0b61e1136.jpg)\n\nFigure 1. Architecture of the gated convolutional network for language modeling.\n\nfrom seeing future context (Oord et al., 2016a). Specifically, we zero-pad the beginning of the sequence with  $k - 1$  elements, assuming the first input element is the beginning of sequence marker which we do not predict and  $k$  is the width of the kernel.\n\nThe output of each layer is a linear projection  $\\mathbf{X}*\\mathbf{W} + \\mathbf{b}$  modulated by the gates  $\\sigma (\\mathbf{X}*\\mathbf{V} + \\mathbf{c})$ . Similar to LSTMs, these gates multiply each element of the matrix  $\\mathbf{X}*\\mathbf{W} + \\mathbf{b}$  and control the information passed on in the hierarchy. We dub this gating mechanism Gated Linear Units (GLU). Stacking multiple layers on top of the input  $\\mathbf{E}$  gives a representation of the context for each word  $\\mathbf{H} = h_{L}\\circ \\dots \\circ h_{0}(\\mathbf{E})$ . We wrap the convolution and the gated linear unit in a preactivation residual block that adds the input of the block to\n\nthe output (He et al., 2015a). The blocks have a bottleneck structure for computational efficiency and each block has up to 5 layers.\n\nThe simplest choice to obtain model predictions is to use a softmax layer, but this choice is often computationally inefficient for large vocabularies and approximations such as noise contrastive estimation (Gutmann & Hyvarinen) or hierarchical softmax (Morin & Bengio, 2005) are preferred. We choose an improvement of the latter known as adaptive softmax which assigns higher capacity to very frequent words and lower capacity to rare words (Grave et al., 2016a). This results in lower memory requirements as well as faster computation at both training and test time.\n\n# 3. Gating Mechanisms\n\nGating mechanisms control the path through which information flows in the network and have proven to be useful for recurrent neural networks (Hochreiter & Schmidhuber, 1997). LSTMs enable long-term memory via a separate cell controlled by input and forget gates. This allows information to flow unimpeded through potentially many timesteps. Without these gates, information could easily vanish through the transformations of each timestep. In contrast, convolutional networks do not suffer from the same kind of vanishing gradient and we find experimentally that they do not require forget gates.\n\nTherefore, we consider models possessing solely output gates, which allow the network to control what information should be propagated through the hierarchy of layers. We show this mechanism to be useful for language modeling as it allows the model to select which words or features are relevant for predicting the next word. Parallel to our work, Oord et al. (2016b) have shown the effectiveness of an LSTM-style mechanism of the form  $\\tanh (\\mathbf{X}*\\mathbf{W} + \\mathbf{b})\\otimes \\sigma (\\mathbf{X}*\\mathbf{V} + \\mathbf{c})$  for the convolutional modeling of images. Later, Kalchbrenner et al. (2016) extended this mechanism with additional gates for use in translation and character-level language modeling.\n\nGated linear units are a simplified gating mechanism based on the work of Dauphin & Grangier (2015) for nondeterministic gates that reduce the vanishing gradient problem by having linear units coupled to the gates. This retains the non-linear capabilities of the layer while allowing the gradient to propagate through the linear unit without scaling. The gradient of the LSTM-style gating of which we dub gated tanh unit (GTU) is\n\n$$\n\\begin{array}{l} \\nabla \\left[ \\tanh  (\\mathbf {X}) \\otimes \\sigma (\\mathbf {X}) \\right] = \\tanh  ^ {\\prime} (\\mathbf {X}) \\nabla \\mathbf {X} \\otimes \\sigma (\\mathbf {X}) \\\\ + \\sigma^ {\\prime} (\\mathbf {X}) \\nabla \\mathbf {X} \\otimes \\tanh  (\\mathbf {X}). \\tag {2} \\\\ \\end{array}\n$$\n\nNotice that it gradually vanishes as we stack layers because of the downscaling factors  $\\tanh^{\\prime}(\\mathbf{X})$  and  $\\sigma^{\\prime}(\\mathbf{X})$ . In con-\n\ntrast, the gradient of the gated linear unit\n\n$$\n\\nabla [ \\mathbf {X} \\otimes \\sigma (\\mathbf {X}) ] = \\nabla \\mathbf {X} \\otimes \\sigma (\\mathbf {X}) + \\mathbf {X} \\otimes \\sigma^ {\\prime} (\\mathbf {X}) \\nabla \\mathbf {X} \\tag {3}\n$$\n\nhas a path  $\\nabla \\mathbf{X} \\otimes \\sigma(\\mathbf{X})$  without downscaling for the activated gating units in  $\\sigma(\\mathbf{X})$ . This can be thought of as a multiplicative skip connection which helps gradients flow through the layers. We compare the different gating schemes experimentally in Section 5.2 and we find gated linear units allow for faster convergence to better perplexities.\n\n# 4. Experimental Setup\n\n# 4.1. Datasets\n\nWe report results on two public large-scale language modeling datasets. First, the Google Billion Word dataset (Chelba et al., 2013) is considered one of the largest language modeling datasets with almost one billion tokens and a vocabulary of over  $800\\mathrm{K}$  words. In this dataset, words appearing less than 3 times are replaced with a special unknown symbol. The data is based on an English corpus of 30,301,028 sentences whose order has been shuffled. Second, WikiText-103 is a smaller dataset of over 100M tokens with a vocabulary of about  $200\\mathrm{K}$  words (Merit et al., 2016). Different from GBW, the sentences are consecutive which allows models to condition on larger contexts rather than single sentences. For both datasets, we add a beginning of sequence marker  $\\langle S\\rangle$  at the start of each line and an end of sequence marker  $\\langle /S\\rangle$  at the end of each line. On the Google Billion Word corpus each sequence is a single sentence, while on WikiText-103 a sequence is an entire paragraph. The model sees  $\\langle S\\rangle$  and  $\\langle /S\\rangle$  as input but only predicts the end of sequence marker  $\\langle /S\\rangle$ . We evaluate models by computing the perplexity  $\\mathrm{e}^{\\frac{1}{N}\\sum_{i}^{N} - \\log p(w_{i}|...,w_{i - 1})}$  on the standard held out test portion of each dataset.\n\n# 4.2. Training\n\nWe implement our models in Torch (Collobert et al., 2011) and train on Tesla M40 GPUs. The majority of our models are trained on single GPU, as we focused on identifying compact architectures with good generalization and efficient computation at test time. We trained larger models with an 8-GPU setup by copying the model onto each GPU and dividing the batch such that each worker computes 1/8th of the gradients. The gradients are then summed using Nvidia NCCL. The multi-GPU setup allowed us to train models with larger hidden units.\n\nWe train using Nesterov's momentum (Sutskever et al., 2013). While the cost in terms of memory is storing another vector of the size of the parameters, it increases the speed of convergence significantly with minimal additional\n\n<table><tr><td>Name</td><td>GCNN-13</td><td colspan=\"2\">GCNN-14B</td><td>GCNN-9</td><td colspan=\"2\">GCNN-8B</td><td>GCNN-8</td><td>GCNN-14</td></tr><tr><td>Dataset</td><td colspan=\"6\">Google Billion Word</td><td colspan=\"2\">wikitext-103</td></tr><tr><td>Lookup</td><td colspan=\"6\">128</td><td colspan=\"2\">280</td></tr><tr><td>Conv1</td><td>[4,1268]  1</td><td colspan=\"2\">[5,512]  1</td><td>[4,807]  1</td><td colspan=\"2\">[1,512]  1</td><td>[4,900]  1</td><td>[6,850]  3</td></tr><tr><td>Conv2.x</td><td>[4,1268/4,1268]  12</td><td>1,1285,1281,512</td><td> 3</td><td>[4,807/4,807]  4</td><td>1,1285,1281,512</td><td> 3</td><td>[4,900]  7</td><td>[1,850]  1</td></tr><tr><td>Conv3.x</td><td></td><td>1,5125,5121,1024</td><td> 3</td><td></td><td>1,2565,2561,512</td><td> 3</td><td></td><td>[5,850]  4</td></tr><tr><td>Conv4.x</td><td></td><td>1,10245,10241,2048</td><td> 6</td><td></td><td>1,10241,10241,2048</td><td> 1</td><td></td><td>[1,850]  1</td></tr><tr><td>Conv5.x</td><td></td><td>1,10245,10241,4096</td><td> 1</td><td></td><td></td><td></td><td></td><td>[4,850]  3</td></tr><tr><td>Conv6.x</td><td></td><td colspan=\"2\"></td><td></td><td></td><td></td><td></td><td>[4,1024]  1</td></tr><tr><td>Conv7.x</td><td></td><td colspan=\"2\"></td><td></td><td></td><td></td><td></td><td>[4,2048]  1</td></tr><tr><td>AdaSoftmax</td><td colspan=\"3\">10k,40k,200k</td><td colspan=\"3\">4k,40k,200k</td><td>2k,10k,50k</td><td>10k,20k,200k</td></tr></table>\n\nTable 1. Architectures for the models. The residual building blocks are shown in brackets with the format  $[k, n]$ . \"B\" denotes bottleneck architectures.\n\ncomputation compared to standard stochastic gradient descent. The speed of convergence was further increased with gradient clipping (Pascanu et al., 2013) and weight normalization (Salimans & Kingma, 2016).\n\nPascanu et al. (2013) argue for gradient clipping because it prevents the gradient explosion problem that characterizes RNNs. However, gradient clipping is not tied to RNNs, as it can be derived from the general concept of trust region methods. Gradient clipping is found using a spherical trust region\n\n$$\n\\begin{array}{l} \\Delta \\theta^ {*} = \\operatorname * {a r g m i n} _ {\\text {s . t .} \\| \\Delta \\theta \\| \\leq \\epsilon} f (\\theta) + \\nabla f ^ {T} \\Delta \\theta \\\\ = - \\max  (\\| \\nabla f \\|, \\epsilon) \\frac {\\nabla f}{\\| \\nabla f \\|}. \\tag {4} \\\\ \\end{array}\n$$\n\nEmpirically, our experiments converge significantly faster with the use of gradient clipping even though we do not use a recurrent architecture.\n\nIn combination, these methods led to stable and fast convergence with comparatively large learning rates such as 1.\n\n# 4.3. Hyper-parameters\n\nWe found good hyper-parameter configurations by cross-validating with random search on a validation set. For model architecture, we select the number of residual blocks between  $\\{1,\\ldots ,10\\}$ , the size of the embeddings with  $\\{128,\\dots ,256\\}$ , the number of units between  $\\{128,\\dots ,2048\\}$ , and the kernel width between  $\\{3,\\dots ,5\\}$ .\n\nIn general, finding a good architecture was simple and the rule of thumb is that the larger the model, the better the performance. In terms of optimization, we initialize the layers of the model with the Kaiming initialization (He et al., 2015b), with the learning rate sampled uniformly in the interval [1., 2.], the momentum set to 0.99, and clipping set to 0.1. Good hyper-parameters for the optimizer are quite straightforward to find and the optimal values do not change much between datasets.\n\n# 5. Results\n\nLSTMs and recurrent networks are able to capture long term dependencies and are fast becoming cornerstones in natural language processing. In this section, we compare strong LSTM and RNN models from the literature to our gated convolutional approach on two datasets.\n\nWe find the GCNN outperforms the comparable LSTM results on Google billion words. To accurately compare these approaches, we control for the same number of GPUs and the adaptive softmax output model (Grave et al., 2016a), as these variables have a significant influence on performance. In this setting, the GCNN reaches 38.1 test perplexity while the comparable LSTM has 39.8 perplexity (Table 2).\n\nFurther, the GCNN obtains strong performance with much greater computational efficiency. Figure 2 shows that our approach closes the previously significant gap between models that use the full softmax and models with the usually less accurate hierarchical softmax. Thanks to the adap\n\n<table><tr><td>Model</td><td>Test PPL</td><td>Hardware</td></tr><tr><td>Sigmoid-RNN-2048 (Ji et al., 2015)</td><td>68.3</td><td>1 CPU</td></tr><tr><td>Interpolated KN 5-Gram (Chelba et al., 2013)</td><td>67.6</td><td>100 CPUs</td></tr><tr><td>Sparse Non-Negative Matrix LM (Shazeer et al., 2014)</td><td>52.9</td><td>-</td></tr><tr><td>RNN-1024 + MaxEnt 9 Gram Features (Chelba et al., 2013)</td><td>51.3</td><td>24 GPUs</td></tr><tr><td>LSTM-2048-512 (Jozefowicz et al., 2016)</td><td>43.7</td><td>32 GPUs</td></tr><tr><td>2-layer LSTM-8192-1024 (Jozefowicz et al., 2016)</td><td>30.6</td><td>32 GPUs</td></tr><tr><td>BIG GLSTM-G4 (Kuchaiev &amp; Ginsburg, 2017)</td><td>23.3*</td><td>8 GPUs</td></tr><tr><td>LSTM-2048 (Grave et al., 2016a)</td><td>43.9</td><td>1 GPU</td></tr><tr><td>2-layer LSTM-2048 (Grave et al., 2016a)</td><td>39.8</td><td>1 GPU</td></tr><tr><td>GCNN-13</td><td>38.1</td><td>1 GPU</td></tr><tr><td>GCNN-14 Bottleneck</td><td>31.9</td><td>8 GPUs</td></tr></table>\n\nTable 2. Results on the Google Billion Word test set. The GCNN outperforms the LSTMs with the same output approximation.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/c82b8587-5472-4e0c-b79c-d8ec5689e29e/0670bbea6c550d1f94de5df7f1caa0b197e49930d40e72902fa73c4ed6fd1f4b.jpg)\n\nFigure 2. In comparison to the state-of-the-art (Jozefowicz et al., 2016) which uses the full softmax, the adaptive softmax approximation greatly reduces the number of operations required to reach a given perplexity.\n\ntive softmax, the GCNN only requires a fraction of the operations to reach the same perplexity values. The GCNN outperforms other single model state-of-the-art approaches except the much larger LSTM of Jozefowicz et al. (2016), a model which requires more GPUs and the much more computationally expensive full softmax. In comparison, the largest model we have trained reaches 31.9 test perplexity compared to the 30.6 of that approach, but only requires training for 2 weeks on 8 GPUs compared to 3 weeks of training on 32 GPUs for the LSTM. Note that these results can be improved by either using mixtures of experts (Shazeer et al., 2017) or ensembles of these models.\n\nAnother relevant concern is if the GCNN's fixed context size can thoroughly model long sequences. On Google Bil\n\n<table><tr><td>Model</td><td>Test PPL</td><td>Hardware</td></tr><tr><td>LSTM-1024 (Grave et al., 2016b)</td><td>48.7</td><td>1 GPU</td></tr><tr><td>GCNN-8</td><td>44.9</td><td>1 GPU</td></tr><tr><td>GCNN-14</td><td>37.2</td><td>4 GPUs</td></tr></table>\n\nTable 3. Results for single models on the WikiText-103 dataset.\n\nlion Word, the average sentence length is quite short  only 20 words. We evaluate on WikiText-103 to determine if the model can perform well on a dataset where much larger contexts are available. On WikiText-103, an input sequence is an entire Wikipedia article instead of an individual sentence - increasing the average length to 4000 words. However, the GCNN outperforms LSTMs on this problem as well (Table 3). The GCNN-8 model has 8 layers with 800 units each and the LSTM has 1024 units. These results show that GCNNs can model enough context to achieve strong results.\n\nWe evaluated on the Gigaword dataset following Chen et al. (2016) to compare with fully connected models. We found that the fully connected and convolutional network reach respectively 55.6 and 29.4 perplexity. We also ran preliminary experiments on the much smaller Penn tree bank dataset. When we score the sentences independently, the GCNN and LSTM have comparable test perplexity with 108.7 and 109.3 respectively. However, it is possible to achieve better results by conditioning on previous sentences. Unlike the LSTM, we found that the GCNN overfits on this quite small dataset and so we note the model is better suited to larger scale problems.\n\n# 5.1. Computational Efficiency\n\nComputational cost is an important consideration for language models. Depending on the application, there are a number of metrics to consider. We measure the throughput\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/c82b8587-5472-4e0c-b79c-d8ec5689e29e/acfcd182d3cda49a411ad94997d51f461c558da429b4b24a183da6d8243316f5.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/c82b8587-5472-4e0c-b79c-d8ec5689e29e/7726206063518343a2b8daf5698d678447813312ea8e1420cba9a9bedd689f4c.jpg)\n\nFigure 3. Learning curves on WikiText-103 (left) and Google Billion Word (right) for models with different activation mechanisms. Models with gated linear units (GLU) converge faster and to a lower perplexity.\n\n<table><tr><td></td><td colspan=\"2\">Throughput</td><td>Responsiveness</td></tr><tr><td></td><td>(CPU)</td><td>(GPU)</td><td>(GPU)</td></tr><tr><td>LSTM-2048</td><td>169</td><td>45,622</td><td>2,282</td></tr><tr><td>GCNN-9</td><td>121</td><td>29,116</td><td>29,116</td></tr><tr><td>GCNN-8 Bottleneck</td><td>179</td><td>45,878</td><td>45,878</td></tr></table>\n\nTable 4. Processing speed in tokens/s at test time for an LSTM with 2048 units and GCNNs achieving 43.9 perplexity on Google Billion Word. The GCNN with bottlenecks improves the responsiveness by 20 times while maintaining high throughput.\n\nof a model as the number of tokens that can be processed per second. Throughput can be maximized by processing many sentences in parallel to amortize sequential operations. In contrast, responsiveness is the speed of processing the input sequentially, one token at a time. Throughput is important because it indicates the time required to process a corpus of text and responsiveness is an indicator of the time to finish processing a sentence. A model can have low responsiveness but high throughput by evaluating many sentences simultaneously through batching. In this case, such a model is slow in finishing processing individual sentences, but can process many sentences at a good rate.\n\nWe evaluate the throughput and responsiveness for models that reach approximately 43.9 perplexity on the Google Billion Word benchmark. We consider the LSTM with 2048 units in Table 2, a GCNN-8 Bottleneck with 7 Resnet blocks that have a bottleneck structure as described by (He et al., 2015a) and a GCNN-8 without bottlenecks. A bottleneck block wedges a  $k > 1$  convolution between two  $k = 1$  layers. This designs reduces computational cost by reducing and increasing dimensionality with the  $k = 1$  layers so that the convolution operates in a lower dimensional space. Our results show that the use of bottleneck blocks is important to maintaining computational efficiency.\n\nThe throughput of the LSTM is measured by using a large batch of 750 sequences of length 20, resulting in 15,000 tokens per batch. The responsiveness is the average speed to process a sequence of 15,000 contiguous tokens. Table 4 shows that the throughput for the LSTM and the GCNN are similar. The LSTM performs very well on GPU because the large batch size of 750 enables high parallelization over different sentences. This is because the LSTM implementation has been thoroughly optimized and uses cuDNN, whereas the cuDNN implementation of convolutions is not been optimized for the 1-D convolutions we use in our model. We believe much better performance can be achieved by a more efficient 1-D cuDNN convolution. Unlike the LSTM, the GCNN can be parallelized both over sequences as well as across the tokens of each sequence, allowing the GCNN to have  $20\\mathrm{x}$  higher responsiveness.\n\n# 5.2. Gating Mechanisms\n\nIn this section, we compare the gated linear unit with other mechanisms as well as to models without gating. We consider the LSTM-style gating mechanism (GTU)  $\\tanh (\\mathbf{X}*\\mathbf{W} + \\mathbf{b})\\otimes \\sigma (\\mathbf{X}*\\mathbf{V} + \\mathbf{c})$  of (Oord et al., 2016b) and networks that use regular ReLU or Tanh activations. Gating units add parameters, so for fair comparison, we carefully cross-validate models with a comparable number of parameters. Figure 3 (left) shows that GLU networks converge to a lower perplexity than the other approaches on WikiText-103. Similar to gated linear units, the ReLU has a linear path that lets the gradients easily pass through the active units. This translates to much faster convergence for both the ReLU and the GLU. On the other hand, neither Tanh nor GTU have this linear path, and thus suffer from the vanishing gradient problem. In the GTU, both the inputs as well as the gating units can cut the gradient when the units saturate.\n\nComparing the GTU and Tanh models allows us to measure\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/c82b8587-5472-4e0c-b79c-d8ec5689e29e/7f6e0bcb76e4ae1a2888c5e56691b179a145e0fb5553b17aba925c814116eae9.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/c82b8587-5472-4e0c-b79c-d8ec5689e29e/e8749826f81544ae889eed1ffa794a76e962b5538473b5a97251232c1f68e1f4.jpg)\n\nFigure 4. Test perplexity as a function of context for Google Billion Word (left) and Wiki-103 (right). We observe that models with bigger context achieve better results but the results start diminishing quickly after a context of 20.\n\nthe effect of gating since the Tanh model can be thought of as a GTU network with the sigmoid gating units removed. The results (Figure 3, left) show that the gating units make a vast difference and provide useful modeling capabilities, as there is a large difference in the performance between GTU and Tanh units. Similarly, while ReLU unit is not an exact ablation of the gating units in the GLU, it can be seen as a simplification  $\\mathrm{ReLU}(\\mathbf{X}) = \\mathbf{X}\\otimes (\\mathbf{X} > 0)$  where the gates become active depending on the sign of the input. Also in this case, GLU units lead to lower perplexity.\n\nIn Figure 3 (right) we repeat the same experiment on the larger Google Billion Words dataset. We consider a fixed time budget of 100 hours because of the considerable training time required for this task. Similar to WikiText-103, the gated linear units achieve the best results on this problem. There is a gap of about 5 perplexity points between the GLU and ReLU which is similar to the difference between the LSTM and RNN models measured by (Jozefowicz et al., 2016) on the same dataset.\n\n# 5.3. Non-linear Modeling\n\nThe experiments so far have shown that the gated linear unit benefits from the linear path the unit provides compared to other non-linearities. Next, we compare networks with GLUs to purely linear networks and networks with bilinear layers in order to measure the impact of the nonlinear path provided by the gates of the GLU. One motivation for this experiment is the success of linear models on many natural language processing tasks (Manning & Schtze, 1999). We consider deep linear convolutional networks where the layers lack the gating units of the GLU and take the form  $h_{l}(\\mathbf{X}) = \\mathbf{X}*\\mathbf{W} + \\mathbf{b}$ . Stacking several layers on top of each other is simply a factorization of the model which remains linear up to the softmax, at which point it becomes log-linear. Another variation of GLUs are bilinear layers (Mnih & Hinton, 2007) which take the form\n\n$$\nh _ {l} (\\mathbf {X}) = (\\mathbf {X} * \\mathbf {W} + \\mathbf {b}) \\otimes (\\mathbf {X} * \\mathbf {V} + \\mathbf {c}).\n$$\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/c82b8587-5472-4e0c-b79c-d8ec5689e29e/0a3c956da8d1edea5e853a736ff5e07b5acf8c335f5e0e63cc2cebdc5d05e103.jpg)\n\nFigure 5. Learning curves on Google Billion Word for models with varying degrees of non-linearity.\n\nFigure 5 shows that GLUs perform best, followed by bilinear layers and then linear layers. Bilinear layers improve over linear ones by more than 40 perplexity points, and the GLU improves another 20 perplexity points over the bilinear model. The linear model performs very poorly at perplexity 115 even compared to 67.6 of a Kneser-Ney 5-gram model, even though the former has access to more context. Surprisingly, the introduction of the bilinear units is enough to reach 61 perplexity on Google Billion Word, which surpasses both Kneser-Ney 5-gram models and the non-linear neural model of (Ji et al., 2015).\n\n# 5.4. Context Size\n\nFigure 4 shows the impact of context size for the gated CNN. We tried different combinations of network depth and kernel widths for each context size and chose the best performing one for each size. Generally, larger contexts\n\nimprove accuracy but returns drastically diminish with windows larger than 40 words, even for WikiText-103 where we may condition on an entire Wikipedia article. This means that the unlimited context offered by recurrent models is not strictly necessary for language modeling. Furthermore, this finding is also congruent with the fact that good performance with recurrent networks can be obtained by truncating gradients after only 40 timesteps using truncated back propagation through time. Figure 4 also shows that WikiText-103 benefits much more from larger context size than Google Billion Word as the performance degrades more sharply with smaller contexts. WikiText-103 provides much more context than Google Billion Word where the average sentence size is 20. However, while the average size of the documents is close to 4000 tokens, we find that strong performance can be achieved with a context size as low as 30 tokens.\n\n# 5.5. Training\n\nIn this section, we perform an ablation study of the impact of weight normalization and gradient clipping. We separately cross-validate the hyper-parameters of each configuration to make the comparison fair. Due to the high cost of each of these experiments, we only consider a single iteration over the training data. Figure 6 shows that both methods significantly speed up convergence. Weight normalization in particular improves the speed by over two times. This speedup is partly due to the ability to use much larger learning rates (1 instead of 0.01) than would otherwise be possible. Both clipping and weight normalization add computational overhead, but it is minor compared to the large gains in convergence speed.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/c82b8587-5472-4e0c-b79c-d8ec5689e29e/84c01f2ad836ac6ddb2f88e01e69573165c3b633e6148afdc5dfda6c246b3fc1.jpg)\n\nFigure 6. Effect of weight normalization and gradient clipping on Google Billion Word.\n\n# 6. Conclusion\n\nWe introduce a convolutional neural network for language modeling with a novel gating mechanism. Compared to recurrent neural networks, our approach builds a hierarchical representation of the input words that makes it easier to capture long-range dependencies, similar in spirit to the tree-structured analysis of linguistic grammar formalisms. The same property eases learning since features are passed through a fixed number of layers and non-linearities, unlike for recurrent networks where the number of processing steps differs depending on the position of the word in the input. The results show that our gated convolutional network achieves a new state of the art on WikiText-103. On the Google Billion Word benchmark, we show competitive results can be achieved with significantly fewer resources.",
    "metadata": {
      "md_filename": "MinerU_markdown_Gating_20260106142839_2008425458597363712.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_Gating_20260106142839_2008425458597363712.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "c26c27154f4e8d9d",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_gRelu_20260106143014_2008425854887792640",
    "title": "gReLU: a comprehensive framework for DNA sequence modeling and design",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# Processing of GM12878 DNase-seq data\n\nFrom the ENCODE portal, we downloaded data corresponding to the GM12878 DNase-seq experiment ENCSR0000EJD. Specifically, we downloaded the read-depth normalized bigwig (ENCFF093VXI) and the peaks narrowPeak file (ENCFF588OCA), aligned to the hg38 reference genome.\n\nUsing gReLU, we extended the peaks 250 bp on each side of the summit and merged overlapping regions. We filtered the regions to autosomes and removed regions overlapping blacklisted regions<sup>30</sup>. We obtained a set of GC-matched negative regions using the grelu.data. preprocess.get_gc(matched_intervals function with binwidth = 0.05. We used sequences from chromosome 10 as the validation set and sequences from chromosome 11 as the test set.\n\n# Architecture and training of the DNase-seq regression model\n\nWe used the DilatedConvModel architecture provided in gReLU, which implements a dilated convolutional architecture, similar to BPNet. The following parameters were used: channels = 512, n_conv = 9. Thus, the model consists of 9 convolution layers with 512 channels each.\n\nWe created a dataset using the BigWigSeqDataset class with input length 2,114 bp and output lengths of 1,000 bp, label_aggfunc = 'sum', label_transform_func = np.log1p, augment_mode = 'random'. Thus, the labels are transformed to the log of the summed counts over the output 1,000-bp region. Therefore, the model is trained to take as input 2,114 bp of sequence and output the log of the total DNase-seq counts in the central 1,000 bp.\n\nWe performed a hyperparameter sweep using Bayesian optimization over different combinations of data augmentation parameters (rc = True or False, max_seq_shift = 0, 1 or 3, max_pairhift = 0, 10, 50 or 100). Each model was trained using the mean squared error loss, a learning rate of  $10^{-4}$  and batch size 512, for a total of 15 epochs. The model with the lowest validation set loss was selected as the best model, evaluated on the test set and used for variant effect prediction.\n\n# Variant effect prediction and interpretation\n\nWe downloaded the previously curated list of 574 lymphoblastoid cell line (LCL) dsQTL single-nucleotide polymorphisms (SNPs) and 27,735 control  $\\mathrm{SNPs}^{24}$ . We removed regions that would be close to chromosome edges using the filter_chrom Ends function with genome = 'hg19' and pad = 98,304 (half the input length for Enformer). This resulted in 574 dsQTL SNPs and 27,700 control SNPs.\n\nWe scored the variants using grelu.variant.predict_variant_effects with compare_func = 'subtract' and genome = 'hg19', using the provided hg19 coordinates. This returns the log fold-change (LFC) of the predicted counts between the reference and the alternate alleles. We ran this step twice, setting rc = True and rc = False, respectively, to test the effect of reverse complement data augmentation. The setting rc = True causes reverse complementation to be applied, that is, for each allele-containing sequence, we predicted its activity in both orientations and averaged the results.\n\nFor gkmSVM, we used the predictions provided by the authors[24]. For Enformer, we used the model from the gReLU model zoo (project = 'enformer', model_name = 'human'). We used grelu variant.predict_variant_effects to compute variant LFCs comparable with our regression model. To do so, we applied a prediction transform that summed predictions in the eight bins centered around the variant (total width of  $128 \\times 8 = 1,024$  bp) for the ENCFF093VXI task and log transformed the sum, along with compare_func = 'subtract'. We ran this step twice, setting rc = True and rc = False, respectively, to test the effect of reverse complement data augmentation.\n\nWe assessed performance using the sklearn.metrics-average_precision_score function with the provided labels and the absolute value of the predicted LFC. We generated the model interpretations at select loci using theGRELU.interpret.score.get_attributes function with the\n\nparameters method = 'saliency', correct_grad = True, which apply the saliency method with gradient correction<sup>31</sup>.\n\nFor a global analysis, we performed TF-MoDISco on 100-bp windows centered at the reference and alternate versions for both dsQTLs and control SNPs using grelu.interpret.modisco.run_modisco with method = 'saliency', correct_grad = True, window = 100. From the output TF-MoDISco object, we plotted the motifs. Then, we extracted the seqlets (that is, motif instances) and computed the number of SNPs overlapping any motif separately for the dsQTL and control sets. A variant was counted as overlapping if either the reference or alternate versions overlap with a motif instance.\n\n# Inference and interpretation using the Borzoi model\n\nWe used the grelu-resources.load_model function to load the Borzoi model (replicate 0) from the public gReLU model zoo. We used the following CD14 $^{+}$  monocyte and CD4 $^{+}$  T cell RNA-seq tracks within Borzoi:\n\nENCFF023YXV+ (CD14-positive monocyte female),\n\nENCCF946ZPT (with multiple sclerosis; CD14-positive monocyte),\n\nENCFF853SNW (CD14-positive monocyte),\n\nENCFF735XXE (CD14-positive monocyte),\n\nENCCF848ZVQ (with multiple sclerosis; CD14-positive monocyte),\n\nENCFF926QTW (CD14-positive monocyte male adult (37 years)),\n\nENCFF623LHV (with multiple sclerosis; CD14-positive monocyte),\n\nENCFF004DOF (CD14-positive monocyte),\n\nENCCF579IBH+ (CD4-positive, alpha-beta T cell male adult (20 years)),\n\nENCF515TIF+ (CD4-positive, alpha-beta T cell male adult (20 years)),\n\nENCFF223MUU (CD4-positive, alpha-beta T cell male adult (37 years)),\n\nENCCF089BOJ (CD4-positive, alpha-beta T cell male adult (21 years)).\n\nGene annotations were loaded usingGRElu.io.genome. read_gtf and filtered to remove overlapping transcripts usingGRElu.data.preprocess.filter_overlapping.\n\nTo predict and visualize RNA-seq coverage across the PPIF gene, we used the model.predict_on_seqs function, followed by conversion from genomic intervals to output bins with model-input_interval_to_output_bins. The resulting predicted tracks were visualized using greluvisualization.plot Tracks.\n\nTo quantify gene expression from model predictions, we used the 'Aggregate' prediction transform to compute the average predicted RNA-seq signal across bins overlapping the exons of the PPIF gene. These scores were used to compare relative expression between cell types and to track changes across sequence edits.\n\nTo produce the attention map surrounding the PPIF gene, we used the grelu.interpret.score.get_interest Scores function with parameter 'block_idx = -1', which extracts attention weights from the model's final attention layer. We then averaged these weights across all attention heads of this layer before visualizing the matrix.\n\n# ISM, sequence design and motif discovery\n\nWe modified the distal enhancer located at chr10:79285732-79287386 (hg38), using the grelu(sequence.mutate function to substitute 5-bp windows with an alternate sequence. Model predictions were computed for each mutated sequence, and expression scores were calculated as above. The predicted expression changes were compared with experimental results from Variant-FlowFISH assays in K562 cells, which were obtained from supplementary table 8 of Martyn et al., and coordinates were converted from hg19 to hg38 using LiftOver $^{32}$ .\n\nWe computed ISM scores using the grelu.interpret.score.ISM_prediction function, comparing predicted expression between the reference sequence and versions with single-base substitutions. These were visualized as  $\\log_2$  fold-change heatmaps using grelu plot_ISM.\n\nTo design sequences with enhanced cell-type specificity, we defined a prediction transform using the 'Specificity' class to compute the difference in predicted RNA-seq expression between monocytes and T cells, averaged across bins overlapping the PPIF exons. This metric served as the objective for optimization. We then used the grelu/design.\n\nevolve function to iteratively modify the enhancer sequence. At each round, a single base was mutated to improve the specificity objective.\n\nTo investigate the mechanisms driving the evolved activity, we scanned the original and evolved enhancer for known motifs using grelu.interpret.motifs.scan_sequences. Motifs were extracted from the HOCOMOCO v12 database<sup>33</sup>. Comparative analysis with the original sequence was performed using grelu.interpret.motifscompare_motifs.\n\n# Validation of the edited PPIF enhancer using orthogonal models\n\nTo validate the design using orthogonal models, we used a chromatin accessibility model (human-atac-catlas) from the gReLU model zoo. This is a multitask binary classification model, trained on binarized pseudobulk single-cell assay for transposase-accessible chromatin using sequencing (ATAC-seq) data from 203 human cell types[22]. Given a 200-bp input sequence, it predicts the probability of accessibility of the sequence in each cell type. We applied this model to the original and evolved PPIF enhancer sequences. Predictions are shown for both monocyte-like (Macrophage General, Macrophage Gen or Alv, Fetal Macrophage Placental, Fetal Macrophage Hepatic 1, Fetal Macrophage General 1, Fetal Macrophage General 2, Fetal Macrophage General 3, Fetal Macrophage Hepatic 2, Fetal Macrophage Hepatic 3 and Fetal Macrophage General 4) and T cell-like (T Lymphocyte 1  $(\\mathrm{CD8^{+}})$ , T Lymphocyte 2  $(\\mathrm{CD4^{+}})$ , Natural Killer T, Naive T, Fetal T Lymphocyte 1  $(\\mathrm{CD4^{+}})$ , Fetal T Lymphocyte 2 (Cytotoxic) and Fetal T Lymphocyte 3  $(\\mathrm{IL2^{+}})$ ) tracks. We further validated that these tracks are similar to the THP-1 and Jurkat cell lines used by Martyn et al. (Supplementary Fig. 4 and Supplementary Methods).\n\nWe additionally validated the cell type specificity of the enhancer using the following DNAse tracks with the Borzoi model: ENCFF678LXL (CD14-positive monocyte female),\n\nENCFF659BVQ (CD14-positive monocyte female), ENCF724HAH (CD14-positive monocyte male adult (21 years)), ENCF722MXM (CD14-positive monocyte male adult (37 years)), ENCF759CKK (CD14-positive monocyte female adult (34 years)), ENCF522NPH (CD4-positive, alpha-beta T cell male adult (37 years)), ENCF133LMW (CD4-positive, alpha-beta T cell male adult (37 years)), ENCF658FXQ (CD4-positive, alpha-beta T cell male adult (21 years)), ENCF502EPK (CD4-positive, alpha-beta T cell female adult (33 years)), ENCF131RGB (CD4-positive, alpha-beta T cell male adult (21 years)).\n\nWe used the same ten DNAse tracks in the Enformer model. In these cases, grelu.transforms_prediction_transforms.Aggregate and score.compute were used to compute accessibility across the distal enhancer.",
    "metadata": {
      "md_filename": "MinerU_markdown_gRelu_20260106143014_2008425854887792640.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_gRelu_20260106143014_2008425854887792640.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "75f0bc44aabc0746",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_HyenaDNA_20260106143006_2008425827142471680",
    "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "In this section, we introduce the HyenaDNA approach to long-range genomic sequence modeling. We start with a description of the model architecture, then discuss sequence length warm-up and soft prompting techniques for downstream adaptation.\n\n# 3.1 The HyenaDNA Model\n\nThe HyenaDNA model is a decoder-only, sequence-to-sequence architecture defined by a stack of blocks consisting of a Hyena operator (Poli et al., 2023), followed by a feed-forward neural network (see Fig. 1.3).\n\nGiven an input  $x \\in \\mathbb{R}^L$  ( $L$  denotes sequence length), a Hyena operator can be defined as:\n\n$$\n(x _ {1}, x _ {2}, v) \\mapsto \\mathsf {H} (x _ {1}, x _ {2}) v\n$$\n\n$$\n\\mathsf {H} \\left(x _ {1}, x _ {2}\\right) = \\mathsf {D} _ {x _ {2}} \\mathsf {T} _ {h} \\mathsf {D} _ {x _ {1}} \\tag {3.1}\n$$\n\nwhere  $x_{1}, x_{2}, v$  are projections of the input, and  $\\mathsf{T}_h \\in \\mathbb{R}^{L \\times L}$  is the Toeplitz matrix constructed from a learnable long convolution filter produced as the output of a neural network,  $(\\mathsf{T}_h)_{ij} = h_{i-j}$ . The convolution filter values themselves are obtained through a small neural network  $\\gamma_{\\theta}$  taking as input the time (position) index and option-\n\nally positional encodings,  $h_t = \\gamma_\\theta(t)$ , which enable the operator to process very long sequences without growing linearly in the number of parameters. Further, the matrices  $\\mathsf{D}_{x_1}, \\mathsf{D}_{x_2} \\in \\mathbb{R}^{L \\times L}$  are constructed with  $x_1, x_2$  on the diagonals, and evaluated as element-wise gating. The projections are obtained by applying a dense linear layer and short convolution to the input sequence, as shown in Figure 3.1.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/33f48360-b496-4e4c-8e6b-30306d5c103e/8933a9e024cfe40b4091bacd16bd2bd6f8527fb3a0a57e967d127760e28ae147.jpg)\n\nFigure 3.1: The Hyena operator is a combination of long convolutions  $\\mathsf{T}$  and data-controlled gating  $\\mathsf{D}$ , and can be a drop-in replacement for attention.\n\nProposition 3.1. A Hyena operator can be evaluated in  $\\mathcal{O}(L\\log_2L)$  time.\n\nEfficient evaluation is crucial on settings involving extremely long sequences such as genomics. In the general case where the embedding dimension  $D > 1$  and  $x \\in \\mathbb{R}^{L \\times D}$ , the linear projections  $\\mathsf{W}_{x_1}, \\mathsf{W}_{x_2}, \\mathsf{W}_v \\in \\mathbb{R}^{D \\times D}$  are right multiplied to  $x$ , and  $D$  independent Hyena operators are then applied to each dimension.\n\n# 3.2 Training Long Sequence Models\n\nTokenization The subquadratic cost of HyenaDNA in sequence length allows the model to process ultralong sequences directly at the single nucleotide level without the need for frequency-based aggregation tokenizers. This enables fine-grain resolution for both short and long sequences, critical for detecting single nucleotide polymorphisms or mutations and modeling long-range dependencies in gene expression.\n\nWe use the natural DNA vocabulary and refer to each nucleotide as a token. The tokens include \"A\", \"G\", \"C\", \"T\", and \"N\" (a non-specific nucleotide) and special character tokens for padding, separation, and unknown characters. Tokens are mapped to embedding dimension  $D$ .\n\nSequence length warm-up for ultralong sequences Directly training on long sequences can affect training stability as the variance in gradient increases (Li et al., 2022). Training on shorter sequences initially (followed by longer sequences) was used by (Press et al., 2020) to train small scale Transformers and reduce training time, while (Li et al., 2022) used sequence length warm-up to address stability on up to  $2\\mathrm{k}$  tokens. For ultralong sequences  $(200\\mathrm{k}+)$ , we develop a new warm-up schedule that gradually increases the sequence length in stages to improve both stability and decrease training time.\n\nOur sequence length schedule starts at  $L_{1} = 64$ , then doubles the window at each stage while keeping the global batch size constant. By doing so, iterations at each consecutive stage will include more tokens, ensuring the scheduler can also act as a form of batch size warm-up. In Fig. 3.2, we observe sequence length scheduling to be particularly important at sequence lengths greater\n\nthan  $450\\mathrm{k}$ , where at this length training time is reduced by  $40\\%$  and improving ultimate accuracy by  $7.5\\%$  points for a species classification task described later in section 4.4.3.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/33f48360-b496-4e4c-8e6b-30306d5c103e/1d6d054adc1922c5a8e5be151d615381d3c6126b7e3063f34df6ad93a710bf68.jpg)\n\nFigure 3.2: Sequence length warm-up reduces the training time of HyenaDNA at sequence length  $450\\mathrm{k}$  by  $40\\%$  and boosts accuracy by 7.5 points on species classification.\n\n# 3.3 Downstream Adaptation\n\nTuneable prompting for long-context models Prompts have been traditionally used to guide the output of a FM (Liu et al., 2023) by prepending additional context to an input. Expanding on this approach, soft tuneable prompting was introduced to inject learnable tokens (as weights) into the input directly (Lester et al., 2021) as an alternative to model fine-tuning.\n\nWith an extended context length  $(L)$ , we're able to explore new paradigms in adapting FMs after pretraining. Given a downstream task with prompts  $x_{p} \\in \\mathbb{R}^{T}$  and corresponding labels  $y_{p}$ , we prepend  $N \\leq L - T$  trainable parameters  $\\theta$  of dimension  $D$  after the embedding step:\n\n$$\nx \\leftarrow \\operatorname {c o n c a t} \\left[ \\operatorname {e m b e d} \\left(x _ {p}\\right), \\theta \\right], \\quad x \\in \\mathbb {R} ^ {L \\times (T + N)} \\tag {3.2}\n$$\n\nThe resulting sequences  $x$  are then processed by the model, and  $\\theta$  is optimized on a loss function involving the input sequence's label  $y_{p}$ . Crucially, soft prompting requires utilization of a small subset of prompt and label pairs to optimize  $\\theta$ .\n\nDuring soft prompting, HyenaDNA only optimizes the parameters of the prompt in the input sequence while keeping all other model parameters fixed. Soft prompting thereby provides a flexible and computationally efficient approach to adapting genomic FMs to new downstream tasks.\n\n# 4 Experiments\n\nIn 4.1, we start with pretraining HyenaDNA on the human reference genome (Genome Reference Consortium, 2013). We then evaluate HyenaDNA on existing short-range (<5k nucleotides) downstream benchmarks in 4.2 to assess the performance of single nucleotide resolution. In 4.3, we explore what new capabilities emerge with longer range genomic modeling in the form of in-context learning. Finally, we push the limits of ultralong context performance in 4.4.\n\n# 4.1 Pretraining on the Human Genome\n\nWe pretrain HyenaDNA on the human reference genome (Genome Reference Consortium, 2013) using next nucleotide (token) prediction. Starting with a stack of decoder-only Transformer blocks, we swap attention for the Hyena operator, and compare against a baseline Transformer (GPT) with Flash Attention (Dao et al., 2022a). We add gradient checkpointing to HyenaDNA to decrease the memory footprint by  $3\\mathrm{x}$  on\n\nlonger sequences ( $>160\\mathrm{k}$ ). We then scale HyenaDNA along dimensions of model depth (2 to 8 layers), width (128 to 256 dimensions), and sequence length (1024 to 1M). At sequence length 1M, HyenaDNA is  $160\\mathrm{x}$  faster than its Transformer counterpart as shown in Fig. 4.1.\n\nAs shown in Fig. 1.2, we observe that as context length increases, perplexity improves during pretraining. However, this improvement comes at the expense of more training time and tokens. For models too shallow to effectively process longer context, perplexity can begin to degrade (increase), observing inflection points with longer sequences. In this way, increasing context can serve as a novel regularization dimension. For genomic pretraining, we provide the following guidelines. 1. In optimizing for faster training time, shorter context enable lower perplexity to be reached faster. 2. In optimizing for best overall perplexity, longer context allows for lower perplexity at the cost of training on more tokens. See A.1 for experiment details.\n\n# 4.2 Single Nucleotide Resolution\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/33f48360-b496-4e4c-8e6b-30306d5c103e/ab59e9fcd66ea974cd9e73fbff0b3e61670d183d273cb2b9c02ccbfe239eb2f7.jpg)\n\nFigure 4.1: Runtime (forward & backward pass) for Transformer and HyenaDNA: 2 layers, width=128, gradient checkpointing, batch size=1, A100 80GB. At 1M tokens HyenaDNA is 160x faster than Transformer.\n\nOur first downstream tasks use short-range genomic sequences  $(< 5\\mathrm{k})$  aimed at evaluating single nucleotide resolution performance on sequence-level classification using standard fine-tuning.\n\nGenomicBenchmarks We start with the newly released GenomicBenchmarks (Gresova et al., 2022), which is comprised of 8 regulatory element classification datasets with sequence lengths of 200-500, and one up to 4,776. The original baseline model uses a short-range CNN. We fine-tune the pretrained Transformer (GPT) and HyenaDNA from 4.1, both having single nucleotide resolution, as well as the DNABERT model (Ji et al., 2021). HyenaDNA sets a new\n\nTable 4.1: GenomicBenchmarks Top-1 accuracy (%) for pretrained HyenaDNA, DNABERT and Transformer (GPT from 4.1), and the previous SotA baseline CNN (scratch).\n\n<table><tr><td>DATASET</td><td>CNN</td><td>DNABERT</td><td>GPT</td><td>HYENADNA</td></tr><tr><td>Mouse Enhancers</td><td>69.0</td><td>66.9</td><td>80.1</td><td>85.1</td></tr><tr><td>Coding vs Intergenomic</td><td>87.6</td><td>92.5</td><td>88.8</td><td>91.3</td></tr><tr><td>Human vs Worm</td><td>93.0</td><td>96.5</td><td>95.6</td><td>96.6</td></tr><tr><td>Human Enhancers Cohn</td><td>69.5</td><td>74.0</td><td>70.5</td><td>74.2</td></tr><tr><td>Human Enhancers Ensembl</td><td>68.9</td><td>85.7</td><td>83.5</td><td>89.2</td></tr><tr><td>Human Regulatory</td><td>93.3</td><td>88.1</td><td>91.5</td><td>93.8</td></tr><tr><td>Human Nontata Promoters</td><td>84.6</td><td>85.6</td><td>87.7</td><td>96.6</td></tr><tr><td>Human OCR Ensembl</td><td>68.0</td><td>75.1</td><td>73.0</td><td>80.9</td></tr></table>\n\nSotA on 7 of 8 datasets and by up to  $20\\%$  points on the human enhancer identification task, as shown in Tab. 4.1. See A.2 for additional experiment details and ablations.\n\nNucleotide Transformer Next, we benchmark against 18 datasets from the Nucleotide Transformer (NT) (Dalla-Torre et al., 2023), which includes predicting regulatory elements for enhancers, promoters, epigenetic marks, and splice sites from DNA sequences of length 200-600 nucleotides. We compare against 3 NT base models, which were pretrained using masked language modeling (BERT) and then fine-tuned. The NT models ranged from 500M to 2.5B parameters, and pretrained on up to 3202 genomes. All NT models use 6-mer sequences of 1000 tokens long. For HyenaDNA, we attach a linear decoder head and fine-tune a pretrained model, surpassing SotA on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data, shown in Tab. 4.2. See A.2 for additional experiment details and ablations.\n\n# 4.3 In-context Learning for Genomic Sequences\n\nCompared to natural language FMs, which have shown strong success with in-context learning, HyenaDNA's vocabulary is very small. DNA sequences are also less diverse in structure, e.g. there's no concept of labels\n\nor descriptions that follow a DNA sequence. This makes it challenging to perform \"pure\" in-context learning (relying only on inference), since new concepts such as classification labels would require new symbols.\n\nTo overcome this limitation and explore the potential for in-context learning in genomics, we make use of two variants of in-context learning: soft prompting and instruction fine-tuning. Each involve a brief tuning phase to introduce the concept of classification using only the existing vocabulary.\n\nProcedure In both variants, we use the GenomicBenchmarks in 4.2, and a HyenaDNA model pretrained on sequence length 160k from 4.1.\n\nIn the first experiment, we evaluate a soft prompting approach by prepending a sequence of soft tuneable tokens (2 to  $32\\mathrm{k}$ ) directly in the input sequences. We include a brief tuning phase ( $< 20$  epochs), updating the soft tokens only, to provide HyenaDNA with the ability to indicate the target classes. To denote classes, we repurpose HyenaDNA's fixed vocabulary: for binary classification, for example, we indicate the two classes with the letters \"A\" and \"N\".\n\nIn the second experiment, we evaluate a few-shot learning approach to in-context\n\nTable 4.2: Nucleotide Transformer (NT) Benchmarks The Matthews correlation coefficient (MCC) is used as the performance metric for the enhancer and epigenetic marks dataset, and the F1-score is used for the promoter and splice site dataset.\n\n<table><tr><td>MODEL</td><td>NT</td><td>NT</td><td>NT</td><td>HyenaDNA</td></tr><tr><td>PARAMS</td><td>500M</td><td>2.5B</td><td>2.5B</td><td>1.6M</td></tr><tr><td># OF GENOMES</td><td>1</td><td>3,202</td><td>850</td><td>1</td></tr><tr><td>Enhancer</td><td>53.5</td><td>59.3</td><td>58.0</td><td>62.6</td></tr><tr><td>Enhancer types</td><td>48.5</td><td>50.0</td><td>47.4</td><td>55.7</td></tr><tr><td>H3</td><td>73.7</td><td>77.6</td><td>81.4</td><td>81.7</td></tr><tr><td>H3K4me1</td><td>35.8</td><td>44.5</td><td>55.9</td><td>57.1</td></tr><tr><td>H3K4me2</td><td>28.1</td><td>30.0</td><td>32.6</td><td>53.9</td></tr><tr><td>H3K4me3</td><td>26.3</td><td>28.1</td><td>42.1</td><td>61.2</td></tr><tr><td>H3K9ac</td><td>46.2</td><td>50.8</td><td>57.5</td><td>65.1</td></tr><tr><td>H3K14ac</td><td>37.7</td><td>47.1</td><td>55.0</td><td>66.3</td></tr><tr><td>H3K36me3</td><td>46.7</td><td>53.3</td><td>63.2</td><td>65.3</td></tr><tr><td>H3K79me3</td><td>57.7</td><td>59.2</td><td>64.2</td><td>71.6</td></tr><tr><td>H4</td><td>76.2</td><td>78.9</td><td>82.2</td><td>79.6</td></tr><tr><td>H4ac</td><td>34.4</td><td>42.3</td><td>50.1</td><td>63.7</td></tr><tr><td>Promoter all</td><td>95.4</td><td>96.6</td><td>97.4</td><td>96.5</td></tr><tr><td>Promoter non-TATA</td><td>95.6</td><td>96.9</td><td>97.7</td><td>96.6</td></tr><tr><td>Promoter TATA</td><td>94.8</td><td>95.8</td><td>96.4</td><td>96.7</td></tr><tr><td>Splice acceptor</td><td>96.5</td><td>98.5</td><td>99.0</td><td>96.6</td></tr><tr><td>Splice donor</td><td>97.2</td><td>98.2</td><td>98.4</td><td>97.3</td></tr><tr><td>Splice all</td><td>97.2</td><td>97.8</td><td>98.3</td><td>97.9</td></tr></table>\n\nlearning (Brown et al., 2020) by prepending, consecutively,  $k$  (2 to 32) demonstrations of each class and its sequence into the prompt. As before, we encode class labels by the use of individual letters of HyenaDNA's existing vocabulary. We additionally perform a brief instruction-tuning period (Wei et al., 2021) for each dataset to familiarize HyenaDNA with this task structure by tuning the pretrained model on a small subset of the dataset.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/33f48360-b496-4e4c-8e6b-30306d5c103e/3b56af64cb28b8d34d0188e396e41c97c057d1927f566d5cbabcc76967f65700.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/33f48360-b496-4e4c-8e6b-30306d5c103e/6126bc5fb2455e98c49117441081f8509a8d9500d7e8127ba5acc222e3714a82.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/33f48360-b496-4e4c-8e6b-30306d5c103e/89f97b3f6aef75e54109e20094679026de5931dc142c53074520df6d0430639b.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/33f48360-b496-4e4c-8e6b-30306d5c103e/891bf6631abfdebc94f31b371c0f38d3f44bf344876b10c068b87f91fcf0724e.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/33f48360-b496-4e4c-8e6b-30306d5c103e/b5cb14a07ff8c4686299b8f91f1d0316997182cdb6bd5f672f18af53ac4bba38.jpg)\n\nFigure 4.2: Filling long-context with soft tuneable tokens. HyenaDNA is able to learn new tasks in-context when adding a sequence of tuneable tokens to the input sequences. Longer sequences of tuneable tokens lead to better performance.\n\nResults In Fig. 4.2, HyenaDNA's performance on novel tasks improves as more tuneable tokens are added into the input sequences, and saturates close to baseline performance (Tab. 4.1; with the exception of the Human Regulatory dataset). By contrast, we find that increasing  $k$ -shot demonstrations to the input does not necessarily improve performance. A higher number of tuning samples is needed before  $k$ -shot demonstrations start to boost accuracy as shown in Tab. A.1. See A.3 for experiment details.\n\n# 4.4 Ultralong-Range Genomics\n\nIn our final experimental section, we focus on pushing the limits of using long context effectively in genomics. In 4.4.1, we tackle a challenging 919 binary multi-task against a sparse-attention baseline. In 4.4.2 we analyze the learned embeddings HyenaDNA and its use in clustering long sequences by functional annotation, and in 4.4.3 we showcase a novel ultralong-range species classification task.\n\n# 4.4.1 Chromatin Profile Prediction\n\nTable 4.3: Chromatin profile prediction Median AUROC computed over three categories: Transcription factor binding profiles (TF), DNase I-hypersensitive sites (DHS) and histone marks (HM).\n\n<table><tr><td rowspan=\"2\">MODEL</td><td rowspan=\"2\">PARAMS</td><td rowspan=\"2\">LEN</td><td colspan=\"3\">AUROC</td></tr><tr><td>TF</td><td>DHS</td><td>HM</td></tr><tr><td>DeepSEA</td><td>40 M</td><td>1k</td><td>95.8</td><td>92.3</td><td>85.6</td></tr><tr><td>BigBird</td><td>110 M</td><td>8k</td><td>96.1</td><td>92.1</td><td>88.7</td></tr><tr><td rowspan=\"2\">HyenaDNA</td><td>7 M</td><td>1k</td><td>96.4</td><td>93.0</td><td>86.3</td></tr><tr><td>3.5 M</td><td>8k</td><td>95.5</td><td>91.7</td><td>89.3</td></tr></table>\n\nThe prediction of chromatin profiles and epigenetic markers from DNA sequences is an impor\n\ntant and challenging task to quantify the functional effects of non-coding variants. These variants include single nucleotide changes in DNA that can affect the downstream expression of genes (Zaina et al., 2010). The DeepSEA dataset (Zhou and Troyanskaya, 2015) is compiled from 919 chromatin features including transcription factor (TF) binding profiles, DNase I-hypersensitive sites (DHS) and histone mark (HM) profiles. For a given sequence, the task is to jointly predict 919 labels corresponding to the chromatin profile (similar to peak detection) of a central region of the sequence, indicating the presence of such functional effects. The input also includes flanking regions that provide broader contextual information needed to incorporate long-range interactions. We fine-tune our pretrained HyenaDNA models from 4.1 and perform competitively against a DeepSea CNN and the SotA sparse attention BigBird (Zaheer et al., 2020) baselines using  $5 - 30\\times$  fewer parameters. See A.4 for experiment details.\n\n# 4.4.2 Biotype Embeddings\n\n# Sequence embeddings, colored by biotype\n\nDNABERT\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/33f48360-b496-4e4c-8e6b-30306d5c103e/0992b1d61e53796d0fb522219ea195e787acdb136fa65d20199d998ea07294dc.jpg)\n\nProtein Coding\n\nsnRNA\n\n$\\bullet$  IncRNA\n\nmiRNA\n\nNucleotide Transformer\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/33f48360-b496-4e4c-8e6b-30306d5c103e/0fd349cdab76ac16b227f8c4555b71801cc97515375a0252373d31492ef40a8c.jpg)\n\n- Processed Pseudogene\n\n$\\bullet$  TEC\n\nHyenaDNA\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/33f48360-b496-4e4c-8e6b-30306d5c103e/dcaa14323585b702aebd24c3b948618211afa551b75a29d5f222d44e1680ea5f.jpg)\n\nUnprocessed Pseudogene\n\nMiscRNA\n\nFigure 4.3: Embedding visualisation. t-SNE of the embeddings generated by DNABERT, Nucleotide Transformer and HyenaDNA coloured by Ensembl biotype annotations.\n\nNext, we analyze the pretrained embeddings from HyenaDNA and compare them with DNABERT (Ji et al., 2021) and the Nucleotide Transformer (Dalla-Torre et al., 2023). We encode sequences of human genes corresponding to different biological function annotations obtained from the Ensembl dataset known as\n\nbiotypes (Cunningham et al., 2022). In cases where the length of the input exceeds the context window of the encoder, the sequence is chunked (by the max length of the encoder) and averaged.\n\nWe fit the embeddings using an XGBoost (Chen and Guestrin, 2016) classifier on the 10 most frequent biotypes, and apply t-SNE (Van der Maaten and Hinton, 2008) for visualization. As shown in 4.3, distinct clusterings emerge visually, while quantitatively, HyenaDNA produces the highest F1 score in biotype classification (with a much smaller model), indicating that during pretraining, HyenaDNA learns informative features related to biological function.\n\nTable 4.4: Embedding quality Weighted F1 classification score on 10 biotypes.\n\n<table><tr><td>MODEL</td><td>PARAMS</td><td>LEN</td><td>F1</td></tr><tr><td>DNABERT</td><td>110 M</td><td>512</td><td>64.6</td></tr><tr><td>NT</td><td>500 M</td><td>6k</td><td>66.5</td></tr><tr><td>HyenaDNA</td><td>7 M</td><td>160k</td><td>72.0</td></tr></table>\n\n# 4.4.3 Species Classification\n\nThe majority of the genome is conserved across species  humans and non-human primates, for example, have  $< 10\\%$  sequence divergence (Rogers and Gibbs, 2014), making them difficult to discriminate. This allows us to design an ultralong-range sequence modeling task to test whether a model can determine the source species of a random genetic sequence. To train, we randomly sample DNA sequences from 5 different species, and fine-tune pretrained HyenaDNA and Transformer models from 4.1 to predict the species label. We observe in Tab. 4.5 that both models struggle on shorter sequences of length 1024, but performance improves with longer contexts as the distinct mutational profile of each species becomes more evident. HyenaDNA effectively solves the task by using a context length of 450k to 1 million, where Transformer cannot due to infeasible training time limitations. See A.6 for experiment details.\n\n# 5 Conclusion\n\nSummary We presented HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths up to 1 million tokens at single nucleotide resolution - an up to  $500\\mathrm{x}$  increase over previous genomic FMs using dense-attention. HyenaDNA is able to learn generalizable features that can then be fine-tuned for tasks including identifying regulatory elements and on a 919-way chromatin profile prediction task. We also explored the first use of in-context learning in genomics to enable simpler adaptation to downstream tasks without any updates to pretrained weights.\n\nLimitations and Future Work While demonstrating competitive results and introducing novel capabilities, it is worth noting that HyenaDNA was pretrained on only one human reference genome. Incorporating genomes of multiple humans and species could increase generalizability in learned features and reduce bias. Furthermore, our current focus in this study was exclusively on DNA sequences. Extending our framework to incorporate other biological or chemical sequences, such as pro\n\nTable 4.5: Species classification Top-1 accuracy  $(\\%)$  for 5-way classification (human, lemur, mouse, pig, hippo). The  $\\pmb{x}$  symbol indicates infeasible training time.\n\n<table><tr><td>MODEL</td><td>LEN</td><td>ACC</td></tr><tr><td>Transformer</td><td>1k</td><td>55.4</td></tr><tr><td>HyenaDNA</td><td>1k</td><td>61.1</td></tr><tr><td>Transformer</td><td>32k</td><td>88.9</td></tr><tr><td>HyenaDNA</td><td>32k</td><td>93.4</td></tr><tr><td>Transformer</td><td>250k</td><td>X</td></tr><tr><td>HyenaDNA</td><td>250k</td><td>97.9</td></tr><tr><td>Transformer</td><td>450k</td><td>X</td></tr><tr><td>HyenaDNA</td><td>450k</td><td>99.4</td></tr><tr><td>Transformer</td><td>1M</td><td>X</td></tr><tr><td>HyenaDNA</td><td>1M</td><td>99.5</td></tr></table>\n\nteins and drug molecules, has the potential to unlock multi-modal capabilities similar to those observed in natural language and vision FMs (Radford et al., 2021; Ramesh et al., 2021; Yu et al., 2022).\n\nWith respect to model size, HyenaDNA is significantly smaller than previous genomic FMs and was pretrained using up to 8 Nvidia A100 (80GB) GPUs. We expect increasing model size, and compute, may lead to additional long-range capabilities. Notably, with model parallelism, it becomes feasible to extend the context length by orders of magnitude beyond this current work, and leave that open to future research.\n\nFurthermore, beyond discriminative applications, the use of long context models in generative tasks unlocks exciting prospects for the design of synthetic regulatory elements, genes and protein complexes. In\n\nconclusion, the continued advancements of long-range sequence models with single nucleotide resolution hold great promise in driving innovation in genomic research and unraveling the complexities of biological systems.",
    "metadata": {
      "md_filename": "MinerU_markdown_HyenaDNA_20260106143006_2008425827142471680.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_HyenaDNA_20260106143006_2008425827142471680.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "efd1c089c8c5de90",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_LegNet_20260106143000_2008425799174848512",
    "title": "Sequence analysis",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# 2.1 Key ideas behind LegNet\n\nTrending deep learning applications in biology are biased toward large-scale attention transformer models (Fudenberg\n\net al. 2020, Avsec et al. 2021, Lin et al. 2023). Transformers indeed perform well in protein structure prediction and text mining, and particular applications of such models are successful in genomic data analysis (Fudenberg et al. 2020, Avsec et al. 2021). Yet, for the latter, it remains unclear whether it was the attention mechanism that contributed the most to their performance. Furthermore, depending on the test scenario, the quality of such models can be overestimated (Sasse et al. 2023), and they may in fact fail to capture long-range interactions (Karollus et al. 2023). In the context of parallel reporter assays with relatively short tested sequences, the application of a transformer-based approach might be an overcomplication compared to advanced fully convolutional networks. With this in mind, we took advantage of the EfficientNetV2 (Tan and Le 2021) with its recent success in image analysis and introduced several modifications to its architecture to account for specifics of the GPRA data.\n\n# 2.2 Experimental data overview\n\n# 2.2.1 Yeast GPRA data of the DREAM2022 challenge\n\nThe initial version of LegNet was constructed to solve the 'sequence-to-expression' problem of the DREAM2022\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/2089372b-84e5-4709-a436-cfcdcc6f7bda/02770b9c2cd83a608c1b6c045ebc5d4c48622ac4b0a035a02e3a9e67892a9d0a.jpg)\n\nFigure 1. Learning and predicting promoter expression and effects of single-nucleotide variants from massive parallel reporter assays with LegNet. (A) An overall pipeline. The regression task is reformulated as the soft-classification problem mirroring the original experimental setup where cells were sorted into different bins depending on reporter protein fluorescence. Bottom: sequence encoding and prediction of the expression bin probabilities with LegNet. (B) Variant effect estimation with LegNet. Both original and mutated promoter sequences are passed separately to the trained neural network. The variant effect is estimated as a difference between corresponding predictions and compared against the ground truth experimental data.\n\nchallenge (Rafi et al. 2023), i.e. predicting promoter activity from GPRAs (Vaishnav et al. 2022). The challenge data consisted of nearly 6.7 million promoter-driven expression measurements in the yeast Saccharomyces cerevisiae cultured in chardonnay grape juice. In the GPRA experiment, yeast cells were transformed with a plasmid containing the YFP gene controlled by 80-bp random DNA sequences put into a promoter-like context, and the constitutively expressed RFP gene. Based on logarithmic relative protein fluorescence, yeast cells were sorted into 18 expression bins (numbered 0-17). The expression estimate for a particular promoter sequence is calculated as a weighted average of the numbers of expression bins where it was found (de Boer et al. 2020).\n\nIn the DREAM challenge, the train set consisted of 6.7 million random promoter sequences. The labels for the independent test set of 71000 promoters were hidden during the challenge and included the higher quality measurements obtained for various classes of promoters, including synthetic sequences and native yeast promoters. The details of the challenge setup, including the train and test data and the description of individual subchallenges, are described in detail in Rafi et al. (2023).\n\nFor LegNet, we used the DREAM2022 challenge data (i) to build and verify the original model architecture, (ii) to perform an ablation study for identifying key model elements contributing to the final performance the most, and (iii) to analyze the performance of the LegNet-ensemble model.\n\n# 2.2.2 Previously published yeast GPRA data\n\nTo further showcase LegNet applicability and verify its predictive performance, we used previously published GPRA results (Vaishnav et al. 2022) that included 30 and 20 million promoter-driven expression measurements in the yeast S. cerevisiae cultured in two media, YPD (complex medium containing yeast extract, peptone, and dextrose) and SD-Ura (synthetic defined medium lacking uracil), respectively. In our study, we neither investigated nor interpreted biological differences between the respective datasets but considered them independent experiments to test the model on additional data.\n\nThe train and test datasets were taken as is from the Vaishnav et al. study (Vaishnav et al. 2022). A total of 20 616 659 (defined medium) and 30 722 376 (complex medium) random promoter sequences were used to train LegNet in each case. The test data were collected by Vaishnav et al. in independent experiments and included only the high-quality measurements obtained for native (i.e. present in the yeast genome) promoter sequences (3928 for the complex medium, 3977 for the defined medium), see the details in Vaishnav et al. (2022). A subset of the test data from the complex medium was used to compare the performance of LegNet against conventional deep learning methods (DeepSEA, DeepAtt, DanQ). To this end, we used 3733 promoter sequences for which the predictions of DeepSEA, DeepAtt, DanQ and Vaishnav et al. attention-based model were available in the GitHub repository that accompanied the original yeast GPRA study (Vaishnav et al. 2022): https://github.com/1edv/evolution.\n\nTo evaluate how LegNet captures the effects of minor sequence alterations, we used the 'genetic drift' data of (Vaishnav et al. 2022) where one to three single-nucleotide substitutions were introduced into 1000 random starting sequences assessed in both defined and complex media. The respective GPRA data are available in GEO (https://www.ncbi.nlm.nih.gov/geo/) under accession numbers GSE104878 and GSE163045.\n\n# 2.3 Sequence to expression as a soft-classification problem\n\nA straightforward application of machine learning to GPRA experimental data is a regression of a single real value, the expression defined by the cell sorting bin, from a fixed-length DNA sequence. However, such a direct approach cannot fully benefit from domain-specific knowledge and specifics of the GPRA experiment.\n\nWe have reformulated the basic sequence-to-expression regression problem as a soft classification task by transforming expression estimates into class probabilities. Given a measured expression  $e$  (the average of the observed bin numbers), we heuristically assume that the real expression is a normally distributed random variable, see Fig. 2b in de Boer et al. (2020):\n\n$$\n\\text {e x p r e s s i o n} \\sim N (\\mu = e + 0. 5, s d = 0. 5). \\tag {1}\n$$\n\nIn this approach, for each class  $i$  from 1 to 16 defined by an original measurement bin, a probability of the class is the cumulative probability of an expression estimate to fall into  $[i,i + 1)$  range, with 0 and 17 classes (bins) represented by special ranges of  $(-\\infty ,1]$  and  $[17, + \\infty)$ , respectively.\n\nThus, for the model loss, we selected the Kullback-Leibler divergence between the distribution derived from the training data and the model output vector containing 18 probabilities corresponding to each class (bin). To obtain a predicted expression value for a sequence during the expression inference step during model validation or test, the predicted probabilities  $p_i$  were multiplied by the corresponding bin numbers. This model layer, if joined with softmax, is called soft-argmax (Luvizon et al. 2017):\n\n$$\n\\text {e x p r e s s i o n} = \\sum_ {i = 0} ^ {1 7} i \\cdot p _ {i}. \\tag {2}\n$$\n\n# 2.4 LegNet architecture\n\nThe original LegNet model that won the DREAM2022 challenge (Supplementary Fig. S1A) is based upon a fully convolutional neural network architecture inspired by EfficientNetV2 (Tan and Le 2021) with selected features from DenseNet (Huang et al. 2018) and additional custom blocks.\n\nThe first LegNet block (Stem block) is a standard convolution with kernel size  $= 7$ , followed by BatchNorm and SiLU activation (Supplementary Fig. S1B). The output of the first block is passed to the sequence of six convolution blocks of EfficientNet-like structure (Supplementary Fig. S1C) but using the grouped convolution instead of the depthwise convolution of the original EfficientNetV2. The standard residual connections are replaced with residual channel-wise concatenation (Supplementary Fig. S1C). Padding mode 'same' is set for all convolutions. The resize block is of the same structure as the stem block at the start of the network (Supplementary Fig. S1B).\n\nThe Squeeze and Excitation (SE) block used as a part of EfficientNet-like block is a modification of that of the original EfficientNetV2 (Supplementary Fig. S1E). The number of parameters in the bilinear block inside of SE block is reduced with low-rank representation of the parameterized tensor via canonical polyadic decomposition implemented in TensorLy (Kossaifi et al. 2018) library.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/2089372b-84e5-4709-a436-cfcdcc6f7bda/93adaf85200de2deb64acdb32bc19347b296ca6ba45aac3ebcceccedb2438cf7.jpg)\n\nFigure 2. A schematic representation of LegNet application to the rational design of promoters in a cold diffusion framework. The hexagonal binning plot shows the correlation between desired (target) and observed (LegNet-predicted) expression for 110 592 designed promoters; the color scale denotes the number of promoters in a bin; Pearson and Spearman correlation coefficients are shown in the top left corner.\n\nThe final block consists of a single point-wise convolutional layer followed by channel-wise Global Average Pooling and SoftMax activation (Supplementary Fig. S1D). We used 256 channels for the first block and 128, 128, 64, 64, 64, and 64 channels for six EfficientNetV2-like blocks, respectively. The total number of parameters in the original LegNet model is 1852846.\n\n# 2.5 Adapting and augmenting GPRA data for deep learning\n\nTo prepare the data, first, we padded the promoter sequences from the  $5^{\\prime}$  end with the respective constant segments of the plasmids to achieve the total fixed length of 150 bps. Next, sequences were encoded into four-dimensional vectors with one-hot encoding.\n\nWe considered the integer expression estimates to belong to the 'singleton' promoters observed only once across all bins. The singletons are more likely to have noisier expression estimates, compared to other promoters with non-integer expression values obtained by averaging two or more observations. To supply this information to the model explicitly, we used a binary issingleton channel (1 for singletons, 0 for other training sequences). The final prediction for evaluation used issingleton  $= 0$ . Since the regulatory elements could be asymmetric with regard to their strand orientation and position relative to the transcription start sites, different scores are expected for the direct and reverse complementary orientation of a particular sequence. Therefore, the training data were augmented by providing each sequence both in native and reverse complementary form, explicitly specifying 0 and 1, respectively, in an additional is_reverse channel. We also performed the test-time augmentation by averaging the predictions made for direct (is_reverse  $= 0$ ) and reverse complementary (is_reverse  $= 1$ ) input of each promoter. A\n\nscheme of the input sequence representation is shown in Supplementary Fig. S2.\n\n# 2.6 LegNet training procedure\n\nTo train the original model, we used One Cycle Learning Rate Policy (OneCycleLR) (Smith and Topin 2017) with FastAI (https://www.fast.ai/) modifications: (i) two phases (instead of the original three), (ii) the cosine annealing strategy instead of the linear one, and (iii) the AdamW optimizer (weight Decay = 0.01) instead of the SGD with momentum. The parameters of the One Cycle Policy were selected using 1/10 of the training data of the DREAM challenge. To select the max learning rate (0.005) for the One Cycle Learning Rate Policy, we used the LR-range test as suggested in Smith and Topin (2017).\n\nEach epoch consisted of 1000 batches of size 1024. The model was trained for 150 epochs (defined medium), 300 epochs (complex medium), and 80 epochs (DREAM2022 chardonnay grape juice medium) achieving a reasonable tradeoff between training time and validation variance. For the original LegNet model, we used the hyperparameters based on the validation on the last  $k$ -fold (10th) of the training data, and the final model was trained from scratch on the whole training dataset.\n\nWe used the same weight initialization as in EfficientNetV2 (Tan and Le 2021). The training of the final model using the NVIDIA RTX A5000 GPU and PyTorch version  $1.11.0 + \\mathrm{cu}113$  took about  $12\\mathrm{h}$  for the defined medium data,  $24\\mathrm{h}$  for the complex medium data, and  $4\\mathrm{h}$  for the chardonnay grape juice medium data.\n\n# 2.7 LegNet ablation study and the optimized LegNet model\n\nTo identify the key elements of the LegNet architecture, we performed a hierarchical step-by-step analysis using the\n\nDREAM2022 train and test GPRA data. We sequentially checked the impact of (i) particular blocks and layers of the network, (ii) the choice of an optimization algorithm, (iii) the soft-classification approach and the singleton channel, and (iv) the reverse-complementary augmentation, see the details below. In each case, we trained five models with five different fixed starting seeds to estimate variability.\n\nWe checked the following features of the network architecture for their impact on model performance:\n\n1) Usage of SiLU activation before average pooling;\n\n2) Type of SE-block (comparing to the original EfficientNetV2 block);\n\n3) Type of residual connections (ResidualConcat versus ResNet Residual block);\n\n4) Different numbers of groups in grouped convolutions;\n\n5) EfficientNetV2 method to estimate the internal dimensionality of the respective block; and\n\n6) Overall number of blocks in the model.\n\nThe complete list of tested model variants is provided in Supplementary Table S1.\n\n# 2.8 Optimization algorithm and learning rate schedule\n\nWe checked if the AdamW optimizer can be replaced with a recent Lion optimizer (Chen et al. 2023), for which the learning rate was 10-fold reduced, and the weight decay was 10-fold increased, as suggested by the authors. Next, we tested if learning the OneCycleLR scheduler is indeed optimal compared to the widely used ReduceLROnPlateau. As the latter requires the validation subset not only for tuning hyperparameters but during the whole model training, in this comparison we used 10 models trained with 10-fold cross-validation, with the final performance measured on the independent test set.\n\n# 2.9 Diffusion model setup for rational promoter design\n\nTo adapt LegNet for the rational design of promoter sequences, we used an approach based on cold diffusion (Bansal et al. 2022). First, we used the original LegNet on several subsets of GPRA data and, with numeric simulation, estimated the expected number of mutations (300) that is sufficient to make the distribution of expression values indistinguishable from that of random sequences (Supplementary Fig. S3). Of note, the mutation process here and below does not have a memory, i.e. it does not check if it accidentally reverts back any previously introduced substitutions.\n\nNext, we trained LegNet-Generator (Supplementary Fig. S4) to iteratively correct multiple single-nucleotide substitutions in sequences with known expression values. As input, we supplied a promoter sequence with 0300 mutations, the respective number of mutations, and the expression of the initial sequence. The cross-entropy loss between the reconstructed and original sequence was used during the network training with the following parameters: 200 epochs, AdamW optimizer, learning rate 0.001, batch size 1024, 1000 batches per epoch, and 4 to 1 train-to-validation ratio.\n\nThe generation of sequences was performed by the iterative correction of mutations in a cold diffusion manner (Fig. 2). As input to LegNet-Generator, we supplied a random sequence, the number of mutations (0300), and the target expression\n\nvalue. The generated sequence was then changed by introducing  $n$ -1-shift substitutions and re-supplied to the LegNet-Generator with  $n$ -1 specified as the number of mutations. This cycle was repeated iteratively until the number of introduced mutations does not reach 0. The usage of a non-zero shift was necessary to trick the model into introducing more changes and drifting farther away from the starting sequence. Finally, we launched the LegNet-Predictor on the obtained sequences and compared the values against the originally defined expression values. As an illustrative example in this study, we used 100 iterations (with 100 introduced mutations at the first step) and the shift of 30, although further optimization of these parameters remains possible.\n\n# 3 Results\n\n3.1 LegNet accurately predicts promoter expression In this study, we present LegNet, a new fully convolutional neural network architecture inspired by EfficientNetV2 (Tan and Le 2021), see Supplementary Fig. S1. First, we evaluated LegNet in predicting native promoter expression for GRPA data from yeast grown in complex (YPD) or defined (SD-Ura) media. In both cases, LegNet demonstrated consistent performance, scoring significantly higher than the state-of-the-art transformer model published along with the GPRA data by Vaishnav et al. (Vaishnav et al. 2022) (Fig. 3). Note that the prediction 'wall' encountered at around expression levels 4 (complex) and 2.5 (defined) is a known issue with the training data also learned by models of (de Boer et al. 2020, Vaishnav et al. 2022), which is likely caused by the cell sorter having limited signal-to-noise ratio in this range or inadvertently truncated distribution. We also compared LegNet against earlier deep learning approaches tested in (Vaishnav et al. 2022) (Supplementary Fig. S5) highlighting the gap between LegNet  $(\\sim 0.96 - 0.98$  Pearson and Spearman correlation against the ground truth test data) and conventional deep learning models such as DeepSEA and DanQ (correlations around  $\\sim 0.92 - 0.94$ ).\n\n# 3.2 LegNet delivers accurate estimates of sequence variant effects\n\nIn the DREAM challenge, LegNet was highly successful in estimating the expression of promoters with single-nucleotide variants. To confirm it with independent data and further explore LegNet reliability in predicting the effects of multiple nucleotide substitutions, we utilized the GRPA data capturing expression divergence under random genetic drift. For 1000 unique random promoter sequences, Vaishnav et al. randomly introduced single-nucleotide mutations for three generations and measured the promoter expression in each.\n\nWe evaluated the capability of LegNet to quantitatively estimate the difference between expression for original and mutated promoter sequences depending on the number of nucleotide substitutions (1, 2 or 3) and compared the performance with the state-of-the-art transformer model of Vaishnav et al. Estimating the single-nucleotide variant effects was the most difficult; yet, LegNet showed significantly better prediction performance in all scenarios (Fig. 4).\n\n# 3.3 Identifying key performance-affecting elements of LegNet and designing the optimized architecture\n\nBy testing different versions of our model (see Section 2 and Supplementary Table S1), we have found a way to further\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/2089372b-84e5-4709-a436-cfcdcc6f7bda/d0381294014cb0bf1d5992db7d68ee9ac672588acf60ff252a03890c47d637f5.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/2089372b-84e5-4709-a436-cfcdcc6f7bda/3827a1c2308a6e4b548e95b0862525569558f80a6f03c3d42a4e8751cf712849.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/2089372b-84e5-4709-a436-cfcdcc6f7bda/d084b22c8e090c7fd97de7c4585afcd73698ea11ac3262f0c42a21148ecb5f2e.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/2089372b-84e5-4709-a436-cfcdcc6f7bda/44051fe41a7e29070a2d2165b321491ef5f1fbff9ddbbfb9520e86297d8686d4.jpg)\n\nFigure 3. LegNet accurately predicts promoter expression. Prediction of native promoter expression for yeast grown in complex medium (YPD, A) and defined medium (SD-Ura, B), hexagonal binning plots, the color scale denotes the number of promoters in a bin. Comparison of LegNet prediction performance for native yeast promoter sequences compared to the transformer model of Vaishnav et al.; (C) Pearson correlation between predictions and ground truth; (D) Spearman correlation; note the Y-axis lower limit. Violin plots show bootstrap with  $n = 10000$ . *P < 0.001, Silver's dependent correlations test (Silver et al. 2004) for the total data.\n\nimprove the original LegNet (Supplementary Fig. S6) on top of the original DREAM2022 version. The primary architectural changes did not affect the model performance significantly, except for the SE block, which had a major added value of more than  $+0.005$  for both Pearson and Spearman correlations as we optimize around 0.975. The other changes were less impactful and we decided to bring the optimized version of LegNet closer to the original EfficientNetV2.\n\nThus, the optimized LegNet uses a standard EfficientNetV2 SE-block instead of an originally used custom variant, depthwise instead of grouped convolutions, the original method of EfficientNetV2 to set the dimensionality of the EfficientNetV2-like block, and does not include activation at the final layer before average pooling. However, we kept ResidualConcat as residual connections were important for reaching optimal scores (see Supplementary Table S1). We also kept the original total number of blocks which provide a 79 base pairs receptive field that is close to the actual variable length of the tested promoter.\n\nThe resulting version of LegNet (2.1M parameters) was used in the consequent tests to prove the importance of soft classification instead of the direct regression of the promoter activity, the singleton channel, and the reverse-complement data augmentation, see Supplementary Table S1.\n\nIn terms of the optimizer used for the model training, a recently introduced Lion (Chen et al. 2023) provided improved performance over AdamW, and in terms of the learning rate scheduler, OneCycleLR was clearly superior compared to the widely used ReduceLROnPlateau. Notably, in terms of the performance differences, the training mode was more important than the minor features of the model architecture, except for the ablation of the SE block.\n\n# 3.4 Predictive performance can be further improved on top of the optimized LegNet\n\nOn top of the optimized model, we constructed and tested a series of ensemble models by averaging the predictions of 2-\n\n# Complex medium\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/2089372b-84e5-4709-a436-cfcdcc6f7bda/62bf07b7027fb5fc6b7725401f471d2e44213c82c8f41b2874ca1592f4cc8b32.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/2089372b-84e5-4709-a436-cfcdcc6f7bda/5109d207ffc6244871d009014181bca638aace28580dc9873e7131ed09c7fd0d.jpg)\n\n# Defined medium\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/2089372b-84e5-4709-a436-cfcdcc6f7bda/43bd3b62f7fda7167c16fa16e3e9e0212f46a384e016b1beacb4f14d35e0a033.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/2089372b-84e5-4709-a436-cfcdcc6f7bda/b3bb5e4f40d7e4f5a09422671c0eac92b6e2b268b2ae9d5b4b83af8c0303d16b.jpg)\n\nFigure 4. LegNet demonstrates better prediction of variant effects for yeast grown in complex (A and B) and defined (C and D) medium compared to the transformer model of Vaishnav et al. (A and C) Pearson correlation between predictions and ground truth; (B and D) Spearman correlation; note the Y-axis lower limit. Violin plots show bootstrap with  $n = 10000$ . * $P < 0.0001$ , Silver's dependent correlations test (Silver et al. 2004) for the total data.\n\n50 optimized LegNets trained with different starting seeds, see Supplementary Figure S7. Interestingly, the performance of the ensemble stably and significantly increased saturating at 0.9766 and 0.98 (Pearson and Spearman correlations, respectively) for averaged predictions of 50 models compared to 0.9756 and 0.979 of the single optimized LegNet. This suggests that, despite being the best-in-class, LegNet does not reach the theoretical bar set by the level of experimental variability and, with the currently available data, there remain further possibilities for improvements.\n\n# 3.5 Setting the ground for the large-scale rational design of regulatory sequences\n\nAs LegNet provides a best-in-class solution for predictive promoter expression, we further explored its capabilities in the rational design of sequences with a desired level of expression using the diffusion approach. First, we train the LegNet-Generator model to correct the artificial noise by reverting back point mutations introduced in sequences with known expression levels. Next, we perform iterative generation by applying LegNet-Generator to induce substitutions in a completely random sequence, i.e. by tricking the model to correct 'errors' in the provided random sequence so that upon full correction the resulting promoter provides a desired expression level. Finally, we verify the results with the LegNet-Predictor based on the original LegNet, see Fig. 2, Supplementary Fig. S4, and Section 2. The Pearson and Spearman correlation betweents the target (as requested) and actually generated (as predicted by the original LegNet) expression reach 0.839 and 0.843, respectively, with imperfect results in a low expression range, but a good agreement\n\nbetween the target and obtained expression for medium-to-highly expressed promoters.\n\n# 4 Discussion\n\nIn this study, we presented LegNet, an EfficientNetV2-based fully convolutional neural network (Tan and Le 2021) employing domain-specific ideas and improvements to reach accurate expression prediction from a DNA sequence.\n\nMultiple factors contribute to the overall LegNet performance, from the up-to-date global model architecture to domain knowledge that facilitated the proper data augmentation and allowed to reformulate the basic regression problem in a way to reflect the scheme of the underlying wet-lab experiment. The data preparation was extended over the standard one-hot encoding approach by discerning whether a target sequence was observed in the experiment only once (a singleton) or multiple times, as the singletons constitute more than half of the training data but eventually provide noisier expression estimates. Next, we augmented the data with reverse complementary sequences. Finally, we reformulated the expression prediction as a soft-classification problem: LegNet was trained to predict not the single expression value but a vector of expression bin probabilities and combine the predicted probabilities into a single predicted expression value at the model evaluation stage.\n\nThe exact values of LegNet parameters are likely specific to a particular GPRA experimental setup, so it is recommended to tune the parameters for better applicability of LegNet to other types of experiments or other species, e.g. to handle other random insert lengths, and adapt the soft classification\n\nstrategy to particular cell sorting techniques or cell typespecific sources of experimental variability.\n\nOf note, many studies of machine learning applications in biology attribute most of the credit to model architecture. However, here, we are showing the critical impact of the training scheme on the final model performance, that is an acknowledged effect in computer vision applications (Bello et al. 2021). Also, the performance of a single optimized LegNet model instance was further improved by the LegNet ensemble, suggesting that the potential for building better models using existing GPRA data has not yet been exhausted.\n\nFinally, we have presented a new model to generate promoters with desired expression level, which is a convenient and computationally efficient way compared to the straightforward greedy sequence expression optimization with a predictive model previously used, e.g. for guided selection and design of bacterial and yeast promoters (Wang et al. 2020, Kotopka and Smolke 2020). A more advanced approach was used for yeast  $5^{\\prime}$  - and  $3^{\\prime}$  UTRs where Wasserstein generative adversarial network was coupled with a predictive convolutional network (Zrimec et al. 2022). However, considering diffusion networks, there are few emerging attempts (Avdeyev et al. 2023), and, to our knowledge, the LegNet adaptation is the first generative diffusion framework for promoters built upon the massive data from parallel reporter assays.\n\nAll in all, by using the data from GPRAs, we have demonstrated LegNet's efficacy in predicting expression per se, quantitatively estimating the effects of sequence variants, and rationally designing promoters with desired expression. We have shown that LegNet shows notably better performance than conventional models and the previous state-of-the-art transformer model. Thus, while today the researchers' preference is biased toward complex architectures, we conclude that the fully convolutional networks should be considered a reliable approach to the computational modeling of short gene regulatory regions and predicting effects of regulatory sequence alterations.",
    "metadata": {
      "md_filename": "MinerU_markdown_LegNet_20260106143000_2008425799174848512.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_LegNet_20260106143000_2008425799174848512.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "1874292325105b89",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_Malinois_20260106142949_2008425751259127808",
    "title": "Machine-guided design of cell-type-targeting cis-regulatory elements",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# Training Malinois, a model of MPRA activity of CREs\n\nTo enable systematic evaluation of parameters governing data preprocessing, model architecture and training, we developed tools for limited automatic machine learning in PyTorch (https://github.com/sjgosai/boda2). We implemented support for regression based on DNA sequences using CNNs. We deployed a containerized application based on this library in conjunction with the Vertex AI platform on Google Cloud to tune all hyperparameters using Bayesian optimization.\n\n# Data preprocessing\n\nMalinois training. To construct the train/ validation/test dataset to train Malinois, we aggregated the  $\\log_2[\\mathrm{FC}]$  output of sequences tested in K562, HepG2 and SK-N-SH cells from multiple projects (OL indexed reference files are shown in Supplementary Table 1). The majority of projects focused on testing the allelic effects of human genetic variation with the remaining projects testing only the reference sequences of the human genome. In total, 798,064 unique oligos were aggregated, originating from ten independent experiments (from three different projects: UKBB (OL27, OL28, OL29, OL30, OL31, OL32, OL33), GTEx (OL41, OL42), OL15). The majority of the sequences used in our study (783,978) were designed to evaluate common human genetic variation associated with heritable complex traits. The majority of sequences (706,054) consisted of testing the reference and alternative allele, typically a single-nucleotide substitution, centred within 200 bp of flanking sequence. Additional sequences (77,924) evaluated the four pairwise combinations of two independent variants. Variants were selected on the basis of genetic fine-mapping, with most variants being linkage disequilibrium partners of causal alleles and therefore likely to not have a meaningful impact on cellular or organismal traits. The remaining sequences (14,086) originated from OL15, from which we selected the known DHSs and H3K27ac sequences. Oligos with a plasmid count of fewer than 20 or no RNA count in any cell type were discarded. If an oligo was present in more than one UKBB library, its  $\\log_2[\\mathrm{FC}]$  values were averaged across libraries. If an oligo in UKBB was also found in GTEx or OL15, only the UKBB readout was collected and the others were discarded. If an oligo in GTEx (but not in UKBB) was also found in OL15, only the GTEx readout was collected and the OL15 readout was discarded. Non-natural sequences from OL15 were discarded. Moreover, oligos with a  $\\log_2[\\mathrm{FC}]$  of 6 s.d. below the global mean were discarded (fewer than 10 oligos). Sequences were padded on both sides with constant sequences from the reporter vector backbone to form 600 bp sequences and converted into one-hot arrays (that is, A:= [1,0,0,0], C:= [0,1,0,0], G:= [0,0,1,0], T:= [0,0,0,1], N:= [0,0,0,0]). Oligos from chromosomes 19, 21 and X were held out from the parameter training loop as a validation set guide hyperparameter tuning. Oligos from chromosomes 7 and 13 were held out from both parameter training and hyperparameter tuning loops as a test set for reporting performance. Oligos from the remaining chromosomes were used in the training loop. Oligos containing alternative alleles were assigned to the same chromosomes as the reference allele oligos. Data augmentation was performed by including into the training set the reverse complement of the (600 bp) sequences, and duplicating oligos that had a  $\\log_2[\\mathrm{FC}]$  greater than 0.5 in any cell type. We also aggregated the  $\\log_2[\\mathrm{FC}]$  output of 318,247 and 442,482 sequences tested in A549 (OL27, OL28, OL29, OL30, OL31, OL32, OL33) and HCT116 (OL41, OL42) cells, respectively, according to the same count filtering steps as described above.\n\nTest set performance metrics and other analyses. For analyses outside Malinois' training loop that leverage the train/Validation/test sets, we aggregated the same 798,064 unique oligos mentioned above initially filtering out only oligos with an RNA count of zero before averaging the  $\\log_2[\\mathrm{FC}]$  across UKBB libraries (no plasmid count filter). Oligos with a  $\\log_2[\\mathrm{FC}]$  standard error greater than 1 in any cell type were then omitted\n\nfrom performance metrics (in the case of oligos with multiple instances across UKBB libraries, oligos of which the highest  $\\log_2[\\mathrm{FC}]$  standard error across libraries was greater than 1 were omitted). For locus-specific benchmarking, we aggregated the  $\\log_2[\\mathrm{FC}]$  of oligos that tile the GATA1 locus (OL43) according to the same count filtering steps as described above. We generated per-genome-base activity measurements by averaging the MPRA activity of each oligo that overlaps that base pair. We removed oligo genomic coordinates that overlap with those in the UKBB and GTEx libraries in scatterplots and correlation calculations.\n\n# Model architecture\n\nThe final Malinois model is composed of three functional segments: (1) three convolutional layers with batch normalization and maximum value pooling; (2) a fully connected linear layer to integrate positional and feature information from the previous hidden state after flattening; and (3) a stack of branched linear layers such that each output feature is a function of four independent transformations. As the first two segments are replicated from the Basset architecture $^4$ , Malinois accepts batches of  $4 \\times 600$  arrays corresponding to one-hot encoded DNA sequences, so predictions for 200-nucleotide MPRA oligos are made by padding inputs on both sides with constant sequences from the reporter vector backbone. This strict input sizing requirement ensures that hidden states are appropriately shaped when transitioning between segments 1 and 2 of the model. Furthermore, this padding strategy enables us to use reverse complement data augmentation with awareness of the orientation of the 200-nucleotide MPRA inserts with respect to the transcription start site in the reporter backbone. Although it was not tested in this study, replacing the final strict max pooling layer with adaptive pooling or padding would allow flexibility in the input sizing requirements while maintaining all other components of the architecture. At training initiation, weights were initialized using pretrained weights from a PyTorch implementation of Basset when segments 1 and 2 were appropriately configured.\n\n# Model fitting\n\nWe trained Malinois using the Vertex AI API on the Google Cloud Platform (GCP). This enabled optimization of all tuneable parameters controlling data preprocessing, model architecture and model training. To do this, first we generated a docker container (gcr.io/sabeti-encode/boda/production:0.0.11) with an installation of CODA using a GCP VM with the following specifications: Debian-based deep learning VM for Pytorch CPU/GPU operating system, a2-highgpu-1g machine type and 1 NVIDIA Tesla A100 40G GPU. The container entrypoint was set to a Python script for model training (boda2/src/main.py). Using this container, we deployed hyperparameter tuning jobs using the default algorithm to optimize the indicated hyperparameters (Supplementary Table 9). We include a notebook for deploying a hyperparameter tuning job using the Vertex AI SDK (boda2/tutorials/vertexsdk.launch.ipynb). We finalized model selection for Malinois by benchmarking candidates on the validation set using predictions calculated as described in the next section. All test set benchmarking was retrospective and did not impact decision making in the study. Two additional models were fitted using a subset of sequences tested in either A549 or HCT116 cells using identical hyperparameter configurations to Malinois.\n\n# Correlation of empirical and predicted MPRA activity\n\nWhen comparing Malinois' predictions to empirical MPRA, we discard any oligo with a replicate  $\\log_2[\\mathrm{FC}]$  standard error greater than 1 in any cell type (see section Data preprocessing above for more details). Malinois' predictions for the (padded) forward and reverse complement sequences are averaged into a single prediction.\n\n# Optimization of cell-type specificity\n\nThe objective function to guide the sequence design with simulated annealing (minimize energy) was the MinGap (Malinois  $\\log_2[\\mathrm{FC}]$\n\n# Article\n\nprediction in the target cell type minus the maximum off-target cell type  $\\log_2[\\mathrm{FC}]$  prediction). The objective function used with the algorithms Fast SeqProp and AdaLead (minimize or maximize, respectively) was the bent-MinGap, which is defined as follows. Let  $y_{+}$  be the Malinois  $\\log_2[\\mathrm{FC}]$  prediction on the target cell type, and  $y_{-}$  the maximum of the  $\\log_2[\\mathrm{FC}]$  predictions on the off-target cell types of a given sequence (so MinGap  $= y_{+} - y_{-}$ ). We constructed a bending function  $g(x) = x - e^{-x} + 1$  to preprocess predictions such that the objective function becomes bent-MinGap  $= g(y_{+}) - g(y_{-})$ . We applied  $g(x)$  to the predictions to incentivize greater MinGaps with low expression in the off-target cell types. For three generative algorithms, to prevent pathologically extreme activity predictions that are common in deep learning methods when computing on sequences highly divergent from the training data, we constrained predictions to a limited interval (default: [-2, 6]) when generating sequences.\n\n# Iterative maximization of sequence function using iterative, generative and evolutionary sequence generation algorithms\n\nFast SeqProp. FastSeqProp $^5$  was selected as a representative gradient-based local optimization method that exploits the structure of deep learning models to conduct greedy search while retaining the ability to pass true one-hot encoded inputs to the model. We implemented this algorithm as described in previous work, but we removed the learnable affine transformation in the instance normalization layer and drew many one-hot encoded samples from the categorical nucleotide probability distribution in each optimization step to more confidently estimate the gradients of the learnable reparameterized input sequence. The input parameters were randomly initialized (drawn from a normal distribution) and optimized using the Pytorch implementation of the Adam optimization algorithm with a learning rate of 0.5, along with a cosine annealing scheduler with a minimum learning rate of  $10^{-6}$  over 300 training steps. In each training step, the loss function value was the negative average bent-MinGap of 20 sequence samples drawn from the categorical nucleotide probability distribution at that step. Once optimization is finalized, instance normalization is applied to the learned input and 20 sequences were sampled from the obtained distribution and the sequence with the highest predicted bent-MinGap was collected unless the value was less than 3.6.\n\nAdaLead. AdaLead $^6$ , another greedy search algorithm, was selected as a representative evolutionary optimization algorithm for its ease of implementation and previously reported success in DNA sequence optimization. We implemented this algorithm as written in the GitHub repository associated with the original paper. In each run, 20 randomly initialized sequences are optimized over 30 generations with mu=1, recomb_rate=0.1, threshold=0.25, rho=2, using bent-MinGap as the fitness (objective) function. Once optimization is finalized, only the sequence with the highest predicted bent-MinGap is collected unless the MinGap was less than 2. We chose to collect only one sequence per run to maximize diversity in the global batch collected from all runs.\n\nSimulated annealing. Simulated annealing<sup>7</sup> was selected as a representative probabilistic optimization algorithm based on a decades-long history of successful application to a wide range of domains for nonconvex optimization. Simulated annealing starts by jumping between regions with different local optima by occasionally accepting proposals that deteriorate the objective when the sampling temperature is high early in the algorithm. In later stages, the algorithm shifts toward greedy hill climbing as low sampling temperatures only allow proposals that improve the objective to be accepted. We implemented simulated annealing based on the Metropolis-Hastings algorithm for Markov chain Monte Carlo simulations. Proposals were generated symmetrically at each step by mutating three random bases. We used negative MinGap (without bending) to simulate the energy landscape of the theoretical system. During optimization, the temperature term was\n\nreduced using a monotonically decreasing function with a diverging infinite sum:\n\n$$\n\\tau = \\frac {1}{1 + s ^ {0 . 5 0 1}}.\n$$\n\nTo produce sequences with high target-specific activity we used negative MinGap (without bending) to simulate energy of the system.\n\n# Motif penalization\n\nTo design a batch of sequences penalizing the enrichment of given motifs in the batch, we introduced to the loss function an additional term explained below. To penalize a single motif of length  $l$ , we construct the motif position-weight matrix (PWM; also known as position-specific scoring matrix, or log probabilities) and use it to score all possible subsequences  $x_{j}$  of length  $l$  in the batch. Let  $s_{j} = \\mathrm{PWM}(x_{j})$  be the motif score of the subsequence  $x_{j}, n$  the number of sequences in the batch, and  $t$  a score threshold. Then, we define the motif penalty as\n\n$$\n\\frac{1}{n}\\sum_{j:s_{j}\\geq t}s_{j},\n$$\n\nwhere  $j$  iterates over all the possible subsequences including their reverse complements. In other words, we sum all the motif scores above the score threshold and divide by the size of the batch. When penalizing  $m$  motifs, the term we introduce is very close to simply averaging the  $m$  motif penalties, except that we introduce a weighting factor for each motif penalty to emphasize the penalization of motifs with lower indices (or in our case below, to prioritize motifs based on their order of inclusion to the motif pool). If we let  $s_j^{(i)} = \\mathrm{PWM}^{(i)}(x_j)$  be the motif score of motif  $i$  of the subsequence  $x_{j}$ , and  $\\tau^{(i)}$  the score threshold of motif  $i$ , then the total motif penalty given a motif pool  $\\{\\mathrm{PWM}^{(1)}, \\dots, \\mathrm{PWM}^{(m)}\\}$  is defined as\n\n$$\n\\frac{1}{mn}\\sum_{i\\in [m]}(m - i + 1)^{\\frac{1}{3}}\\sum_{j:s_{j}^{(i)}\\geq t^{(i)}}s_{j}^{(i)},\n$$\n\nwhere the term  $(m - i + 1)^{1/3}$  is the weighting factor increasing the value of the motif penalties with lower index  $i$ .\n\nWe used this motif penalty expression to iteratively design sequences subject to an increasing pool of motifs. We call these iterations penalization tracks. A single penalization track starts with the generation of a batch of 500 (non-penalized) sequences, which is then analysed for motif enrichment (top 10 motifs of length 8 to 15) using STREME through a Python wrapper function. We collect the top motif  $\\mathsf{PWM}^{(1)}$  from the analysis and design a second batch of 250 sequences (which we call round-1 penalized sequences) penalizing the motif pool  $\\{\\mathsf{PWM}^{(1)}\\}$ . We then extract the top motif  $\\mathsf{PWM}^{(2)}$  enriched in the round-1 penalized sequences and design a third batch of 250 sequences (round-2 penalized sequences) penalizing the motif pool  $\\{\\mathsf{PWM}^{(1)}, \\mathsf{PWM}^{(2)}\\}$ . We continue this process till we generate 250 round-5 penalized sequences penalizing the motif pool  $\\{\\mathsf{PWM}^{(1)}, \\mathsf{PWM}^{(2)}, \\dots, \\mathsf{PWM}^{(5)}\\}$ .\n\nWe generated four penalization tracks for each target cell type, for all three cell types. We defined the score threshold for each motif as a percentage of the motif score of its consensus sequence. The percentages used were 0 for K562-target sequences, and 0.25 for HepG2- and SK-N-SH-target sequences. The reason behind the different choice for K562 cells is that we found that the optimization process could more easily escape the penalization of GATA by still using suboptimal instances of the motif, so a more stringent penalty was of interest for us. The motivation for using a weighting factor was that we hypothesize that sequence design optimization gravitates more strongly to motifs captured in enrichment analyses of early penalization rounds, so we wanted to keep emphasizing the penalization of motifs extracted from earlier rounds.\n\nIn Supplementary Note 2, the motif-presence score ( $y$  axis) of a motif in each sequence was calculated by summing all the motif-match scores that pass the Patser score threshold (as defined in Biopython $^{83}$ ), and then dividing by the maximum possible motif score (the match score of the motif consensus sequence).\n\n# $k$ -mer analysis\n\nWe calculated 4-mer and 7-mer content for sequences in the CODA MPRA library as well as various other sets of reference sequences including 200-mers upstream of RefGene annotated transcription start sites, shuffled CODA sequences and random 200-mers. We calculated the average Manhattan distance to the  $k$ -nearest neighbours distances for 200-mers ( $k = 4$ ) by splitting sequences into groups based on design method, target cell line and penalty level and using the NearestNeighbors module from scikit-learn (v.1.2.2). We embedded sequences in two-dimensional space based on 4-mer content using the uniform manifold approximation and projection implemented by the umap-learn (v.0.5.2) Python package.\n\n# Homology search using Nucleotide BLAST\n\nWe conducted a homology search using NCBI ElasticBLAST to determine whether synthetic sequences had measurable homology to any sequences in the Nucleotide Collection. We used the BLASTn algorithm, the dc-megablast task and a word size of 11 and maintained the defaults for all other settings.\n\n# Selection of naturally occurring cell-type-specific sequences by DNase and Malinois-driven GenomeScan\n\nDHS-natural. To identify CREs broadly replicating across experimental approaches, using a uniformly processed dataset from ENCODE, we first selected DNase peaks from each of the three cell lines (K562, HepG2 and SK-N-SH). To further select for active CREs, we subsetted DHS peaks that intersect with H3K27ac peaks from the same cell type. For each cell type, we then identified cell-type-specific peaks by requiring a that a  $\\mathrm{DHS^{+}H3K27ac^{+}}$  peak had no overlap with a DHS peak in the other two cell types. For these DHS-H3K27ac peaks, in each cell type, we scored the K562, HepG2 and SK-N-SH DHS signal in the peak coordinates of the target cell type. We then selected the top 4,000 peaks with the highest ratio of on-target cell type's DHS signal to the maximal off-target cell type's DHS signal, mirroring our efforts to maximize MinGap of  $\\log_2$ -space MPRA activity with other CREs.\n\nMalinois-natural. To nominate cell-type-specific natural sequences with Malinois, we tiled the whole human genome into 200 bp windows using a 50 bp stride and generated predictions for each window sequence. The cell-type specificity of each sequence was obtained by evaluating the objective function mentioned above (bent-MinGap), and the top 4,000 best-performing sequences were selected for each cell type.\n\n# Genome annotation of natural sequences\n\nMalinois-natural sequences capture a unique component of the genome compared with DHS-natural, with  $2.7\\%$  of Malinois-natural sequences overlapping sequences in our DHS-natural set, and  $65.8\\%$  residing outside any previously annotated CREs. cCRE BED files for promoter-like sequences, proximal enhancer-like sequences, distal enhancer-like sequences and CTCF-only were downloaded from the ENCODE SCREEN Portal<sup>12</sup> and concatenated into a single BED file for intersection with DHS-natural and Malinois-natural BED files using a custom script. Intersections were performed using bedtools (v.2.30.0)<sup>84</sup> and pybedtools (v.0.9.0)<sup>85</sup> with the following command 'Malinois/DHS-natural_BED.intersect(ENCODE_cCRE BED,wa=True,u=True)' and the number of intersections was reported. To determine the genomic features overlapping DHS-natural and Malinois-natural sequences, the same BED files were used as an input for annotatePeaks.pl from the homer suite (v.4.11)<sup>86</sup> with the following command ' annotatePeaks.pl inputBED\n\nhg38 -annStats annStats.txt > annotatePeaksOut.txt'. Annotations for the whole genome (hg38) were generated by dividing the genome into 200 bp intervals using the bedtools makewindows command 'bedtools makewindows-g hg38.txt -w 200 > hg38_200bp.bed'. Annotations were generated for each cell type (K562, HepG2, SK-N-SH) and sequence selection method (DHS-natural, Malinois-natural).\n\n# Sampled integrated gradients to compute contribution scores of Malinois predictions\n\nWe calculated nucleotide contribution scores for each sequence in the proposed library using an adaptation of the input attribution method Integrated Gradients $^{58}$ . Sampled Integrated Gradients (SIG) considers the expected gradients along the linear path in log-probability space from the background distribution to the distribution that samples the input sequence almost surely. In each point of the linear path, a sequence probability distribution (also known as a position probability matrix (PPM)) is obtained from the log-probability space parameters by applying the SoftMax function along the nucleotide axis, and a batch of sequences is sampled from that distribution to be fed into the model. We then calculate the gradients of the batch model predictions with respect to the parameters in the log-probability space, using the straight-through estimator to backpropagate through the sampling operation. The batch gradients are averaged for each point in the path and approximate the gradient integral as in the original formulation of the method. In our case, the subtraction of the baseline input from the input of interest involves the parameters in log-probability space. This adaptation of Integrated Gradients provides two useful features. First, the sequence inputs being fed to the model are always in one-hot form, avoiding evaluations of inputs off the vertices of the simplex on which the model was trained which could more easily lead to pathological predictions. Second, the original method relies on choosing an appropriate single baseline input against which to compare the input of interest, which might not always be straightforward, whereas our adaptation uses a background distribution of sequences as the baseline. Favourably, when choosing the uniform background (0.25, 0.25, 0.25, 0.25), the parameters in log-probability space where the line path is traversed become the zero matrix, which removes the need to subtract the baseline from the input of interest. We can then more easily extract integrated gradients for all tokens in all positions (by omitting masking the gradients with the one-hot input), which we found useful as hypothetical scores for TF-MoDISco.\n\n# Contribution block ablation\n\nTo test the value of contribution scores obtained with SIG, we conducted an in silico ablation study of the library sequences using contribution blocks (defined below) to randomize segments of the sequences. The goal of the study was to investigate the predicted  $\\log_2[\\mathrm{FC}]$  effects of randomizing positions within the sequences corresponding to blocks of either positive or negative contribution, or random positions outside blocks. The result of the study is summarized in Supplementary Note 4. Overall, randomizing segments of the sequences associated with negative contributions resulted in an increase in the predicted activity in either the target or off-target cell type, while randomizing those associated with positive contribution completely destroyed the activity in the target cell type, and marginally decreased the (already repressed) activity in off-target cell types. To make calls of contribution blocks in any given sequence, we took the 200 contribution scores and built a smoothed contribution signal using a one-dimensional Gaussian filter (scipy.ndimage.gaussian_filter1d) with a sigma of 1.15. We defined a positive contribution block whenever the smoothed signal was above a threshold of 0.015 for 4 contiguous positions or more, and negative whenever it was below 0.015 for 4 contiguous positions or more. Outside positions were those not assigned to a contribution block. For each target cell type group (25,000 sequences), contribution block calls and ablations were performed for all three prediction\n\n# Article\n\ntasks. For example, taking the K562-target sequences, three different ablations and call sets were carried out: (1) block calls using contribution scores in K562 cells assessing the K562 activity effect (target cell type); (2) block calls using contribution scores in HepG2 cells assessing the HepG2 activity effect (off-target cell type); (3) and block calls using contribution scores in SK-N-SH cells assessing the SK-N-SH activity effect (off-target cell type). This resulted in a total of nine sets of calls and ablations. When assessing the effect of disrupting positions outside contribution blocks, we subsampled the outside coverage (number of positions not in blocks) to match the upper half of the distribution of coverage sizes of positive and negative contribution blocks together, whenever possible. For the SK-N-SH-target group, for example, such a distribution match was not possible as the total number of available positions from which to sample was simply not large enough globally. The same was true for the target cell type outside ablation in K562 and HepG2 cells, which might be expected as positive contribution blocks alone have large coverages. We performed this outside subsampling to have comparable ablation sizes across categories, but also because disrupting all of the positions outside blocks that have low coverage (resulting in very high outside coverages) introduces too much noise into the sequence when most of the sequence is disrupted. We set a minimum of five positions to be disrupted by outside coverages.\n\n# Propeller plots\n\nA propeller dot plot (Fig. 2e (top row)) is a two-dimensional plot scheme of our own device that seeks to elucidate the cross-dimensional non-uniformity of three-dimensional points. In this coordinate system, a point's radial distance from the origin corresponds to the difference between the maximum and minimum values. Its deviant angle from the axis corresponding to the maximum value quantifies the position of the median value within the range of the minimum and maximum values. Namely, the angle is proportional to the ratio between two differences: (1) the difference of the median and minimum values; and (2) the difference of the maximum and minimum values. This ratio represents the  $60^{\\circ}$ -angle fraction deviating from the axis corresponding to the maximum value towards the axis corresponding to the median value. A higher angle of deviation (maximum of  $60^{\\circ}$ ) indicates that the median value is closer to the maximum value, while a lower angle (minimum of  $0^{\\circ}$ ) of deviation indicates that the median value is closer to the minimum value.\n\nThis can also be formulated in terms of the MinGap (maximum - median) and MaxGap (maximum - minimum). In our coordinate system, the MaxGap corresponds to the radial distance. The difference (1 - MinGap/MaxGap) corresponds to the  $60^{\\circ}$ -angle fraction deviating from the axis corresponding to the maximum value towards the axis corresponding to the median value. The MinGap:MaxGap ratio controls how much a point gravitates toward a main axis and away from the in-between-axis areas. A ratio of 0 means that the MinGap is zero and therefore the median value is equal to the maximum, so the point will be exactly between two axes. If the ratio is 1, it means that the median and the minimum values are equal, therefore the point will fall exactly in the axis corresponding to the maximum value. Note that, for this point of view to work with target and off-target cell type activities, we assume that the maximum cell type activity is the intended target cell type. This implies that, when counting sequences that pass specificity thresholds in Fig. 2e, some sequences get their target cell type reassigned to the cell type with the maximum activity, with DHS-natural sequences being the group that most benefits from the reassignment. A total of 652 sequences pass the lenient specificity threshold of MaxGap > 1 and MinGap/MaxGap > 0.5 by getting their target cell type reassigned (DHS-natural, 565; Malinois-natural, 39; AdaLead, 12; Simulated Annealing, 5; FastSeqProp, 0; FastSeqProp penalized, 4). However, only 16 sequences pass the stringent specificity threshold of MaxGap > 4 and MinGap/MaxGap > 0.5 by getting their target cell type reassigned (DHS-natural, 15; Malinois-natural, 0; AdaLead, 1; Simulated Annealing, 0; FastSeqProp, 0; FastSeqProp penalized, 0).\n\nAs an example of coordinate calculation, take the point  $(5, 3, 1)$ . This point would have a radial distance of  $5 - 1 = 4$  and an angle of deviation from the axis of the first dimension of  $(3 - 1) / (5 - 1) \\times (60^{\\circ}) = 30^{\\circ}$  (in the direction of the axis of the second dimension). In terms of the MinGap:MaxGap ratio, the angle of deviation from the axis of the first dimension (the dimension of the maximum value) towards the axis of the second dimension would be  $(1 - (5 - 3) / (5 - 1))(60^{\\circ}) = 30^{\\circ}$ . Observe that all the points of the form  $(x + 4, x + 2, x)$ , for any real value of  $x$ , will have the same coordinates as the point  $(5, 3, 1)$ .\n\nA propeller count plot (Fig. 2e (bottom row)) shows the percentage of points that fall in each given area of a propeller dot plot. The teal, yellow and red regions capture sequences in which the median value is closer to the minimum value than to the maximum value. Teal, yellow and red areas represent sequences in which the MinGap:MaxGap ratio is greater than 0.5.\n\nThe two synthetic groups in Fig. 2e were randomly subsampled to have exactly 12,000 sequences each and avoid over-plotting compared to the plots of the two natural groups. Supplementary Fig. 9 shows the complete propeller plots broken down by design method.\n\nOligos with a replicate  $\\log_2[\\mathrm{FC}]$  standard error greater than 1 in any cell type were omitted from the plots.\n\n# Motif discovery\n\nWe used TF-MoDISco Lite $^{59,60}$  to extract sequence motifs to be predicted as functional by Malinois through contribution scores obtained through SIG. As described above, SIG naturally provides hypothetical contribution scores (as defined by TF-MoDISco) when selecting the uniform random background by simply carrying out the equivalent of the full process minus masking out using the input sequence one-hot matrix. The final contribution scores can then be retrieved masking out the hypothetical contribution using the input sequence one-hot matrices, as required by TF-MoDISco. We computed hypothetical contribution scores for each of the three prediction tasks and ran TF-MoDISco Lite with 100,000 seqlets and a window size of 200 (equivalent results were obtained using 1,000,000 seqlets). We aggregated the discovered patterns across prediction tasks following their provided example using modiscolite.aggregator.SimilarPatternsCollapseer. TF-MoDISco Lite results are provided as positive and negative patterns.\n\n# TF-MoDISco patterns to PwMs\n\nTo convert a TF-MoDISco positive pattern living in the hypothetical-contribution-score space into a PWM, we divided the pattern scores by the maximum position score sum and multiplied by 10. To obtain the PPM, we applied the SoftMax function to each position vector. Some of our TF-MoDISco negative patterns are a combination of a negative pattern (negative contributions) and a positive one (positive contributions). Thus, to convert a TF-MoDISco negative pattern into a PWM, we first reversed the sign directionality of the negative portions (as informed by the pattern scores living in contribution-score space, not hypothetical) and compensated their magnitude by multiplying by 1.2 (because our negative contribution scores are in general smaller in magnitude than positive ones perhaps due to the nature of the training data target distribution that has a positive bias). We then proceed as for the positive patterns.\n\n# Core motifs (TF-MoDISco)\n\nAs TF-MoDISco, in addition to capturing isolated ungapped motifs, is able to capture patterns that are combinations of motifs, we heuristically extracted core ungapped patterns that, to varying degrees, account for all the of the combinations observed in the TF-MoDISco merged results. To manually define the starts and stops of core motifs, we relied on scoring the full pattern PWMs against themselves using TOMTOM $^{87}$ , information content contours and visual examination. The core motif IDs are derived from the IDs of the original patterns from which they were extracted. To convert the patterns into PWMs and\n\nPPMs, we applied the same operations as described above. Matches to human known TF-binding motifs were assigned using TOMTOM with the default parameters against the databases JASPAR CORE (2022) $^{61}$  and HOCOMOCO Human (v11 FULL) $^{62}$ .\n\n# Core motifs (STREME)\n\nIn addition to extracting sequence motifs with TF-MoDISco, we also performed a motif enrichment analysis using STREME. First, to assess the agreement between a given STREME motif and its predicted functionality as measured by contribution scores, we performed weighted-averaging of the hypothetical contribution scores corresponding to all the sequence segments that were determined to be a match to the motif (as provided by FIMO with default parameters, using motif scores as weights), and compared the score averages (one set of averages per prediction task) to the motif's information-content matrix. We will refer to the weighted average hypothetical scores as the contribution-score projection. All motifs with overall positive contribution scores that had a strong agreement with their contribution-score projection had been already captured by TF-MoDISco, suggesting that the TF-MoDISco positive pattern results are very comprehensive. However, we found a small number of STREME motifs with negative contribution scores that had a strong agreement with their contribution-score projection, so we decided to include them to the list of core motifs. Note that these motifs had negative contribution scores with moderate-to-low magnitude. We speculate that the reason TF-MoDISco might not have been able to detect them is because the contribution allocated in the seqlets that would correspond to these motifs too often falls below the threshold of the distribution of negative scores, making it hard to discriminate them from noise or insignificant scores. Running TF-MoDISco with 1 million seqlets did not change the results. We retrieved 11 such STREME motifs with strong agreement with their contribution-score projection not captured by TF-MoDISco, 9 of which were clustered together into 3 groups with nearly identical contribution-score projection (up to 1 or 2 additional positions to the left or right). This gave us a total of five STREME negative patterns in contribution-score projection form that were included to the list of core motifs. Their conversion to PWM and PPM forms followed the same process as for the TF-MoDISco patterns. Matches to human known TF-binding motifs were assigned using TOM-TOM with the default parameters against the databases JASPAR CORE (2022) $^{61}$  and HOCOMOCO Human (v11 FULL) $^{62}$ .\n\n# Contribution score-based motif scan\n\nTo find instances of the core motifs present in the CODA sequence library, we leveraged the hypothetical contribution scores of the sequences to match sequence segments to the core motifs in hypothetical-contribution-score form. First, we padded with zeros left and right all the sequence hypothetical contribution scores, yielding a matrix of dimensions  $3 \\times 75,000 \\times 4 \\times 210$ . Second, for a core motif of length  $l$ , we computed all the Pearson correlation coefficients between every possible subsequence hypothetical contribution scores of length  $l$  (matrices of size  $75,000 \\times 4 \\times l$ ) and the core motif's hypothetical contribution scores in forward and reverse complement orientations. For each cell type dimension, we randomly sampled 500,000 Pearson correlation coefficients (arising from a single core motif) to obtain the value  $\\min(0.75, \\mu + 4\\sigma)$  to serve as a coefficient threshold, where  $\\mu$  and  $\\sigma$  represent the mean and the s.d., respectively, of the subsampled distribution. All subsequences for which the hypothetical contribution scores scored above their coefficient threshold were collected as motif hits for the given core motif. We repeated this process for all core motifs across all cell types.\n\n# Motifs embedded in random background\n\nWe embedded single motifs in random sequences to measure their standalone predicted effect compared to fully random sequences. For each motif, we built a  $200 \\times 4$  PPM consisting of the motif's PPM in the\n\nmiddle and random background ([0.25, 0.25, 0.25, 0.25]) everywhere else. We sampled 5,000 sequences from it and fed them to Malinois to obtain predictions in each cell type. We also sampled 5,000 sequences from a  $200 \\times 4$  PPM of uniform background everywhere (no motif in the middle), and fed them to Malinois to serve as baseline.\n\n# Motifablation\n\nWe sought to assess the predicted effect of disrupting all instances of a single motif in our sequence library. For each motif, we collected the particular batch of sequences that had at least one instance of such motif, replaced all of the instances with random segments (sampled from uniform background), and fed them to Malinois to obtain predictions in each cell type. We performed this step five times, averaged the five predictions of each disrupted sequence and subtracted from the average the batch's original predicted activities to obtain the predicted disrupting effect. For example, say that a sequence has one instance of a given motif in positions 20-32. We inserted a random sequence segment in those positions and got the disrupted sequence's predictions. We did this five times, so five different random segments (with five different predictions) in positions 20-32, and averaged the five predictions (to mildly marginalize potential effects of replacing with random segments). The disrupting effect would be this average prediction minus the sequence's original predicted activity. We aggregated the disrupting effects by motif presence (as defined above in the last paragraph of motif penalization in this section). To find instances of core motifs, we used the contribution-score-based motif scan described above. To find instances of the original TF-MoDISco patterns, we used FIMO (with the default parameters), as our contribution score-based motif scan might not handle gapped patterns as well as FIMO. When submitting the pattern PPMs to FIMO, we trimmed the patterns at both ends such that the start/stop of the pattern is the first/last position to have an information content of at least 0.15 bits.\n\n# Motif contributions\n\nTo get a motif's overall contribution, we performed a weighted average of the contribution score sums contained in all of the motif instances provided by our motif hit method across the three prediction tasks. The average was weighted using the motif scores corresponding to the Pearson correlation coefficients mentioned above. The overall regulatory directionality of a motif (activator or repressor) is given by the sign of the mean of the weighted averages across cell types. For all motifs, the overall regulatory directionality agrees with the original TF-MoDISco designation as a positive or negative pattern.\n\n# Motifco-occurrence\n\nWe say that a pair of motifs co-occur whenever a sequence has at least one instance of each motif. By co-occurrence percentage of a motif pair, we mean the percentage of sequences in a given group in which the motif pair co-occurs.\n\n# NMF analysis of motif programs\n\nWe used NMF, a parts-based representation of data $^{74}$ , to model semantic relationships between motifs in our sequence library (scikit-learn v.1.2.2, initialized with NNDSVDAR, Frobenius loss). First we counted motif matches in each sequence with the contribution score-based motif scan described above $^{88}$  to generate  $X \\in \\mathbb{N}^{n \\times f}$ , where rows represent sequences in the library and columns correspond to motifs. The sample matrix  $X$  can then be decomposed into the coefficients and features matrices  $W \\in \\mathbb{R}^{n \\times k}$  and  $H \\in \\mathbb{R}^{k \\times f}$ , respectively. These  $k$ -dimensional representations are referred to as 'topics' in natural language processing and 'programs' in gene expression analysis $^{89,90}$ . These programs capture the frequency of TF motifs appearing in semantically similar CREs, and the CREs themselves are modelled as compositions of programs. We tested decomposing sequences into  $k \\in [8,28]$  programs using bi-cross-validation $^{91}$  and identified an 'elbow'\n\n# Article\n\nin the reconstruction error at  $k = 12$  (data not shown). When plotting the coefficient matrix comparative analysis, we normalize the coefficient matrix such that the rows sum to 1. We quantified the function of each decomposed program by calculating a weighted average of motif contributions (see the 'Motif contributions' section above) for each program using the motif weights in the features matrix. Motif contributions were clipped to an upper bound of 3 to mitigate the impact of extreme outliers.\n\n# MPRA saturation mutagenesis plot\n\nThe saturation mutagenesis study (Supplementary Table 10) of the sequence in Fig. 4g consisted in empirically testing the activity of all the possible 600 variants of the sequence (3 variants per position, 200 positions). We followed an identical protocol to the previous MPRAs in SK-N-SH cells with this saturation mutagenesis library. We visualized the effect of each variant as the subtraction of the activity of the original sequence from each variant-sequence's activity, resulting in the lollipops in Fig. 4h. The mean variant effect is represented in the height of the logo sequence letters but in the opposite direction.\n\n# CODAMPRA\n\nMPRA library construction. The CODA MPRA library was constructed according to previously described protocols<sup>8</sup>. In brief, oligos were synthesized (Twist Bioscience) as 230 bp sequences containing 200 bp of genomic sequences and 15 bp of adaptor sequence on either end. The oligo library was PCR amplified with primers MPRA_v3_F and MPRA_v3_20I_R to add unique 20 bp barcodes along with arms for Gibson assembly into a backbone vector. The oligonucleotide library was assembled into pMPRAv3:luc:xbal (Addgene plasmid, 109035) and expanded by electroporation into Escherichia coli. Seven of the ten expanded cultures were purified using Qiagen Plasmid Plus Midi Kit to reach 200300 colony-forming units (barcodes) per oligonucleotide. The expanded plasmid library was sequenced on the Illumina NovaSeq system using  $2 \\times 150$  bp chemistry to acquire oligobarcode pairings. The library underwent AsiSI restriction digestion, and GFP with a minimal promoter amplified from pMPRAv3:minP-GFP (Addgene plasmid, 109036) using primers MPRA_v3_GFP_Fusion_F and MPRA_v3_GFP_Fusion_R was inserted by Gibson assembly resulting into the 200 bp oligo sequence positioned directly upstream of the promoter and the 20 bp barcode falling in the  $3^{\\prime}$  UTR of GFP. Finally, the library was expanded within  $E.$  coli and purified using the Qiagen Plasmid Plus Giga Kit.\n\nMPRA library transfection into cells. All cell culture and transfection conditions followed previously established protocols $^{27}$ . For each of the three cell types, K562, SK-N-SH and HepG2, we collected two hundred million cells for transfections using the Neon Transfection System  $100\\mu \\mathrm{l}$  Kit with  $5\\mu \\mathrm{g}$  or  $10\\mu \\mathrm{g}$  of the MPRA library per 10 million cells. Cells were collected  $24\\mathrm{h}$  after transfection, rinsed with PBS and collected by centrifugation. After adding RLT buffer (RNeasy Maxi kit), dithiothreitol and homogenization, cell pellets were frozen at  $-80^{\\circ}\\mathrm{C}$  until further processing. For each cell type, three biological replicates were performed on different days. All cell lines were acquired from ATCC, authenticated using genotyping and gene expression signatures, and routinely tested for Mycoplasma and other common contaminants by The Jackson Laboratory's Molecular Diagnostic Laboratory.\n\nRNA isolation and MPRA RNA library generation. RNA was extracted from frozen cell homogenates using the Qiagen RNeasy Maxi kit. After DNase treatment, a mixture of three GFP-specific biotinylated primers was used to capture GFP transcripts using Sera Mag Beads (Thermo Fisher Scientific). After a second round of DNase treatment, cDNA was synthesized using SuperScript III (Life Technologies) and the GFP mRNA abundance was quantified using quantitative PCR (qPCR) to determine the cycle at which linear amplification begins for each replicate.\n\nReplicates were diluted to approximately the same concentration based on the qPCR results, and a first round of PCR (8 or 9 cycles) with primers MPRA_Illumina_GFP_F_v2 and Ilmn_P5_1stPCR_v2 was used to amplify barcodes associated with GFP mRNA sequences for each replicate. A second round of PCR (6 cycles) was used to add Illumina sequencing adaptors to the replicates. The resulting Illumina indexed MPRA barcode libraries were sequenced on the Illumina NovaSeq system using  $1 \\times 20$  bp chemistry.\n\n# CRE prioritization for in vivo validation\n\nEnformer analysis of epigenetic signatures. To simulate epigenetic and gene expression signatures in silico we collected the nucleotide sequence from chromosome 11: 3101137-3493091 of the mouse reference genome (mm10). The expected insertion sequence using an H11 targeting vector with a lacZ:P2A:GFP open reading frame was added. As a control, the expected CRE insertion site was simulated as a 200 nucleotide sequence of N. We simulated all possible CRE insertions corresponding to our cell-type-specific MPRA by replacing the oligo-N sequence with 200-mers from our library. We inferred epigenetic signatures for all of these sequences using Enformer by modifying the notebook available online (https://colab.research.google.com/github/deepmind/deepmind_research/blob/master/enformer/enformer-usage.ipynb). To estimate CRE-induced transcriptional activation in various tissues, we collected 128 nucleotide resolution DHS, H3K27ac, ATAC and CAGE datasets overlapping the expected insertion (35 bins). To calculate an aggregate effect for each tissue, we calculated the maximum signal for each feature over the insertion, followed by a feature-specific Yeo-Johnson power transformation. Normalized features were then selected based on tissue correspondence (Supplementary Table 8) and averaged to estimate CRE activity in ten different tissues. We calculated MinGap values for the spleen, liver and brain using these ten measurements for each CRE.\n\nManual sequence prioritization. Sequences were prioritized on the basis of review of empirical MPRA measurements, contribution scores, motif matches, sequence content and predicted epigenetic signatures. We looked for sequences that displayed a high separation between the MPRA measures of the target and the off-target cell types. We also looked to capture variations of combinations of motif matches, and we used the contribution scores to visually examine the motif matches and other potentially important sequence content and motif organization. Finally, we selected sequences with at least moderate tissue specificity in predicted epigenetic signatures.\n\n# Transgenics\n\nTransient zebrafish synthetic enhancer assay. To build the synthetic CRE eGFP reporter, double-stranded oligonucleotides corresponding to synthetic CREs (200 bp) were synthesized by IDT (GeneBlock). Synthetic CREs were amplified by PCR with primers that included homology to the plasmid vector E1b-GFP-Tol2 (Addgene plasmid, 37845)[75] and were cloned upstream of the minimal promoter (E1b) to generate the synthetic enhancer eGFP plasmid reporter (pTol2-synthetic CRE-E1b-eGFP-Tol2) using HiFi DNA Assembly according to the manufacturer's instructions (New England Biolabs). We also created 'empty vectors', which were identical to CODA CRE vectors except for the lack of a 200 bp insert. Reporter plasmid sequences were verified by Sanger sequencing. To transiently express the synthetic CRE reporter in zebrafish, plasmids were co-injected with tol2 transposase mRNA into one-cell-stage zebrafish embryos according to established methods[92]. A minimum of 15, one-cell zebrafish embryos of either sex were injected per construct. Injected embryos were imaged at the indicated days (2 or 4 days after fertilization) either using the dissecting (Olympus) or confocal fluorescence (Leica SP8) microscope. Injected embryos were not randomized, and researchers were not blinded. All zebrafish procedures were\n\napproved by the Yale University Institutional Animal Care and Use Committee (2022-20274).\n\nMouse transgenic reporter assay. An H11 targeting vector with an lacZ:P2A:GFP open reading frame was linearized using PCR containing 2 ng of template,  $1\\mu \\mathrm{l}$  of KOD Xtreme Hot Start DNA Polymerase (Sigma-Aldrich, 71975),  $25\\mu \\mathrm{l}$  of Xtreme buffer and  $0.5\\mu \\mathrm{M}$  forward and reverse primers (H11_bxb_lacZ:GFP_lin_F, pGL_minP_GFP_R; Supplementary Table 11) cycled with the following conditions:  $94^{\\circ}C$  for 2 min; 20 cycles of  $98^{\\circ}C$  for 10 s,  $56^{\\circ}C$  for 30 s and  $68^{\\circ}C$  for 13 min; and then  $68^{\\circ}C$  for 5 min. Amplified fragments were treated with  $0.5\\mu \\mathrm{l}$  of DpnI (NEB, R0176S) for 30 min at  $37^{\\circ}C$ , purified using  $1\\times$  volume of AMPure XP (Beckman Coulter, A63881) and eluted with water. Double-stranded oligonucleotides corresponding to synthetic enhancers with Gibson arms were synthesized by IDT (GeneBlock) and assembled into the targeting vector using  $5\\mu \\mathrm{l}$  of NEBuilder HiFiDNA Assembly Master Mix (NEB, E2621S),  $36\\mathrm{ng}$  of linearized vector and  $10\\mathrm{ng}$  of the synthesized fragment in a total volume of  $20\\mu \\mathrm{l}$  for  $45\\mathrm{min}$  at  $50^{\\circ}C$ . Transgenic mice were created according to the enSERT protocol[76]. A mixture of  $20\\mathrm{ng}\\mu \\mathrm{l}^{-1}$  Cas9 protein (IDT, 1074181),  $50\\mathrm{ng}\\mu \\mathrm{l}^{-1}$  single guide RNA (sgRNA_H11lacZ; Supplementary Table 11),  $25\\mathrm{ng}\\mu \\mathrm{l}^{-1}$  donor plasmid,  $10\\mathrm{mM}$  Tris, pH 7.5, and  $0.1\\mathrm{mM}$  EDTA was injected into pronuclear of FBV zygotes. Each group was tested with a predetermined sample size of 3 l and all of the samples were stained regardless of their genotype and sex. Embryos were collected and stained blindly with respect to their genotype. The whole embryo at embryonic day 14.5 or isolated brain at 5 weeks postnatal were fixed at  $4^{\\circ}C$  for 1 h in PBS supplemented with  $2\\%$  paraformaldehyde,  $0.2\\%$  glutaraldehyde and  $0.2\\%$  IGEPAL CA-630. After washing with PBS, the embryos were stained at  $37^{\\circ}C$  overnight in a solution in PBS supplemented with  $0.5\\mathrm{mg}\\mathrm{ml}^{-1}$  X-gal (Sigma-Aldrich, B4252),  $5\\mathrm{mM}$  potassium hexacyanoferrate(II) trihydrate,  $5\\mathrm{mM}$  potassium hexacyanoferrate(III),  $2\\mathrm{mM}$ $\\mathrm{MgCl}_2$  and  $0.2\\%$  IGEPAL CA-630. The images were taken using the Leica M165 system for embryos or the Leica M125 system for brains. All mice were housed in duplexed pens containing five or less mice and under a  $12\\mathrm{h}-12\\mathrm{h}$  light-dark cycle at  $18-23^{\\circ}C$  with  $40-60\\%$  humidity. All mouse procedures were performed in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals, and were approved by the Institutional Animal Care and Use Committees of The Jackson Laboratory (18038).\n\nHistology and immunofluorescence staining. After LacZ staining, mouse brains were sectioned with a vibratome (Leica VT100s) and free-floating  $70 - \\mu m$  thick sagittal sections were collected in ice-cold PBS. The sections were then rinsed in  $1\\times$  PBS for 5 min and incubated for  $30\\mathrm{min}$  in a blocking solution consisting of  $0.3\\%$  Triton X-100,  $0.3\\%$  mouse on mouse blocking reagent (Vector laboratories, MKB-2213-1),  $10\\%$  normal goat serum (Abcam, ab7481) and  $5\\%$  BSA in  $1\\times$  PBS with gentle agitation at room temperature. Immunostaining was then performed with a mixture of primary antibodies in the blocking solution at  $4^{\\circ}C$  on a shaker overnight. The sections were rinsed in  $1\\times$  PBS three times for 5 min each and then incubated with corresponding fluorescence conjugated secondary antibodies for 2 h. After treatment with secondary antibodies, the slices were then further rinsed with PBS three times, followed by staining for nuclei with DAPI (Thermo Fisher Scientific, 62248). The sections were mounted onto slides with Prolong Gold antifade reagent (Cell Signalling Technology, 9071). The following primary antibodies were used during the staining procedure: mouse anti-NeuN (Abcam, ab104224), chicken anti-GFAP (OriGene Technologies, TA309150), rabbit anti-IBA1 (Abcam, ab178846). Secondary antibodies used were as follows: goat anti-mouse Alexa Fluor 488 (Thermo Fisher Scientific, AB_2534069), goat anti-chicken Alexa Fluor 568 (Thermo Fisher Scientific, AB_2534098), goat anti-rabbit Alexa Fluor 568 (Abcam, ab175471). All primary and secondary antibodies were used at 1:500 dilutions. Image acquisition for whole-brain\n\nsagittal slice mosaic images was performed using the Thunder Imager (Leica Microsystems) system using a  $\\times 10 / 0.8$  NA dry lens. Fluorescence imaging was combined with bright-field imaging to visualize LacZ staining. Computational tissue clearing was applied systematically to reduce background noise (Leica acquisition software). After obtaining mosaic scans, higher-magnification images of regions of interest (ROIs) were acquired on the Stellaris 8 (Leica Microsystems) equipped with a Diode, Ar gas and He/Ne adjustable wavelength lasers using  $\\times 40 / 1.2$  NA and  $\\times 63 / 1.4$  NA oil objectives for quantification and representative images, respectively. The pinhole size was set to 1 a.u. and the samples were illuminated with 405, 488, 561 and  $633\\mathrm{nm}$  lasers sequentially. Six-micrometre  $z$ -stack images with a  $2\\mu m$ $z$ -step size and with a  $4,096\\times 4,096$  pixel resolution were acquired using HyD detectors with a line average of 3. Fluorescent LacZ staining was visualized using the confocal microscope using the  $633\\mathrm{nm}$  laser[93]. For the representative images shown, bright outliers were removed using the default 2-pixel radius and 20 threshold. A Gaussian blur was then applied with a sigma radius of 1.\n\nLacZ layer intensity analysis. Acquired mosaic bright-field images underwent auto-thresholding using the default algorithm in the FIJI software (NIH). Quantification of LacZ signal intensity was achieved using the plot profile tool with ROIs drawn from superficial cortical layers down to the corpus callosum. Depth information for cortical layers was acquired from the Allen Brain atlas. Multiple ROIs were taken in different cortical areas to verify the distribution of the signal. Representative images are ROIs taken from the somatosensory and visual cortices. For cell quantification and overlap analysis, to quantify cell populations, using FIJI software, maximum-intensity projection of the  $z$ -stack of images acquired with a confocal microscope was performed, and background removal was applied with rolling ball radius of 50. The images were then processed for autothresholding using the Moments algorithm. The signal to noise ratio was uniform across ROIs and a single thresholding algorithm yielded reproducible results. Cells were then quantified using the Analyse particle function. By varying the particle size, accurate quantification of neurons, astrocytes and microglia was achieved. To calculate the overlap between LacZ expression and the cell-type-specific markers, each binarized LacZ image was multiplied with corresponding binarized neuronal, astrocytic and microglia ROIs and the residual signals were quantified using the Analyse particle function. In total, five sagittal slices were analysed per mouse and a total of  $n = 3$  mice was used for both controls and LacZ-positive brains.\n\nRNA-seq analysis. Three replicates each from transgenic mice of CODA-designed SK-N-SH-specific CRE and empty vector were collected at 5 weeks postnatally. The liver, spleen and the right half of the brain were soaked into RNA later (Thermo Fisher Scientific) overnight at  $4^{\\circ}\\mathrm{C}$  and homogenized in QIAzol, followed by total RNA isolation using the RNeasy mini kit (Qiagen) with on-column DNase treatment. The RNA-seq library was generated from  $1\\mu \\mathrm{g}$  of total RNA using the NEBNext Ultra II RNA Library Prep Kit for Illumina (NEB) and NEBNext Poly(A) mRNA Magnetic Isolation Module (NEB) according to the manufacturer's protocol. The libraries were indexed using i7 and i5 primers with the following conditions:  $98^{\\circ}\\mathrm{C}$  for 30 s; 10 cycles of  $98^{\\circ}\\mathrm{C}$  for 10 s,  $65^{\\circ}\\mathrm{C}$  for 75 s; then  $65^{\\circ}\\mathrm{C}$  for 5 min. Indexed samples were purified using  $0.9\\times$  volume of AMpure XP, eluted in  $20~\\mu \\mathrm{l}$  of EB, pooled equimolarly and sequenced using  $2\\times 150$  bp chemistry on the Illumina NovaSeq X+ instrument at the Jackson Laboratory. The sequencing reads were mapped onto a modified mouse genome (GRCm38/mm10) with the lacZ-GFP sequence as an additional chromosome using STAR[94] (v.2.5.2b). After removing duplicates using picard MarkDuplicates (MIT, v.3.1.1), the mapped reads were counted using featureCount (v.2.0.6, options: -p-B-Q 20-T 16-s 2--countReadPairs). DESeq2 (v.1.32.0)[95] was used to normalize the read counts and calculate the  $\\log_2[\\mathrm{FC}]$ , standard error and Wald-test Pvalues.",
    "metadata": {
      "md_filename": "MinerU_markdown_Malinois_20260106142949_2008425751259127808.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_Malinois_20260106142949_2008425751259127808.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "759d8a6dc18c12ba",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_MPAC_20260106142928_2008425664583831552",
    "title": "Identifying non-coding variant effects at scale via machine learning models of cis-regulatory reporter assays",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# Data preprocessing and model training\n\nWe partitioned a previously generated MPRA dataset  $^{66}$  into 11 roughly even sized splits based on the chromosome pairs from which the oligo sequences were derived. Pairs of source chromosomes were selected such that the sums of the chromosome identifiers were 23 (e.g., chr3 and chr20 form a pair). Next, we organized the data into 110 combinations of training, validation, and test sequences composed of 10, 1, and 1 pairs of source chromosomes, respectively. The training and validation groups were used to fit and select model parameters, respectively. Therefore, for every pair of source chromosomes, 10 models are fitted using unique training/validation combinations to use for benchmarking performance and making predictions. We then trained 110 models as previously described using these data set combinations using the code base available at https://github.com/sjgosai/boda2 and hyperparameter settings defined in (Supplementary Data 10).\n\n# Predicting variant effects using MPAC\n\nCommand line tools, the original Malinois model, and code base is available at (https://github.com/sjgosai/boda2). The 110 MPAC models are available at Zenodo (https://doi.org/10.5281/zenodo.15178434). Predictions are generated with the `vcf_prediction.py' script as shown below:\n\npython /opt/boda2/src/vcf_prediction.py --artifact_path {10X $MODEL} --vcf_file ${VCF} --fasta_file ${FASTA} --output ${OUTPUT} --relative_start 9 --relative_end 181 --step_size 10 --strand_reduction mean --window_reduction mean\n\nThe above example generates 18 predictions per variant, tested in positions 9 to 181 (0-based, 10 bp intervals) which are reduced to a single reference and alternate activity  $(\\log_2FC)$  by averaging. Allelic skew is then calculated as the difference between alternate and reference activity. GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta was downloaded from https://www.encodeproject.org/files/GRCh38 no.alt_analysis_set_GCA_000001405.15/ and used as the reference genome for all analyses. All predictions were generated with the above prediction schema except for correlation with empirical MPRA activity (Figure 1B) which used the following:\n\npython /opt/boda2/src/vcf_prediction.py --artifact_path {10X $MODEL} --vcf_file ${VCF} --fasta_file ${FASTA} --output ${OUTPUT} --relative_start 99 --relative_end 100 --step_size 1 --strand_reduction mean --window_reduction mean\n\nWhich generates a single prediction with the variant centered at position 99 (0-based) to exactly reproduce the sequences tested by the empirical MPRA used for comparison  $^{66}$ .\n\n# MPAC correlation with empirical MPRA\n\nSingle, variant centered predictions were generated for all empirical MPRA sequences from  $^{66}$  that passed QC, K562 (n=485,180), HepG2 (n=499,820), and SK-N-SH (n=485,034). Empirical  $\\log_2\\mathrm{FC}$  was compared to predicted  $\\log_2\\mathrm{FC}$  for computing Pearson's correlation. Next, MPAC allelic skew and empirical Log2Skew were compared for empirically validated emVars ((logPadj_BF  $\\gtrsim$  -log10(0.01) & abs(log2FC) >= 1) & Skew_logPadj >= -log10(0.1)) in K562 ( $n = 16,187$ ), HepG2 ( $n = 14,029$ ), and SK-N-SH ( $n = 14,723$ ) from the same experimental dataset. Predictions and empirical measurements were correlated using Pearson's method.\n\n# Correlation to DNase allele-specific effects variants\n\nDNase allele specific effects (ASE) data were obtained for the three MPAC cell types (K562, HepG2, and SK-N-SH) from Jeff Vierstra (unpublished). Significant ASE variants were called\n\nusing a minimum FDR threshold of <= 0.05. As SK-N-SH samples had no significant hits they were dropped from subsequent analysis. ASE mean effect sizes were multiplied by -1 to match directionality of MPAC allelic skew (Alt - Ref). MPAC allelic skew was compared to DNase skew for all K562 and HepG2 ASE variants as well as the subset of MPAC emVars (|allelic skew| > 0.5). Significant ASE variants were identified in previously performed MPRA experiments  $^{66}$  and Log2Skew was compared to ASE effect size for both all ASE variants and those called emVars by MPRA ((logPadj_BF  $\\gtrsim$  -log10(0.01) & abs(log2FC) >= 1) & Skew_logPadj >= -log10(0.1)). All correlations were performed using Pearson's method.\n\nFor additional model comparisons precomputed Enformer 1KG predictions were downloaded from\n\n(https://console.cloud.google.com/storage/browser/dm-enformer/variant-scores/1000-genomes/enformer) and filtered for significant ASE variants (HepG2  $n = 331$ , K562:  $n = 855$ ). All Pearson's correlations presented in Fig. 1D reflect this subset. Sei  $^{81}$  predictions were generated for these same variants using the web interface (https://hb.flatironinstitute.org/deepsea/) and filtered for HepG2 and K562 DNase. For both Enformer and Sei the highest Pearson's correlation to empirical DNase was reported.\n\nCorrelation of Enformer and Sei DNase predictions to MPRA emVars\n\nAll empirical emVars  $^{66}$  present in the precomputed Enformer 1KG variants predictions (HepG2 n = 10,407, K562 = 11,993) were downloaded from (https://console.cloud.google.com/storage/browser/dm-enformer/variant-scores/1000-genomes/enformer). Sei was downloaded and installed from (https://github.com/FunctionLab/sei-framework). Variant effect predictions for all emVars were generated using the following command 'sh 1_variant-effect_prediction.sh ../vcfs2sei/emvar_vcf.vcf hg38 ../sei_preds/mpra_emvars --cuda.' For both Enformer and Sei predictions all HepG2 and K562 DNase predictions were compared to MPRA skew and the highest Pearson's correlation was used.\n\n# Precision-Recall of high PIP GWAS variants\n\nWe generated MPAC predictions for a set of statistically fine-mapped variants from the UK Biobank and Biobank Japan previously tested by empirical MPRA  $^{66}$ . After filtering for autosomal variants tested in K562, HepG2, or SK-N-SH, the test set contained 1,869 variants: 934 positive (PIP > 0.9) and 935 negative (PIP < 0.01). MPAC predictions were reduced to a single score using the max(|allelic skew|) of K562, HepG2, or SK-N-SH. We also generated predictions for these variants with Sei  $^{81}$  and LS-GKM  $^{86,140}$ . Sei predictions were generated with the following command 'sh 1_variant_effects_prediction.sh ../vcfs2sei/traits_prc_hg38_vals_for_SEI_preds.vcf hg38 ../sei_preds/ukbb_gtex --cuda' and the max(|allelic skew|) prediction for DNase in all cell types was used. LS-GKM and model weights were downloaded from (https://www.beerlab.org/deltasvm/). HepG2 predictions were generated using the following command 'perl deltasvm.pl ref_fasta alt_fasta SupplementaryTable_hepg2weights.txt hepg2_out' and K562 predictions with 'perl deltasvm.pl ref_fasta alt_fasta SupplementaryTable_k562weights.txt k562_out'. The max(|allelic skew|) prediction from K562 and HepG2 was used. Precision and recall curves, as well as AUPRC measurements were generated using scikit-learn 1.4.1.post1  $^{141}$ .\n\n# Precision-Recall of high PIP GTEx variants\n\nMPAC predictions were generated for a set of previously described high (> 0.9) and low (< 0.01) PIP eQTLs  $^{66}$ . After filtering for autosomal variants empirically tested in K562, HepG2, and SK-N-SH, SNVs the test set contained 22,720 variants: 11,354 positive and 11,366 negative.\n\nMPAC predictions were reduced to a single score using the max(|allelic skew|) of K562, HepG2, and SK-N-SH. Sei  $^{81}$  predictions were generated with 'sh 1_variant Effect_prediction.sh {./vcfs2sei/gtex_prc_hg38_vars_for_SEI_preds.vcf} {hg38} {./sei_preds/ukbb_gtex} --cuda' and the max(|allelic skew|) prediction for DNase in all cell types was used. LS-GKM  $^{86,140}$  predictions were generated using the command 'perl deltasvm.pl {ref_fasta} {alt_fasta} {SupplementaryTable_hepg2weights.txt} {hepg2_out}' for HepG2 and 'perl deltasvm.pl {ref_fasta} {alt_fasta} {SupplementaryTable_k562weights.txt} {k562_out}' for K562.The max(|allelic skew|) prediction from K562 and HepG2 was used. As before LS-GKM and model weights were downloaded from (https://www.beerlab.org/deltasvm/). Precision and recall curves, and AUPRC measurements were generated using scikit-learn 1.4.1.post1  $^{141}$ .\n\n# Missed MPAC prediction analysis\n\nTo understand sources of MPAC missed predictions compared to empirical MPRA, we subset all MPRA emVars for those with an |allelic skew|  $>0.5$  and an MPAC predicted |allelic skew|  $< 0.05$ . These incorrectly predicted emVars were independently identified from each cell type, split into 80/20 training and test sets and matched with a randomly sampled set of correctly predicted sequences. An LS-GKM model  $^{86,140}$  was trained for each cell type using the command 'gkmtrain {positive_train_fasta} {negative_train_fasta} {output}.'\n\nUsing these models, importance scores were generated using gkmexplain  $^{88}$  and passed to tfmodisco-lite (https://github.com/jmschrei/tfmodisco-lite)  $^{87}$  for de-novo motif identification. tfmodisco-lite motifs were matched to known TF motifs using TOMTOM (MEME version 5.5.7, HOCOMOCOv11_core_HUMAN)  $^{142}$ . We next used FIMO  $^{143}$  ('fimo -no-pgc -o {outfile} {modisc_pwms} {emvar_fast}') to quantify the presence of motifs identified by tfmodisco-lite in correctly predicted emVar sequences. FIMO hits were considered if they overlapped the variant (oligo position 100) and had a q-value  $< 0.01$ . Lastly, to determine abundance of de-novo motifs in the correctly predicted emVars the total number of significant FIMO hits with a significant TOMTOM match ( $q$ -value  $< 0.05$ ) to a known motif was divided by the total number of correctly predicted emVar sequences.\n\n# ClinVar analysis\n\nThe ClinVar VCF was downloaded from (https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/archive_2.0/2023/clinvar_20230930.vcf.gz fileDate: 2023-09-30) and MPAC predictions were generated for all ClinVar variants prior to filtering for non-coding variants. To focus on non-coding SNVs, we filtered transcripts from GENCODE v44 basic annotations  ${}^{144}$  (downloaded from https://www.gencodegenes.org/human/release_44.html) that were located on chr1-22, had a transcript_type column = \"protein_coding\", a non-missing hgnc_id, and tag column matching \"Ensembl_canonical\" but not matching \"readthrough_gene\". We then removed variants intersecting any exons. To remove potential splice disrupting variations, we also removed SNVs in intronic regions up to -20 from 3' splice sites or +6 from 5' splice sites, consistent with splice region definitions from MaxEntScan  ${}^{144,145}$ . After filtering a total of 180,032 variants classified as pathogenic (ClinVar 'Pathogenic' or 'Likely_pathogenic') or benign (ClinVar 'Benign' or 'Likely_benign') were used for all downstream analyses. ClinVar variants were intersected with peaks of DNase I Hypersensitivity (DHS) downloaded from (https://www.encodeproject.org/files/ENCFF503GCK/)  ${}^{146}$  and promoters. Promoters were defined as 250 bp immediately upstream of the TSS of filtered GENCODE transcripts. emVar proportions for pathogenic and benign variants are calculated as the number of emVars labeled non-DHS, DHS, or Promoter over the total number of variants in each category. Significance was tested by the Chi-Square Test. Enrichments for pathogenic and benign variants were\n\ncalculated by odds ratio of emVars in each respective group and tested by Fisher's exact test to calculate  $p$  values.\n\nSei  $^{81}$  predictions were generated for all non-coding SNVs using the command 'sh 1_variant_EFFECT_prediction.sh {clinvar_exonNSSVariables.vcf} {hg38} {/clinvar_preds} --cuda' and filtered for K562, HepG2, and SK-N-SH ENCODE DHS predictions. For a given category (All ClinVar, DHS, Promoter) all pathogenic variants were compared to an equal random sample of benign variants 100 times. For both MPAC and Sei predictions the max(|allelic skew|) prediction from K562, HepG2, and SK-N-SH was used in precision recall calculations. Precision and recall curves, and AUPRC measures were generated using scikit-learn 1.4.1.post1  $^{141}$ .\n\n# COSMIC analysis\n\nMPAC predictions were generated for all non-coding COSMIC  $^{23}$  (v98, downloaded from (https://cancer.sanger.ac.uk/cosmic/download/cosmic/v98/noncodingvariantsvcf). Variants were filtered for only SNVs derived from whole genome sequencing ('Whole_Genome_Reseq' == 'y' and 'Whole_Exome' == 'n') from (https://cancer.sanger.ac.uk/cosmic/download/cosmic/v98/noncodingvariantsstsv). Variant recurrence was calculated as the number of COSMIC mutation IDs ('GENOMIC_MUTATION_ID') present after filtering. MPAC predictions were generated for ETS-TF disrupting putative drivers previously tested by MPRA  $^{98}$ . We calculated percent emVar recovery as the number of MPAC emVars divided by empirically defined emVars.\n\nVariants were subset into recurrent  $(n\\geq 2)$  or unique  $(n = 1)$  by ID count and feature annotations, promoters (as defined in ClinVar analysis), cancer promoters (CNC)  $^{99}$ , recurrent promoters, and recurrent CNC promoters were assigned by intersection. CNC promoters were downloaded from (https://cncdatabase.med.cornell.edu/) with the query 'promoters' and using all unique gene names for the final list. Variants were categorized into putative regulatory elements by intersection with DHS elements from (https://www.encodeproject.org/files/ENCFF503GCK/)  $^{146}$ . Odds ratios (OR) were calculated for the proportion of emVars in the category of interest versus the remainder of COSMIC variants. For allelic skew bin comparisons bins were defined as the mean allelic skew of all predictions. ORs were calculated for promoter variants in the allelic skew bin versus distal variants. For activity comparisons variants were binned by the max(ref | alt) log $_2$ FC and the OR that a recurrent or unique variant was an emVar was calculated. For all ORs Fisher's exact test was used to calculate  $p$  values and  $95\\%$  confidence intervals.\n\n# MPAC gnomAD SNV predictions and analysis\n\nWe used MPAC to predict variant effects for all 646,033,065 SNVs reported in 76,156 whole genomes analyzed by gnomAD v3.1.2 on hg38 (downloaded from https://gnomad.broadinstitute.org/downloads#v3-variants). We filtered to include SNVs that: 1) passed gnomAD QC, 2) were located on autosomes (chr1-22), 3) had non-zero minor allele frequency (MAF = min(AF, 1 - AF) > 0), 4) non-zero allele count (AC > 0), and 5) total number of alleles observed is at least half of gnomAD (AN  76,156).\n\nWe annotated SNVs by 1) overlapping Zoonomia 241-way base-level phyloP scores  $^{26}$  (downloaded from https://cgl.gi.ucsc.edu/data/cactus/241-mammalian-2020v2-hub/Homo_sapiens/241-mammalian-2020v2/bigWig), and 2) annotations of mutation rate from Roulette predictions  $^{147}$  (as annotated in CADD v1.6 downloaded from\n\nhttps://krishna.gs.washington.edu/download/CADD/v1.6/GRCh38/whole_genome_SNVs.tsv.gz). We removed variants without a phyloP annotation (1.7% of variants).\n\nTo focus on non-coding SNVs, we removed variants intersecting our previously defined exons and splice regions on filtered GENCODE transcripts (see \"ClinVar analysis\"). This resulted in a final set of 513,886,257 SNVs for downstream analyses.\n\nGWAS Catalog (accessed 2024-12-19)  $^{10}$  SNVs were expanded to include all variants in high LD ( $r^2 \\geq 0.7$ ) in each of three superpopulations (ASN, AFR, EUR). This set was assigned MPAC predictions by subsetsing our filtered GWAS prediction set to those with a MAF  $\\geq 1\\%$ , then joining by rsID.\n\n# gnomAD SNV functional annotations\n\nWe categorized non-coding SNVs by intersecting with the ENCODE cCRE class annotations  $^{37,144,145}$  (downloaded from https://downloads.wenglab.org/Registry-V4/GRCh38-cCREs.bed), specifically promoter-like signatures (PLS), proximal enhancer-like signatures (pELS), and distal enhancer-like signatures (dELS). SNVs not falling into the above categories were further classified as \"Other cCREs\" if they overlapped any remaining cCRE annotations, or labeled as \"non-cCRE\" if overlapping no annotations.\n\nFor the TF binding enrichment analysis, we labeled SNVs as overlapping DNase I TF footprints if they intersected at least one consensus footprint from Vierstra et al.  $^{104}$  (downloaded from https://zenodo.org/records/3905306) and overlapping TF ChIP-seq peaks if they intersected at least one peak from ChIP-Atlas  $^{105,106}$ . Odds ratios (OR) were calculated using Fisher's exact test with a +1 pseudocount correction to calculate the OR,  $p$ -value, and 95% CI.\n\nTo compare mean phyloP between non-coding and coding functional variants, we annotated SNVs by Ensembl VEP  $^{105,148}$  based on gnomAD v3.1.2 annotations using the worst predicted consequence as reported in the sequence ontology ranking provided in the Ensembl VEP documentation (https://ensembl.org/info/genome/variation/prediction/predicted_data.html retrieved 2024-06-10) prior to removing SNVs overlapping exons or splice regions.\n\n# gnomAD evolutionary constraint analyses\n\nWe binned SNVs by their MPAC predicted mean  $\\log_2\\mathrm{FC}$  in activity across all cell lines or mean  $\\log_2\\mathrm{FC}$  allelic skew. We used the enrichment of rare (AF  $< 0.1\\%$  ) vs. common (AF  $\\geq 0.1\\%$  ) SNVs as a measure of recent purifying selection and compared across MPAC allelic-skew bin for each ENCODE cCRE class. We omitted bins with less than 50 variants. We calculated the odds ratios (OR) based on counts of rare vs. common SNVs belonging to a focal subset of SNVs (MPAC allelic-skew bin x ENCODE cCRE class) relative to that of all other SNVs in the genome, using Fisher's exact test with a +1 pseudocount correction to calculate the OR,  $p$ -value, and 95% CI.\n\n# MPAC promoter saturation mutagenesis predictions\n\nStarting from the filtered list of 18,761 GENCODE transcripts (see \"ClinVar analysis\"), we used MPAC to conduct in silico saturation mutagenesis of all 3,000 possible single nucleotide mutations across the 1 kb promoter regions located upstream of the TSS. We masked mutations intersecting any exons or splice regions of filtered GENCODE transcripts (see \"ClinVar analysis\"), resulting in 18,658 promoter regions with any mutations after masking. At each base position, the mean activity  $(\\log_2FC)$  or allelic skew  $(\\log_2FC)$  is that of all three non-reference mutations averaged across all three cell lines (nine values total). The 1 kb\n\npromoter regions were used for Supplementary Fig. 21 and 22. All subsequent analyses were conducted on the 250 bp promoter region upstream of the TSS. Means at each distance from the TSS were calculated using promoters that had unmasked values at that specific position.\n\n# Motif analysis of promoter regions\n\nDNA sequences for 250 bp promoter regions were scanned for TF binding motifs using FIMO from the memes R package  $^{143,149}$  and position weight matrices from JASPAR 2024  $^{150}$  with accessions: TATA (MA0108.3), SP1 (MA0079.5), NRF1 (MA0506.1), ELK1 (MA0028.2), NFYC (MA1644.1), CREB1 (MA0018.3), YY1 (MA0095.2), CTCF (MA0139.1), and ZNF143 (MA0088.2). Significant motifs were called at default  $p < 4 \\times 10^{-4}$ , which corresponds to rescaling the default FIMO value of  $1 \\times 10^{-4}$  to 250 bp promoters as specified in https://memesuite.org/meme/doc/fimo-tutorial.html. Motif count profiles were based on the positions of motif midpoints relative to the TSS.\n\n# Gene-level expression annotations\n\nGene expression in transcripts per million (TPM) were downloaded from the Human Protein Atlas for K562, HepG2, and SK-N-SH cell lines  $^{151}$ . Using Pearson's  $r$ , the  $\\log_{10}$  TPMs were then correlated against the mean MPAC-predicted activity of each 250 bp promoter region, computed as the mean of activities centered on every position in the region. Gene expression annotations were joined to filtered GENCODE transcripts (see \"ClinVar analysis\") by matching Ensembl gene IDs, Ensembl transcript IDs, or gene names.\n\n# Gene-level coding constraint stratification\n\nPromoters were annotated by four different scores of gene-level coding constraint: GeneBayes  $S_{\\text{het}}$  LoF-intolerance  $^{125}$ , LOEUF LoF-intolerance and MOEUF missense-intolerance from gnomAD v4.0  $^{124}$ , and mean AlphaMissense missense pathogenicity  $^{126}$ . Each of these were binned into deciles for the coding constraint stratification analyses. Gene-level coding constraint annotations were joined to filtered GENCODE transcripts (see \"ClinVar analysis\") by matching Ensembl gene IDs, Ensembl transcript IDs, or gene names.\n\n# Promoter correlations between function and constraint\n\nFor each promoter, we calculated Spearman correlation  $(\\rho)$  between MPAC-predicted allelic skew at each position (mean log2FC of the three possible mutations across three cell lines) and across-species base-level constraint (max(0, phyloP) scores). We performed this calculation separately for negative or positive allelic-skew variants, requiring 50 unmasked positions that had both a phyloP score and a non-zero skew prediction. Among analyzed promoters, 17,392 had defined correlations for at least one category (positive or negative allelic-skew variants), while 16,995 had both. We identified promoters with significant correlations by comparing them to a permutation-based null distribution created by shuffling base positions among 250 bp promoter regions, where we defined promoters as significant if their correlation values fell below the bottom  $0.01\\%$  (for negative allelic-skew) or above the top  $0.01\\%$  (for positive allelic-skew) of this null distribution.\n\nGene set enrichment analyses were conducted using the Enrichr web server  $^{152}$  with the negative allelic-skew correlation, positive allelic-skew correlation, or both correlation gene sets using the background set of 17,392 genes where either the positive or negative allelic-skew correlations was well-defined, focusing on Reactome Pathways 2024, GO Biological Process 2025, and MGI Mammalian Phenotype Level 4 gene sets with FDR < 0.05.",
    "metadata": {
      "md_filename": "MinerU_markdown_MPAC_20260106142928_2008425664583831552.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_MPAC_20260106142928_2008425664583831552.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "68b2727da5ed175b",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_NAS_20260106142922_2008425642756669440",
    "title": "Neural Architecture Search for Genomic Sequence Data",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# A. GenomeNAS Search Space\n\nThe GenomeNAS search space is created by combining both the NASNet search space for CNNs and the ENAS search space for RNNs. The NASNet search space here consists of the configurations of two disjoint DAGs that describe each the \"normal\" and a \"reduction\" convolutional cell; the ENAS RNN search space is the configuration space of the DAG describing the recurrent cell. The operations used for the convolutional cells differ necessarily from the operations used in other work focusing on image data, since they are applied to one-dimensional data, and are chosen to have potentially relatively large receptive field sizes because DNA data may have relatively long-range interactions [38].\n\nThe operations  $\\mathcal{O}_{\\mathrm{cnn}}$  used for the convolutional cells are:\n\n- average pooling (size 5)\n\nmax pooling (size 5)\n\nconvolution (size 15)\n\n- depth-wise separable convolution (size 9 and size 15)\n\n- dilated depth-wise separable convolution (size 9 and size 15, both dilation 2)\n\n- identity (i.e. skip connection)\n\nzero-operation\n\nThe non-dilated depth-wise separable convolutions are applied twice to the hidden state as in [26], and as in [16] each convolution operation is preceded by ReLU activation and followed by batch normalization. The operations  $\\mathcal{O}_{\\mathrm{rn}}$  used for the recurrent cell are:\n\ntanh\n\n- sigmoid\n\nReLU\n\n- identity (i.e. skip connection)\n\nzero-operation\n\nNote that for all cells, the zero-operation is only used by DARTS-based methods during model search and is never part of a final network. We follow [16] in that the value of the output node of each cell is the concatenation (CNN) or average (RNN) of all intermediate nodes within that cell.\n\nAs in [26], the reduction convolutional cells are used to decrease the input size, while the normal convolutional cells keep the input size constant. However, since input lengths for genome sequence tasks can be thousands of nucleotides, it is beneficial to have more aggressive input size reduction, thereby increasing the effective receptive field size of later convolutional operations. We therefore typically use more reduction cells than used in [26], and also allow reduction cells to have a step size greater than 2.\n\nThe overall network built from the individual cells consists of normal and reduction convolutional cells stacked as in [26] that are followed by a single recurrent cell. Both the convolutional and the recurrent network are then optimized jointly.\n\nThe GenomeNAS search space is used together with the methods described in Section II-B to form the following methods: Genome-RS (performing random search over the GenomeNAS space), Genome-SH (performing successive halving), Genome-DARTS (based on [16]), Genome-P-DARTS (based on [19]), and Genome-BONAS (based on [15]). Because neural networks for genome sequence data do not tend to be as deep as for image data, we limit the number of convolutional cells in the final model. The consequence of this is that, for Genome-DARTS, the one-shot model used during architecture search can have the same size as the model used on the ultimate evaluation task while still fitting in GPU memory, reducing the optimization gap. This is also true for Genome-P-DARTS, which, unlike the original P-DARTS, does not (need to) change the number of cells between optimization stages.\n\n# B. CWP-DARTS\n\nThe original P-DARTS implementation [19] uses a different number of cells for each optimization stage, which makes it necessary to re-initialize the network weights  $\\mathbf{w}$  after each stage. This means that the architecture search will, at the beginning of each stage, proceed for a while with a one-shot model that is relatively distant from an optimal parameterization  $\\mathbf{w}^*$ , leading to wasted optimization steps.\n\nWe, therefore, propose the CWP-DARTS (Continuous Weight Sharing Progressive DARTS) method, which does not change the number of cells between optimization stages, and which therefore makes it possible to continue using the network weights when starting a new optimization stage. The other innovations of P-DARTS, namely the continuous reduction of the search space over operations  $\\mathcal{O}_k^{(i,j)}$  for each stage  $k$ , as well as the search space regularization, can be used as before. If the final model evaluation task involves a model with more cells than used during one-shot model optimization, then this method slightly increases the optimization gap compared to P-DARTS. It is traded for more efficient optimization within the optimization stages.\n\nIn the context of genome data using the GenomeNAS search space, we refer to this method as Genome-CWP-DARTS.\n\n# C. EDP-DARTS\n\nThe original P-DARTS method [19] closes the optimization gap to the final model by (among others) reducing the search space over operations  $\\mathcal{O}_k^{(i,j)}$  for each stage  $k$ , thereby \"progressively\" reducing the search space towards the final result and at the same time making the one-shot model more similar to the ultimate single result architecture. However, even with this approach, the one-shot model is still a DAG where most intermediate nodes have more incoming edges than they will in the final model since the edges are pruned and each intermediate node retains only one (RNN) or two (CNN) edges at the end.\n\nWe, therefore, propose EDP-DARTS (Edge Discarding P-DARTS), which builds on the idea of P-DARTS and expands it by also pruning edges between optimization stages. This is done by specifying a schedule  $E_{k}$  for the maximum number of edges that each intermediate node may have at most at the end of optimization stage  $k$ . After each stage, all edges that are not among the best  $E_{k}$  edges based on their largest softmax  $(\\alpha^{(\\mathbf{i},\\mathbf{j})})$  value (where the component of the zero operation is ignored) are dropped. This mirrors the method of selection for edges to use at the end of the optimization. This way, the idea behind P-DARTS is taken a step further and the optimization stages narrow the search space down to the final model in an even more principled way. In the context of genome data using the GenomeNAS search space, we term this method GenomeEDP-DARTS.\n\n# IV. EXPERIMENTS\n\nDataset and Methods We performed all of our experiments on the DeepSEA dataset, which is a popular and widely used non-coding DNA sequence benchmark. The input, a one-hot encoded 1000-bp DNA sequence, is used to predict 919 binary chromatin features in a multi-label prediction setting. The data, provided by  $[5]^2$  under a CC Attribution 3.0 license, is already split into training, validation, and test sets. In all our experiments, we limit the size of one epoch to 3000 samples which are randomly drawn without replacement from the full dataset. Different samples from the DeepSEA data are drawn in each epoch, for each experimental replicate and for each method. The DeepSEA architecture [5], DanQ [6], and NCNet [7] are popular deep learning architectures that we investigate as human-designed baselines, and we compare these architectures to those chosen by NAS algorithms. Each training run was conducted on a single Nvidia A100 40GB accelerator card on an Nvidia DGX A100 server\n\nExperimental Design We closely follow the NAS procedure suggested by [16] in our experiments. It consists of three defined steps: architecture search, architecture selection, and architecture evaluation. For architecture search, the aim is to learn the optimal architecture by running the NAS algorithms for a pre-defined number of epochs. After the search phase is completed, we obtain the final architecture. Each final architecture is then trained for a specific number of replications on the test set and average results are reported. The overall experiments proceed as follows:\n\n(i) Preliminary hyperparameter optimization: The learning rate and dropout hyperparameters of the RNN part were optimized in a preliminary step; see Appendix B for details.\n\n(ii) Search phase: Each NAS algorithm is run four times, and four final architectures are obtained.\n\n(iii) Selection phase: Each of these four final architectures is trained for 50 epochs and the validation performance of each architecture is evaluated. The architecture that achieves the highest performance on the validation data is chosen as the final architecture of the NAS algorithm.\n\n(iv) Evaluation phase: The performance of each NAS algorithm is evaluated by training the selected architecture from scratch for 50 epochs. We report the performance on the test set.\n\nWe perform all runs with batch size of 100 as used by DeepSEA, DanQ, and NCNet. For most hyperparameter settings of the Genome-X NAS methods we use the settings provided by the original authors, see Appendix A for more details on the chosen hyperparameters.\n\n# V. RESULTS AND DISCUSSION\n\nSelection phase There was about a  $10\\%$  difference in median F1 score of the architecture found by the random baseline and the best-performing method, which was Genome-SH, closely followed by Genome-DARTS (Fig. 2a). In certain cases the variability among different architecture search runs of the same method was considerable. Different runs of Genome-CWP-DARTS resulted in both worst- and third best-performing architectures, with a gap of  $25\\%$  in F1 score. This algorithm was also the worst in terms of median performance, which was even lower than a purely random search. On the contrary, the best and worst architectures found by Genome-BONAS were separated by only about  $8\\%$ . Despite producing the best of the worst architectures, Genome-BONAS could only increase its performance by 0.02 points.\n\nEvaluation phase Except for Genome-BONAS, all Genome-NAS consistently outperformed the baseline algorithms, as well as random search and successive halving (Table I and Fig. 2b). Random search achieved an average PR-AUC [39] score of 21.90 and an average ROC-AUC score [40] of 84.56, which was comparable to the results from NCNet-RR [7] and NCNet-bRR [7] and better than the results of the DeepSEA [5] and the DanQ algorithm [6]. This suggests an advantage of our novel search space, which combines a convolutional and a recurrent DAG. The best result was achieved by Genome-DARTS, with Genome-SH within 0.5 percentage points in term of PR-AUC. The best architecture is shown in Fig. 1. With an average PR-AUC score of 21.20 and an average ROC-AUC score of 84.19, the final architecture of Genome-BONAS is the worst-performing architecture. Our Genome-EDP-DARTS algorithm performed better than the original Genome-P-DARTS, which suggests that we achieved a further decrease of the optimization gap between search and evaluation [19].\n\nRuntime The runtime of all GenomeNAS algorithms was between 8 and 10 GPU-days, except for Genome-BONAS, which required more than 24 GPU-days on average. In spite of the superior performance, the final networks found by the GenomeNAS family of algorithms were considerably smaller compared to the baseline models, respectively 26-29 and 46-57 million parameters. However, such networks required on average 156 minutes per epoch to train, compared to 7, 13, 34, and 52 minutes for DeepSEA [5], DanQ [6], NCNet-bRR and NCNet-RR, and converged after about 40 training epochs.\n\nDiscussion Neural architecture search performs very well on genome data, outperforming prior hand-crafted models [6], [7] while also being significantly smaller. Successive Halving performs surprisingly strong considering its simplicity\n\nIt is also noticeable that P-DARTS, which was specifically designed to outperform DARTS, does not work better in this setting. This is likely due to the fact that one of its main advantages, the shrinking of the optimization gap, is diminished in this setting where the size of the network does not change between architecture search and evaluation. It is also likely that, since the more complicated NAS methods have a variety of hyperparameters on their own, they have so far been tuned for more popular deep learning tasks such as computer vision. This likely explains some part of the relatively weak performance of the more complex algorithms and also creates hope that these algorithms could be tweaked to work even better in this field.\n\nLimitations Our work has shown that NAS methods can easily be adapted to the field of genome sequence data, where it outperforms human-designed baseline models. However, we only conducted a benchmark experiment on a single dataset that exemplifies the data and tasks in this field. It is likely that for other genome sequence tasks that predict other features, the results would be slightly different. Further research should therefore focus on evaluating our methods on other tasks where sequence models are being used, such as DNA methylation [41] or gene expression prediction [42]. While our specific implementation was set up to make a single prediction for a given input sequence it is straightforward to adapt the RNN layer to give a sequential output instead of a single vector.\n\nA general limitation of the NAS approach is its requirement for a large amount of resources. Using smaller proxy tasks may be one approach to alleviate this, as it has been successfully in other domains [16].\n\nBroader impact statement The fact that our NAS algorithms were able to beat expert-designed architectures on a benchmark dataset is a promising sign that NAS algorithms are able to provide a better design and a better performance for deep learning in genomics. Although NAS methods may seem expensive on the surface, they are likely more systematic and therefore more efficient than human experimentation for architecture design. We consider deep learning on genomic data a promising field that has the potential to bring new insights for biomedical research and therefore ultimately benefit society. Neural networks optimized for genomic data modalities could e.g. benefit outbreak detection due to the optimization of taxonomic assignments [43] and could contribute to the design of novel therapeutica due to the modeling of protein folding [44].\n\n# VI. CONCLUSION\n\nIn this paper, we have adapted a diverse set of popular NAS algorithms (random search, Successive Halving, DARTS, P-DARTS, BONAS) to the field of genomic sequence analysis. Moreover, we have extended the P-DARTS algorithm in two novel ways, by introducing continuous weight sharing (CWP-DARTS), and edge discarding (EDP-DARTS). The new algorithms can generate accurate combined CNN and RNN architectures that are capable of modeling genomic sequences.\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/a081cdb2-f1fa-4e9a-a6fb-c245ed406515/69a4afbad05d30227a02863b1d2423b6c867bdfb38f0a1af0e8b45c1e1a9b725.jpg)\n\n![image](https://cdn-mineru.openxlab.org.cn/result/2026-01-06/a081cdb2-f1fa-4e9a-a6fb-c245ed406515/7bb98109c23300b08cf1cc7ddc912a16c461930a251a75a3459be578937faaab.jpg)\n\nFigure 2. Performance of algorithms in the two phases of the evaluation. (a) selection phase: validation set performance of models, which is used to select models for the evaluation phase. Architecture search was performed four different times, each dot represents the performance of a separate NAS run. For each method, the architecture with the highest performance was chosen to be evaluated in the evaluation phase. (b) evaluation phase: The model architecture selected in the selection phase was trained from scratch four separate times and evaluated on the test set. Each dot represents the performance of the selected architecture for each NAS method, trained with different weight initializations.\n\nTable I COMPARISON OF PERFORMANCE VALUES OBTAINED WITH VARIOUS METHODS. PR-AUC IS THE PRECISION-RECALL-AUC [39] AS RECOMMENDED BY [6], ROC-AUC THE AREA UNDER THE RECEIVER OPERATOR CHARACTERISTIC ([40], MORE IS BETTER); VALUES ARE AVERAGED OVER ALL 919 LABELS. TRAINING WAS DONE WITH SUBSAMPLED EPOCHS, SO VALUES MAY NOT BE COMPARABLE WITH OTHER VALUES IN THE LITERATURE.\n\n<table><tr><td></td><td>Method</td><td>Params. (M)</td><td>PR-AUC</td><td>ROC-AUC</td><td>Train Time (GPU-Days)</td></tr><tr><td rowspan=\"4\">Baselines</td><td>DeepSEA</td><td>52.84</td><td>6.44</td><td>71.22</td><td>-</td></tr><tr><td>DanQ</td><td>46.93</td><td>17.27</td><td>81.5</td><td>-</td></tr><tr><td>NCNet-RR</td><td>57.58</td><td>22.15</td><td>84.01</td><td>-</td></tr><tr><td>NCNet-bRR</td><td>47.69</td><td>22.05</td><td>84.31</td><td>-</td></tr><tr><td rowspan=\"7\">Ours</td><td>Genome-RS</td><td>27.01</td><td>21.90</td><td>84.56</td><td>10.22</td></tr><tr><td>Genome-SH</td><td>26.86</td><td>23.44</td><td>85.23</td><td>9.91</td></tr><tr><td>Genome-DARTS</td><td>29.22</td><td>23.90</td><td>85.47</td><td>10.21</td></tr><tr><td>Genome-P-DARTS</td><td>27.08</td><td>22.24</td><td>84.68</td><td>11.61</td></tr><tr><td>Genome-BONAS</td><td>26.25</td><td>21.20</td><td>84.19</td><td>24.03</td></tr><tr><td>Genome-CWP-DARTS</td><td>26.54</td><td>22.97</td><td>84.98</td><td>8.62</td></tr><tr><td>Genome-EDP-DARTS</td><td>26.73</td><td>22.87</td><td>84.95</td><td>10.45</td></tr></table>\n\nIt can be summarized that our uniquely designed search space works very well, as all GenomeNAS algorithms showed strong performance on the DeepSEA task and most outperformed current state-of-the-art baseline models as well as randomly sampled models. We conclude that our unique combination of DAG search spaces for both CNN and RNN cells has great potential for further research.\n\n# APPENDIX A\n\n# APPENDIX: DETAILS ABOUT HYPERPARAMETER SETTINGS\n\nMost hyperparameters were set as in the original publications of the respective methods. We use momentum SGD to ensure that our optimizer is similar to the original DARTS; the CNN weights are optimized with momentum set to 0.9 and weight decay set to  $3 \\times 10^{-4}$ ; the weights of the recurrent part of the network are optimized without momentum and with weight decay set to  $5 \\times 10^{-7}$ .\n\n- Genome-RS: To get an optimization runtime similar to Genome-DARTS, 20 architectures are sampled randomly and trained for 7 epochs before being evaluated.\n\n- Genome-SH: An initial sample of 25 architectures is sampled. Halving happens every 3 epochs by discarding the worse half of models by performance.\n\n- Genome-DARTS: The  $\\alpha$  parameters are optimized as in DARTS with Adam, with initial learning rate  $3 \\times 10^{-3}$ , weight decay  $1 \\times 10^{-3}$ , momentum parameters  $\\beta = (0.9, 0.999)$ . Weight clipping of size 0.25 is applied for stability. As in DARTS, the architecture search runs for 50 epochs.\n\n- Genome-P-DARTS: As in P-DARTS, the initial dropout probability of skip-connects for search space regularization is set to 0.1 in the first stage, 0.2 in the second stage and 0.3 in the third stage. The one-shot model is trained for 25 epochs in each stage, where in the first 10 epochs, only network weights are trained. Network and architecture weights are optimized jointly in the last 15 epochs of each stage. As in P-DARTS,  $\\alpha$  are optimized with Adam with learning rate 0.0006, weight decay 0.001 and momentum  $\\beta = (0.5, 0.999)$ .\n\n- Genome-CWP-DARTS: Unlike Genome-P-DARTS, only the first stage is trained for 25 epochs with the first 10\n\nepochs network parameter optimization only; the other two stages optimize only 15 epochs, optimizing network and architecture weights jointly. This reduces the overall number of epochs from 75 to 55.\n\n- Genome-EDP-DARTS is optimized as Genome-P-DARTS. The maximum number of input edges kept for all vertices is 4 after the first stage, 3 after the second stage, and 2 after the third (final) stage for the CNN networks and 5 after the first stage, 3 after the second stage, and 1 after the third (final) stage for the RNN network.\n\n- Genome-BONAS starts with 60 randomly sampled architectures trained for 60 epochs. Then, two BO iterations are executed where 1000 randomly sampled architectures are evaluated by the surrogate model and the top 60 networks by UCB are proposed for evaluation, again for 60 epochs.\n\nDuring the model search, the batch size is set to 64, and epochs are limited to random a subset of the entire data of 2000 steps (i.e.  $64 \\times 2000$  samples). The final training of architectures is done with 3000 steps and batch size 100.\n\n# APPENDIX B APPENDIX: PRELIMINARY HYPERPARAMETER OPTIMIZATION\n\nDARTS and ENAS run experiments on image classification and language modeling tasks [16], [17]. For image classification using CNNs, an initial learning rate of 0.025 is used, while the RNNs used for language modeling have an initial learning rate of 20. Due to the large difference between the learning rates, we decided to use different learning rates for the convolutional and recurrent parts of our search space.\n\nInitial experiments showed low sensitivity to the learning rate of the convolutional part and high sensitivity to the learning rate of the recurrent part. We therefore decided to run a preliminary grid search to optimize the learning rate and variational dropout rate of the recurrent network. We evaluated the Genome-DARTS method for recurrent learning rate values of 2, 8, and 12, and dropout rates (input / \"embedding\" dropout, recurrent dropout) of (0,0), (0.1,0.05), and (0.3,0.1). The best-performing settings that were chosen were learning rate 8, input dropout 0.1, and recurrent dropout 0.05.",
    "metadata": {
      "md_filename": "MinerU_markdown_NAS_20260106142922_2008425642756669440.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_NAS_20260106142922_2008425642756669440.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "521cee80eb0552bc",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_NT_20260106142915_2008425609286127616",
    "title": "Nucleotide Transformer: building and evaluating robust foundation models for human genomics",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# Models\n\nLMs have been primarily developed within NLP to model spoken languages $^{1,2}$ . An LM is a probability distribution over sequences of tokens (often words) that is given any sequence of words; an LM will return the probability for that sentence to exist. LMs gained in popularity thanks to their ability to leverage large unlabeled datasets to generate general-purpose representations that can solve downstream tasks even when few supervised data are available $^{49}$ . One technique to train LM task models to predict the most likely tokens at masked positions in a sequence is often referred to as masked language modeling (MLM). Motivated by results obtained with MLM in the field of protein research $^{4,5}$ , where proteins are considered as sentences and amino acids as words, we apply MLM to train LMs transformers in genomics, considering sequences of nucleotides as sentences and k-mers (with  $k = 6$ ) as words. Transformers are a class of DL models that achieved breakthroughs in machine-learning fields, including NLP and computer vision. They consist of an initial embedding layer that transform positions in the input sequence into an embedding vector, followed by stack of self-attention layers that sequentially refine these embeddings. The main technique to train LM transformers with MLM is called bidirectional encoder representations from transformers (BERT) $^{1}$ . In BERT, all positions in the sequence can attend to each other allowing the information to flow in both directions, which is essential in the context of DNA sequences. During training, the final embedding of the network is fed to a language model head that transforms it into a probability distribution over the input sequence.\n\nArchitecture. All our models follow an encoder-only transformer architecture. An embedding layer transforms sequences of tokens into sequences of embeddings. Positional encodings are then added to each embedding in the sequence to provide the model with positional information. We use a learnable positional encoding layer that accepts a maximum of 1,000 tokens. We used six-mer tokens as a trade-off between sequence length (up to 6 kb) and embedding size, and because it achieved the highest performance when compared to other token lengths. The token embeddings are then processed by a transformer layer stack. Each transformer layer transforms its input through a layer normalization layer followed by a multi-head self-attention layer. The output of the self-attention layer is summed with the transformer layer input through a skip connection. The result of this operation is then passed through a new layer normalization layer and a two-layer perceptron with GELU activations<sup>50</sup>. The number of heads, the embedding dimension, the number of neurons within the perceptron hidden layer and the total number of layers for each model can be found in Supplementary Table 1. During self-supervised training, the embeddings returned by the final layer of the stack are transformed by a language model head into a probability distribution over the existing tokens at each position in the sequence.\n\nOur second version NT-v2 models include a series of architectural changes that proved more efficient: instead of using learned positional embeddings, we use rotary embeddings $^{51}$ , which are used at each attention layer; we use gated linear units with swish activations without bias, making NLPs more efficient. These improved models also accept sequences up to 2,048 tokens, leading to a longer context window of  $12\\mathrm{kb}$ .\n\nTraining. The models are trained following BERT methodology<sup>1</sup>. At each training step a batch of tokenized sequences is sampled. The batch size is adapted to available hardware and model size. We conducted all experiments on clusters of A100 GPUs and took batches of sizes 14 and 2 sequences to train the '500M' and '2.5B' parameter models, respectively. Within a sequence, of a subset of  $15\\%$  of tokens,  $80\\%$  are replaced by a special masking ('MASK') token. For training runs on the Human reference genome and multispecies datasets, an additional  $10\\%$\n\nof the  $15\\%$  subset of tokens are replaced by randomly selected standard tokens (any token different from the class ('CLS'), padding ('PAD') or MASK token), as was carried out in BERT. For training runs on the 1000G dataset, we skipped this additional data augmentation, as the added noise was greater than the natural mutation frequency present in the human genome. For each batch, the loss function was computed as the sum of the cross-entropy losses, between the predicted probabilities over tokens and the ground-truth tokens, at each selected position. Gradients were accumulated to reach an effective batch size of 1 million tokens per batch. We used the Adam optimizer<sup>52</sup> with a learning rate schedule and standard values for exponential decay rates and epsilon constants,  $\\beta_{1} = 0.9$ ,  $\\beta_{2} = 0.999$  and  $\\epsilon = 1 \\times 10^{-8}$ . During a first warmup period, the learning rate was increased linearly between  $5 \\times 10^{-5}$  and  $1 \\times 10^{-4}$  over 16,000 steps before decreasing following a square root decay until the end of training.\n\nWe slightly modified the hyperparameters of our NT-v2 models: the optimizer and learning rate schedule are kept the same; however, we increased batch size to 512 (1 million tokens per batch). Inspired by Chinchilla scaling laws $^{45}$ , we also trained our NT-v2 models for longer duration compared to other DL models. Specifically, we pre-trained our NT-v2 50M- and 250M-parameters models for 300 billion tokens, while our '250M' and '500M' parameters models were trained for up to 1 trillion tokens to understand the scaling laws at play. In comparison, the NT-v12.5B-parameters models were trained for 300 billion tokens and their 500M counterparts were trained for 50 billion tokens. In the end we used the following model checkpoints for the NT-v2 models: checkpoint 300 billion tokens for the '50M' and '100M' models, checkpoint 800 billion tokens for the '250M' model and checkpoint 900 billion tokens for the '500M' model.\n\nProbing. We refer to probing the assessment of the quality of the model embeddings to solve downstream tasks. After training, for each task, we probe each layer of the model and compare several downstream methods to evaluate in depth the representations capabilities of the model. In other words, given a dataset of nucleotide sequences for a downstream task, we compute and store the embeddings returned by ten layers of the model. Then, using the embeddings of each individual layer as inputs, we trained several downstream models to solve the downstream task. We tested logistic regression with the default hyperparameters from scikit-learn $^{53}$  and an MLP. As we observed that the choice of hyperparameters, such as the learning rate, the activation function and the number of layers per hidden layer impacted final performance, we also ran hyperparameters sweeps for each downstream model. We used a tenfold validation scheme, where the training dataset was split ten times in a training and validation set, that contain different shuffles with  $90\\%$  and  $10\\%$  of the initial set. For a given set of hyperparameters, ten models were trained over the ten splits and their validation performances were averaged. This procedure is run 100 times with a Tree-structured Parzen Estimator solver $^{54}$  guiding the search over the hyperparameters space, before evaluating the best-performing set of models on the test set. Therefore, for each downstream task, for ten layers of each pre-trained model, the performance on the test set is recorded at the end of the hyperparameters search. The hyperparameters of the best-performing probe across the pre-trained models and their layers are reported in Supplementary Table 8. This probing strategy resulted in 760,000 downstream models trained, which provides detailed analysis into various aspects of training and using LMs, such as the role of different layers on downstream task performance.\n\nAs a baseline, we evaluated the performance of a logistic regression model that takes as input the tokenized sequence, before passing the tokens through the transformer layers. Using the raw tokenized sequences as input yielded much better performance than using a vector where the token ids were one-hot encoded and passed through a pooling layer (summing or averaging, over the sequence length axis).\n\nFine-tuning. In addition to probing our models through embedding extraction at various layers, we also performed parameter-efficient fine-tuning through the IA $^3$  technique $^{35}$ . Using this strategy, the language model head is replaced by either a classification or regression head depending on the task at hand. The weights of the transformer layers and embedding layers are frozen and new, learnable weights are introduced. For each transformer layer, we introduced three learned vectors  $l_{k} \\in \\mathbb{R}^{d_{k}}$ ,  $l_{v} \\in \\mathbb{R}^{d_{v}}$  and  $l_{\\mathrm{ff}} \\in \\mathbb{R}^{d_{\\mathrm{ff}}}$ , which were introduced in the self-attention mechanism as:\n\n$$\n\\operatorname {s o f t m a x} \\left(\\frac {Q (l _ {k}) \\odot K ^ {T}}{\\sqrt {d _ {k}}}\\right) (l _ {v} \\odot V)\n$$\n\nand in the position-wise feed-forward networks as  $(l_{\\mathrm{ff}}\\gamma (W_1x))W_2$ , where  $\\gamma$  is the feed-forward network nonlinearity and  $\\odot$  represents the element-wise multiplication. This adds a total of  $L(d_k + d_\\nu +d_{\\mathrm{ff}})$  new parameters, where  $L$  is the number of transformer layers. We refer to these learnable weights as rescaling weights. The intuition is that during fine-tuning these weights will weigh the transformer layers to improve the final representation of the model on a downstream task, so that the classification/regression head can more accurately solve the problem. As we observed layer specialization during probing, we speculate that this fine-tuning technique will similarly select layers with greater predictive ability for particular tasks.\n\nIn practice, the number of additional parameters introduced by rescaling weights and the classification/regression head weights represented approximately  $0.1\\%$  of the total number of weights of the model. This increased fine-tuning speed as just a fraction of parameters needed updating. Similarly, it alleviated storage requirements, needing to create space for just  $0.1\\%$  new parameters over 500 million and 2.5 billion for each downstream task using traditional fine-tuning. For instance, for the 2.5B-parameter models, the weights represent 9.5 GB. Considering 18 downstream tasks, classical fine-tuning would have required  $9.5 \\times 18 = 171$  GB, whereas parameter-efficient fine-tuning required only 171 MB.\n\nLike in the probing scheme, the training dataset was split ten times in a training and validation set, which contains different shuffles with  $90\\%$  and  $10\\%$  of the initial set. For each split, the model was fine-tuned for 10,000 steps and parameters yielding the highest validation score were then used to evaluate the model on the test set. We used a batch size of eight and the Adam optimizer with a learning rate of  $3 \\times 10^{-3}$ . Other optimizer parameters were maintained from training regimens. Each model is fine-tuned for 10,000 steps for each task. These hyperparameters were selected as they led to promising results in the field of NLP<sup>35</sup>. Diverging hyperparameter choices did not yield significant gains in our experiments. We have also compared this approach with fine-tuning from a randomly initialized checkpoint.\n\nComparison with supervised BPNet baseline. As baseline supervised models, we trained different variants of the BPNet convolutional architecture from scratch on each of the 18 tasks. We tested the original architecture (121,000 parameters), a large one (28 million) and an extra-large (113 million) one. For each, the hyperparameters were manually adjusted to yield the best performance on the validation set. We implemented a tenfold cross-validation strategy to measure the performance of each of the 18 models per architecture.\n\nComparison with published pre-trained genomics models. We compared the fine-tuned performance of NT models on the 18 downstream tasks with three different pre-trained models: DNABERT-2 (ref. 23), HyenaDNA (1-kb and 32-kb-context length,[25]) and Enformer[19]. We excluded DNABERT-1 from this comparison as it can only handle a maximum input length of 512 bp and thus could not be used for most tasks[20]. We ported the architecture and trained weights of each model to our code framework and performed parameter-efficient fine-tuning\n\non the transformer part of every model as described above, using the same cross-validation scheme for a fair comparison. All results can be visualized in an interactive leaderboard (https://huggingface.co/spaces/InstaDeepAI/nucleotide_transformer_benchmark). Only for HyenaDNA we performed full fine-tuning due to the incompatibility of our parameter-efficient fine-tuning approach with the model architecture.\n\nNote that Enformer has been originally trained in a supervised fashion to solve chromatin and gene expression tasks $^{19}$ . For the sake of benchmarking, we reused the provided model torso as a pretrained model for our benchmark, which is not the intended and recommended use of the original paper; however, we think this comparison is interesting to highlight the differences between self-supervised and supervised learning for pretraining and observe that the Enformer is indeed a very competitive baseline even for tasks that differ from gene expression. Still, we note that Enformer has been pretrained originally using different data splits and thus its performance in our benchmark evaluation might be inflated due to potential data leakage.\n\n# Pre-training datasets\n\nThe Human reference genome dataset. The Human reference dataset was constructed by considering all autosomal and sex chromosomes sequences from the reference assembly GRCh38/hg38 (https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.26) and reached a total of 3.2 billion nucleotides.\n\nThe 1000G dataset. To inform the model on naturally occurring genetic diversity in humans, we constructed a training dataset including genetic variants arising from different human populations. Specifically, we downloaded the variant calling format (VCF) files (http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/dataCollections/1000G_2504_high_coverage/working/20201028_3202_phased/) from the 1000 Genomes (1000G) project $^{55}$ , which aims at recording genetic variants occurring at a frequency of at least  $1\\%$  in the human population. The dataset contained 3,202 high-coverage human genomes, originating from 27 geographically structured populations of African, American, East Asian and European ancestry as detailed in Supplementary Table 2, making up a total of 20.5 trillion nucleotides. Such diversity allowed the dataset to encode a better representation of human genetic variation. To allow haplotype reconstruction in the FASTA format from the VCF files, we considered the phased version of the data, which corresponded to a total of 125 million mutations, 111 million and 14 million of which are single-nucleotide polymorphisms (SNPs) and indels, respectively.\n\nThe Multispecies dataset. The selection of genomes for the Multispecies dataset was primarily based on two factors: (1) the quality of available reference genomes and (2) diversity among the species used. The genomes chosen for this dataset were selected from the ones available at the Reference Sequence (RefSeq) collection of NCBI (https://www.ncbi.nlm.nih.gov/). To ensure a diverse set of genomes, we randomly selected one genome at the genus level from each of the main groupings available in RefSeq (archaea, fungi, vertebrate_mammalian, vertebrate_other, etc.). However, due to the large number of available bacterial genomes, we opted to include only a random subset of them. Plant and virus genomes were not taken into account, as their regulatory elements differ from those of interest in this work. The resulting collection of genomes was downsampled to a total of 850 species, whose genomes add up to 174 billion nucleotides. The final contribution of each class, in terms of number of nucleotides, to the total number of nucleotides in the dataset, displayed in Supplementary Table 3, is the same as in the original collection parsed from NCBI. Finally, we enriched this dataset by selecting several genomes that have been heavily studied in the literature (Supplementary Table 4).\n\n# Data preparation\n\nOnce the FASTA files of each genome/individual were collected, they were assembled into one unique FASTA file per dataset that was then pre-processed before training. During this data-processing phase, all nucleotides other than A, T, C, G were replaced by N. A tokenizer was employed to convert strings of letters to sequences of tokens. The tokenizer used as the alphabet the  $4^{6} = 4,096$  possible six-mer combinations obtained by combining A, T, C, G, as well as five extra tokens to represent stand-alone A, T, C, G and N. It also included three special tokens, namely the padding (PAD), masking (MASK) and the beginning of sequence (also called class (CLS)) token. This adds to a vocabulary of 4,104 tokens. To tokenize an input sequence, the tokenizer will start with a class token and then convert the sequence starting from the left, matching six-mer tokens when possible or falling back on the stand-alone tokens when needed (for instance when the letter N is present or if the sequence length is not a multiple of 6).\n\nFor the Multispecies and Human reference dataset, genomes were split into overlapping chunks of 6,100 nucleotides, each sharing the first and last 50 nucleotides with the previous and last chunk, respectively. As a data augmentation exercise, for each epoch and chunk, a starting nucleotide index was randomly sampled between 0 and 100, and the sequence was then tokenized from this nucleotide until 1,000 tokens was reached. The number of epochs was determined depending on the dataset so that the model processed a total of 300 billion tokens during training. At each step, a batch of sequences sampled randomly within the epoch set was fed to the model. For the 1000G dataset, batches of sequences from the Human reference genome, prepared as specified above, were sampled at each step. Then, for each sampled chunk, an individual from the 1000G dataset was randomly selected, and if that individual carried mutations at the positions and chromosome corresponding to that chunk, these mutations were introduced into the sequence and the corresponding tokens were replaced. This data-processing technique ensured uniform sampling both over the genome and over the individuals during training, as well as enabled us to efficiently store only mutations for each individual, instead of full genomes.\n\nHardware. All models were trained on the Cambridge-1 Nvidia supercomputer system, using 16 nodes, each equipped with eight A100 GPUs, leading to a total of 128 A100 GPUs used. During training, model weights were replicated on each GPU, while batches were sharded across GPUs. Gradients were computed on each shard and accumulated before being averaged across devices and backpropagated. We relied on the Jax library (https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html), which relies on the NCCL (https://developer.nvidia.com/nccl) protocol to handle communications between nodes and devices, and observed an almost linear decrease of the training time with respect to the number of GPUs available. The 500M-parameter models were trained on a single node for a day, whereas the 2.5B-parameter models required the whole cluster for 28 days to be trained. The NT-v2 models with varying number of parameters ranging from 50 million to 500 million were similarly trained on a single node for a single day. All fine-tuning runs were performed on a single node with eight A100 GPUs. As for the training runs, the models weights were replicated and batches were distributed across GPUs. As we used a batch size of eight for fine-tuning, each GPU processed a single sample before averaging the gradients and applying them. On average, a fine-tuning run lasted 20 min for the 500M-parameter models and 50 min for the 2.5B-parameter models.\n\nFor the probing experiments, all embeddings (for all sequences in all downstream tasks, for selected layers of each model) were computed and stored on a single node with eight A100 GPUs, requiring 2 days to compute. Then, 760,000 downstream models were fit on a cluster of 3,000 CPUs, requiring 2.5 days.\n\n# Nucleotide Transformer downstream tasks\n\nEpigenetic marks prediction. Histone ChIP-seq data for ten histone marks in the K562 human cell line were obtained from ENCODE $^{31}$  (https://www.encodeproject.org/). We downloaded bed narrow-Peak files with the following identifiers: H3K4me3 (ENCFF706WUF), H3K27ac (ENCFF544LXB), H3K27me3 (ENCFF323WOT), H3K4me1 (ENCFF135ZLM), H3K36me3 (ENCFF561OUZ), H3K9me3 (ENCFF963GZJ), H3K9ac (ENCFF891CHI), H3K4me2 (ENCFF749KLQ), H4K20me1 (ENCFF909RKY) and H2AFZ (ENCFF213OTI). For each dataset, we selected 1-kb genomic sequences containing peaks as positive examples and all 1-kb sequences not overlapping peaks as negative examples.\n\nPromoter sequence prediction. We built a dataset of promoter sequences to evaluate the capabilities of the model to identify promoter motifs. We downloaded all human promoters from the Eukaryotic Promoter Database $^{30}$  (https://epd.expasy.org/epd/), spanning 49 bp upstream and 10 bp downstream of transcription start sites (https://epd.expasy.org/ftp/expdnew/H_sapiens/006/Hs_EPDnew_006_hg38.bed). This resulted in 29,598 promoter regions, 3,065 of which were TATA-box promoters (using the motif annotation at https://epd.expasy.org/ftp/expdnew/H_sapiens/006/db/promoter_motifs.txt). We selected 300-bp genomic sequences containing promoters as positive examples and all 300-bp sequences not overlapping promoters as negative examples. These positive and negative examples were used to create three different binary classification tasks: presence of any promoter element (Promoter all), a promoter with a TATA-box motif (Promoter TATA) or a promoter without a TATA-box motif (Promoter non-TATA).\n\nEnhancer sequence prediction. Human enhancer elements were retrieved from ENCODE's SCREEN database (https://screen.wenglab.org/)32. Distal and proximal enhancers were combined. Enhancers were split in tissue-specific and tissue-invariant based on the vocabulary from Meuleman et al.33 (https://www.meuleman.org/research/dhsindex/). Enhancers overlapping regions classified as tissue-invariant were defined as that, while all other enhancers were defined as tissue-specific. We selected 400-bp genomic sequences containing enhancers as positive examples and all 400-bp sequences not overlapping enhancers as negative examples. We created a binary classification task for the presence of enhancer elements in the sequence (Enhancer) and a multilabel prediction task with labels being tissue-specific enhancer, tissue-invariant enhancer or none (Enhancer types).\n\nSplice site prediction. We obtained all human annotated splice sites from GENCODE $^{29}$  V44 gene annotation. Annotations were filtered to exclude level-3 transcripts (automated annotation), so that all training data were annotated by a human. We used extract_spliceSites.py from HISAT2 (ref. 56) (https://github.com/DaehwanKimLab/hisat2/blob/master/hisat2extract_spliceSites.py) to extract respective splice site annotations. We selected 600-bp genomic sequences containing a splice acceptor or donor site in the center as positive examples and all 600-bp sequences not overlapping splice sites as negative examples. We used these sequences to create three different tasks to evaluate splice prediction: a multilabel prediction task with labels being acceptor, donor or none (Splice site all); a binary classification task for the prediction of splice acceptors (Splice acceptor); and a binary classification task for the prediction of splice donors (Splice donor).\n\nDataset splits and performance evaluation. Model training and performance evaluation were performed on different sets of chromosomes from the human genome. Namely, sequences from chromosomes 20 and 21 were used for the test and the remainder were used for training the different models. Sequences with Ns were removed. We balanced each dataset by subsampling the negative examples to\n\nthe same number of positive examples. To obtain small-sized datasets that can be used to benchmark any new design choice or model quickly, we further randomly subsampled examples to a maximum of 30,000 for training and 3,000 for validation and testing (see details in Supplementary Table 5).\n\nWe used a tenfold cross-validation scheme to evaluate each model, where the training set is split into ten folds, and each time we used nine of those parts for training and reserved one-tenth for validation and selecting the final checkpoint. We repeated this procedure ten times per model, each time reserving a different tenth for validation and evaluating the final performance on the same held-out test set. We used the median performance across the ten folds as the final performance metric of each model on a given task.\n\nFor a consistent performance evaluation across tasks, we used the binary or multi-class MCC as the metric as it is robust to uneven label ratios. For a final comparison across models, we calculated for each model the mean MCC across the three different categories of task (chromatin profiles, regulatory elements and splicing), where for each category we used the median MCC across tasks.\n\n# Additional downstream tasks\n\nChromatin profile prediction. We used the DeepSEA dataset (http://deepsea.princeton.edu/media/code/deepsea_train Bundle.v0.9.tar.gz) compiled by Zhou et al. $^{10}$  for chromatin profile prediction. The dataset is composed of 2.4 million sequences, each of size 1,000 nucleotides, and associated with 919 chromatin features. These include 690 TF, 125 DNase and 104 histone features. As in the original publication, our model was trained simultaneously on the 919 classification tasks, with 919 independent classification heads, and a loss was taken as the average of the cross-entropy losses. As each label is highly unbalanced and is composed mostly of negative samples, the losses associated with positive samples are upscaled by a factor of 8. Contrary to the DeepSEA method $^{10}$ , which trained two models independently, one on the forward sequences and one on the corresponding reverse-complementary, and evaluated the average of their predictions, the model presented here was trained only on the forward sequences.\n\nSpliceAl benchmark. We used the scripts available at the Illumina Base-space platform (https://basespace.illumina.com/projects/66029966/) to reproduce the training dataset presented in SpliceAl $^{36}$ . In brief, this training dataset was constructed using GENCODE V24lift37 annotations and RNA-seq data from the GTEx cohort, focusing solely on splice site annotations from the principal transcript. The training dataset comprised annotations from genes located on chromosomes 2, 4, 6, 8 and 1022, as well as chromosomes X and Y, while annotations from genes on the remaining chromosomes, which were not paralogs, constituted the test dataset. Each sequence produced by this pipeline had a length of 15,000 bp, with the central 5,000 bp containing the sites to be predicted. Additional details about the construction of the training dataset can be found in the original publication. We adapted this original SpliceAl dataset to be able to run our models by reducing the sequence length to 6,000 bp (for NT-v1 model) and 12,000 bp (for NT-v2 model), reducing the flanking contexts but keeping the central 5,000 bp. We also removed sequences that contained Ns. When comparing against SpliceAl on the first dataset, which we refer to as SpliceAl-6k, we appended 9,000 N nucleotides as a flanking sequence as SpliceAl is based on a model with a 15,000-bp input. When comparing against SpliceAl on the second dataset, we report the performance presented in the original publication, which, compared to this dataset, included a sequence length of 15,000 bp instead of 12,000 bp.\n\nThis task is a multilabel classification for each of the input sequence's nucleotides similar to SpliceAl $^{36}$  (Fig. 5d). From each embedding outputted by the transformer model, a head predicts, for each of the six nucleotides represented by the token embedding, three label probabilities: splice acceptor, splice donor or none. The head is a simple\n\nclassification layer that predicts 18 classes (three labels for each of the six nucleotides). To ensure that each embedding is associated with a six-mer, the sequences were cut so that their length was divisible by 6. Furthermore, all sequences with Ns were removed from both training and test set, which represented a negligible portion of the data. Note that if we were to use a Byte Pair Encoding tokenizer such as DNABERT-2 (ref. 23), the number of nucleotides represented by each embedding would vary and make nucleotide-level prediction tasks substantially trickier to implement.\n\nEnhancer activity prediction. We used the DeepSTARR enhancer activity dataset released by de Almeida et al. $^{12}$  (https://zenodo.org/ record/5502060#.Y9qO7hzMLUc). The dataset is composed of 484,052 DNA sequences of size 249 nucleotides, each measured for their quantitative enhancer activity toward a developmental or a housekeeping promoter. We added two independent regression heads to our models to predict both enhancer activities in simultaneous. Following the methodology used elsewhere $^{3}$ , we chose to treat this regression task as a multilabel classification problem. Specifically, each label  $y$  was discretized over a set of 50 values  $(b_{i})_{i\\in [1,50]}$ , evenly spaced between the minimum and maximum value. For each label, the model predicts the normalized weights  $(w_{i})_{i\\in [1,50]}$  such that\n\n$$\ny = \\sum_ {i = 1} ^ {5 0} w _ {i} b _ {i}.\n$$\n\n# Additional performance analysis\n\n$t$ -SNE projections of embeddings.  $t$ -distributed stochastic neighbor embedding ( $t$ -SNE) was used to reduce NT inner embeddings to two-dimensional vectors to visualize the separation of different genomic elements. NT embeddings for each genomic element were computed at several transformer layer outputs and the mean embeddings computed across the sequence locations corresponding to the element were calculated. These mean embeddings were then passed as input into a  $t$ -SNE reducer object with default parameters from the sklearn Python package<sup>53</sup>.\n\nReconstruction accuracy and perplexity. We studied how pretrained models could reconstruct masked tokens. We considered a trained language model with parameters  $\\theta$ . Within a nucleotide sequence  $\\mathbf{s}$  of interest, we masked tokens using one of two strategies (we replaced the tokens at these positions with the masking (MASK) token). We either masked the central token of the sequence only, or we masked randomly  $15\\%$  of the tokens within the sequence. The masked sequence is then fed to the model and the probabilities over tokens at each masked position are retrieved. The loss function  $l(\\theta, \\mathbf{s})$  and the accuracy  $acc(\\theta, \\mathbf{s})$  are defined as follows:\n\n$$\n\\left\\{ \\begin{array}{l} l (\\theta , \\mathbf {s}) = \\sum_ {i \\in \\mathcal {P} _ {\\text {m a s k e d}}} \\sum_ {\\operatorname {t o k} \\in \\mathcal {V}} \\log p (\\theta , i, \\operatorname {t o k}) \\cdot \\mathbf {1} (\\operatorname {t o k} = \\mathbf {s} (i)) \\\\ a c c (\\theta , \\mathbf {s}) = \\frac {1}{| \\mathcal {P} _ {\\text {m a s k e d}} |} \\sum_ {i \\in \\mathcal {P} _ {\\text {m a s k e d}}} \\mathbf {1} \\left(\\underset {\\operatorname {t o k} \\in \\mathcal {V}} {\\operatorname {a r g m a x}} \\left(\\log p (\\theta , i, \\operatorname {t o k}\\right)\\right) = \\mathbf {s} (i) \\end{array} \\right.\n$$\n\nwhere  $\\mathcal{P}_{\\mathrm{masked}}$  is the set of masked positions and  $\\nu$  is the vocabulary (the set of all existing tokens). The perplexity is usually defined in the context of autoregressive generative models. Here, we rely on an alternative definition used in Rives<sup>4</sup>, and define it as the exponential of the loss function computed over the masked positions:\n\n$$\n\\text {p e r p l e x i t y} (\\theta , \\mathbf {s}) = 2 ^ {l (\\theta , \\mathbf {s})}. \\tag {1}\n$$\n\nPerplexity measures how well a model can reconstruct masked positions and is a more-refined measure than accuracy as it does also account for magnitude. In contrast to accuracy, lower perplexity suggests better reconstruction ability, thus better performance.\n\nReconstruction of tokens in different genomics elements. We also performed this token-reconstruction approach across the full chromosome 22 in 6-kb windows. We only kept windows without Ns. For each window masked each token at a time, recovering the predicted probability for the original token in the sequence. We display these scores as a WashU Epigenome Browser session (https://shorturl.at/jov28). To obtain average token probabilities across different types of genomic elements, we retrieved gene annotation regions from Ensembl (exons, introns, splice acceptors and donors,  $5^{\\prime}$  UTR,  $3^{\\prime}$  UTR, transcription start site and transcription end site) (https://www.ensembl.org/info/data/ftp/index.html), polyA signal sites from GENCODE (https://www.gencodegenes.org/human/) and regulatory elements from ENCODE (enhancers, promoters and CTCF-bound sites from the SCREEN database) (https://api.wenglab.org/screen_v13/f downloads/GRCh38-ccREs.bed).\n\nFunctional variant prioritization. To obtain genetic variants with varying levels of associated severity we used Variant Effect Prediction (VEP) software $^{40}$  and annotated sequences across the human genome. Specifically, we randomly sampled sequences throughout the human genome and kept genetic variants within those sequences annotated to any of the following classes: 'intron variant', 'intergenic variant', 'regulatory region variant', 'missense variant', '3 prime UTR variant', 'synonymous variant', 'TF binding site variant', '5 prime UTR variant', 'splice region variant' and 'stop-gained variant'. After keeping only SNPs and filtering out variants annotated to more than one consequence (for example those annotated as stop-gained and splice variants), we obtained a final dataset composed of 920 genetic variants per class.\n\nAs a positive set of functional genetic variants, we compiled SNPs from four different resources. We used SNPs associated with gene expression (eQTLs) and to meQTLs from the GRASP database $^{57}$  with a  $P$  value  $< 10^{-12}$ , SNPs with 'likely pathogenic' annotations from ClinVar $^{58}$  and SNPs reported in HGMD (public v.2020.4) $^{59}$ . After these filters, we retained a total of 80,590, 11,734, 70,224 and 14,626 genetic variants for the eQTLs, meQTLs, ClinVar and HGMD SNP datasets, respectively. For each of these four datasets, we then constructed a set of negative variants based on SNPs from the 1000G Project with a minor allele frequency  $>5\\%$  that did not overlap with any variant reported in the dataset tested and that were within 100 kb of the associated variants, resulting in four balanced datasets.\n\nTo compute zero-shot-based scores for a given site of interest we did the following. For each SNP, we obtained a 6,000-bp sequence centered on the SNP of interest based on the Human reference genome. We then created two sequences, one carrying the reference allele and a second carrying the alternative allele at the SNP position. We then computed several zero-shot scores that captured different aspects of the vector distances in the embedding space between those two sequences, namely: (1) the L1 distance (Manhattan); (2) the L2 distance (Euclidean); (3) the cosine similarity; and (4) the dot-product (not normalized cosine similarity). We also computed the loss of the alternative allele and the difference in the loss between the sequence carrying the alternative and reference alleles, as two additional zero-shot scores. In the case of functional variants, in addition to zero-shot scores, we also fine-tuned the transformer models to classify positive and negative variants. We employed a similar strategy to the one previously described, with the primary difference being that the training and test sets were divided by chromosomes and strictly kept as nonoverlapping. Specifically, we divided the 22 chromosomes into five sets and sequentially used each of them as a test set and the four others as a training set. By fine-tuning the training set we could derive the probabilities of being a positive variant for each sequence in the test set. We used those probabilities as a score for each SNP.\n\nTo compare these predictions against other methods, we randomly sampled 10,000 positive and negative SNPs from each of the four datasets. We then used the Combined Annotation Dependent Depletion tool (v.GRCh38-v1.6) to compute CADD, GERP, phastCons and phyloP\n\nscores. The DeepSEA scores were computed using the Beluga model available at https://hb.flatironinstitute.org/sei/. The score considered was the 'disease impact score' reported for each SNP.\n\nAttention maps analysis. We analyzed how attention maps gathered from the pretrained models capture key genomic elements. We followed a methodology proposed in previous work<sup>38</sup>. For a genomic element, we defined the indicator function over tokens  $f(i)$  that equals 1 if one or several nucleotides within token  $i$  belong to the element, and 0 otherwise. We computed the average proportion of attention focused on that genomic element, in one attention head, aggregated over a dataset of nucleotide sequences  $\\mathbf{X}$  as:\n\n$$\np _ {\\alpha} (f) = \\frac {1}{| \\mathbf {X} |} \\sum_ {\\mathbf {x} \\in \\mathbf {X}} \\frac {\\sum_ {i} \\sum_ {j} f (i) \\mathbf {1} (\\alpha (i , j) > \\mu)}{\\sum_ {i} \\sum_ {j} \\mathbf {1} (\\alpha (i , j) > \\mu)}\n$$\n\nwhere  $\\alpha(i,j)$  is the attention coefficient between tokens  $i$  and tokens  $j$ , defined such that  $\\sum_{i} \\alpha_{i,j} = 1$  and  $\\mu$  is a confidence threshold.\n\nWe computed the values of  $p_{\\alpha}(f)$  for all the heads and all the layers of all models, and considered nine elements ('5' UTR', '3' UTR', 'exon', 'intron', 'enhancer', 'promoter', 'CTCF binding site', 'open chromatin' and 'transcription factor binding sites'). We perform these analyses over a dataset made of 90,000 sequences, 10,000 per feature, of length 6 kb extracted from the Human reference genome. The average proportion of tokens belonging to each element can be found in Supplementary Table 10. For each sequence, the position of the feature within the sequence was sampled uniformly during the dataset creation. As suggested in previous work[38], we selected a confidence threshold  $\\mu = 0.3$  for all experiments.\n\nWe considered that a feature was captured by an attention head if the quantity  $p_{\\alpha}(f)$  was significantly greater than the natural occurring frequency of the feature within the dataset (Supplementary Table 10). To validate this, we conducted a two-proportion  $z$ -test with the null hypothesis as the natural frequency of the feature, and the alternate hypothesis as  $p_{\\alpha}(f)$ . The total number of heads of each model was used as a Bonferroni correction to the significance level ( $\\alpha$ ) of 0.05. We computed  $z$ -scores and associated  $P$  values for each head in every model for every genomic element as follows:\n\n$$\nZ = \\frac {\\hat {p} _ {1} - \\hat {p} _ {2}}{\\sqrt {\\hat {p} (1 - \\hat {p}) \\left(\\frac {1}{n _ {1}} + \\frac {1}{n _ {2}}\\right)}}\n$$\n\nwhere  $\\hat{p}_1$  represents the proportion of attention above  $\\mu$  associated with each genomic element,  $\\hat{p}_2$  represents the proportion of the sequence occupied by the genomic element,  $n_1$  is the total number of sequence positions with attention above  $\\mu$  and  $n_2$  is the total number of sequence positions. Attention heads with  $P$  values below the Bonferroni-corrected significance level were considered to be significant.\n\nPrediction of important TF motif instances from DeepSTARR data. We retrieved experimental mutagenesis data from the DeepSTARR dataset $^{12}$ , where individual TF motif instances are mutated and their impact is measured in the activity of developmental and housekeeping enhancers. We assessed the performance of the fully fine-tuned NT 2.5B Multispecies model (as it was the model with the highest test set performance) to predict the contribution of each TF motif instance by predicting the activity of the wild-type and respective motif-mutant sequence and calculating their  $\\log_2$  fold change. We compared our predicted mutation effects with the ones predicted by the original method DeepSTARR and the experimentally derived  $\\log_2$  fold changes.",
    "metadata": {
      "md_filename": "MinerU_markdown_NT_20260106142915_2008425609286127616.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_NT_20260106142915_2008425609286127616.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "cc433a2c3f8bb288",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_Orca_20260106142905_2008425565627621376",
    "title": "Sequence-based modeling of three-dimensional genome architecture from kilobase to chromosome scale",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "Orca model architecture for multiscale 3D genome prediction. The Orca model architecture is composed of a hierarchical sequence encoder and a multilevel cascading decoder, designed to provide a 'zooming' series of predictions at multiple scales (Fig. 1). The hierarchical sequence encoder transforms a large input sequence up to  $256\\mathrm{Mb}$  to a series of sequence representations at multiple resolutions. A series of cascading decoders each predict an interaction matrix, which represents all pairwise genome interactions within a window of varying sizes from 1 to  $256\\mathrm{Mb}$  at different resolutions. All predicted interaction matrices are of size  $250\\times 250$  and all predicted scores represent log fold over distance-based background. The decoder at each level takes the sequence encoding at the corresponding resolution as input. The top-level decoder receives input from the entire sequence at the lowest resolution, and lower levels receive sequence representations at higher resolutions. For example, the 32-Mb level decoder receives 128-kb resolution sequence encoding for 32-Mb sequence and the 1-Mb level decoder receives 4-kb resolution sequence encoding for 1-Mb sequence. In addition, except for the top-level decoder, lower-level decoders also receive the prediction from the upper level as input (for example, 1-Mb level decoder receives 2-Mb level prediction, cropped to the 1-Mb region), and all multilevel decoders also receive a distance encoding matrix as input. The encoder computation starts from a bottom-up pass (high resolution to low resolution) starting from raw sequence with one-hot encoding, followed by a top-down pass to introduce longer-range information to the finer-resolution representations (Supplementary Fig. 1). The decoder computation follows a top-down order (long maximum distance to short maximum distance, low resolution to high resolution) and each lower-level decoder receives the upper-level prediction as input. The architecture and input are described in more detail below, and the detailed architecture of models is available in Supplementary Fig. 1 and the code repository.\n\nBoth the encoder and decoders are convolutional networks with residual connections. The hierarchical sequence encoder alternates between one-dimensional (1D) residual convolution blocks and max-pooling layers. More specifically, the first section of the sequence encoder converts the one-hot sequence encoding into 4-kb-resolution sequence representations with a convolutional architecture adapted from the Sei model[42] that uses a dual linear + nonlinear path design that stacks nonlinear blocks with residual connections on top of the linear blocks (Supplementary Fig. 1). The first section of the encoder contains 28 convolution layers each with 64-128 channels. With the 4-kb-resolution sequence encoding as input, the upper sections of the encoder create a series of sequence encoding at 4-kb, 8-kb, ..., 1,024-kb resolutions with factors of 2 with a similar residual block structure, using 4 convolution layers per resolution with 128 channels.\n\nTo predict two-dimensional (2D) interaction matrices at multiple scales, a cascading series of sequence decoders was used, each predicting a genome interaction matrix with a different length and resolution. The 2D convolution architecture consists of 2D residual convolution blocks with the linear + nonlinear path design. The 2D convolution blocks cycle through dilation factors of 1, 2, 4, 8, 16, 32, 64 for four full passes with a total of 112 convolution layers per decoder. Decoders at lower levels receive input from the corresponding level of sequence representations, the interaction matrix prediction from one-level above and a 2D pairwise distance encoding matrix as an auxiliary input. 1D sequence representations are transformed to 2D with the pairwise sum operation  $(Y_{ij} = X_i + X_j)$ . The lower-level decoders predict for a subregion half the window size of the upper-level prediction, and the prediction from the upper-level corresponding to this region was upsampled by a factor of 2 and provided as input. For the distance encoding matrix  $D$ , for each cell type,  $D_{ij}$  is the log distance-based expected balanced contact score at each genomic distance  $|i - j|$  for intrachromosomal pairs  $\\{i,j\\}$ , and interchromosomal pairs are filled with a constant of average interchromosomal log expected balanced contact score. The distance-based expectation scores for 32-256 Mb were monotonically transformed so that the scores for longer distances are no higher than the shorter distances. The distance encoding matrix and the upsampled prediction from the upper level are combined with the 2D sequence representation by concatenation followed by a convolution block (Supplementary Fig. 1). The final model prediction is symmetrized by averaging with its transpose. The model predictions are averaged between the predictions from the forward- and the reverse-complement sequences.\n\nThe sequence encoder is also trained with an auxiliary task of predicting DNase-seq and ChIP-seq chromatin profile labels (Supplementary Table 6), which improved performance. To simultaneously predict chromatin profile labels and genome interactions, a 1D convolution block for predicting chromatin profiles is introduced which receives input from the 4-kb-resolution output of the sequence encoder.\n\nModel training and evaluation. The processed micro-C datasets for H1-ESCs and HFF cells $^{15}$  were downloaded from the 4D Nucleome (4DN) data portal (accession IDs 4DNF19GMP2J8 and 4DNF1643OYP9). The genomic sequences were retrieved from the GRCh38/hg38 reference genome. Training data were generated on-the-fly during training by uniformly sampling the genome from training chromosomes with the Selene deep-learning sequence modeling library $^{21}$ . A separate model was trained for each micro-C dataset. The on-the-fly sampling generated new training samples for every training step. Each training sample consists of a sequence\n\n(the input) and the corresponding multilevel distance-normalized contact matrices (the target), which were also referred to as the genome interaction matrices.\n\nTo compute the genome interaction matrices, the iterative correction matrix balancing algorithm $^{43}$  and adaptive coarse graining procedures were applied to the contact matrices retrieved from the micro-C datasets with cooler and cooltools packages $^{26}$ . Adaptive coarse graining is a preprocessing procedure implemented in the cooltools package that smooths the low-coverage areas of the contact map with adaptive window size and this step eliminates zeros by pooling the reads from the local neighborhood. No further smoothing was applied to preserve the spatial resolution of the data. The processed matrix was then divided by the background matrix which is the exponential of the distance encoding matrix as described in the previous section (all operations are elementwise), and the minimum value of the background matrix was added to both nominator and denominator for numerical stability and noise reduction. The distance-based expectations are computed per chromosome with cooltools and then aggregated over all chromosomes. The distance expectation curve beyond a 1.6-Mb distance is smoothed with lowess. The chromosomes were divided into the training set (all chromosomes except for chr8, 9 and 10), the validation set (chr8) and the test set (chr9, 10).\n\nThe main loss function is the mean squared error between the predictions and the targets, or  $\\frac{1}{N}\\| \\mathrm{prediction} - \\mathrm{target}\\| _2^2$ , where prediction and target are both  $250\\times 250$  square matrices,  $N$  indicates the number of elements in the matrix to average over and the norm sign indicates the Frobenius norm. Missing values in the genome interaction matrices, which are typically due to low or no coverage, are ignored in the loss and gradient computation. An auxiliary binary cross-entropy loss function is also used to train the 4-kb-resolution sequence encoding to simultaneously predict chromatin accessibility and ChIP-seq chromatin profile labels, or specifically the auxiliary loss is\n\n$$\n\\frac {1}{N} \\sum_ {i j} \\left[ \\operatorname {t a r g e t} _ {i j} ^ {c} \\cdot \\log \\left(\\operatorname {p r e d i c t i o n} _ {i j} ^ {c}\\right) + \\left(1 - \\operatorname {t a r g e t} _ {i j} ^ {c}\\right) \\cdot \\log \\left(1 - \\operatorname {p r e d i c t i o n} _ {i j} ^ {c}\\right) \\right]\n$$\n\nwhere  $\\text{target}^c$  is the binary chromatin profile target matrix of size  $d \\times 250$  ( $d$  is the number of chromatin profiles), and prediction  $^c$  is the predicted probability matrix of the same size,  $i$  and  $j$  are the indices of the matrices, and  $N$  is the total number of elements in the matrix. The auxiliary loss is simultaneously trained on the same set of sequences as the main loss function. The list of chromatin profiles used is provided in Supplementary Table 6. The chromatin profile labels are generated for 4-kb bins and labeled one or zero based on whether any peak overlaps with the 4-kb bin.\n\nTo allow training of large-scale sequence models that do not fit into GPU memory with standard techniques, a horizontal checkpointing method was devised, leveraging the hierarchical structure of the model (Methods section 'Scaling hierarchical deep-learning model training' for details). Other training optimizations include parallelizing training data generation on CPU and randomly selecting either forward- or reverse-complement sequence for prediction, which can be seen as an unbiased stochastic approximation for averaging predictions from forward- and reverse-complement sequences.\n\nFor both flexibility in model application and efficiency in model training, the model was designed to be composed of three stackable modules (1 Mb, 1-32 Mb, 32-256 Mb), which were trained in three stages. In the first stage, the sequence encoding at 4-kb resolution was pretrained with the task of predicting genome interactions within 1-Mb distance at 4-kb resolution and the auxiliary task of predicting chromatin profile labels at the same resolution (the cohesin-depleted HCT116 model was trained without auxiliary task). The encoder up to 4-kb resolution and the decoder trained in the first stage are also called Orca-1Mb. In the second stage, with the pretrained first section of the sequence encoder from the 1-Mb module, the multiscale 1-32-Mb model was trained to predict at 1-Mb, 2-Mb, 4-Mb, 8-Mb, 16-Mb and 32-Mb levels. For training multiscale prediction models, a series of subregions with increasingly smaller window size and finer resolution at each level, or the 'zooming' series, was selected. For example, for a 32-Mb sequence, a 16-Mb subregion was randomly selected, then an 8-Mb subregion within the 16-Mb region was randomly selected and this continued until a 1-Mb region was selected. The encoder up to 128-kb resolution and decoders trained in the second stage are also called Orca 32-Mb. In the third stage, the 32-256-Mb model is trained for both intrachromosomal and interchromosomal interactions, with the pretrained sequence encoder up to 128-kb resolution from the 1-32-Mb model. The full encoder and the third-stage decoders are also called Orca 256-Mb. The training data for the 32-256-Mb model were sampled from multiple chromosomes with the following process: a chromosome is first sampled, then add the full length of that chromosome to the sequence; then sample another chromosome, and add the full-length chromosome if not exceeding 256 Mb, and otherwise sample a subregion on that chromosome that makes up a total of 256 Mb; continue adding new chromosomes until 256-Mb sequence is filled; randomly permute the order of the sequence segments sampled and randomly select a strand direction for each segment; retrieve the corresponding sequence, intrachromosomal and interchromosomal genome interactions, and distance encodings as described above. The training process with stochastic gradient descent took about 480,000 steps for the first stage (1-Mb sequence and batch size 16, learning rate 0.002 with momentum 0.98 and the last 1/3 of steps are trained with stochastic weight averaging $^{44}$ ), 150,000 steps for the second stage (32-Mb sequence with batch size 4 and learning rate 0.001 with momentum 0.98) and\n\n20,000 steps for the third stage (256-Mb sequence with batch size 4 and learning rate 0.001 with momentum 0.98). The training hardware was one server equipped with four NVIDIA Tesla V100 32GB GPUs. The code for training Orca models with full details of the implementation is provided at the code repository.\n\nEach training stage generates training data from micro-C data processed to different resolutions. The training data were sampled from the micro-C contact matrices at 1-kb resolution for the 1-Mb model, 4-kb resolution for the 1-32-Mb model and 32-kb for the 32-256-Mb model, and these high-resolution matrices are downsampled to the prediction resolutions of the decoders. Downsampling is performed by taking the average of the multiple entries that are collapsed into one, excluding the missing values. To further reduce overfitting, the input sequences for training are shifted by a random offset within  $100\\mathrm{bp}$  for the 1-Mb model,  $1\\mathrm{kb}$  for the 1-32-Mb model and  $4\\mathrm{kb}$  for the 32-256-Mb model.\n\nModel prediction evaluation on holdout test chromosomes. To evaluate the model prediction performance on holdout test chromosomes, multiscale genome interaction matrices were systematically predicted on the test chromosomes and the predictions were compared with the observed micro-C data. The evaluation data were processed in the same procedure as for training data generation. Missing values in the micro-C target matrices are excluded from the evaluation (missing values are typically due to low or no coverage). Because target matrices at lower resolutions are downsampled from higher-resolution matrices by the binning procedure described above, a downsampled value is computed from averaging multiple values from high-resolution matrix while excluding missing values, and if  $>25\\%$  of these values are missing then the downsampled value is also skipped in evaluation. Specifically, for evaluating the predictions at 1-32-Mb levels, the test set chromosomes were tiled with 32-Mb windows at a step size of  $0.5\\mathrm{Mb}$ . For each 32-Mb window the genome interactions were predicted at all scales from 1 Mb to 32 Mb by sequentially zooming into 16-Mb, 8-Mb, 4-Mb, 2-Mb, 1-Mb subwindows each located at the center of the higher-level region. All prediction matrices were concatenated and flattened, and Pearson correlation was computed between the predictions and micro-C observations. The 1-Mb level performance of the 1-32-Mb models was also compared with the 1-Mb module predictions on the same 1-Mb windows.\n\nFor evaluating the intrachromosomal 32-256-Mb-scale predictions, two 256-Mb sequences each containing a test chromosome were first generated, with the rest of the 256-Mb length padded with sequence from chr1 (only the intrachromosomal interactions were evaluated). For predictions at 128-Mb, 64-Mb and 32-Mb levels, the same starting positions that tile the test chromosomes with step size of  $5,120\\mathrm{kb}$  were used. Windows that extended beyond the test chromosome boundaries were discarded from evaluation.\n\nFor evaluating interchromosomal predictions for 32-256-Mb-scale predictions, multichromosomal 256-Mb sequences were constructed by randomly sampling sequence segments from test chromosomes and concatenation. Specifically, the length of each sequence segment was uniformly chosen at random between 64 and  $128\\mathrm{Mb}$ , and the last segment was truncated to  $256\\mathrm{Mb}$  when the total length exceeded  $256\\mathrm{Mb}$ , then the orders of the sampled segments were randomly shuffled. Distance encoding matrices were constructed accordingly. Then, 100 sequences of  $256\\mathrm{Mb}$  were constructed and multiscale predictions zooming into the center of each 256-Mb sequence were generated (128-Mb, 64-Mb, 32-Mb regions at the center of each 256-Mb sequence were selected). Only interchromosomal predictions were evaluated.\n\nFor comparison with Akita $^{23}$  on submegabase-scale predictions, predictions from Akita were generated on its test set samples that are also located in Orca test chromosomes 9 and 10. Orca predictions for the same genomic regions were then generated with the Orca 1-32-Mb models and only predictions at the 1-Mb level were used. The Orca 1-Mb-level predictions and target genome interaction matrices were resized using bilinear upsampling with a factor of 2 and cropped to the Akita output region, and additional Gaussian filtering with sigma 1 and kernel size 5 and clipping to  $(-2, 2)$  was then applied to match the Akita data-processing steps. For each test sample, background-subtracted Pearson correlations were computed against the Akita targets and Orca targets processed as described above. To compute background-subtracted Pearson correlation, for any prediction or target matrix, each score was subtracted by the average scores at the same distance in the same matrix before computing correlation. The background subtraction has minimal effects on preserving the genome structure information and improves robustness to different data preprocessing.\n\nScaling hierarchical deep-learning model training. To scale deep-learning sequence models to hundreds of megabases, a scalable memory-efficient training algorithm was devised to dramatically reduce the memory requirement. As illustrated in Supplementary Fig. 25, the regular training procedure for deep learning is layer-wise and stores all internal representations in memory for computing gradients, which results in extremely high memory demand for large model input. Checkpointing is a memory-saving technique first developed for residual networks with a high number of layers<sup>45</sup>. With checkpointing, only internal representations at the checkpoint layers are stored and other internal representations can be recomputed on-the-fly when gradient computation is needed. However, even with the checkpointing technique, training is still infeasible\n\nfor very large sequence input because the memory requirement of computing even only the first layer for a single sequence is beyond the maximum capacity of currently available GPUs.\n\nLeveraging the hierarchical structure of the sequence model, the memory consumption of the bottom layers, which use the most memory, can be greatly reduced by executing them in horizontal blocks and only storing the output of the blocks. This approach fixed the memory usage of the lower layers to the memory needed to compute the block, with the minimum block size being the receptive field of the block output layer (recommended sizes are at least twofold of the minimum for computational efficiency). For example, the receptive field of the 4-kb-resolution layer output of the Orca sequence encoder is  $212\\mathrm{kb}$ , which is less than  $1/150$  of  $32\\mathrm{Mb}$  or  $1/1,200$  of  $256\\mathrm{Mb}$ , allowing great reduction of memory usage. Because the memory consumption in the bottom layers is orders of magnitude larger compared with the upper layers, this essentially resolved the memory consumption issue for Orca models and allowed us to scale to and beyond whole-chromosome-scale input. I refer to this technique as horizontal checkpointing. Horizontal checkpointing was used to allow the model to scale to large input for training and prediction of Orca 32-Mb and Orca 256-Mb models. Horizontal checkpointing also allows gradient computation during model training, and while this capability was not utilized in the current models due to the increased training time, such capability could be useful in future studies.\n\nSV impact on multiscale genome interactions. Orca models allow the prediction of the multiscale genome organization impact of almost any genome variant at any size. This is naturally achieved by comparing the model predictions of chromosomal sequences of the reference allele and the alternative allele. The capability of using up to  $256\\mathrm{Mb}$  as input allows the analysis of even very large variants as well as including large context sequence up to the whole chromosome. This approach is also extendable to analyzing the joint effects of multiple variants in the same haplotype or even whole individual genomes. Specifically, to predict the structural impact for each variant, multiple series of multiscale prediction were generated, each zooming into a breakpoint introduced by the variant in the alternative allele sequence, or their corresponding positions in the reference sequence.\n\nFor prediction of transposon insertion effects, the sequences after insertions were computationally generated based on the report by Zhang et al. [28]. Experimental in situ Hi-C data that measured the insertion effects were also obtained from the same study. To quantify the insertion effects by insulation score changes (Mut-WT, the mutant insulation score subtracted by the wildtype insulation score). The insulation score is measured as the average intra-region interaction (cis) for the two 200-kb regions before and after the insertion site, subtracted by the average inter-region (trans) interaction scores between the two regions (Extended Data Fig. 5e). The interaction scores are quantified by log fold over distance-based background. Cosine similarity was used to compare the predicted and observed insulation score changes across 14 insertion sites (two sites, C21S8 and C21S9, are excluded because of missing values in the in situ Hi-C data).  $P$  values are computed with an empirical null distribution of 100,000 cosine similarities generated by randomly flipping WT and Mut labels for each insertion site.\n\nMultiplexed in silico mutagenesis. To systematically identify sequences underlying submegabase-scale genome interactions at the single motif scale, an in silico mutagenesis approach that uses the Orca sequence models to predict the effects of a large number of mutations that cover the genome was designed. In this study, a score was assigned to all genomic sequences in 10-bp bins on autosomes representing the structural impact of their disruption. To perform a genome-scale screen, the analysis was sped up by introducing a multiplexed approach to in silico mutagenesis. Since 10-bp sequences with strong structural impacts are sparse (most disruptions have near zero effects), multiple random disruptions can be introduced to the same sequence with a very low probability that more than one disruption will have a strong effect. The multiplexed design ensures that for each 10-bp sequence, multiple random disruptions are introduced in different sequences, each with a different set of random disruptions. The 10-bp site-specific sequence disruption effect was then deconvolved by taking the minimum effect of all sequences that carry a disruption of the 10-bp sequence. The disruption impact on local genome interactions is measured by 1-Mb structural impact score, which is the average absolute log fold change of interactions between the disruption position and all other positions in the 1-Mb window. The Orca 1-Mb modules for H1-ESCs and HFFs are used for all predictions to allow fast screening of a large number of sequences.\n\nMore specifically, the genome was tiled with 1-Mb windows at 0.8-Mb step size across all autosomes. Each 1-Mb window is considered as a  $25 \\times 40$ -kb region each containing  $4,000 \\times 10$ -bp disruption sites. Thus, 12,000 mutated sequences are generated for each 1-Mb window. Each generated sequence contains 20 disruptions in each of the center  $20 \\times 40$ -kb regions. Each 10-bp is disrupted in three different sequences. This multiplexed design can be generated by assigning all 10-bp sequences to a  $20 \\times 4,000$  matrix with each row containing all 10-bp sites of a 40-kb region, then randomly shuffling each row independently, resulting in 4,000 columns each corresponding to a mutated sequence. This process was repeated three times to generate 12,000 sequence designs. According to these designs, 10-bp\n\nsequence disruptions are introduced by replacing the original 10-bp sequence with random nucleotides that match the nucleotide composition in the 1-Mb window.\n\nFor motif enrichment analysis, vertebrate nonredundant motifs were downloaded from the JASPAR database $^{46}$ . Motif matches for each 10-bp site were scanned for after extending by 10-bp flanking sequence on each side, and a maximum log-odds score over the 30-bp window for each motif was obtained. The maximum motif log-odds score in this window was also referred to as the maximum motif log-odds score for the 10-bp site. To avoid overlap of extended sequences, only one 10-bp site every 30-bp was considered for statistical tests. To analyze non-CTCF motif enrichments, 10-bp sites with 1-Mb structural impact score  $>0.01$  and without nearby CTCF motif matches (CTCF max motif log-odds  $<6$  within 200 bp) or CTCF binding sites (CTCF ChIP-seq fold over control  $<4$ ; ENCODE accession IDs ENCFF473IZV, ENCFF761RHS) were used. Next, to quantify the enrichment of motifs, two-sided  $t$ -test (without assuming equal variance) was performed to compare the motif log-odds scores of these filtered sites against the background of 100,000 sites randomly drawn among all 10-bp sites screened. Fold enrichment was also computed on the same sites with a motif log-odds threshold of 12.\n\nFor pileup analysis of H1-ESC and HFF micro-C datasets at POU5F1::SOX2 and FOS::JUN structural impact sites, the average interaction matrix (log fold over background scores within 1-Mb window) centered at all non-CTCF sites (as defined above) across the genome with motif log-odds  $>10$  and  $>0.02$  Mb structural impact score for the same cell type that matches the micro-C datasets was computed. For pileup analysis of CTCF structural impact sites, similarly, the average over all sites of CTCF motif log-odds  $>10$  and 1-Mb structural impact score  $>0.1$  was computed.\n\nVirtual genetic screen for chromatin compartment activity. For performing virtual screens of sequence chromatin compartment activity, an Orca model was first trained for the cohesin-depleted (after 6h of auxin treatment) in situ Hi-C HCT119 dataset $^{36}$ , in which the TADs were eliminated while chromatin compartments were intact or strengthened. The dataset was downloaded from the 4DN data portal (accession ID 4DNFILP99QJS). The cohesin-depleted HCT119 Orca model was trained from scratch with a similar procedure as described above, with a difference that the HCT119 model was trained without the auxiliary loss function of predicting chromatin accessibility and ChIP-seq chromatin profiles at any stage of training.\n\nTo screen for sequence activity of chromatin compartment alteration, a virtual screen with ectopic insertions of genomic sequences was designed. For each screen, a pool of source sequences and one or more target positions were selected. For every source sequence and target location pair, the source sequence was 'inserted' into the target location by swapping out the original sequence at the target position, then the genome interaction pattern changes were predicted and quantified by the 32-Mb structural impact score (the average absolute log fold change of interactions between the target position and all other positions in the 32-Mb window). Because a large proportion of the mutated sequence is in common with the original sequence, the computation was sped up by only recomputing the internal representations that are affected by the change.\n\nThe source sequences were generated from a large genomic region or across entire chromosomes by dividing the region into fixed-sized segments, and the structural impact scores of the source sequences at all positions were visualized as a chromatin compartment alteration activity profile. For exploratory virtual screens, a 32-Mb region, chr10:77,072,000-109,072,000 covering multiple A compartment, B compartment and intermediate regions, was used. Activities of source sequences tiling this region were screened for at nine target positions that are uniformly spaced in the same region. For the large-scale screen, source sequences with 12,800-bp length tiling all of the holdout chromosomes chr8, 9 and 10 were used, with sequences overlapping with blacklisted regions removed. Here the blacklisted regions were defined as 4-kb genomic bins with missing values in the Hi-C datasets used in this manuscript, or with more than 10 unknown bases ('N's) in the reference genome sequence. Then 200 target positions spanning all holdout chromosomes were randomly chosen from source sequence start positions. The 32-Mb windows for Orca prediction in the large-scale screen were centered at the target positions. The  $\\mathrm{A > B}$  or  $\\mathrm{B > A}$  chromatin compartment activity of each 12,800-bp sequence was quantified from the large-scale screen of 200 targets, by taking the first principal component across the 200 compartment activity profiles (one for each target position). The sign of a principal component is arbitrary, but the direction that corresponds to compartment A activity can be easily detected, such as based on TSS enrichment. The top  $2\\%$  of sequences with the strongest compartment A activity were used for downstream enrichment analysis.\n\nFor enrichment analysis of the chromatin compartment activities, the FANTOM CAGE signal profile (maximum count across samples) was downloaded from the UCSC table browser with a filter of count  $>1$ , and the annotations\n\nfor TSS,  $5^{\\prime}$  UTR,  $3^{\\prime}$  UTR, exon and genes were from Ensembl release 97. The chromatin state annotations for HCT116 are from EpiMap[47].\n\nFor performing analyses with random permutation of sequences, the sequence to be permuted was first divided into segments of the same specified length, then the order of the sequence segments was randomly permuted. As random permutation disrupts any sequence patterns larger than the segment length, this analysis can be used to reveal the length scale of the sequence dependencies.\n\nReporting Summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article.",
    "metadata": {
      "md_filename": "MinerU_markdown_Orca_20260106142905_2008425565627621376.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_Orca_20260106142905_2008425565627621376.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "e8f4c94cf249e71a",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_Pangolin_20260106142859_2008425543418777600",
    "title": "Open Access",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "# Deep neural network architecture\n\nPangolin's models are dilated convolutional neural networks with an architecture allowing features to be extracted from up to 5000 bases upstream and downstream each target position in the genome. Each model takes as input a one-hot encoded sequence of  $N$  bases, where  $N \\geq 10,001$ , and predictsfor the middle  $N - 10,000$  basesthe probability that these sites are splice sites (probability output model) and the usage of each site (usage output model) in heart, liver, brain, and testis (see \"Generating training and test sets\" section for details on the format of the input/output, and see Additional file 1: Supplementary Note 1 for characterization of the two model types). In particular,  $N$  is 10,001 when making predictions for individual sites.\n\nMore specifically, the neural networks consist of 16 stacked residual blockswhich are composed of batch normalization, ReLU activation, and convolutional layersas well as skip connections, which add the model outputs before residual blocks 1, 5, 9, and 13 to the input of the penultimate layer. The convolutional layers of each residual block are dilatedmeaning the convolution filter sees every  $k$ th base,  $k > 1$ , rather than every single baseallowing the receptive field width of the model to increase exponentially with the number of layers. Besides the residual blocks, the networks only contains three other\n\nlayersthe first and penultimate layers, which are convolutional layers that transform their inputs into the proper dimensions for later layers; and the final activation layer that applies either a softmax or sigmoid activation to produce Pangolin's probability or usage predictions respectively. In comparison to SpliceAI's architecture, the last two layers are the primary points of differenceSpliceAI does not make probability/usage predictions for multiple tissues, but rather makes a prediction for the probability that a site is a splice donor, splice acceptor, or not a splice site which is invariable across tissues.\n\n# Generating training and test sets\n\nTo identify splice sites and quantify their usages, we processed RNA-seq data from four tissuesheart, liver, brain, and testisacross four specieshuman, rhesus macaque, mouse, and rat [5]. For heart, liver, and brain, we analyzed 8 samples for each species, while for testis, we analyzed 8 samples for human and 4 for rhesus macaque, mouse, and rat. Samples were chosen from developmental periods subsequent to all periods of large transcriptional changes (Extended Data Fig. 5 from Cardoso-Moreira et al. [5]). RNA-seq reads were mapped to their respective genomes with annotations using STAR 2.7.5 [11] using its multi-sample 2-pass mode (genomes and annotations used: GRCh38 with GENCODE release 34 comprehensive annotations for human; Mmul_10 with ENSEMBL release 100 for rhesus macaque; GRCm38 with GENCODE release M25 for mouse; and Rnor_6.0 with ENSEMBL release 101 for rat). We then assigned multimapped reads to a single location using the multi-mapper resolution (MMR) tool [16].\n\nTo create training and test datasets for Pangolin for each tissue, we labeled every position within a gene body as spliced or not spliced and quantified the usage of each splice site. While the first label is binary, the second labelsplice site usageis a continuous value between 0 and 1 representing the proportion of a gene's transcripts that use a given splice site. Specifically, we labeled all sites within gene bodies supported by 1 split read in at least 2 samples each as spliced, and we labeled all other sites as unspliced. Then, to estimate a splice site's usage level, we used SpliSER 1.3 [10] to calculate a per-tissue Splice-Site Strength Estimate (SSE). SpliSER considers four types of reads to estimate usage for a target site:  $\\alpha$  and  $\\beta_{1}$  reads, which are split and non-split reads respectively that map to or across the target site; and  $\\beta_{2-}\\mathrm{SIMPLE}$  and  $\\beta_{2-CRYPTIC}$  reads, which are split reads that provide direct and indirect evidence against usage respectively [10]. Then, SSE is calculated as:\n\n$$\n\\alpha \\Bigg{/}\\left(\\alpha +\\beta_{1} + \\beta_{2 - \\text{SIMPLE}} + \\frac{1}{\\alpha}\\sum_{p\\in \\{\\text{partners of the target site}\\}}\\alpha_{p}\\beta_{2 - \\text{CRYPTIC},p}\\right)\n$$\n\nWe used the SSE metric to estimate the usage of all sites for which  $\\alpha + \\beta_{1} + \\beta_{2 - \\mathrm{SIMPLE}} \\geq 5$  in at least 2 samples. Some sites that we labeled as spliced (1 split read in at least 2 samples) did not meet these criteria and were excluded from both training and testing sets. All sites labeled as not spliced were assigned 0 usage.\n\nFor the test set, we set aside genes from human chromosomes 1, 3, 5, 7, and 9, and for the training set, we used all genes from the remaining human chromosomes 2, 4, 6, 8, 10-22, X, and Y that do not show orthology or paralogy to test set genes. We also excluded genes in rhesus macaque, mouse, and rat that show orthology to human test set genes from our training data set. More specifically, we used annotations from Ensembl BioMart\n\n(accessed 11/14/2020) to exclude all genes with either low or high \"orthology confidence\" to a test set gene from the training set.\n\nNext, we prepared the training set as follows. For the model inputs, we extracted the sequence between the annotated  $5^{\\prime}$  most transcription start and  $3^{\\prime}$  most transcription end sites for each gene; padded them with Ns (representing unknown bases) so that each site is surrounded by at least 5000 bases on either side; and split the resulting sequence into overlapping blocks of 15,000 base pairs such that the first block contained positions 0 to 15,000, the second positions 5000 to 20,000, and the  $i$ th block positions  $5000(i - 1) - 5000$  to  $5000(i - 1) + 5000$ . We chose such a block size because it allows many predictions to be made for a single input block (specifically, predictions for the middle 5000 positions), greatly reducing the training time required in comparison to predicting one base at a time, which would require 5000 blocks of 10,001 bases each. We then one-hot encoded each input sequence, representing A, C, G, T/U, andfor unknown basesN by  $[1,0,0,0]$ ,  $[0,1,0,0]$ ,  $[0,0,1,0]$ ,  $[0,0,0,1]$ , and  $[0,0,0,0]$  respectively.\n\nFor the output labels, we assigned each target site a vector of length 12, with positions 0-3, 4-6, 7-9, and 10-12 corresponding to labels for heart, liver, brain, and testis respectively. For each tissue, the first two positions represent whether or not a site is splicedunspliced sites were labeled as [1,0], spliced sites as [0,1], and padding or unknown sites as [0,0]. The third position is a number between 0 and 1 representing the estimated usage level of the site. For each target site, Pangolin outputs an identically-sized vector of numbers, with each position representing the predicted values.\n\n# Training Pangolin\n\nDuring training, we randomly held out  $10\\%$  of the 15,000 base-pair blocks to determine an early-stopping point, which was the epoch when the average training loss stopped decreasing. We first trained the network using the AdamW optimizer and a warm-restarts learning-rate schedule with cycle lengths of 2 and 4 epochs (total training time of 6 epochs) [23]. With this schedule, we initialized the learning rate to  $5 \\times 10^{-4}$  at the start of each cycle and decayed it to 0 using a cosine annealing by the end of each cycle. For each input, we computed losses for the model's probability predictions with a categorical cross-entropy loss function and losses for the model's usage predictions with a binary cross-entropy loss function; then summed over the losses across all tissues to calculate a total loss. Total losses for the inputs were used to update the model's weights through backpropagation.\n\nWe further trained the model on each tissue and label type (spliced/unspliced and splice site usage) separately so that the loss for each input was computed using only one tissue and label type at a time (we trained each tissue and label type combination for 4 epochs, initializing the learning rate to  $5 \\times 10^{-4}$  and decaying it to 0 using a cosine annealing). In addition, we ran the training procedure 5 times, resulting in 5 models per tissue and label type combination (heart-spliced, heart-usage, liver-spliced, liver-usage, etc. for 40 total models). For all predictions of splicing probabilities or splice site usage for a tissue, unless otherwise specified, we took the mean prediction across the 5 models.\n\nFinally, we trained a version of Pangolin by fine-tuning on human data after removing, for each tissue, sequences containing no splice sites; and with the use of label smoothing, a regularization technique wherein unspliced and spliced sites are labeled using the vectors [0.95, 0.05] and [0.05, 0.95] respectively rather than the one-hot encodings [1, 0] and\n\n[0, 1] (4 epochs; learning rate was initialized to  $5 \\times 10^{-5}$  and decayed to 0 with a cosine annealing). We repeated this training process 3 times to obtain 3 models per tissue. We found that this fine-tuned version of Pangolin generally performed better at predicting splice variants, and use it for the analyses in Figs. 1c, d and 2a, d-g.\n\n# Evaluation on held-out test set\n\nFor each tissue, we evaluated Pangolin's ability to predict splice sites in genes from human chromosomes 1, 3, 5, 7, and 9, excluding genes with low expression levels (mean transcripts per million (TPM) across samples  $<2.5$  as determined using RSEM 1.3.3 [21]). For this evaluation, we used Pangolin's predictions of tissue-specific splice site probabilities. We first computed, for each tissue, the average top-1 and top-0.5 accuracy over all genes from these chromosomes. The top-1 accuracy is defined as the fraction of sites within the top  $N$  predicted splice sites that are labeled as splice sites, where  $N$  is the number of labeled splice sites in the test dataset (i.e., the fraction of the top  $N$  predicted splice sites that are correct). Similarly, the top-0.5 accuracy is defined as the same fraction but for the top  $\\lfloor N/2\\rfloor$  predicted splice sites. We also computed the area under the precision-recall curve (AUPRC) for each tissue. For SpliceAI (version 1.3.1), we computed the probability that a site is spliced as the maximum of SpliceAI's  $5'$  and  $3'$  scores. Similarly, for MMSplice (version 2.2.0), we scored each site as the maximum of MMSplice's  $5'$  and  $3'$  scores following a logit transformation, where the input for each  $5'$  site was the sequence 13 bp into the intron and 5 bp into the exon, and where the input for each  $3'$  site the sequence 50 bp into the intron and 3 bp into the exon. For HAL, we scored each site used the function score_seq_pos (from Cell2015_N8_HAL_Genome_Predictions.ipynb in the GitHub repository https://github.com/Alex-Rosenberg/cell-2015) to obtain  $5'$  splice site scores, using the sequence 80 bp into the intron and 80 bp into the exon as input. Since HAL does not score  $3'$  splice sites, we excluded  $3'$  splice sites when evaluating HAL's predictions. Finally, for MaxEntScan, we scored each site as the maximum of MaxEntScan's  $5'$  and  $3'$  splice scores following the transformation  $2^{s} / (2^{s} + 1)$  for each score  $s$ , where the input for the  $5'$  model was the sequence 6 bp into the intron and 3 bp into the exon, and where the input for the  $3'$  model was the sequence 20 bp into the intron and 3 bp into the exon. When running MMSplice, HAL, and MaxEntScan, we excluded inputs that contained \"N\" bases (unknown bases or padding).\n\n# Definition of maximum difference in probability scores\n\nFor some applications of Pangolin, we calculated the splice score of a variant as the maximum difference in probability scores across tissues between the reference and mutated sequence. Here, we define this difference. Let  $P_{\\mathrm{ref,tissue}}$  be the predicted probability that a splice site in the reference sequence context is spliced in a tissue, and  $P_{\\mathrm{alt,tissue}}$  be this probability for a splice site in the mutated sequence context. Let  $\\Delta$  scores be the vector  $[P_{\\mathrm{alt,heart}} - P_{\\mathrm{ref,heart}}, P_{\\mathrm{alt,liver}} - P_{\\mathrm{ref,liver}}, P_{\\mathrm{alt,brain}} - P_{\\mathrm{ref,brain}}, P_{\\mathrm{alt,testis}} - P_{\\mathrm{ref,testis}}]$ . Then, we define maximum difference in probability scores as the element in  $\\Delta$  scores corresponding to max  $|\\Delta$  scores|, i.e.  $\\Delta$  scores  $_{\\mathrm{argmax}|\\Delta \\mathrm{scores}|}$ .\n\n# MFASS and MaPSy evaluation\n\nCheung et al. [8] used a Sort-seq assay (MFASS) to quantify the effects of 27,733 exonic and intronic variants from the Exome Aggregation Consortium (ExAC) on exon recognition. More specifically, the effects of these variants on splicing were assayed using\n\nminigene reporters each containing an exon and its surrounding intronic sequences. Variants with a  $\\Delta$ inclusion index of  $\\leq -0.5$  were classified as splice-disrupting variants (SDVs), where  $\\Delta$ inclusion index is defined as the difference in percent-spliced-in, the ratio of transcripts containing an exon, between the alternative and reference alleles. To predict the effect of a variant on exon splicing using Pangolin, we took the mean over Pangolin's scores for the  $5'$  and  $3'$  splice sites of each exon, where each site was scored using the maximum difference in probability scores across tissues between the alternative and reference sequences. Specifically, we used sequences  $\\pm 5000$  bp of the  $5'$  and  $3'$  sitesobtained from the GRCh37 human reference assemblyas the reference sequence inputs to the model, and we used their mutated versions as the alternative sequence inputs. For SpliceAI, we scored each variant as the mean of SpliceAI's scores for the exon's  $5'$  and  $3'$  sites, where each site was scored as the difference in score between the alternative and reference alleles. As before, we used the maximum of SpliceAI's  $5'$  and  $3'$  scores as the score for each site. Variant scores for MMSplice and HAL were previously computed [7]. For Pangolin and SpliceAI, we then computed precision and recall using the precision_recall Curve function from the Python package scikit-learn, and AUPRC using the auc function. For MMSplice, we computed precision and recall using scripts from the MMSplice paper [7] (https://github.com/gagneurlab/MMSplice_paper), and for HAL, we used the precision and recall statistics provided in the MFASS paper [8] (https://github.com/KosuriLab/MFASS).\n\nWe further compared the performance of Pangolin, SpliceAI, and MMSplice on the MaPSy dataset [28], obtained from the MMSplice GitHub repository (https://github. com/gagneurlab/MMSplice_paper). Soemedi et al. [28] used a splicing reporter system (MaPSy) to test the effects of 4964 variants from the Human Gene Mutation Database (HGMD) on splicing efficiency, i.e., the proportion of spliced RNAs in the set of total RNAs. They tested the effects of variants both in vitro (cell nuclear extract) and in vivo (HEK293T cells). The effect of a variant on splicing efficiency is calculated as  $\\log_2\\left(\\frac{m_o / m_i}{w_o / w_i}\\right)$ , where  $m_{o}$  and  $w_{o}$  are the mutant and wild-type spliced-RNA read counts, respectively, and  $m_{i}$  and  $w_{i}$  are the mutant and wild-type unspliced-RNA read counts respectively. For Pangolin and SpliceAI, we scored each variant as the mean of the score for the  $5^{\\prime}$  splice site and the score for the  $3^{\\prime}$  splice site, following the procedure described above for scoring MFASS variants. We excluded sites for which the measured effect of a variant was undefined (for example, if  $\\frac{m_o / m_i}{w_o / w_i} = 0$ ). We then calculated the Pearson correlations between Pangolin's and SpliceAI's predicted scores and the measured effects from MaPSy. For MMSplice, we report the Pearson correlations provided in the MMSplice paper, which were calculated for a smaller test set of variants as MMSplice was trained to predict splicing efficiency using a subset of the variants.\n\n# FAS exon 6 evaluation\n\nJulien et al. [15] quantified the effects of all possible single mutations (189 total) in FAS exon 6 using a minigene reporter covering FAS exons 5-7. In a subsequent study, Baeza-Centurion et al. [3] quantified the effects of several single, double, and higher-order combinations of 12 single mutations (3072 total) in FAS exon 6 using the same minigene reporter assay. We used the first dataset to evaluate Pangolin's performance on single mutations; and used sequences with  $>1$  mutation from the second dataset (3059 out of 3072) to evaluate Pangolin's performance on multiple mutations. For the first dataset, we\n\nconverted enrichment scores to PSI estimates by fitting an exponential calibration curve using 24 mutants with experimentally determined inclusion levels. For the second dataset, PSI estimates for each variant were provided in Baeza-Centurion et al. [3]. We scored each variant by computing  $\\max(P_{\\text{heart}}, P_{\\text{liver}}, P_{\\text{brain}}, P_{\\text{testis}})$  for the  $5'$  and  $3'$  splice sites, where  $P_{\\text{tissue}}$  is the predicted probability that a site is spliced in the specified tissue. We then used the mean of the scores for the  $5'$  and  $3'$  splice sites to predict exon inclusion levels for each variant. As inputs to Pangolin, we extracted sequences from the GRCh38 reference assembly. To understand the effects of epistatic interactions, Baeza-Centurion et al. [3] developed a linear model with 12 parameters, one for each single base-pair mutation, to predict the PSIs of all exons in the library. For Additional file 1: Fig. S3, we used this model to predict the PSIs for all exons with  $>1$  mutation, and calculated the Spearman's  $r$  correlation coefficient between the predicted and observed PSIs.\n\n# Tissue-specific splicing\n\nTo evaluate Pangolin's ability to predict tissue-specific splicing differences, we considered a subset of splice sites in the test genes with higher confidence usage estimates: sites with at least  $10\\alpha + \\beta_{1} + \\beta_{2}$  reads per sample (see \"Generating training and test sets\" section for definitions of the read types) for at least three samples and with standard deviations of  $< 0.1$  for the usage estimates. We also required that sites be expressed in all tissues (mean TPM across samples for each tissue  $\\geq 2.5$ ) and that for at least one tissue, the splice site usage differs from the mean splice site usage across tissues by  $>0.2$ . For each tissue, we computedfor each splice site meeting the above criteriathe observed difference in usage from the mean usage across all tissues. Similarly, we computed each predicted difference as the difference between a splice site's predicted usage in a tissue and the mean predicted usage across tissues. Then, we computed the Spearman's  $r$  correlation coefficient between these predicted and measured differences.\n\n# In silico mutagenesis\n\nWe performed in silico mutagenesis by predicting the splicing effects of all possible single base mutations for positions 8 bp into the intron and 4 bp into the exon for  $5^{\\prime}$  splice sites, and for positions 15 bp into the intron and 3 bp into the exon for  $3^{\\prime}$  splice sites. For each splice site, we predicted the effect of each mutation on splice site usage by computing the mean predicted difference across tissues between the reference and mutated sequences. We performed this analysis for the splice sites of protein-coding genes in human chromosomes 7 and 8, limiting our analysis to the most representative transcript per gene as determined by the presence of an Ensembl_canonical tag in the annotation file. Furthermore, we excluded the start of the first exon and end of the last exon of each transcript.\n\n# Splicing QTLs evaluation\n\nTo evaluate Pangolin's ability to predict the effects of common variants in their extant biological contexts, we used Pangolin to distinguish SNPs that are putatively causal for splicing differencesas determined from a splicing QTL (sQTL) analysisfrom other nearby SNPs tested in our sQTL analysis. We used a previously analyzed set of sQTLs generated using RNA-seq data from whole blood samples from 922 genotyped individuals in the Depression Genes and Networks (DGN) cohort [24]. For each sQTL, we defined\n\nthe putatively causal SNP as the SNP with the most significant association (lowest  $p$  value) with the splicing phenotype out of all tested SNPs. Next, we considered sQTLs whose causal SNPs were within 1000 bp of the intron's  $5'$  or  $3'$  sites, and analyzed the 500 sQTLs that had the most significant causal SNPs. For each of these sQTLs, we used both Pangolin and SpliceAI to predict the splicing effects of the causal SNP as well as all other SNPs within 1000 bp of the  $5'$  and  $3'$  sites. Specifically, we predicted the effect of each SNP on the nearest splice site by taking the absolute value of the predicted change in splice score. If both the  $5'$  and  $3'$  splice site were within 1000 bp of the SNP, we took the mean over the predictions for both sites; otherwise, we used the prediction for the nearest splice site. For Pangolin, we used the absolute value of the maximum difference in probability score (defined earlier in the \"Methods\" section) as the prediction for each site.\n\nNext, for Pangolin and SpliceAI, we generated empirical cumulative distribution function (eCDF) plots for the ratio (predicted  $p$  value)/(putative  $p$  value), where predicted  $p$  value for a given QTL is the  $p$  value of the SNP with the largest predicted effect on splicing as determined by Pangolin or SpliceAI, and putative  $p$  value is the  $p$  value of the putatively causal SNP. As baselines (100 in total), we repeatedly selected a random SNP for each QTL and generated eCDF profile for the ratio (random  $p$  value)/(putative  $p$  value), where random  $p$  value is the  $p$  value of the randomly-chosen SNP.\n\n# Splice site evolution\n\nTo predict variants responsible for differences in splice site usage between species, we analyzed RNA-seq data from human, chimpanzee, and rhesus macaque prefrontal cortex [17]. Kanton et al. [17] performed bulk RNA sequencing separately on sliced sections of prefrontal cortex samples. We analyzed two samples per species (one sample per individual), and after combining RNA-seq reads from cortex sections for each sample, mapped reads to their respective genome assemblies with annotations using STAR 2.7.5 in its multi-sample 2-pass mode (assemblies and annotations: GRCh38 with GENCODE release 34 for human; Mmul_10 with ENSEMBL release 100 for rhesus macaque; and PanTro_3.0 with ENSEMBL release 101 for chimpanzee). To convert coordinates between genomes, we again used Liftoff [27] to map genomic features from the human genome assembly to the chimpanzee and rhesus macaque genome assemblies, and vice versa. For further analysis, we calculated usage for splice sites with at least  $50\\alpha + \\beta_{1} + \\beta_{2}$  reads in each sample and with standard deviations of  $< 0.05$  for the usage estimates. We also considered sites that had no reads at all as having 0 usage, and required that sites be in expressed genes (mean TPM across samples  $\\geq 2.5$ ). For comparisons of splice site usage between human and chimpanzee, we considered annotated human (resp. chimpanzee) sites that mapped to the chimpanzee (resp. human) genome with alignment coverage  $\\geq 0.75$  and exon/CDS sequence identity  $\\geq 0.75$  as determined by Liftoff; were on genes of the same strand; mapped to a single location on the target genome; and were one-to-one orthologs. Next, we considered the differentially used splice sites with |usage in human - usage in rhesus|  $\\geq 0.5$ . To score each differentially spliced site using Pangolin and SpliceAI, we computed the maximum difference in predicted splice score between chimp and human using the sequence contexts surrounding the splice site and its lifted-over coordinates as inputs. With these predictions, we then calculated false sign rates (FSR) for a range of predicted score cutoffs (Additional file 1: Supplementary Note 7).\n\nTo identify sites where a single mutation is sufficient to explain the difference in splice scores, we first limited analysis to sites predicted to be differentially used (5% FSR, score cutoff = 0.14) for which chimpanzee and human sequences showed at most 10% divergence in regions near the splice site (20 differences within 100 bp upstream and downstream of the splice site). By visualizing the positional distributions of divergent bases, we found that this cutoff kept only sequence differences that are likely to be substitutions (differences were generally isolated to single bases). Next, we kept sites where the predicted difference in usage is explained mostly by these nearby differences rather than by more distal ones (>100 bases from the splice site), and furthermore, where a single nearby mutation is sufficient to explain the difference in splice scores (see Additional file 1: Fig. S12). Examples of such sites are shown in Fig. 2b and c.\n\n# BRCA1 evaluation and ClinVar prediction\n\nFindlay et al. [12] performed saturation genome editing to test the effects of 3893 SNVs in 13 exons and nearby intronic regions of the  $BRCA1$  gene (96.5% of all possible SNVs) to determine their functional consequences. Specifically, they performed editing in the HAP1 cell line, where  $BRCA1$  is essential for cell survival, and calculated variant function scores using depletion of each variant over time from the plasmid library, a metric for cell survival (negative function scores correspond to a decline in  $BRCA1$  function). For each variant, we used Pangolin to compute the largest decrease in splice score (largest across tissues) at the closest annotated splice site ( $BRCA1$  transcript  $BRCA1-203$ ). In addition, if the other splice site for the corresponding exon was within 100 bp, we used the mean predicted decrease across both splice sites as the predicted effect of the variant. We used the GRCh37 reference assembly to extract input sequences for Pangolin. To classify variants as missense, nonsense, intronic, synonymous, splice region, or canonical splice variants, we used the labels provided by Findlay et al. [12]. In particular, variants in splice regions are those that are located up to 3 bp into the exon and 8 bp into the intron that do not disrupt canonical splice sites or alter the amino acid sequence. We define variants in extended splice regions similarly, but include variants 15 bp of the exon-intron boundary. In addition, we classified variants as loss-of-function (LOF), intermediate, or functional using the function score thresholds determined in Findlay et al. [12]. For all analyses, such as computing precision-recall curves and AURPC, we considered only LOF and functional variants.\n\nThe ClinVar database contains variants found in patient samples, many of which are classified as Pathogenic, Likely pathogenic, Likely benign, Benign, or Uncertain Significance. We applied Pangolin to ClinVar variants downloaded from https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz on 05/04/2021. Specifically, for all variants passing certain criteria (listed below), we computed the maximum decrease in splice score at an annotated splice site within 50 bases of the variant, using the GRCh38 genome assembly and GENCODE Release 38 gene annotations filtered for the most representative (Ensembl_canonical tagged) transcripts. These criteria were: the variant is a substitution or simple insertion/deletion (insertion/deletion where either the REF or ALT field is a single base); is contained in a gene body; is not within 5000 bases of the chromosome ends; and is not a deletion larger than 100 bp. We also ran SpliceAI on the same variants, similarly by computing the maximum decrease in splice score at an annotated splice site within 50 bases of the variant (same genome and annotations). For further\n\nanalyses, we considered variants in protein-coding genes that were either classified by ClinVar as Benign, Pathogenic, Likely Benign, Likely Pathogenic, or Uncertain significance; and required that each variant be in only one gene, not be a nonsense or missense variant as determined using the molecular consequence (MC) field in the ClinVar VCF (variants with no such field were excluded), and be within 15 bp of an annotated splice site (excluding the start of the first exon and end of the last exon of each transcript). We also only considered variants that could be scored by both Pangolin and SpliceAI. To distinguish between variants in annotated splice sites and all other variants, we looked for the presence of the splice acceptor variant and splice_donorvariant tags in the MC field.\n\n# Supplementary Information\n\nThe online version contains supplementary material available at https://doi.org/10.1186/s13059-022-02664-4.\n\nAdditional file 1: Supplementary Note 1: Binary versus continuous prediction output. Supplementary Note 2: Identifying a test set with minimal similarity to the training data. Supplementary Note 3: Training Pangolin using quantitative splicing data from multiple species improve prediction. Supplementary Note 4: Pangolin versus MTSplice on predicting tissue-type-specific splicing. Supplementary Note 5: Identifying motifs involved in tissue-specific splicing. Supplementary Note 6: Mutations away from G at the -1 position of the  $5^{\\prime}$  splice site cause strong decreases in  $5^{\\prime}$  splice site usage. Supplementary Note 7: Predicting causal variants that explain inter-species divergence in splice site usage. Figure S1: Precision and recall at different distances from a splice site. Figure S2: Pangolin scores correlate with changes in splicing efficiency. Figure S3: Prediction of epistatic effects on RNA splicing as a combination of single SNP effects. Figure S4: Prediction of tissue specific splice site usage using Pangolin. Figure S5: Motifs characterizing tissue-specific splice sites. Figure S6: False sign rates of predicted causal variants underlying inter-species divergence in splice site usage. Figure S7: Survival function plots of tested BRCA1 variants. Figure S8: Precision and recall for BRCA1 predictions and fraction LOF at different cutoffs. Figure S9: Predicted effects of variants in BRCA1. Figure S10: Precision and recall for pathogenic versus benign variant classification using Pangolin versus SpliceAl. Figure S11: Predicted effects of variants in CHEK2. Figure S12: Schematic for identifying single causal variants. Figure S13: Comparison of training on binary classification of splice sites versus continuous usage estimates.\n\nFigure S14: Comparison between Pangolin and SpliceAl for predicting inter-species variation in splice site usage. Table S3: Evaluations on subsets of test genes. Table S4: Comparison of AUPRC between models trained on multiple species and on human only.\n\nAdditional file 2: Table S1: Mapping statistics for RNA-seq datasets from Cardoso-Moreira et al., 2019 used for Pangolin training and testing. Table S2: Mapping statistics for RNA-seq datasets from Kanton et al., 2019 used in the chimp-human splice site divergence analysis.\n\nAdditional file 3: Review history.\n\n# Acknowledgements\n\nWe thank Benjamin Fair and Xuanyao Liu for comments on the manuscript.\n\n# Peer review information\n\nAndrew Cosgrove was the primary editor of this article and managed its editorial process and peer review in collaboration with the rest of the editorial team.\n\n# Review history\n\nThe review history is available as Additional file 3.\n\n# Authors' contributions\n\nY.I.L. conceived of the project. T.Z. performed the analyses and implemented the software. T.Z. and Y.I.L. wrote the manuscript. The authors read and approved the final submission.\n\n# Funding\n\nThis work was supported by the US National Institutes of Health (R01GM130738 to YI Li). This work was completed with resources provided by the University of Chicago Research Computing Center.\n\n# Availability of data and materials\n\nRNA-seq reads for primary model training and evaluation are available through ArrayExpress (mouse: https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-6798, rat: https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-6811, rhesus macaque: https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-6813, human: https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-6814. RNA-seq reads used in the splice site evolution analysis are available through ArrayExpress (rhesus macaque, chimpanzee, and human: https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-8231). MFASS data are available on GitHub at https://github.com/KosuriLab/MFASS. FAS exon 6 data are available in Supplementary Data 1 of Julien et al. [15]. Processed DGN data are available through Mu et al. [24]. BRCA1 data are available in\n\nSupplementary Table 1 of Findlay et al. [12]. Pangolin is available under a GPL-3.0 License on GitHub at https://github.com/tkzeng/Pangolin and Zenodo [32].\n\n# Declarations\n\n# Ethics approval and consent to participate\n\nNot applicable.",
    "metadata": {
      "md_filename": "MinerU_markdown_Pangolin_20260106142859_2008425543418777600.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_Pangolin_20260106142859_2008425543418777600.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "7c924f17293a47ac",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_Saluki_20260106142855_2008425523906875392",
    "title": "The genetic and biochemical determinants of mRNA degradation rates in mammals",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "mRNA half-life data collection and pre-processing\n\nWe manually collected the processed half-life values from all of the studies indicated (Table 1). In cases in which genes were provided as gene names or RefSeq IDs, the names were converted to their corresponding Ensembl IDs using information from the Ensembl BioMart. Half-life measurements were log-transformed after adding a pseudocount of 0.1 or 1, depending upon whether the unit of the provided half-life was measured in hours or minutes, respectively. Duplicated IDs were then averaged to compute a single half-life measurement for each gene. Five human datasets from four studies provided data as degradation rates rather than half-lives [29,38,46,49], so their transformed half-life values were negated. Finally, we generated a sparse matrix of half-lives for all genes x samples, containing missing values for genes in which the sample did not provide a measured half-life (Additional file 2: Dataset S1).\n\nTo evaluate the relatedness between samples, we first extracted the subset of genes in the aforementioned matrix for which  $\\geq 10$  human samples (or  $\\geq 5$  mouse samples) reported non-missing values. We then z-score transformed the half-lives from each sample to standardize the scale of each sample. To impute the missing values of the matrix, we used the estim_ncpPCA function in the missMDA R package to estimate the appropriate number of principal components (PC) for the imputePCA function, considering between 0 to 20 components for the human samples (or 0 to 10 components for the mouse samples). Finally, the data was quantile normalized using the normalizequantiles function in the preprocessCore R package. The first PC of this imputed matrix (as computed by the prcomp function in R) was used as a robust cell-type-independent measurement of mRNA half-life, and computed for human and mouse\n\nspecies separately. For human, the samples from a single study [44] were removed prior to computing the PC because the samples from this study represented large outliers relative to all other samples (Additional file 3: Dataset S2).\n\nFor evolutionary comparisons between species, one-to-one human-to-mouse orthologs were acquired from the Ensembl v90 BioMart [99] by extracting the \"Mouse gene stable ID\" and \"Mouse homology type\" with respect to each human gene.\n\nTo examine sample relatedness, the first two PCs were computed using the transpose of the imputed (gene x sample) matrix, and samples were annotated and colored by their cell type of origin, study of origin, and half-life measurement technique.\n\n# Gene annotation set\n\nGene annotations for protein coding genes were derived from Ensembl v83 (hg38 genome build) and v90 (mm10 genome build) for human and mouse, respectively [99]. Only protein-coding genes were carried forward for analysis. Out of all transcripts corresponding to each gene, the one with the longest ORF, followed by the longest  $5^{\\prime}$  UTR, followed by the longest  $3^{\\prime}$  UTR was chosen as the representative transcript for that gene [2,24,25].\n\n# Basic mRNA features to predict half-life\n\nThe G/C content and lengths of each of these functional regions (i.e.,  $5^{\\prime}$  UTRs, ORFs, and  $3^{\\prime}$  UTRs), intron length, and ORF exon junction density (computed as the number of exon junctions per kilobase of ORF sequence) were gathered as \"basic\" mRNA features associated with mRNA\n\nhalf-life [2,6,7]. All length-related features were transformed such that:  $\\hat{x} \\gets \\log_{10}(x + 0.1)$  to reduce the right skew [2].\n\n# Sequence features to predict mRNA half-life\n\nAll genetically encoded features are summarized in Table 2. Codon frequencies were extracted from ORF sequences using the oligonucleotideFrequency function (parameters width=3, step=3) in the Biostrings R package, and normalizing the counts for each codon by the sum of all codon counts for the gene. K-mer frequencies were computed similarly from each of the  $5^{\\prime}$  UTR, ORF, and  $3^{\\prime}$  UTR sequences (parameters width={1..7} depending on the size of the k-mer, step=1).\n\nMicroRNA features were collected for mammalian-conserved miRNA families using TargetScanHuman7.2 and TargetScanMouse7.2 [24]. We computed a miRNA target binding score by negating the cumulative weighted context++ score for each miRNA family.\n\nThe degree of predicted binding to a number of RBPs was computed for  $5^{\\prime}$  UTR, ORF, and  $3^{\\prime}$  UTR sequences separately using SeqWeaver [68] and DeepRiPE [69]. For SeqWeaver, we generated predictions on each 50 nt window, padding the sequence with its neighboring 475 nt upstream and downstream sequence (or Ns in the case of sequences at the boundary of a region) to generate a 1000 nt input sequence. For DeepRiPE, we generated predictions on each 50 nt window, padding the sequence with its neighboring 50 nt upstream and downstream sequence (or Ns in the case of sequences at the boundary of a region) to generate a 150 nt input sequence. DeepRiPE additionally required information about the functional region of interest, which we provided according to the functional region being considered. After generating predictions for all\n\nhuman and mouse RBPs for each functional region and gene, we computed the average value of the binding by normalizing the sum of values for each predicted RBP to the total length of the functional region.\n\n# Biochemical features\n\nAll biochemical features are summarized in Table 2. The number of CLIP peaks associated with each gene for each of 133 RBPs was downloaded from the ENCORI database [71]. eCLIP peaks were collected from the ENCORE browser as narrowPeak BED files [72]. The peaks for each RBP were intersected with gene body annotations and counted for each gene using bedtools intersect (parameters -s -c) [100]. PAR-CLIP peaks were downloaded from a previous study [70] and processed similarly to count the total peaks overlapping the gene body. m6A pathway (i.e., a6A, m6Am, YTHDF2, METTL3, METTL14, and WTAP) CLIP peaks were collected from an assortment of previous studies [37,42,7377], and if not already provided, the number of peaks intersecting gene bodies was computed. For all CLIP assays, genes with missing values (i.e., those without an annotated peak) were considered to have zero peaks. All peak counts were transformed as such:  $\\hat{x} \\gets \\log_{10}(x + 1)$  to reduce the right skew of the distributions.\n\nProcessed RIP-seq and translational efficiency data was downloaded from previous studies [36,78-80]. Merging these values with half-lives resulted in missing values; these were imputed using the impute function of the imputeR R package (parameter lmFun=\"plsR\") when considered alongside the \"basic\" continuous features describing mRNA properties.\n\n# Lasso regression modeling\n\nAll features being considered in a model (i.e., basic, genetic, biochemical, and/or a subset within each category) and half-life values were concatenated together and z-score normalized by subtracting their respective mean values and dividing by their standard deviations. We trained a lasso regression model on each of 10 folds of the data. A lasso regression model was chosen specifically because it employs an L1 regularization penalty, which leads to the selection of the fewest features that maximally explain the data. The strength of the regularization was controlled by a single  $\\lambda$  parameter, which was optimized using 10-fold CV on the entire dataset. To evaluate the usefulness of different subsets of features in improving predictive performance, we evaluated the Pearson correlation of the predictions of the lasso regression model on each of the 10 held-out folds of data, and performed paired t-tests to evaluate significant improvements in performance. To interpret the best model, we trained the lasso regression model on the full dataset and visualized the top 30 coefficients with the greatest magnitude.\n\n# Saluki model architecture and training\n\nWe trained a hybrid convolutional and recurrent deep neural network to predict half-life from its spliced mRNA sequence with several important gene structure annotations. Following previous work on applying such models to DNA/RNA sequence, we one-hot encoded the nucleotide sequence to four input tracks. Due to the strong influence of splicing on RNA stability, we added a fifth binary track to mark the positions of exon junctions at each exon's  $5^{\\prime}$  nucleotide. Due to the strong influence of mRNA codon composition, we added a sixth binary track to mark the beginning nucleotide of each codon, which implicitly labels the  $5^{\\prime}$  and  $3^{\\prime}$  UTRs due to their absence of codon markers. Although a nucleotide-only strategy may be preferable for mutation\n\neffect prediction, these mRNA features are important and could not easily be predicted by the model without adding substantial auxiliary training information.\n\nBecause mRNA lengths vary by orders of magnitude, we designed a model architecture to work with variable length sequences. We capped the length of an input mRNA to the model at 12,288 nt for practical purposes, finding that consideration of longer sequences led to equal performance at the cost of slower training speed. To accommodate the rare scenario in which an mRNA exceeded 12,288 nt, we truncated such cases from the  $5^{\\prime}$  end, restricting the model to access the  $3^{\\prime}$ -most 12,288 nt. Conversely, for the common scenario in which an mRNA was shorter, we padded such cases with zeros at their  $3^{\\prime}$  ends, representing Ns. We made use of the ten folds of human genes described above, and divided the mouse genes into ten folds that maintain the closest homologous genes in the same fold across species based on Ensembl annotations [99].\n\nOur model architecture, which we refer to as Saluki, consists of a tower of six convolution blocks to reach a resolution for which each position represents 128 nt (Fig. 5a and Additional file 1: Fig. S6a). Each block includes the following operations: (i) layer normalization [101], (ii) ReLU activation, (iii) 1D convolution with kernel width 5, (iv) dropout, and (v) max pooling with width 2. Overall, the model consists of 155,521 learnable parameters. We chose layer normalization over batch normalization because most of the  $3^{\\prime}$  positions are zero padded and would confuse the batch statistics. In contrast, layer normalization is computed independently at each position and simply maintains a zero vector for the padded regions.\n\nTo make a single numeric prediction for each sequence, we must aggregate information across the variable lengths. To achieve this, we use a common recurrent neural network block called the Gated Recurrent Unit (GRU) [102]. After layer normalization and ReLU activation, the GRU runs backward from the often-padded  $3^{\\prime}$  end to the information dense  $5^{\\prime}$  end. We take the final GRU hidden representation from the most  $5^{\\prime}$  position as a summary of the entire sequence. We apply a subsequent dense block, consisting of batch normalization, ReLU, and a dense layer. Finally, we apply one more block of batch normalization, ReLU, and a final dense transformation to produce the half-life prediction.\n\nWe trained with the MSE loss function using the Adam optimizer on batches of 64 examples and learning rate 0.001, beta1 0.9, and beta2 0.98. We clipped gradients to a global norm of 0.5. We used dropout probability 0.3 throughout and added L2 regularization on all convolution, GRU, and dense layer weights with coefficient 0.001. We used skopt to optimize these hyperparameters as well as the number of channels throughout the model. The hyperparameters specified here and 64 channels achieved the greatest validation set accuracy and compose our final model. For model training, we performed early stopping after 25 epochs without improvement and took the final parameters that achieved the greatest Pearson correlation on the validation set.\n\nWe trained models using both the human and mouse data using a published approach [5], in which all parameters are shared except for two separate final dense blocks for human and mouse. During training, we iterate between interwoven batches of human and mouse genes.\n\nOur ten folds of genes allowed us to train multiple models, in which each fold was held out as a test set (and another was held out as a validation set). Because we observed variance from training run to training run, we trained five replicate models for each held out test fold, producing a total of fifty trained parameter settings. After preliminary analyses to examine the predictions' variance, we averaged the predictions of the five replicates per test set as an ensemble, which improves accuracy and robustness. For all downstream analyses involving Saluki predictions on third-party test sets, we averaged the predictions from all fifty models.\n\n# In silico mutagenesis with Saluki\n\nWe performed in silico saturation mutagenesis (ISM) to predict the effect on mRNA half-life of all transcriptomic nucleotides. For each position, we ran three Saluki forward passes, mutating the reference nucleotide to each of the three possible alternative alleles. For each mutation, we compared the half-life prediction to the reference.\n\nFor mutations that modify stop codons, we optionally set the coding track  $3^{\\prime}$  onwards to zeros. This mode creates disproportionately large effect predictions for these mutations, which can be inconvenient for analyses focused on alternative aspects. We therefore decided not to modify the coding track downstream of the stop codon.\n\n# Insertional motif analysis with Saluki\n\nUsing our ISM scores as input, we ran TF-MoDISco [83] on each functional region (i.e.,  $5^{\\prime}$  UTR, ORF, and  $3^{\\prime}$  UTR) independently to identify the enriched motifs associated with changes in half-life. We selected several of the resulting PWMs to perform an insertional analysis, choosing\n\nthe consensus sequence as a representative kmer to insert. We inserted each kmer into one of 50 evenly-divided positional bins along each functional region of a valid mRNA, replacing the reference sequence with the inserted kmer to preserve the length of the mRNA. A valid mRNA was defined as one whose  $5^{\\prime}$  UTR length was  $\\geq 100\\mathrm{nt}$ , ORF length was  $\\geq 500\\mathrm{nt}$ , and  $3^{\\prime}$  UTR length was  $\\geq 500\\mathrm{nt}$ . For each insertion, we recorded the predicted change in half-life relative to the corresponding wild-type mRNA. Finally, for each of the 150 positional bins, we averaged the predicted changes in half-lives across all valid mRNAs to calculate the average influence of the motif across heterogeneous sequence contexts. We performed insertional analysis identically with the 61 non-stop codons, except that each codon was inserted into the first reading frame within the ORF in which it was inserted.\n\n# DECLARATIONS\n\n# AVAILABILITY OF DATA AND MATERIALS\n\nThe code to reproduce the core results of this work is provided under the Apache2.0 open access license at the following links: https://github.com/vagarwal87/saluki_paper (to reproduce figures) and https://github.com/calico/basenji/tree/master/manuscripts/saluki (to train deep learning models).",
    "metadata": {
      "md_filename": "MinerU_markdown_Saluki_20260106142855_2008425523906875392.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_Saluki_20260106142855_2008425523906875392.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "8b8449827950aa33",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_scBasset_20260106142850_2008425504122347520",
    "title": "scBasset: sequence-based modeling of single-cell ATAC-seq using convolutional neural networks",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "scATAC-seq preprocessing. We downloaded the processed peak set for Buenrostro2018 generated by Chen et al. at https://github.com/pinellolab/scATAC-benchmarking/blob/master/Real_Data/Buenrostro_2018/input/combined.sorted.merged.bed, which involved calling peaks on the aggregated profile of each cell type and merging them into a single atlas. We downloaded the aligned bam files from https://github.com/pinellolab/scATAC-benchmarking/tree/master/Real_Data/Buenrostro_2018/input/sc-bams_nodup, also provided by Chen et al. Peaks accessible in fewer than  $1\\%$  cells were filtered out. The final dataset contains 103,151 peaks and 2,034 cells.\n\nWe downloaded the 10x multiome datasets from 10x Genomics: https:// support.10xgenomics.com/single-cell-multiome-atac-gex/datasets/2.0.0/pbmc_ granulocyte_sorted_3k for the PBMC dataset and https://support.10xgenomics. com/single-cell-multiome-atac-gex/datasets/2.0.0/e18 Mouse_brain_fresh_5k for the mouse brain dataset. Genes expressed in fewer than  $5\\%$  cells were filtered out. Peaks accessible in fewer than  $5\\%$  cells were filtered out.\n\nscRNA-seq preprocessing. For the 10x multiome datasets, we processed the expression data with scVI v.0.6.5 with n_layers, 1; n Hidden, 768; latent, 64 and a dropout rate of 0.2 (ref. [36]). We trained scVI for 1,000 epochs with a learning rate of 0.001, using the option to reduce the learning rate upon plateau using options lr_patience of 20 and lr_factor of 0.1. We enabled early stopping when there was no improvement on the evidence lower bound loss for 40 epochs. To generate denoised expression profiles, we used the get_sample_scale() function to sample from the generative model ten times and took the average.\n\nBriefly, scVI performs denoising by modeling single-cell gene counts by negative binomial distributions and infers the parameters of these distributions with a variational autoencoder. We used scVI-denoised expression profiles to benchmark scATAC denoising and integration performance as previous work has demonstrated that denoised expression values reflect the true values in the cell more accurately than the observed counts, and we observed better integration performance when both RNA and ATAC profiles were denoised (Extended Data Fig. 5). We used the learned latent cell representations to build nearest-neighbor graphs and perform cell clustering.\n\nPBMC cell annotations. For multiome PBMC datasets, we performed a simple cell-type annotation based on gene expression data following a scanty tutorial (https://scany-tutorials.readthedocs.io/en/latest/pbmc3k.html). Briefly, we first clustered the cells based on scVI latent cell embeddings using the Leiden algorithm. Then we normalized a cell-by-gene expression matrix by log(reads per 10,000). We ran rank_genies_groups() on the normalized gene expression matrix and plotted the top 25 enriched genes in each Leiden cluster. We compared the top enriched genes in each cluster with PBMC marker genes provided in the tutorial to assign cell type annotation to each cluster. Clusters where no marker genes were found in the top 25 enriched genes were assigned to 'other'.\n\nModel architecture. scBasset is a neural network architecture that predicts binary accessibility vectors for each peak based on its DNA sequence. scBasset takes as input a 1,344-bp DNA sequence from each peak's center and one-hot encodes it as a  $1,344 \\times 4$  matrix. The neural network architecture includes the following blocks:\n\n1D convolution layer with 288 filters of size  $17 \\times 4$ , followed by batch normalization, GELU and width 3 max pooling layers, which generates a  $488 \\times 288$  output matrix.\n\n- Convolution tower of six convolution blocks each consisting of convolution, batch normalization, max pooling and GELU layers. The convolution layers have increasing numbers of filters (288, 323, 363, 407, 456 and 512) and kernel width 5. The output of the convolution tower is a  $7 \\times 512$  matrix.\n\n- 1D convolution layer with 256 filters of width 1, followed by batch normalization and GELU. The output is a  $7 \\times 256$  matrix, which is then flattened into a  $1 \\times 1,792$  vector.\n\n- Dense bottleneck layer with 32 units, followed by batch normalization, dropout with rate 0.2, and GELU. The output is a compact peak representation vector of size  $1 \\times 32$ .\n\n- Final dense layer predicting continuous accessibility logits for the peaks in every cell.\n\n- (Optional) to perform batch correction, we attach a second parallel dense layer to the bottleneck layer predicting batch-specific accessibility. This batch-specific accessibility is multiplied by the batch-by-cell matrix to compute the batch contribution to accessibility in every cell. This vector is then added to the previous continuous accessibility logits per cell (Extended Data Fig. 3). L2 regularization can be optionally applied to the cell-embedding path (with hyperparameter  $\\lambda_{1}$ ) or the batch-specific path (with hyperparameter  $\\lambda_{2}$ ) to tune the contribution of the batch covariate to the predictions.\n\n- Final sigmoid activation to [0,1] accessibility probability.\n\nThe total number of trainable parameters in the model is a function of the number of cells  $(n)$  in the dataset. Specifically, the model will have  $4,513,960 + 33 \\times n$  trainable parameters. Due to extensive previous work establishing high-performing model architecture hyperparameter ranges $^{11-13}$ ,\n\nwe only performed hyperparameter searches for the size of the bottleneck layer and optimization parameters, including batch size, learning rate,  $\\beta 1$  and  $\\beta 2$ . For the optimization parameters, we chose the values that minimized training loss. For the bottleneck layer, we also examined cell-embedding metrics.\n\nTraining approach. We used a binary cross-entropy loss and monitored the training auROC after every epoch. We stopped training when the maximum training auROC improved by less than  $1 \\times 10^{-6}$  in 50 epochs. This stopping criterion led to training for around 600 epochs for the Buenrostro2018 dataset, 1,100 epochs for the 10x multiome PBMC dataset and 1,200 epochs for the 10x multiome mouse brain dataset.\n\nWe focused on training auROC instead of validation auROC for model selection because we observed that the model continues to improve cell embeddings even after the point where the validation auROC has plateaued (Supplementary Fig. 9). Stopping criteria based on training set loss are typical for optimization of many statistical models but atypical for overparameterized deep-learning models that are prone to overfitting. The primary overfitting risk is reduced performance on held-out data, which we do not observe; validation auROC during the later stages of training is stable. Our hyperparameter analyses indicate that the 32-unit bottleneck layer is a major impediment to true overfitting. Thus, although the convolution towers may learn sequence factors that do not generalize well during the later training phase, the final layer weights (which serve as cell embeddings) are constrained and continue to learn from the cell-cell accessibility correlations in the training data.\n\nWe updated model parameters using stochastic gradient descent using the Adam update algorithm. We performed a random search for optimal hyperparameters including batch size, learning rate and  $\\beta 1$  and  $\\beta 2$  for the Adam optimizer. The best performance was achieved with a batch size of 128, learning rate of 0.01,  $\\beta 1$  of 0.95 and  $\\beta 2$  of 0.9995.\n\nWe focused on the Buenrostro2018 dataset to select the optimal bottleneck layer size. We trained models with bottleneck sizes of 8, 16, 32, 64 and 128 and observed that bottleneck size 32 gave the best performance (Supplementary Fig. 9).\n\nscBasset trained on shuffled labels. To establish baseline performance, for each of the datasets, we trained scBasset on a training set with labels shuffled. For each cell in the training set, we first binarized the accessibility vector and then randomly shuffled the positives (accessibility regions), while the total number of positives (coverage) was not affected, and re-trained the scBasset model.\n\nPerformance evaluation on data dropout. To benchmark model performance as a function of data sparsity, we choose a scATAC dataset with relatively high sequencing depth, the 10x multiome PBMC dataset. The original scATAC peak-by-cell matrix contains  $21.2\\%$  nonzero entries. We downsampled reads from this matrix and generated datasets of the same size but increasing sparsity. The sampled datasets contain  $16.9\\%$ ,  $12.7\\%$ ,  $8.45\\%$ ,  $4.22\\%$ ,  $2.11\\%$  and  $1.06\\%$  nonzero entries, which is  $80\\%$ ,  $60\\%$ ,  $40\\%$ ,  $20\\%$ ,  $10\\%$  and  $5\\%$  of the original data. Then we trained scBasset models on each of these dropout datasets and evaluated the training area under the curve and validation area under the curve, as well as clustering performance (neighbor score), as a function of sparsity.\n\nBenchmarking existing methods. For evaluation of cell embeddings, we compared scBasset to principal component analysis (PCA) implemented in scikit-learn $^{38}$ , latent semantic indexing (LSI) implemented in cicero $^{7}$ , cisTopic $^{5}$ , SCALE $^{8}$ , chromVAR with motifs or k-mer features $^{9}$ , ArchR $^{23}$ , snapATAC $^{39}$ , peakVI $^{40}$ , and scDEC $^{41}$ .\n\nFor evaluation of batch correction performance, we compared scBasset to Harmony $^{19}$ , peakVI $^{40}$ , scDEC $^{41}$ , cisTopic $^{5}$  and SCALE $^{8}$ .\n\nFor evaluation of scATAC denoising performance, we compare scBasset to cisTopic $^{5}$ , peakVI $^{40}$ , MAGIC $^{42}$ , SCALE $^{8}$  and scOpen $^{22}$ .\n\nCell-embedding evaluation. For implementation details of embedding methods, see Supplementary Notes.\n\nClustering-based metrics. We evaluated learned cell embeddings by comparing the clustering to the ground-truth labels (FACS-sorted cell-type labels for Buenrostro2018, RNA-based cell cluster labels for multiome data). We first built a nearest-neighbor graph using scany with default  $n$  neighbors of 15. Then we followed a previous study to tune for a resolution that outputs 10 clusters for Buenrostro2018, 18 clusters for multiome PBMC and 21 clusters for multiome mouse brain so that they match the number of ground-truth labels<sup>6</sup>. Finally, we compared the clustering outcome to the ground-truth cell type labels using ARI, AMI and homogeneity as implemented in sklearn.metrics.\n\nCell type average silhouette width. Silhouette width evaluates whether cells of the same label are embedded close together by quantifying the distance of a cell to other cells of the same label, as compared to distance to cells of different labels. We evaluated cell embeddings by cell type ASW as proposed in previous single-cell studies $^{18}$ , which is the silhouette score average across all cells and re-normalized to 0 and 1.\n\nLabel score. We evaluated the learned cell embeddings using label score for all three datasets. For a given nearest-neighbor graph, label score quantifies what percentage of each cell's neighbors share its same label in a given neighborhood. For each cell-embedding method, we computed the label score across a neighborhood of 10, 50 and 100. As the ground-truth cell types for the multiome datasets are unknown, we used cluster identifiers from scRNA-seq Leiden clustering as cell-type labels.\n\nNeighbor score. We evaluated the learned cell embeddings using neighbor score for the 10x multiome datasets. For a 10x multiome dataset, we built independent nearest-neighbor graphs from the scRNA (using scVI) and scATAC (using the cell-embedding method we wanted to evaluate) and quantified the percentage of each cell's neighbors that were shared between the two graphs across neighborhoods of size 10, 50 and 100.\n\nBatch correction evaluation. For implementation details of batch correction methods, see Supplementary Notes.\n\nChemistry-mixed PBMC dataset. We first evaluated batch correction performance on a dataset with perfect batch design. We mixed PBMC populations from 10x PBMC multiome chemistry (https://cf.10xgenomics.com/samples/cell-arc/1.0.0/ pbmc_granulocyte_sorted_10k/) and 10x PBMC next GEM chemistry (https:// cf.10xgenomics.com/samples/cell-atac/2.0.0/atac_pbmc_10k_nextgem/). We generated a shared atlas of 21,017 peaks from the two datasets by resizing the 10x peak calls from the two datasets to 1,000 bp and took the intersection. We subsampled 2,000 cells from each dataset and merged them over the shared atlas.\n\nBuenrostro2018 dataset. We compared the batch correction performance of different methods on the Buenrostro2018 dataset. This dataset has an unbalanced batch design and represents a more practical case for batch correction application. Since popular metrics for batch correction such as kBET and iLISI assume all batches are present in a local neighborhood in a batch-corrected population $^{21,43}$ , we sampled the Buenrostro2018 dataset to contain only cells from batch 'BM0828' and 'BM1077' to compute kBET and iLISI metrics.\n\n$k$ -nearest-neighbor batch-effect test acceptance rate. kBET acceptance rate measures batch mixing by the concordance of local batch distribution with the global batch distribution<sup>20</sup>. Higher acceptance rate indicates better mixing. We implemented the kBET R package (v.0.99.6) to compute kBET acceptance rate.\n\nIntegration local inverse Simpson's index. iLISI measures batch mixing by the effective number of batch labels in a local neighborhood $^{19}$ . Higher iLISI score indicates better mixing. We implemented the lisR package (v.1.0) to compute iLISI scores.\n\nLabel score. We quantified the conservation of biological variation after batch correction by evaluating the cell embeddings with label score. Ground-truth cell-type labels for Buenrostro2018 are provided by FACS-sorting. Ground-truth cell-type labels for multiome PBMCs are generated by annotating the matched RNA profiles as described previously.\n\nDenoising evaluation. For implementation details of denoising methods, see Supplementary Notes.\n\nTo compute denoised and normalized accessibility across cells for a query peak with scBasset, we ran a forward pass on the input DNA sequence to compute the latent embedding for the peak. Then we generated the normalized accessibility across all cells through dot product of the peak, embedding with the weight matrix of the final layer. As sequencing depth information is entirely captured by the intercept vector of the final layer, we excluded the intercept term so that scBasset generates denoised profiles normalized for sequencing depth.\n\nFollowing a previous study, we evaluated the denoising performance of scBasset for cell-cell distance estimation and cell embedding $^{22}$ .\n\n- Cell type ASW: we computed a cell-cell distance matrix from the denoised cell-by-peak matrix using 1 - PearsonR as the distance metric and asked whether cells of the same label are closer together, using the cell type ASW.\n\n- Label score or neighbor score: we performed PCA embedding  $(\\mathrm{PC} = 50)$  on the denoised cell-by-peak matrix and asked whether cells of the same type embed closer together, as evaluated by our label score for Buenrostro2018 dataset and neighbor score for multiome datasets.\n\nThen we evaluated additional multiome-specific metrics for 10x multiome datasets. Our evaluation is based on the hypothesis that effective denoising would improve the correlation between accessibility at genes' promoters and the genes' expression in multiome measurements[7,23]. For each gene, we computed a gene accessibility score by averaging accessibility values for peaks at the gene's promoter (2kb from transcription start site). We evaluated denoising performance by:\n\nCorrelation per cell: computing the Pearson correlation between the gene accessibility score and gene expression (after scVI denoising) across all genes for each individual cell.\n\nCorrelation per gene: computing the Pearson correlation between the gene accessibility score and gene expression (after scVI denoising) across all cells for each gene.\n\nIntegration evaluation. To evaluate integration performance, we treated the 10x multiome scRNA and scATAC profiles as originated from two independent experiments. We summarized the accessibility profile to the gene level by computing the gene accessibility score as described above and integrated the scRNA and scATAC data by embedding them into a shared space using Seurat FindTransferAnchors() and TransferData() functions $^{24}$ .\n\nTo quantify the integration performance, we measured a 'RNA/ATAC embedding distance'  $R_{\\mathrm{c}}$  between the RNA embedding and the ATAC embedding of each cell  $c$  in the co-embedding space. We use  $R_{\\mathrm{rna}}$  to represent the ranking of the Euclidean distance between RNA embedding and ATAC embedding of cell  $c$  among all neighbors of  $c$ 's RNA embedding and  $R_{\\mathrm{atac}}$  to represent the ranking of the same distance among all neighbors of  $c$ 's ATAC embedding.  $R_{\\mathrm{c}}$  is computed as the average of  $R_{\\mathrm{rna}}$  and  $R_{\\mathrm{atac}}$ . A smaller  $R_{\\mathrm{c}}$  indicates better integration, whereas a higher  $R_{\\mathrm{c}}$  indicates worse integration.\n\nspear-ATAC analysis. spear-ATAC preprocessed count matrix 'K562-Pilot-scATAC-Peak-Matrix-SE.rds' was downloaded from the Gene Expression Omnibus (accession code GSE168851) $^{27}$ . This dataset contains a pool of nine CRISPRi sgRNAs targeting GATA1 (sgGATA1) and GATA2 (sgGATA2) and inert sgRNA controls (sgNT) that were introduced into K562 cells expressing a dCas9-KRAB367 cassette. Cells with unknown sgRNAs were filtered out (sgAssignFinal, 'UNK'). We kept cells with at least  $5\\%$  peaks accessible and peaks accessible in at least  $5\\%$  cells for training the scBasset model.\n\nWe used the cell embeddings generated by scBasset for visualization using UMAP. We scored GATA1 and GATA2 motif activity using either an scBasset motif insertion approach or using chromVAR. We compared scBasset and chromVAR in distinguishing sgGATA1 cells from sgNT using the predicted GATA1 scores and distinguishing sgGATA2 cells from sgNT cells using the predicted GATA2 scores. Prediction performance was evaluated by auPR and auROC.\n\nsci-ATAC human atlas analysis. We downloaded the processed peak-by-cell matrix from the sci-ATAC human atlas stored at http://renlab.sdc.edu/kai/ Key_Processed_Data/Cell_by_cCRE $^{31}$ . We kept peaks accessible in more than  $0.5\\%$  cells, and cells with at least 500 peaks accessible. The filtered matrix contains 1,114,621 cells and 118,043 peaks. Storing such a matrix in a dense format would take more than 1 terabyte of disk space. The data are thus stored in h5ad and sequences used for training are also stored in h5 format. scBasset can easily be trained on a dataset of this size because it takes sparse data as input and interacts with batches of input at training time.\n\nWe trained scBasset on the whole sci-ATAC atlas as well as a sampled dataset with 10,000, 20,000, 50,000, 100,000, 200,000, 400,000, 600,000, 800,000 and 1,000,000 cells. We measured CPU memory, GPU memory and runtime when training scBasset on each dataset. CPU memory is monitored by psutil.Process. memory info() command after reading or creating matrices and peak memory usage is reported. GPU memory is monitored using Tensorboard Profiler. Runtime per epoch is reported by Tensorflow during training.\n\nMotif insertion. We performed motif insertion on scBasset to compute a TF activity score for each TF for each cell. Specifically, we first generated 1,000 genomic background sequences by performing dinucleotide shuffling of 1,000 randomly sampled peaks from the atlas using fasta umhuffle<sup>44</sup>. For each TF in the motif database, we sampled a motif sequence from the PWM and inserted it into the center of each of the genomic background sequences. We ran forward passes through the model for both the motif-inserted sequences and background sequences to predict normalized accessibility across all cells. We took the difference in predicted accessibility between the motif-inserted sequences and background sequences as the motif influence for each sequence. We averaged this influence score across all 1,000 sequences for each cell to generate a cell-level prediction of raw TF activity. Finally, we  $z$  score-normalized the raw TF activities to generate the final TF activity predictions across all cells.\n\nWe used CIS-BP 1.0 single species DNA database motifs downloaded from https://meme-suite.org/meme/db/motifs for our motif analysis<sup>45</sup>.\n\nIn silico saturation mutagenesis. We performed ISM to compute the importance scores of all single nucleotides on a sequence of interest. For each position, we ran three scBasset forward passes, each time mutating the reference nucleotide to an alternative. For each mutation, we compared the alternative accessibility prediction to that of the reference to compute the change in accessibility for each cell. We normalized the ISM scores for the four nucleotides at each position such that they summed to zero. We then took the normalized ISM score at the reference nucleotide as the importance score for that position.\n\nIn the  $\\beta$ -globin enhancer ISM analysis, we labeled TF motifs using the following procedure. First, we scanned the DNA sequence for candidate motif matches using FIMO with a permissive  $P$  value threshold of  $1\\times 10^{-3}$  (ref. [46]). For any motif match, we assigned a score using a Pearson correlation or dot product\n\nbetween the PWM and ISM. Finally, we performed a statistical test on the match score by comparing the observed correlation with a null distribution computed from shuffled input.\n\nReporting summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article.",
    "metadata": {
      "md_filename": "MinerU_markdown_scBasset_20260106142850_2008425504122347520.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_scBasset_20260106142850_2008425504122347520.md",
      "has_methods": true
    }
  },
  {
    "doc_id": "59d91d5c14223e74",
    "source": "Core Papers MD",
    "source_id": "MinerU_markdown_Sei_20260106142843_2008425478490947584",
    "title": "OPEN",
    "abstract": "",
    "authors": "",
    "journal": "",
    "date": "",
    "doi": "",
    "url": "",
    "keywords": [],
    "full_text": "",
    "methods": "Training data. A total of 21,907 cis-regulatory profiles in peak format were compiled from the processed files of the Cistrome $^4$ , ENCODE $^2$  and Roadmap Epigenomics projects $^3$ . The Cistrome Project, which systematically processes publicly available cis-regulatory profiles, contributed most of the profiles predicted in Sei  $(n = 19,905)$ . We excluded profiles from Cistrome with fewer than 1,000 peaks. Genome sequences are from the GRCh38/hg38 human reference genome. The full list of cis-regulatory profiles is available in Supplementary Table 1.\n\nDeep learning sequence model training. The Sei model was trained to predict 21,907 TF binding, histone marks and DNA accessibility from cis-regulatory profile peaks at the center of 4-kb length sequences. The model is trained on chromatin profile peak calls, which are binary (presence/absence), but the model output is continuous, representing probabilities of peaks.\n\nThe model architecture is composed of three sequential sections: (1) a convolutional network with dual linear and nonlinear paths; (2) residual dilated convolution layers; (3) spatial basis function transformation and output layers. A detailed specification of the model is available in Supplementary Fig. 21 and in the code repository (https://github.com/FunctionLab/sei-framework, downloadable from https://doi.org/10.5281/zenodo.4906996). In the convolutional architecture, we introduced a new design composed of both linear and nonlinear convolution blocks. The linear path allows for fast and statistically efficient training, while the nonlinear path offers strong representation power and the capability to learn complex interactions. The nonlinear blocks consist of convolution layers and rectified linear activation functions, similar to regular convolutional networks. The linear blocks have the same structure as the nonlinear blocks but do not include activation functions to facilitate learning of linear dependencies. Each nonlinear block is stacked on top of a linear block with a residual connection adding the input of the nonlinear block to the output, allowing the computation to go through either the linear or nonlinear path. Dilated convolutional layers with residual connections further expand the receptive fields without reducing spatial resolution. Finally, for scaling and performance, we introduced a layer of spatial basis functions, which integrates information across spatial locations with much higher memory efficiency than fully connected layers. Spatial basis functions are used to reduce dimensionality of the spatial dimension while preserving the capability to discriminate spatial patterns of sequence representations. Specifically, in the Sei model, a B-spline basis matrix  $(256\\times 16)$  with 16 degrees of freedom across 256 uniformly spaced spatial bins is generated and multiplied with the convolutional layers output to reduce the 256 spatial dimensions to 16 spline basis function dimensions. After the spline basis function transformation, a fully connected layer and an output layer are used to integrate information across the whole sequence and generate the final 21,907 dimensional predictions.\n\nOur model training pipeline was updated $^{35}$  to improve training speed and performance by using on-the-fly sampling, which reduces overfitting by generating new training samples for every training step. Training, validation and testing datasets are specified by different sets of chromosomes in the hg38 genome (holding out chromosome 8 and 9 for the test set and chromosome 10 for the validation set) and samples drawn uniformly across the hg38 genome for these partitions, excluding regions specified in the ENCODE blacklist $^{36}$ . For training, we sampled training sequences and their labels on the fly from the training set of chromosomes using Selene $^{35}$ . Thus almost all training samples are drawn from unique genomic intervals with distinct start and end positions to reduce overfitting during the training process. For each 4-kb region, a 21,907 dimensional binary label vector was created for the 21,907 cis-regulatory profiles based on whether the center bp overlaps with a peak in each of the profiles. The model was implemented in PyTorch and trained with Selene. A detailed training configuration file is available at https://github.com/FunctionLab/sei-framework/blob/main/train/train.yml.\n\nModel performance. We computed the AUROC and AUPRC for all cis-regulatory profiles predicted by Sei on the test holdout dataset, excluding profiles that had fewer than 25 positive samples in the test set. Additionally, to assess the correlation structure of the predictions, we compared the rank-transformed pairwise Spearman rank correlations for the predicted cis-regulatory profiles to the pairwise correlations for the true labels (peak calls provided in the Cistrome Data Browser).\n\nThe model performance comparison between DeepSEA and Sei was computed on the 2,002 cis-regulatory profiles from Roadmap and ENCODE that both DeepSEA and Sei predict. Because both models have the same chromosomal test holdout (chromosomes 8 and 9), we use the regions specified in the DeepSEA test holdout set to create a common test dataset of sequences and labels on which to evaluate the models.\n\nSequence classes. We selected 30 million genomic positions that uniformly tile the genome with a 100-bp step size and then computed Sei predictions for 4-kb sequences centered at each position. Sequences overlapping with ENCODE blacklist regions $^{36}$  or assembly gaps (Ns) were removed. To process the 30 million  $\\times$  21,907 predictions matrix, the dimensionality was first reduced with principal component analysis (PCA). The PCA transformations were fitted with incremental PCA using a batch size of 1,000,000 for 1 pass of the whole dataset;\n\ngenomic positions were randomly assigned to batches. The top 180 principal components, scaled to unit variance, were used for constructing a nearest neighbor graph where each node is connected to its  $k$ -nearest neighbors by Euclidean distance ( $k = 14$ ). Louvain community clustering with default parameters was applied to the nearest neighbor graph with the python-louvain package (version 0.6.1), which resulted in 61 clusters. We refer to the largest 40 clusters as sequence classes and exclude the remaining (smallest) 21 clusters, which constitute  $< 2.6\\%$  of the genome, from our analyses due to their size. These 21 clusters mainly displayed L or HET-like enrichment (Supplementary Fig. 20). We refer to this cluster assignment to sequence classes at 100-bp resolution as sequence class annotations. We visualized the genome-wide predictions by computing uniform manifold approximation and projection (UMAP) embedding with a subsample of PCA-transformed Sei predictions of 30 million sequences and then fine-tuned the visualization with openTSNE (version 0.6.0). The detailed procedures are available in our code repository (https://github.com/FunctionLab/sei-manuscript).\n\nSequence class scores. Each sequence class is represented as a unit vector in the 21,907 dimensional cis-regulatory profile space, in the direction of the average prediction of all sequences assigned to this sequence class among the 30 million. In more formal notation, the vector for sequence class  $i$  is  $\\nu_{i} = \\frac{\\overline{p}_{s \\in \\text{Sequence class } i}}{||\\overline{p}_{s \\in \\text{Sequence class } i}||^{2}}$ , where  $p_{s}$  represents the 21,907 dimensional Sei prediction for sequence  $s$ . Each Sei prediction can then be projected onto any sequence class vector to obtain a sequence class-level representation of the prediction, which we call sequence class score or  $\\text{score}_{s,i} = p_{s}^{T} \\cdot \\nu_{i}$ . In addition, predicted sequence class-level variant effects are represented by the difference between the sequence class scores of the sequences carrying the Ref and Alt alleles or  $\\text{score}_{\\nu,i} = \\text{score}_{\\text{Alt},i} - \\text{score}_{\\text{Ref},i}$ . To better represent predicted variant effects on histone marks, it is necessary to normalize for nucleosome occupancy (for example, a LoF mutation near the TSS can decrease H3K4me3 modification level while increasing nucleosome occupancy, resulting in an overall increase in observed H3K4me3 quantity). Therefore, for variant effect computation, we used the sum of all histone profile predictions as an approximation to nucleosome occupancy and adjusted all histone mark predictions to remove the impact of nucleosome occupancy change (nonhistone mark predictions are unchanged):\n\n$$\np _ {\\text {R e f}} ^ {\\text {h m} *} = p _ {\\text {R e f}} ^ {\\text {h m}} \\frac {\\sum_ {k} p _ {\\text {R e f}} ^ {\\text {h m} k} + \\sum_ {k} p _ {\\text {A l t}} ^ {\\text {h m} k}}{2 \\sum_ {k} p _ {\\text {R e f}} ^ {\\text {h m} k}}; p _ {\\text {A l t}} ^ {\\text {h m} *} = p _ {\\text {A l t}} ^ {\\text {h m}} \\frac {\\sum_ {k} p _ {\\text {R e f}} ^ {\\text {h m} k} + \\sum_ {k} p _ {\\text {A l t}} ^ {\\text {h m} k}}{2 \\sum_ {k} p _ {\\text {A l t}} ^ {\\text {h m} k}}\n$$\n\nwhere  $\\sum_{k}p_{\\mathrm{Ref}}^{\\mathrm{hm}^k}$  represents the sum over all histone mark predictions (among 21,907\n\ndimensions of a prediction) for the Ref allele. We generally excluded L sequence classes in sequence class-level variant effect analyses because they lack an intuitive biological interpretation.\n\nSequence class enrichment of chromatin profiles and genome annotations. We computed the log fold change enrichment of various chromatin profiles and genome annotations for each sequence class based on sequence class annotations (described above); log fold change enrichment is computed by taking the log ratio of the proportion of a sequence class intersecting with the annotation versus the background proportion of the annotation, where we consider all regions assigned to any sequence class. We computed enrichment for all 21,907 profiles predicted by Sei, filtered the chromatin profiles for each sequence class to only those having Benjamini-Hochberg-corrected (two-sided Fisher's exact test)  $P < 2.2 \\times 10^{-16}$  and selected the top 25 profiles based on log fold change enrichment. Cistrome Project profile enrichment is computed over 2 million random genomic positions.\n\nThe annotation of centromere repeats was obtained from the University of California, Santa Cruz RepeatMasker track and annotations of histone marks over multiple cell types were obtained from the Roadmap Epigenomics projectenrichments for both of these sets of annotations were computed over the entire genome. In addition, we obtained ChromHMM chromatin states from ENCODE<sup>2</sup> and tissue- and cell type-specific DHS vocabulary from Meuleman et al.<sup>18</sup>.\n\nEnhancer sequence class correlations with cell type-specific gene expression. Tissue expression profiles are from  $\\mathrm{GTEx}^{19}$ , Roadmap Epigenomics $^3$  and ENCODE $^2$  and transformed to log RPKM (reads per kilobase of transcript, per million mapped reads) scores as previously described $^7$  and normalized by tissue average. Specifically, a pseudocount was added before log transformation (0.0001 for GTEx tissues, which are averaged across individuals, and 0.01 for Roadmap and ENCODE tissues). After log transformation, the average scores across tissues were subtracted for each gene; thus, the processed scores represent log fold change relative to tissue average.\n\nGene-wide expression prediction was evaluated on sequence class annotations (from Louvain community clustering) for positions within  $\\pm 10\\mathrm{kb}$  of the TSS for these genes. For each enhancer sequence class and tissue, we computed a Spearman correlation between the sequence class annotation coverage and gene expression.\n\nSequence class variant effect correlation with directional eQTL variant effect sizes. We collected the eQTLs within  $\\pm 5\\mathrm{kb}$  of gene TSS from GTEx v8, combined across all GTEx tissues, and computed the Spearman correlation between the top\n\n15,000 variant effect predictions for each sequence class and the eQTL variant effect sizes (averaged across multiple tissues if the variant was an eQTL in multiple tissues).  $P$  values were derived from a two-sided Spearman rank correlation test and Benjamini-Hochberg correction was applied. L and HET sequence classes were excluded from this analysis due to lack of interpretation for their variant effect scores in this context.\n\nAdditionally, we collected fine-mapped GTEx eQTLs from the eQTL Catalogue $^{20}$  and obtained sequence class scores for eQTLs with a posterior inclusion probability greater than 0.95. Variants were assigned to sequence classes based on the sequence class annotation for the reference genome (that is, variants were not further selected based on variant effect predictions). For each sequence class, we computed the Spearman rank correlation between sequence class scores and eQTL variant effect sizes in the same way we described above.\n\nEvolutionary constraints on variant effects. We computed sequence class-level variant effects for all 1000 Genomes Project phase 3 variants $^{21}$ . Variants were assigned to sequence classes based on the 100-bp resolution genome-wide assignment derived from Louvain community clustering. For each sequence class, we divided variants into six bins based on their effects in the same sequence class as illustrated in Fig. 3 and summarized the common variant (AF > 0.01) frequencies in each bin by mean and s.e.m. We also estimated statistical significance of AF dependency on sequence class-level variant effects. For each sequence class, we applied logistic regression separately for positive effect and negative effect variants to predict common variants (AF > 0.01) from the absolute value of the sequence class-level variant effect score and obtained the significance  $z$ -score of the regression coefficient of variant effect. The bidirectional evolutionary constraint  $z$ -score is defined as the negative value of the combined  $z$ -scores from positive and negative effect variants with Stouffer's method.\n\nPartitioning GWAS heritability by sequence classes. The UKBB GWAS summary statistics were obtained from Loh et al.[25]. To study the association of sequence class genome annotation and sequence class variant effects and trait heritability, we performed partitioned heritability LDSR as described by Finucane at al.[22]. To partition the heritability as sums of heritability explained by each sequence class, we ran LDSR with only sequence class annotations and a baseline all-ones annotation. We obtained the estimated proportion of  $h^2$  explained by each sequence class and its s.e. with LDSR as implemented in https://github.com/bulik/ldsc. Because the estimated proportions can have high variance or even be negative (the true value of heritability explained can only be nonnegative), we used a robust and conservative estimator that is the estimated proportion of  $h^2$  subtracted by one s.e. and then lower-bounded by zero. (The s.e. of the estimated proportion of  $h^2$  explained is given by LDSR and was estimated with the block jackknife procedure as described by Finucane et al.[22].)\n\nTo assess the contribution of sequence classes to explaining additional heritability when conditioned on known baseline annotations, we also ran LDSCORE v.2.2 with the baseline annotations (https://alkesgroup.broadinstitute.org/LDSCORE/).  $P$  values were derived from the coefficient  $z$ -score; Benjamini-Hochberg correction was applied.\n\nSequence class variant effect analysis of noncoding pathogenic mutations. We obtained all mutations assigned 'DM' and 'regulatory' annotation in the HGMD database (2019.1 release). RMRP gene mutations were excluded because they are likely pathogenic due to impacting RNA function instead of regulatory perturbations, despite being annotated to the regulatory category in HGMD. For every mutation, we predicted the sequence class scores for both the Ref and Alt alleles and computed the sequence class-level variant effect as the predicted scores for the Alt allele subtracting the scores for the Ref allele. To provide an overview of sequence class-level effects of human noncoding pathogenic mutations, mutations were first assigned to sequence classes based on the sequence class annotations of the mutation position. For mutations with a strong effect in a different sequence class than the originally assigned sequence class (absolute value higher than the\n\noriginal sequence class by  $>1$  absolute difference and  $>2.5$ -fold relative difference), we reassigned the mutation to the sequence class with the strongest effects.\n\nReporting summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article.",
    "metadata": {
      "md_filename": "MinerU_markdown_Sei_20260106142843_2008425478490947584.md",
      "md_path": "d:\\RE-Agent\\Knowledge_Corpus\\core_papers_md\\MinerU_markdown_Sei_20260106142843_2008425478490947584.md",
      "has_methods": true
    }
  }
]